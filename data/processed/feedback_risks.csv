Organization,Feedback reference,Submitted on,User type,Organisation size,Country of origin,text,language,env_sentences,category
Consumer Technology Association (United States),F551055,10 September 2020,Business association,Medium (50 to 249 employees),United States,"September 10, 2020 Consumer Technology Association Comments on European Commission Inception Impact Assessment on Artificial Intelligence Ref. Ares(3896535 The Consumer Technology Association ( CTA ) 1 respectfully submits these comments in response to the European Commission ( Commission ) Inception Impact Assessment2 ( the Impact Assessment ) analyzing the potential impact of adopting sweeping new legislation on artificial intelligence ( AI ), as outlined in the Commission s White Paper.3 As explained in prior comments, CTA applauds the Commission s thoughtful work on these issues, including release of the Impact Assessment, which raises important questions concerning the potential pecuniary impact of adopting new legislation in this area. Respectfully, CTA urges the Commission to undertake additional review and analysis of the potential economic impact of formal Commission action. This is necessary to better understand and evaluate the potential economic costs, and benefits, of taking formal action that may lead to new rules and restrictions on the development, use and sale of AI. In order to meet the Commission s own high standard for analyzing the impact of potential legislation, the Commission must (in its own words) analyse in more detail the issue to be addressed, whether action should be taken at EU level and the potential economic, social and environmental effects of the different solutions outlined. 4 To meet this high bar the Commission must recognize the very significant potential economic costs of imposing burdensome new rules upon a still nascent and emerging technology. In recognition of this fact, the Commission should further develop its Impact Assessment to more fully consider the potential economic impact of adopting new legislation in this area. That further assessment should specifically analyze the potential impact on small to 1 CTA is the tech sector. Our members are the world s leading innovators from startups to global brands helping support millions of jobs. CTA owns and produces CES the largest, most influential tech event on the planet. 2 White Paper on Artificial Intelligence, A European Approach to Excellence and Trust; COM( 65 Final, Brussels 2020 ( White Paper ). 3 European Commission Inception Impact Assessment on Artificial Intelligence, Ref. Ares(3896535 2020 ( Impact Assessment ). 4 See European Commission, Planning and Proposing Law process/planning-and-proposing-law_en#how-their-scope-is-defined (emphasis added). 2 medium sized businesses, the potential costs of adopting AI-specific liability schemes (such as a strict liability framework), and how to avoid creating potentially costly new requirements by leveraging consensus-based standards, voluntary governance and risk assessment processes in lieu of broad new AI legislation. I. Certain Preliminary Findings in the Impact Assessment Will be Central to the Development of a More Fulsome Analysis by the Commission The Commission s Impact Assessment correctly makes preliminary findings for several important issues, including the need to: properly define the scope of any potential legislation; avoid fragmentation amongst EU member states; and, recognize that if legislation is overreaching and pervasive, the costs of compliance will outweigh the potential opportunities and create disincentives for developing this new technology. CTA agrees with these findings and offers the following additional considerations for these three points. In the Impact Assessment the Commission recognizes that a core question relates to the scope of the initiative, i.e., how should AI be defined (narrowly or broadly). 5 CTA submits that addressing this core question at the outset is critically important to completion of a probative and meaningful final impact assessment. How can the Commission assess the impact of potential additional compliance costs, if it has not yet determined how narrowly or broadly those costs may extend? Without addressing this threshold question the Commission s broad, general pronouncements in the White Paper are too vague to properly assess or calibrate (let alone implement). To address this issue the Commission should adopt a nuanced approach that recognizes that the breadth of AI use cases will require a focused response, which may differ by application. To that end, the Commission should avoid using broad, ill-defined concepts in any future legislation. Use of untethered terms such as automated decision making conflicts with the thoughtful and focused risk-based constructs outlined in the White Paper. Similarly, the Commission must retain the risk/harm framework articulated in the White Paper and avoid adoption of new rules or requirements that do not distinguish between AI applications that may pose risk of injury or harm to individuals, and those applications that pose little, or no, risk. Retention of this risk-based framework is essential to maintaining proportionality in any new framework. Further, the Commission should differentiate types of harm and carefully calibrate the responses to such harm. Indeed, as CTA explained in its prior comments to the Commission, the Commission s approach should begin with a narrow focus by identifying challenges that are unique to AI and not already covered by existing regulation.6 To begin with, the Commission should adopt a definition of AI that narrowly focuses on those unique aspects of the technology (i.e., systems that are capable of learning on their own), but which avoids capturing general concepts or constructs used in computer science (i.e., algorithms) that do not, on their own, constitute AI. The Commission can do so by utilizing the definition set forth in its April 2018 5 Impact Assessment at 6 CTA Consumer Technology Association Comments on European Commission White Paper on Artificial Intelligence A European Approach (filed June 12, ( CTA AI Comments ). 3 Communication on AI.7 A more nuanced approach would embrace an application-specific framework for consideration of any new mandates, which would also ensure that AI policy does not duplicate existing policy frameworks in those fields in which AI technologies are already in use today (such as healthcare, financial services or energy). CTA also agrees with the Commission s finding that it must avoid fragmentation amongst EU member states that may consider differing rules as one potential benefit of this process.8 While sovereignty must be respected, the potential emergence of varied regulatory or legislative proposals within the EU would create an unworkable environment for many technology providers, who would be forced to operate and comply with numerous different rules and regulations. The operational costs and burdens of doing so are well documented. Indeed, as the Commission itself noted: increasing fragmentation can hamper the confidence of European businesses to innovate and the development of beneficial AI solutions, and jeopardize the goals of a single digital market. 9 Narrow, focused Commission action supported by a detailed, and evidence-based assessment will reduce the potential for fragmentation among member states. Finally, CTA applauds the Commission for recognizing that if compliance costs outweigh the benefits, some desirable AI systems may not be developed at all. Specifically, the Commission finds that new legislation may impose additional compliance costs because AI systems may have to adhere to new requirements and processes. 10 Indeed, this finding is followed by an acknowledgement that [i]f compliance costs outweigh the benefits, it may even be the case that some desirable AI systems may not be developed at all. This principle should be the touchstone of any truly accurate and meaningful impact assessment: over-regulation will lead to under-investment and loss of innovation. Thus, the Commission must be mindful of the potential for taking action that has the effect of simply discontinuing further development of this technology in the EU. II. Additional Analysis Is Necessary to Fully Assess the Impact on Nascent and Developing Technology A. Further Development of the Cost-Benefit Analysis Is Necessary to Ensure Commission Action Does Not Undermine Investment and Innovation Although the Impact Assessment acknowledges the potentially deleterious impact on investment and innovation, it does not give sufficient consideration to the potential economic impact of adopting over-reaching new rules. The Commission s analysis of the potential economic impact of new rules11 is insufficient and must be more fully developed. The assessment does not give necessary, or sufficient, consideration to the significant potential costs of new requirements and processes, and specifically any new requirements that may include ex 7 Artificial intelligence (AI) refers to systems that display intelligent behaviour by analysing their environment and taking actions with some degree of autonomy to achieve specific goals. AI- based systems can be purely software-based, acting in the virtual world (e.g. voice assistants, image analysis software, search engines, speech and face recognition systems) or AI can be embedded in hardware devices (e.g. advanced robots, autonomous cars, drones or Internet of Things applications). COM( 237 Final, p. 8 Impact Assessment at at at at 5-4 ante regulation. If ex ante rules are adopted, many companies developing this technology will likely face costly and burdensome obligations that would deter innovation and discourage investment. The need for a fuller impact assessment of the potential economic costs of regulations is most clearly framed by the Commission s consideration of a restrictive and complex Option 3 regulatory approach, as described in the Impact Assessment.12 As outlined in numerous comments before the Commission, new recordkeeping and disclosure obligations could greatly increase the operational and compliance costs of AI developers, including many small emerging companies in this market. Indeed, the Impact Assessment significantly underestimates the potential administrative burdens of operating under a new regulatory regime for AI, as reflected in the finding that binding requirements could create some administrative burden that is not totally compensated by additional benefits. 13 This finding ignores the likely significant administrative burden if recordkeeping, reporting, training and oversight obligations are imposed. More significantly, the Impact Assessment does not fully analyze the potential impact of adopting limits on the use of certain training data, which may impact the accuracy and robustness of these systems; and the impact of imposing new obligations to utilize human oversight of systems that may be unnecessary or redundant.14 Nor does the analysis give sufficient consideration to the numerous societal, economic and other benefits provided by emerging AI systems and technologies, such as the numerous new AI-powered technology solutions being deployed to respond to COVID-15 Accordingly, the Commission should further develop its assessment to ensure that it considers the impact of policies that may deter the continued development and deployment of AI products and services which enhance the lives, safety and interests of European consumers. The analysis also fails to fully evaluate the potential impact of providers that may need to operate under a regime which creates duplicative or overlapping obligations, and the legal and administrative costs of operating in that environment. As CTA demonstrated in its comments, the potential for conflicting or duplicative laws in certain markets such as autonomous vehicles, which may already have existing duties and obligations, is particularly problematic.16 For example, the creation of conformity assessment procedures for high-risk AI applications, if applied to autonomous vehicles, could be duplicative of other existing requirements applicable to autonomous vehicles. To mitigate this potential overlap the Commission should identify those areas, such as autonomous vehicles, which may already have existing duties and obligations and carve out such areas that are subject to other regulatory requirements. at 4- at 7 (emphasis added). 14 See CTA Comments at 11-12 (noting that imposing an obligation of human oversight may be appropriate in certain circumstances, but is fundamentally at odds with the objective of developing and deploying fully automated vehicles intended to operate free from human intervention). 15 As previously noted, during the global COVID-19 pandemic AI is driving important research and testing necessary to defeat the virus. For example, French AI company Iktos has partnered with SRI International, based in Menlo Park, to discover and develop new anti-viral therapies using deep-learning models. Id. at 1- at 5 B. Cost-benefit Analysis Should Specifically Consider the Impact on Small and Medium Sized Business In discussing potential compliance costs for potentially affected entities,17 the Commission focuses primarily on the aggregate costs on the industry (i.e., the greater the potential costs of compliance, the higher the costs that industry must assume). But the Impact Assessment fails to sufficiently account for the disproportionate impact of significant compliance costs on small to mid-sized entities that do not have the margin or operational budget to assume significantly greater compliance costs. For that reason the final Impact Assessment must specifically consider how such compliance costs would impact small to medium-sized businesses. While some AI developers are large concerns with significant resources, this is an area with many new entrants (small and medium sized enterprises) that may not have the same resources to devote to respond to new compliance, paperwork or reporting obligations. The potential impact on overreaching compliance costs must consider the proportional impact on such small and medium sized businesses. C. Final Impact Assessment Must Recognize that AI Specific Liability Schemes Could Create a Competitive Disadvantage for AI Technologies The Impact Assessment fails to explore the potential impact of adopting AI-specific liability rules, including a strict liability scheme for AI. CTA believes that adoption of AI- specific liability rules could lead to a competitive disadvantage for AI technologies competing against traditional technologies that may create similar risks. For example, many studies have recognized that implementation of autonomous vehicle systems is likely to increase road safety in the years ahead, but if such systems are subjected to new or greater liability duties than that which applies to existing systems, that could create a significant disincentive for further investment in autonomous vehicle systems. In this way, an AI-specific-liability scheme that creates an uneven playing field may hinder the development and deployment of many different types of AI technologies in the EU. Similarly, the Impact Assessment fails to explore the potential impact of adopting a strict liability scheme for AI. To address that shortcoming the Commission should acknowledge that a strict liability scheme would, if adopted, likely increase economic and societal costs across the board, which would require a very different cost-benefit framework and analysis. In exploring some of the issues surrounding the adoption of a strict liability scheme, the Impact Assessment fails to recognize that there are numerous economic actors involved in the lifecycle of an AI system including entities: developing the technology, those deploying the technology and potentially others, such as distributors, resellers, service providers and individuals who use the technology in an integrated platform or solution. Under this framework numerous private economic actors play a role in the development and delivery chain and will be impacted by the adoption of an overly broad, ill-conceived strict liability system. This shortcoming may stem from the fact that neither the White Paper nor the accompanying Report on the Safety and Liability Implications of Artificial Intelligence, the Internet of Things and Robotics,18 properly evaluates the potential ramifications of adopting a strict liability framework for all AI systems. 17 Impact Assessment at 18 COM(64, Report on the Safety and Liability Implications Of Artificial Intelligence, The Internet of Things and Robotics (rel. 19 Feb. . 6 Such a decision would have wide-ranging implications that could seriously undermine the development and expansion of important AI systems, and must be considered in any further Impact Assessment. D. Final Impact Assessment Should Rely Upon Evidence from Companies Leading in the Development and Deployment of this Technology Although the Impact Assessment states that there is not a lot of currently valid evidence [] available at this stage this belies the very robust record already developed by the Commission itself in this proceeding. There is significant evidence already in the record in the form of public comments made in response to the White Paper. Many leading developers of AI systems and technology, such as Google, Microsoft and others have filed comments with the Commission which provide significant factual record to address the many important questions raised in the White Paper. To fail to account for, and rely upon, such evidence would be a mistake. E. Final Impact Assessment Should Further Analyze Value of Industry Consensus and Voluntary Standards in Lieu of New Rules While the Impact Assessment acknowledges the potential value of adopting a soft law approach ( Option 1 ), the analysis does not fully consider the potential value of leveraging voluntary standards and industry consensus in lieu of more formal rules. This approach should be used whenever possible to leverage existing voluntary standards and ethical frameworks adopted by many actors. As CTA has explained, voluntary consensus-based standards can reduce the burden of complying with ill-formed regulation, eliminate the administrative costs of developing state-mandated standards, and decrease the overall cost of goods procured and the burden of complying with agency regulation. This is precisely why CTA and its members have taken a leadership position in AI standard-setting, both in North America19 and internationally.20 Consensus-based standards, like those being developed by CTA and its member companies, often have broad support from industry, are more likely to reflect the most current technological developments, and reflect the most practical solutions available to the marketplace. III. Conclusion CTA and its members have a significant interest in ensuring that European consumers benefit from AI-powered products and services. The Commission should further develop its assessment of the economic impact of over-reaching and burdensome new legislation in this area. Instead, the Commission should proceed carefully to ensure that its policies promote continued development and deployment of AI products and services that enhance the lives, safety and interests of European consumers. CTA stands ready to continue its central role in the 19 For example, CTA is developing standards focused on AI in healthcare. See The Use of Artificial Intelligence in Health Care: Trustworthiness; or Definitions and Characteristics of Artificial Intelligence; and, Riya Anandwala and Danielle Cassagnol, CTA, Press Release, CTA Launches First-Ever Industry-Led Standard for AI in Health Care (rel. Feb. 25, , available at Launches-First-Ever-Industry-Led-Standard . 20 For example, CTA has been actively participating in ISO/IEC JTC 1/SC 42, the international standards committee responsible for standardization in the area of Artificial Intelligence. 7 development of consensus-based standards that advance these goals and promote continued dynamic growth and innovation throughout the consumer technology industry. /s/ Douglas K. Johnson Douglas K. Johnson Vice President, Technology Policy /s/ Michael Petricone Michael Petricone Sr. Vice President, Government and Regulatory Affairs Filed: September 10, 2020",en,"In order to meet the Commission s own high standard for analyzing the impact of potential legislation, the Commission must (in its own words) analyse in more detail the issue to be addressed, whether action should be taken at EU level and the potential economic, social and environmental effects of the different solutions outlined. 3 Communication on AI.7 A more nuanced approach would embrace an application-specific framework for consideration of any new mandates, which would also ensure that AI policy does not duplicate existing policy frameworks in those fields in which AI technologies are already in use today (such as healthcare, financial services or energy).",risk
IEEE Standards Association (United States),F551045,10 September 2020,Business association,Large (250 or more),United States,"ETHICALLY ALIGNED DESIGN A Vision for Prioritizing Human Well-being with Autonomous and Intelligent SystemsFirst EditionThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems United States License. The views and opinions expressed in this collaborative work are those of the authors and do not necessarily reflect the official policy or position of their respective institutions or of the Institute of Electrical and Electronics Engineers (IEEE). This work is published under the auspices of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems for the purposes of furthering public understanding of the importance of addressing ethical considerations in the design of autonomous and intelligent systems. Please see page 290, How the Document Was Prepared, for more details regarding the preparation of this document.Introduction 2 Executive Summary 3 -6 Acknowledgements 7-8 Ethically Aligned Design From Principles to Practice 9 -16 General Principles 1 7-35 Classical Ethics in A/IS 3 6-67 Well-being 6 8-89 Affective Computing 90 -109 Personal Data and Individual Agency 1 10-123 Methods to Guide Ethical Research and Design 1 24-139 A/IS for Sustainable Development 1 40-168 Embedding Values into Autonomous and Intelligent Systems 1 69-197 Policy 198-210 L a w 2 11-281 About Ethically Aligned Design The Mission and Results of The IEEE Global Initiative 2 82 From Principles to Practice Results of Our Work to Date 2 83-284 IEEE P7000 Approved Standardization Projects 2 85-286 Who We Are 2 87 Our Process 2 88-289 How the Document was Prepared 2 90 How to Cite Ethically Aligned Design 290 Key References 2 91 United States License.2 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Introduction This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.As the use and impact of autonomous and intelligent systems (A/IS) become pervasive, we need to establish societal and policy guidelines in order for such systems to remain human-centric, serving humanity s values and ethical principles. These systems must be developed and should operate in a way that is beneficial to people and the environment, beyond simply reaching functional goals and addressing technical problems. This approach will foster the heightened level of trust between people and technology that is needed for its fruitful use in our daily lives. To be able to contribute in a positive, non-dogmatic way, we, the techno-scientific communities, need to enhance our self-reflection. We need to have an open and honest debate around our explicit or implicit values, including our imaginary 1 around so-called Artificial Intelligence and the institutions, symbols, and representations it generates. Ultimately, our goal should be eudaimonia , a practice elucidated by Aristotle that defines human well-being, both at the individual and collective level, as the highest virtue for a society. Translated roughly as flourishing , the benefits of eudaimonia begin with conscious contemplation, where ethical considerations help us define how we wish to live. Whether our ethical practices are Western (e.g., Aristotelian, Kantian), Eastern (e.g., Shinto, /School of Mo, Confucian), African (e.g., Ubuntu), or from another tradition, honoring holistic definitions of societal prosperity is essential versus pursuing one-dimensional goals of increased productivity or gross domestic product (GDP). Autonomous and intelligent systems should prioritize and have as their goal the explicit honoring of our inalienable fundamental rights and dignity as well as the increase of human flourishing and environmental sustainability. The goal of The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems ( The IEEE Global Initiative ) is that Ethically Aligned Design will provide pragmatic and directional insights and recommendations, serving as a key reference for the work of technologists, educators and policymakers in the coming years. Ethically Aligned Design sets forth scientific analysis and resources, high-level principles, and actionable recommendations. It offers specific guidance for standards, certification, regulation or legislation for design, manufacture, and use of A/IS that provably aligns with and improves holistic societal well-being. 1The symbols, values, institutions, and norms of a societal group through which people imagine their lives and constitute their societies.3 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Introduction This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.I. Purpose of Ethically Aligned Design, First Edition (EAD1e) Autonomous and intelligent technical systems are specifically designed to reduce the necessity for human intervention in our day-to-day lives. In so doing, these new systems are also raising concerns about their impact on individuals and societies. Current discussions include advocacy for a positive impact, such as optimization of processes and resource usage, more informed planning and decisions, and recognition of useful patterns in big data. Discussions also include warnings about potential harm to privacy, discrimination, loss of skills, adverse economic impacts, risks to security of critical infrastructure, and possible negative long-term effects on societal well-being. Because of their nature, the full benefit of these technologies will be attained only if they are aligned with society s defined values and ethical principles. Through this work we intend, therefore, to establish frameworks to guide and inform dialogue and debate around the non-technical implications of these technologies, in particular related to ethical aspects. We understand ethical to go beyond moral constructs and include social fairness, environmental sustainability, and our desire for self-determination. Our analyses and recommendations in Ethically Aligned Design address values and intentions as well as implementations, both legal and technical. They are both aspirational, what we hope or wish should happen, and practical, what we the techno-scientific community and every group involved with and/or affected by these technologies could do for society to advance in positive directions. The analyses and recommendations in EAD1e are offered as guidance for consideration by governments, businesses, and the public at large in the advancement of technology for the benefit of humanity. Chapters in Ethically Aligned Design , First Edition F rom Principles to Practice G eneral Principles C lassical Ethics in A/IS Well-being A ffective Computing P ersonal Data and Individual Agency7. M ethods to Guide Ethical Research and Design A /IS for Sustainable Development E mbedding Values into Autonomous an d Intelligent Systems Policy L a wExecutive Summary4 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Introduction This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.II. General Principles The ethical and values-based design, development, and implementation of autonomous and intelligent systems should be guided by the following General Principles: H uman Rights A /IS shall be created and operated to respect, pr omote, and protect internationally re cognized human rights. Well-being A /IS creators shall adopt increased human w ell-being as a primary success criterion f or development. D ata Agency A /IS creators shall empower individuals with t he ability to access and securely share their d ata, to maintain people s capacity to have c ontrol over their identity. Effectiveness A /IS creators and operators shall provide e vidence of the effectiveness and fitness f or purpose of A/IS. T ransparency T he basis of a particular A/IS decision should al ways be discoverable. Accountability A /IS shall be created and operated to provide an u nambiguous rationale for all decisions made. A wareness of Misuse A /IS creators shall guard against all potential m isuses and risks of A/IS in operation. Competence A /IS creators shall specify and operators shall a dhere to the knowledge and skill required f or safe and effective operation.III. Ethical Foundations Classical E thics By drawing from over two thousand five hundred years of classical ethics traditions, the authors of Ethically Aligned Design explored established ethics systems, addressing both scientific and religious approaches, including secular philosophical traditions, to address human morality in the digital age. Through reviewing the philosophical foundations that define autonomy and ontology, this work addresses the alleged potential for autonomous capacity of intelligent technical systems, morality in amoral systems, and asks whether decisions made by amoral systems can have moral consequences. IV. Areas of Impact A/IS for Sustainable Development Through affordable and universal access to communications networks and the Internet, autonomous and intelligent systems can be made available to and benefit populations anywhere. They can significantly alter institutions and institutional relationships toward more human-centric structures, and they can address humanitarian and sustainable development issues resulting in increased individual societal and environmental well-being. Such efforts could be facilitated through the recognition of and adherence to established indicators of societal flourishing such as the United Nations Sustainable Development Goals so that human well-being is utilized as a primary success criteria for A/IS development. 5 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Introduction This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Personal Data Rights and Agency Over Digital Identity People have the right to access, share, and benefit from their data and the insights it provides. Individuals require mechanisms to help create and curate the terms and conditions regarding access to their identity and personal data, and to control its safe, specific, and finite exchange. Individuals also require policies and practices that make them explicitly aware of consequences resulting from the aggregation or resale of their personal information. Legal Frameworks for Accountability The convergence of autonomous and intelligent systems and robotics technologies has led to the development of systems with attributes that simulate those of human beings in terms of partial autonomy, ability to perform specific intellectual tasks, and even a human physical appearance. The issue of the legal status of complex autonomous and intelligent systems thus intertwines with broader legal questions regarding how to ensure accountability and allocate liability when such systems cause harm. It is clear that: A utonomous and intelligent technical systems should be subject to the applicable regimes of property law. G overnment and industry stakeholders should identify the types of decisions and operations that should never be delegated to such systems. These stakeholders should adopt rules and standards that ensure effective human control over those decisions and how to allocate legal responsibility for harm caused by them. T he manifestations generated by autonomous and intelligent technical systems should, in general, be protected under national and international laws. S tandards of transparency, competence, accountability, and evidence of effectiveness should govern the development of autonomous and intelligent systems. Policies for Education and Awareness Effective policy addresses the protection and promotion of human rights, safety, privacy, and cybersecurity, as well as the public understanding of the potential impact of autonomous and intelligent technical systems on society. To ensure that they best serve the public interest, policies should: S upport, promote, and enable internationally recognized legal norms. Dev elop government expertise in related technologies. E nsure governance and ethics are core components in research, development, acquisition, and use. R egulate to ensure public safety and responsible system design. E ducate the public on societal impacts of related technologies.6 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Introduction This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.V. Implementation Well-being Metrics For autonomous and intelligent systems to provably advance a specific benefit for humanity, there need to be clear indicators of that benefit. Common metrics of success include profit, gross domestic product, consumption levels, and occupational safety. While important, these metrics fail to encompass the full spectrum of well-being for individuals, the environment, and society. Psychological, social, economic fairness, and environmental factors matter. Well-being metrics include such factors, allowing the benefits arising from technological progress to be more comprehensively evaluated, providing opportunities to test for unintended negative consequences that could diminish human well-being. A/IS can improve capturing of and analyzing the pertinent data, which in turn could help identify where these systems would increase human well-being, providing new routes to societal and technological innovation. Embedding Values into Autonomous and Intelligent Systems If machines engage in human communities as quasi-autonomous agents, then those agents must be expected to follow the community s social and moral norms. Embedding norms in such quasi-autonomous systems requires a clear delineation of the community in which they are to be deployed. Further, even within a particular community, different types of technical embodiments will demand different sets of norms. The first step is to identify the norms of the specific community in which the systems are to be deployed and, in particular, norms relevant to the kinds of tasks that they are designed to perform. Methods to Guide Ethical Research and Design To create autonomous and intelligent technical systems that enhance and extend human well-being and freedom, values-based design methods must put human advancement at the core of development of technical systems. This must be done in concert with the recognition that machines should serve humans and not the other way around. Systems developers should employ values-based design methods in order to create sustainable systems that can be evaluated in terms of not only providing increased economic value for organizations but also of broader social costs and benefits. Affective Computing Affect is a core aspect of intelligence. Drives and emotions such as anger, fear, and joy are often the foundations of actions throughout our lives. To ensure that intelligent technical systems will be used to help humanity to the greatest extent possible in all contexts, autonomous and intelligent systems that participate in or facilitate human society should not cause harm by either amplifying or dampening human emotional experience.7 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Introduction This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Acknowledgements Our progress and the ongoing positive influence of this work are due to the volunteer experts serving on all our Committees and IEEE P7000 Standards Working Groups, along with the IEEE professional staff who support our efforts. Thank you for your dedication toward defining, designing, and inspiring the ethical principles and standards that will ensure that autonomous and intelligent systems and the technologies associated with them will positively benefit humanity. We wish to thank the Executive Committee and Committees of The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems: Executive Committee Officers Raja Chatila, ChairKay Firth-Butterfield, Vice ChairJohn C. Havens, Executive Director Executive Committee Members Dr. Greg Adamson, Karen Bartleson, Virginia Dignum, Danit Gal, Malavika Jayaram, Sven Koenig, Eileen M. Lach, Raj Madhavan, Richard Mallah, AJung Moon, Monique Morrow, Francesca Rossi, Alan Winfield, and Hagit Messer Yaron Committee Chairs G eneral Principles: Mark Halverson, and Peet van Biljon E mbedding Values into Autonomous Intelligent Systems: Francesca Rossi and Bertram F. Malle M ethodologies to Guide Ethical Research and Design: Raja Chatila and Corinne Cath S afety and Beneficence of Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI): Malo Bourgon and Richard Mallah P ersonal Data and Individual Agency: Katryna Dow and John C. Havens R eframing Autonomous Weapons Systems: Peter Asaro S ustainable Development: Elizabeth Gibbons L aw: Nicolas Economou and John Casey Af fective Computing: John Sullins and Joanna J. Bryson C lassical Ethics in A/IS: Jared Bielby P olicy: Peter Brooks and Mina Hannah E xtended Reality: Monique Morrow and Jay Iorio W ell-being: Laura Musikanski and John C. Havens E diting: Karen Bartleson and Eileen M. Lach Ou treach: Maya Zuckerman and Ali Muzaffar C ommunications: Leanne Seeto and Mark Halverson H igh School: Tess Posner G lobal Coordination: Victoria Wang, Arisa Ema, Pavel Gotovtsev 8 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Introduction This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Programs and Projects Inspired by The IEEE Global Initiative: E thically Aligned Design University Consortium: Hagit Messer, Chair E thically Aligned Design Community: Lisa Morgan, Program Director, Content and Community E thics Certification Program for Autonomous and Intelligent Systems: Meeri Haataja, Chair; Ali Hessami, Vice-Chair G lossary: Sara M. Jordan, Chair People We would like to warmly recognize the leadership and constant support of The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems by Dr. Ing. Konstantinos Karachalios, Managing Director of the IEEE Standards Association. We would also like to thank Stephen Welby, Executive Director and Chief Operating Officer of IEEE for his generous and insightful support of the Ethically Aligned Design , First Edition process and The IEEE Global Initiative overall. We would especially like to thank Eileen M. Lach, the former IEEE General Counsel and Chief Compliance Officer, whose heartfelt conviction that there is a pressing need to focus the global community on highlighting ethical considerations in the development of autonomous and intelligent systems served as a strong catalyst for the development of the Initiative within IEEE.Finally, we would like to also acknowledge the ongoing work of three Committees of The IEEE Global Initiative regarding their chapters of Ethically Aligned Design that, for timing reasons, we were not able to include in Ethically Aligned Design , First Edition. These Committees include: Reframing Autonomous Weapons Systems, Extended Reality (formerly Mixed Reality) and Safety and Beneficence of Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI). We would like to thank Peter Asaro, Monique Morrow and Jay Iorio, Malo Bourgon and Richard Mallah for their leadership in these groups along with all their Committee Members. Once these chapters have completed their review and been accepted by IEEE they could either be included in Ethically Aligned Design , published by The IEEE Global Initiative, or in other publications of IEEE. For information on disclaimers associated with EAD1e, see How the Document Was Prepared.9 From Principles to Practice Ethically Aligned Design Conceptual Framework The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Ethically Aligned Design , First Edition (EAD1e) represents more than a comprehensive report, distilling the consensus of its vast community of creators into a set of high-level ethical principles, key issues, and practical recommendations. EAD1e is an in-depth seminal work, a one-of-a-kind treatise, intended not only to inform a broader public but also to inspire its audience and readership of academics, engineers, policy makers, and manufacturers of autonomous and intelligent systems 1 (A/IS) to take action. This Chapter, From Principles to Practice , provides a mapping of the conceptual framework of Ethically Aligned Design . It outlines the logic behind Three Pillars that form the basis of EAD1e, and it connects the Pillars to high-level General Principles which guide all manner of ethical A/IS design. Following this, the content of the Chapters of EAD1e is mapped to the Principles. Finally, examples of EAD1e already in practice are described. Sections in this Chapter: The Three Pillars of the Ethically Aligned Design Conceptual Framework The General Principles of Ethically Aligned Design Mapping the Pillars to the Principles Mapping the Principles to the Content of the Chapters From Principles to Practice Ethically Aligned Design in Implementation From Principles to Practice Ethically Aligned Design Conceptual Framework 10 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems From Principles to Practice Ethically Aligned Design Conceptual Framework This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.The Three Pillars of the Ethically Aligned Design Conceptual Framework The Pillars of the Ethically Aligned Design Conceptual Framework fall broadly into three areas, reflecting anthropological, political, and technical aspects: U niversal Human Values: A/IS can be an enormous force for good in society provided they are designed to respect human rights, align with human values, and holistically increase well-being while empowering as many people as possible. They should also be designed to safeguard our environment and natural resources. These values should guide policy makers as well as engineers, designers, and developers. Advances in A/IS should be in the service of all people, rather than benefiting solely small groups, a single nation, or a corporation. P olitical Self-Determination and Data Agency: A/IS if designed and implemented properly have a great potential to nurture political freedom and democracy, in accordance with the cultural precepts of individual societies, when people have access to and control over the data constituting and representing their identity. These systems can improve government effectiveness and accountability, foster trust, and protect our private sphere, but only when people have agency over their digital identity and their data is provably protected. T echnical Dependability: Ultimately, A/IS should deliver services that can be trusted.2 This trust means that A/IS will reliably, safely, and actively accomplish the objectives for which they were designed while advancing the human-driven values they were intended to reflect. Technologies should be monitored to ensure that their operation meets predetermined ethical objectives aligning with human values and respecting codified rights. In addition, validation and verification processes, including aspects of explainability, should be developed that could lead to better auditability and to certification 3 of A/IS. 11 The General Principles of Ethically Aligned Design The General Principles of Ethically Aligned Design have emerged through the continuous work of dedicated, open communities in a multi-year, creative, consensus-building process. They articulate high-level principles that should apply to all types of autonomous and intelligent systems (A/IS). Created to guide behavior and inform standards and policy making, the General Principles define imperatives for the ethical design, development, deployment, adoption, and decommissioning of autonomous and intelligent systems. The Principles consider the role of A/IS creators, i.e., those who design and manufacture, of operators, i.e., those with expertise specific to use of A/IS, other users, and any other stakeholders or affected parties. The General Principles4 of Ethically Aligned Design H uman Rights A/IS shall be created and operated to respect, promote, and protect internationally recognized human rights. W ell-being A/IS creators shall adopt increased human well-being as a primary success criterion for development. D ata Agency A/IS creators shall empower individuals with the ability to access and securely share their data, to maintain people s capacity to have control over their identity.E ffectiveness A/IS creators and operators shall provide evidence of the effectiveness and fitness for purpose of A/IS. T ransparency The basis of a particular A/IS decision should always be discoverable. A ccountability A/IS shall be created and operated to provide an unambiguous rationale for all decisions made. A wareness of Misuse A/IS creators shall guard against all potential misuses and risks of A/IS in operation. C ompetence A/IS creators shall specify and operators shall adhere to the knowledge and skill required for safe and effective operation.12 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems From Principles to Practice Ethically Aligned Design Conceptual Framework This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Mapping the Pillars to the Principles Whereas the Pillars of the Ethically Aligned Design Conceptual Framework represent broad anthropological, political, and technical aspects relating to autonomous and intelligent systems, the General Principles provide contextual filters for deeper analysis and pragmatic implementation. It is also important to recognize that the General Principles do not live in isolation of EAD s Pillars and vice versa. While the General Principle of Transparency may inform the design of a specific autonomous or intelligent system, the A/IS must also account for universal human values, political self-determination, and data agency. Moreover, Transparency goes beyond technical features. It is an important requirement also for the processes of policy and lawmaking. In this way, EAD1e s Pillars form the holistic ethical grounding upon which the Principles can build, and the latter may apply in various spheres of human activity. EAD Pillars Universal H uman Values Political S elf-Determination Data Agency Technical D ependability Human Rights n n Well-being n n Data Agency n n n Effectiveness n Transparency n n n Accountability n n n Awareness of Misuse n Competence nEAD General PrinciplesEAD1e Pillars Mapped to General Principles Universal Human ValuesTechnical DependabilityPolitical Self-Determination Data Agency n Indicates General Principle mapped to Pillar. 13 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems From Principles to Practice Ethically Aligned Design Conceptual Framework This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Mapping the Principles to the Content of the Chapters The Chapters of Ethically Aligned Design provide in-depth subject matter expertise that allows readers to move from the General Principles to more deeply analyze ethical A/IS issues within the context of their specific work. The mapping or indexing provided in the table below serve as directional starting points since elements of a Principle like Competence may resonate in several EAD1e Chapters. In addition, where core subjects are primarily covered by specific Chapters, we have done our best to indicate this via our mapping below. EAD1e General Principles Mapped to Chapters EAD Chapters General PrinciplesClassical Ethics in A/ISWell-beingAffective ComputingData & Individual AgencyMethods A/IS DesignA/IS f or Sustainable Dev.Embedding Values into A/ISPolicy Law Human Rights n n n n n n n n n n Well-being n n n n n n n n n n Data Agency n n n n n n n n n Effectiveness n n n n n n Transparency n n n n n n Accountability n n n n n n n Awareness of Misuse n n n n n n n Competence n n n n n nEAD General Principles n Indicates General Principle mapped to Chapter. n Indicates primary EAD Chapter providing elaboration on a General Principle .14 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems From Principles to Practice Ethically Aligned Design Conceptual Framework This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.From Principles to Practice It is at this step of the Ethically Aligned Design Conceptual Framework that readers will be able to identify the Principles and Chapters of key relevance to their work. Content provided in EAD1e Chapters is organized by Issues identified as the most pressing ethical matters surrounding A/IS design to address today and Recommendations on how it should be done. By reviewing these Issues and Recommendations in light of a specific A/IS product, service, or system being designed, readers are provided with a simple form of impact assessment and due diligence process to help put their Principles into Practice for themselves. Of course, more fine-tuned customization and adaptation of the content of EAD1e to fit specific sectors or applications are possible and will be pursued in the near future. See below for some implementation examples already happening.15 Ethically Aligned Design in Implementation Ethically Aligned Design , First Edition represents the culmination of a three-year process guided bottom-up since 2015 by the rigor and standards of the engineering profession and by a globally open and iterative process involving hundreds of global experts. The analysis of the Principles, Issues, and Recommendations generated as part of an iterative process have already inspired the creation of fourteen IEEE Standardization Projects, a Certification Program, A/IS Ethics Courses, and multiple other action-oriented programs currently in development. In its earlier manifestations, Ethically Aligned Design informed collaborations on A/IS governance with a broad range of governmental and civil society organizations, including the United Nations, the European Commission, the Organization for Economic Cooperation and Development and many national and municipal governments and institutions. 5 Moreover, the engagement in all of these arenas and with such partners has put the collective knowledge and creativity of The IEEE Global Initiative in the service of global policy-making with tangible and visible results. Beyond inspiring the policy arena, EAD1e and this growing body of work has also been influencing the development of industry-related resources. 6 It is time to move From Principles to Practice in society regarding the governance of emerging autonomous and intelligent systems. The implementation of ethical principles must be validated by dependable applications of A/IS in practice while honoring our desire for political self-determination and data agency. To achieve societal progress, the autonomous and intelligent systems we create must be trustworthy, provable, and accountable and must align to our explicitly formulated human values. It is our hope that Ethically Aligned Design and this conceptual framework will provide action-oriented inspiration for your work as well. Ethically Aligned Design Conceptual Framework From Principles to Practice For information on disclaimers associated with EAD1e, see How the Document Was Prepared.16 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems From Principles to Practice Ethically Aligned Design Conceptual Framework This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Endnotes 1 We prefer not to use as far as possible the vague term AI and use instead the term autonomous and intelligent systems (A/IS). This terminology is applied throughout Ethically Aligned Design, First Edition to ensure the broadest possible application of ethical considerations in the design of the addressed technologies and systems. 2 See also Draft Ethics Guidelines for Trustworthy AI of The European Commission s High Level Expert Group on AI. 3 A/IS should be subject to specific certification procedures by competent and qualified agencies with participation or control of public authorities in the same way other technical systems require certification before deployment. The IEEE has launched one of the world s first programs dedicated to creating A/IS certification processes. The Ethics Certification Program for Autonomous and Intelligent Systems (ECPAIS) offers processes by which organizations can seek certified A/IS products, systems, and services. It is being developed through an extensive and open public-private collaboration. 4 For their overall framing, see the General Principles Chapter. 5 As an example, the recently published report Draft Ethics Guidelines for Trustworthy AI of The European Commission s High Level Expert Group on AI explicitly mentions EAD as a major source of their inspiration. EAD has also been guiding policy creation for efforts of the United Nations and the Organization for Economic Cooperation and Development. 6 Everyday Ethics for Artificial Intelligence: A Practical Guide for Designers and Developers17 General PrinciplesThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.The General Principles of Ethically Aligned Design articulate high-level ethical principles that apply to all types of autonomous and intelligent systems (A/IS), regardless of whether they are physical robots, such as care robots or driverless cars, or software systems, such as medical diagnosis systems, intelligent personal assistants, or algorithmic chat bots, in real, virtual, contextual, and mixed-reality environments. The General Principles define imperatives for the design, development, deployment, adoption, and decommissioning of autonomous and intelligent systems. The Principles consider the role of A/IS creators, i.e., those who design and manufacture, of operators, i.e., those with expertise specific to use of A/IS, other users, and any other stakeholders or affected parties. We have created these ethical General Principles for A/IS that: Embody the highest ideals of human beneficence within human rights. Prioritize benefits to humanity and the natural environment from the use of A/IS over commercial and other considerations. Benefits to humanity and the natural environment should not be at odds the former depends on the latter. Prioritizing human well-being does not mean degrading the environment. Mitigate risks and negative impacts, including misuse, as A/IS evolve as socio-technical systems, in particular by ensuring actions of A/IS are accountable and transparent. These General Principles are elaborated in subsequent sections of this chapter of Ethically Aligned Design , with specific contextual, cultural, and pragmatic explorations which impact their implementation. General Principles18 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.General Principles as Imperatives We offer high-level General Principles in Ethically Aligned Design that we consider to be imperatives for creating and operating A/IS that further human values and ensure trustworthiness. In summary, our General Principles are: Human Rights A/IS shall be created and operated to respect, promote, and protect internationally recognized human rights. Well-being A/IS creators shall adopt increased human well-being as a primary success criterion for development. Data Agency A/IS creators shall empower individuals with the ability to access and securely share their data, to maintain people s capacity to have control over their identity. Effectiveness A/IS creators and operators shall provide evidence of the effectiveness and fitness for purpose of A/IS. Transparency The basis of a particular A/IS decision should always be discoverable. Accountability A/IS shall be created and operated to provide an unambiguous rationale for all decisions made. Awareness of Misuse A/IS creators shall guard against all potential misuses and risks of A/IS in operation. Competence A/IS creators shall specify and operators shall adhere to the knowledge and skill required for safe and effective operation.19 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Principle 1 Human Rights A/IS shall be created and operated to respect, promote, and protect internationally recognized human rights. Background Human benefit is a crucial goal of A/IS, as is respect for human rights set out in works including, but not limited to: The Universal Declaration of Human Rights , the International Covenant on Civil and Political Rights, the Convention on the Rights of the Child, the Convention on the Elimination of all forms of Discrimination against Women , the Convention on the Rights of Persons with Disabilities , and the Geneva Conventions . Such rights need to be fully taken into consideration by individuals, companies, professional bodies, research institutions, and governments alike to reflect the principle that A/IS should be designed and operated in a way that both respects and fulfills human rights, freedoms, human dignity, and cultural diversity. While their interpretation may change over time, human rights , as defined by international law, provide a unilateral basis for creating any A/IS, as these systems affect humans, their emotions, data, or agency. While the direct coding of human rights in A/IS may be difficult or impossible based on contextual use, newer guidelines from The United Nations provide methods to pragmatically implement human rights ideals within business or corporate contexts that could be adapted for engineers and technologists. In this way, technologists can take into account human rights in the way A/IS are developed, operated, tested, and validated. In short, human rights should be part of the ethical risk assessment of A/IS. Recommendations To best respect human rights, society must assure the safety and security of A/IS so that they are designed and operated in a way that benefits humans. Specifically: G overnance frameworks, including standards and regulatory bodies, should be established to oversee processes which ensure that the use of A/IS does not infringe upon human rights, freedoms, dignity, and privacy, and which ensure traceability. This will contribute to building public trust in A/IS. A w ay to translate existing and forthcoming legal obligations into informed policy and technical considerations is needed. Such a method should allow for diverse cultural norms as well as differing legal and regulatory frameworks.20 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A /IS should always be subordinate to human judgment and control. F or the foreseeable future, A/IS should not be granted rights and privileges equal to human rights. Further Resources The following documents and organizations are provided both as references and examples of the types of work that can be emulated, adapted, and proliferated regarding ethical best practices around A/IS to best honor human rights: T he Universal Declaration of Human Rights , N . Wiener, The Human Use of Human Beings , New York: Houghton Mifflin, T he International Covenant on Civil and Political Rights , T he International Covenant on Economic , Social and Cultural Rights , T he International Convention on the Elimination of All Forms of Racial Discrimination , T he Convention on the Rights of the Child , T he Convention on the Elimination of All Forms of Discrimination against Women , T he Convention on the Rights of Persons with Disabilities , T he Geneva Conventions and Additional Protocols , I RTF s Research into Human Rights Protocol Considerations , T he UN Guiding Principles on Business and Human Rights , B ritish Standards Institute BS861 2016, Robots and Robotic Devices. Guide to the Ethical Design and Application of Robots and Robotic Systems21 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Principle 2 Well-being A/IS creators shall adopt increased human well-being as a primary success criterion for development. Background For A/IS technologies to demonstrably advance benefit for humanity, we need to be able to define and measure the benefit we wish to increase. But often the only indicators utilized in determining success for A/IS are avoiding negative unintended consequences and increasing productivity and economic growth for customers and society. Today, these are largely measured by gross domestic product (GDP), profit, or consumption levels. Well-being, for the purpose of Ethically Aligned Design , is based on the Organization for Economic Co-operation and Development s (OECD) Guidelines on Measuring Subjective Well-being perspective that, Being able to measure people s quality of life is fundamental when assessing the progress of societies. There is now widespread acknowledgement that measuring subjective well-being is an essential part of measuring quality of life alongside other social and economic dimensions as identified within Nassbaum-Sen s capability approach whereby well-being is objectively defined in terms of human capabilities necessary for functioning and flourishing.Since modern societies will be largely constituted of A/IS users, we believe these considerations to be relevant for A/IS creators. A/IS technologies can be narrowly conceived from an ethical standpoint. They can be legal, profitable, and safe in their usage, yet not positively contribute to human and environmental well-being. This means technologies created with the best intentions, but without considering well-being, can still have dramatic negative consequences on people s mental health, emotions, sense of themselves, their autonomy, their ability to achieve their goals, and other dimensions of well-being. Recommendation A/IS should prioritize human well-being as an outcome in all system designs, using the best available and widely accepted well-being metrics as their reference point. Further Resources I EEE P7010 , Well-being Metric for Autonomous and Intelligent Systems . T he Measurement of Economic Performance and Social Progress now commonly referred to as The Stiglitz Report , commissioned by the then President of the French Republic, From the report: the time is ripe for our measurement system to shift emphasis from measuring economic production to measuring 22 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.people s well-being emphasizing well-being is important because there appears to be an increasing gap between the information contained in aggregate GDP data and what counts for common people s well-being. O ECD Guidelines on Measuring Subjective Well-being , O ECD Better Life Index , W orld Happiness Reports , 2012 Uni ted Nations Sustainable Development Goal (SDG) Indicators , B eyond GDP , European Commission, From the site: The Beyond GDP initiative is about developing indicators that are as clear and appealing as GDP, but more inclusive of environmental and social aspects of progress. G enuine Progress Indicator , State of Maryland (first developed by Redefining Progress), T he International Panel on Social Progress, Social Justice, Well-Being and Economic Organization , R . Veenhoven, World Database of Happiness, Erasmus University Rotterdam, The Netherlands, Accessed 2018 at: . R oyal Government of Bhutan, The Report of the High-Level Meeting on Wellbeing and Happiness: Defining a New Economic Paradigm , New York: The Permanent Mission of the Kingdom of Bhutan to the United Nations, 23 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Principle 3 Data Agency A/IS creators shall empower individuals with the ability to access and securely share their data, to maintain people s capacity to have control over their identity. Background Digital consent is a misnomer in its current manifestation. Terms and conditions or privacy policies are largely designed to provide legally accurate information regarding the usage of people s data to safeguard institutional and corporate interests, while often neglecting the needs of the people whose data they process. Consent fatigue , the constant request for agreement to sets of long and unreadable data handling conditions, causes a majority of users to simply click and accept terms in order to access the services they wish to use. General obfuscation regarding privacy policies, and scenarios like the Cambridge Analytica scandal in 2018, demonstrate that even when individuals provide consent, the understanding of the value regarding their data and its safety is out of an individual s control. This existing model of data exchange has eroded human agency in the algorithmic age. People don t know how their data is being used at all times or when predictive messaging is honoring their existing preferences or manipulating them to create new behaviors. Regulations like the EU General Data Protection Regulation (GDPR) will help improve this lack of clarity regarding the exchange of personal data. But compliance with existing models of consent is not enough to safeguard people s agency regarding their personal information. In an era where A/IS are already pervasive in society, governments must recognize that limiting the misuse of personal data is not enough. Society must also recognize that human rights in the digital sphere don t exist until individuals globally are empowered with means including tools and policies that ensure their dignity through some form of sovereignty, agency, symmetry, or control regarding their identity and personal data. These rights rely on individuals being able to make their choices, outside of the potential influence of biased algorithmic messaging or bad actors. Society also needs to be confident that those who are unable to provide legal informed consent, including minors and people with diminished capacity to make informed decisions, do not lose their dignity due to this. Recommendation Organizations, including governments, should immediately explore, test, and implement technologies and policies that let individuals specify their online agent for case-by-case authorization decisions as to who can process what personal data for what purpose. For minors and those with diminished capacity to make informed decisions, current guardianship approaches should be viewed to determine their suitability in this context.24 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.The general solution to give agency to the individual is meant to anticipate and enable individuals to own and fully control autonomous and intelligent (as in capable of learning) technology that can evaluate data use requests by external parties and service providers. This technology would then provide a form of digital sovereignty and could issue limited and specific authorizations for processing of the individual s personal data wherever it is held in a compatible system. Further Resources The following resources are designed to provide governments and other organizations corporate, for-profit, not-for-profit, B Corp, or any form of public institution basic information on services designed to provide user agency and/or sovereignty over their personal data. T he European Data Protection Supervisor defines personal information management systems (PIMS) as: ...systems that help give individuals more control over their personal data...allowing individuals to manage their personal data in secure, local or online storage systems and share them when and with whom they choose. Providers of online services and advertisers will need to interact with the PIMS if they plan to process individuals data. This can enable a human centric approach to personal information and new business models. For further information and ongoing research regarding PIMS, visit Crtl-Shift s PIMS monthly archive . I EEE P7006 , IEEE Standards Project for Personal Data Artificial Intelligence (AI) Agent describes the technical elements required to create and grant access to a personalized Artificial Intelligence that will comprise inputs, learning, ethics, rules, and values controlled by individuals. I EEE P7012 , IEEE Standards Project for Machine Readable Personal Privacy Terms is designed to provide individuals with a means to proffer their own terms respecting personal privacy in ways that can be read, acknowledged, and be agreed to by machines operated by others in the networked world. 25 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Principle 4 Effectiveness Creators and operators shall provide evidence of the effectiveness and fitness for purpose of A/IS. Background The responsible adoption and deployment of A/IS are essential if such systems are to realize their many potential benefits to the well-being of both individuals and societies. A/IS will not be trusted unless they can be shown to be effective in use. Harms caused by A/IS, from harm to an individual through to systemic damage, can undermine the perceived value of A/IS and delay or prevent its adoption. Operators and other users will therefore benefit from measurement of the effectiveness of the A/IS in question. To be adequate, effective measurements need to be both valid and accurate, as well as meaningful and actionable. And such measurements must be accompanied by practical guidance on how to interpret and respond to them. Recommendations C reators engaged in the development of A/IS should seek to define metrics or benchmarks that will serve as valid and meaningful gauges of the effectiveness of the system in meeting its objectives, adhering to standards and remaining within risk tolerances. Creators building A/IS should ensure that the results when the defined metrics are applied are readily obtainable by all interested parties, e.g., users, safety certifiers, and regulators of the system. C reators of A/IS should provide guidance on how to interpret and respond to the metrics generated by the systems. T o the extent warranted by specific circumstances, operators of A/IS should follow the guidance on measurement provided with the systems, i.e., which metrics to obtain, how and when to obtain them, how to respond to given results, and so on. T o the extent that measurements are sample- based, measurements should account for the scope of sampling error, e.g., the reporting of confidence intervals associated with the measurements. Operators should be advised how to interpret the results. C reators of A/IS should design their systems such that metrics on specific deployments of the system can be aggregated to provide information on the effectiveness of the system across multiple deployments. For example, in the case of autonomous vehicles, metrics should be generated both for a specific instance of a vehicle and for a fleet of many instances of the same kind of vehicle. I n interpreting and responding to measurements, allowance should be made for variation in the specific objectives and circumstances of a given deployment of A/IS. 26 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.T o the extent possible, industry associations or other organizations, e.g., IEEE and ISO, should work toward developing standards for the measurement and reporting on the effectiveness of A/IS. Further Resources R . Dillmann, KA 10 Benchmarks for Robotics Research , A . Steinfeld, T.W. Fong, D. Kaber, J. Scholtz, A. Schultz, and M. Goodrich, Common Metrics for Human-Robot Interaction , 2006 Human-Robot Interaction Conference, March, R . Madhavan, E. Messina, and E. Tunstel, Eds., Performance Evaluation and Benchmarking of Intelligent Systems , Boston, MA: Springer, I EEE Robotics & Automation Magazine , Special Issue on Replicable and Measurable Robotics Research , Volume 22, No. 3, September C . Flanagin, A Survey on Robotics Systems and Performance Analysis , T ransaction Processing Performance Council (TPC) Establishes Artificial Intelligence Working Group (TPC-AI) tasked with developing industry standard benchmarks for both hardware and software platforms associated with running Artificial Intelligence (AI) based workloads, 27 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Principle 5 Transparency The basis of a particular A/IS decision should always be discoverable. Background A key concern over autonomous and intelligent systems is that their operation must be transparent to a wide range of stakeholders for different reasons, noting that the level of transparency will necessarily be different for each stakeholder. Transparent A/IS are ones in which it is possible to discover how and why a system made a particular decision, or in the case of a robot, acted the way it did. The term transparency in the context of A/IS also addresses the concepts of traceability, explainability, and interpretability. A/IS will perform tasks that are far more complex and have more effect on our world than prior generations of technology. Where the task is undertaken in a non-deterministic manner, it may defy simple explanation. This reality will be particularly acute with systems that interact with the physical world, thus raising the potential level of harm that such a system could cause. For example, some A/IS already have real consequences to human safety or well-being, such as medical diagnosis or driverless car autopilots. Systems such as these are safety-critical systems. At the same time, the complexity of A/IS technology and the non-intuitive way in which it may operate will make it difficult for users of those systems to understand the actions of the A/IS that they use, or with which they interact. This opacity, combined with the often distributed manner in which the A/IS are developed, will complicate efforts to determine and allocate responsibility when something goes wrong. Thus, lack of transparency increases the risk and magnitude of harm when users do not understand the systems they are using, or there is a failure to fix faults and improve systems following accidents. Lack of transparency also increases the difficulty of ensuring accountability (see Principle 6 Accountability). Achieving transparency, which may involve a significant portion of the resources required to develop the A/IS, is important to each stakeholder group for the following reasons: F or users, what the system is doing and why. F or creators, including those undertaking the validation and certification of A/IS, the systems processes and input data. F or an accident investigator, if accidents occur. F or those in the legal process, to inform evidence and decision-making. F or the public, to build confidence in the technology.28 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Recommendation Develop new standards that describe measurable, testable levels of transparency, so that systems can be objectively assessed and levels of compliance determined. For designers, such standards will provide a guide for self-assessing transparency during development and suggest mechanisms for improving transparency. The mechanisms by which transparency is provided will vary significantly, including but not limited to, the following use cases: F or users of care or domestic robots, a why- did-you-do-that button which, when pressed, causes the robot to explain the action it just took. F or validation or certification agencies, the algorithms underlying the A/IS and how they have been verified. F or accident investigators, secure storage of sensor and internal state data comparable to a flight data recorder or black box. IEEE P7001 , IEEE Standard for Transparency of Autonomous Systems is one such standard, developed in response to this recommendation. Further Resources C . Cappelli, P. Engiel, R. Mendes de Araujo, and J. C. Sampaio do Prado Leite, Managing Transparency Guided by a Maturity Model, 3rd Global Conference on Transparency Research 1 no. 3, pp. 1 17, Jouy-en-Josas, France: HEC Paris, J .C. Sampaio do Prado Leite and C. Cappelli, Software Transparency. Business & Information Systems Engineering 2, no. 3, pp. 127 139, A , Winfield, and M. Jirotka, The Case for an Ethical Black Box, Lecture Notes in Artificial Intelligence 10454, pp. 262 273, R . R. Wortham, A. Theodorou, and J. J. Bryson, What Does the Robot Think? Transparency as a Fundamental Design Requirement for Intelligent Systems, IJCAI-2016 Ethics for Artificial Intelligence Workshop , New York, M achine Intelligence Research Institute, Transparency in Safety-Critical Systems , August 25, M . Scherer, Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies , Harvard Journal of Law & Technology 29, no. 2, U .K. House of Commons, Decision Making Transparency, Report of the U.K. House of Commons Science and Technology Committee on Robotics and Artificial Intelligence , pp. 17-18, September 13, 29 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Principle 6 Accountability A/IS shall be created and operated to provide an unambiguous rationale for decisions made. Background The programming, output, and purpose of A/IS are often not discernible by the general public. Based on the cultural context, application, and use of A/IS, people and institutions need clarity around the manufacture and deployment of these systems to establish responsibility and accountability, and to avoid potential harm. Additionally, manufacturers of these systems must be accountable in order to address legal issues of culpability. It should, if necessary, be possible to apportion culpability among responsible creators (designers and manufacturers) and operators to avoid confusion or fear within the general public. Accountability and partial accountability are not possible without transparency, thus this principle is closely linked with Principle 5 Transparency. Recommendations To best address issues of responsibility and accountability: L egislatures/courts should clarify responsibility, culpability, liability, and accountability for A/IS, where possible, prior to development and deployment so that manufacturers and users understand their rights and obligations. D esigners and developers of A/IS should remain aware of, and take into account, the diversity of existing cultural norms among the groups of users of these A/IS. M ulti-stakeholder ecosystems including creators, and government, civil, and commercial stakeholders, should be developed to help establish norms where they do not exist because A/IS-oriented technology and their impacts are too new. These ecosystems would include, but not be limited to, representatives of civil society, law enforcement, insurers, investors, manufacturers, engineers, lawyers, and users. The norms can mature into best practices and laws. 30 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.S ystems for registration and record-keeping should be established so that it is always possible to find out who is legally responsible for a particular A/IS. Creators, including manufacturers, along with operators, of A/IS should register key, high-level parameters, including: I ntended use, T raining data and training environment, if applicable, S ensors and real world data sources, A lgorithms, P rocess graphs, M odel features, at various levels, U ser interfaces, A ctuators and outputs, and O ptimization goals, loss functions, and reward functions.Further Resources B . Shneiderman, Human Responsibility for Autonomous Agents, IEEE Intelligent Systems 22, no. 2, pp. 60 61, A . Matthias, The Responsibility Gap: Ascribing Responsibility for the Actions of Learning Automata. Ethics and Information Technology 6, no. 3, pp. 175 183, A . Hevelke and J. Nida-R melin, Responsibility for Crashes of Autonomous Vehicles: An Ethical Analysis, Science and Engineering Ethics 21, no. 3, pp. 619 630, A n example of good practice (in relation to Recommendation # can be found in Sciencewise the U.K. national center for public dialogue in policy-making involving science and technology issues.31 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Principle 7 Awareness of Misuse Creators shall guard against all potential misuses and risks of A/IS in operation. Background New technologies give rise to greater risk of deliberate or accidental misuse, and this is especially true for A/IS. A/IS increases the impact of risks such as hacking, misuse of personal data, system manipulation, or exploitation of vulnerable users by unscrupulous parties. Cases of A/IS hacking have already been widely reported, with driverless cars , for example. The Microsoft Tay AI chatbot was famously manipulated when it mimicked deliberately offensive users. In an age where these powerful tools are easily available, there is a need for a new kind of education for citizens to be sensitized to risks associated with the misuse of A/IS. The EU s General Data Protection Regulation (GDPR) provides measures to remedy the misuse of personal data. Responsible innovation requires A/IS creators to anticipate, reflect, and engage with users of A/IS. Thus, citizens, lawyers, governments, etc., all have a role to play through education and awareness in developing accountability structures (see Principle , in addition to guiding new technology proactively toward beneficial ends. Recommendations C reators should be aware of methods of misuse, and they should design A/IS in ways to minimize the opportunity for these.R aise public awareness around the issues of potential A/IS technology misuse in an informed and measured way by: P roviding ethics education and security awareness that sensitizes society to the potential risks of misuse of A/IS. For example, provide data privacy warnings that some smart devices will collect their users personal data. D elivering this education in scalable and effective ways, including having experts with the greatest credibility and impact who can minimize unwarranted fear about A/IS. E ducating government, lawmakers, and enforcement agencies about these issues of A/IS so citizens can work collaboratively with these agencies to understand safe use of A/IS. For example, the same way police officers give public safety lectures in schools, they could provide workshops on safe use and interaction with A/IS. Further Resources A . Greenberg, Hackers Fool Tesla S s _Autopilot to Hide and Spoof Obstacles , Wired, August C . Wilkinson and E. Weitkamp, Creative Research and Communication: Theory and Practice , Manchester, UK: Manchester University Press, 2016 (in relation to Recommendation #. E ngineering and Physical Sciences Research Council, Anticipate, Reflect, Engage and Act (AREA) , Framework for Responsible Research and Innovation, Accessed 32 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Principle 8 Competence Creators shall specify and operators shall adhere to the knowledge and skill required for safe and effective operation. Background A/IS can and often do make decisions that previously required human knowledge, expertise, and reason. Algorithms potentially can make even better decisions, by accessing more information, more quickly, and without the error, inconsistency, and bias that can plague human decision-making. As the use of algorithms becomes common and the decisions they make become more complex, however, the more normal and natural such decisions appear. Operators of A/IS can become less likely to question and potentially less able to question the decisions that algorithms make. Operators will not necessarily know the sources, scale, accuracy, and uncertainty that are implicit in applications of A/IS. As the use of A/IS expands, more systems will rely on machine learning where actions are not preprogrammed and that might not leave a clear record of the steps that led the system to its current state. Even if those records do exist, operators might not have access to them or the expertise necessary to decipher those records. Standards for the operators are essential. Operators should be able to understand how A/IS reach their decisions, the information and logic on which the A/IS rely, and the effects of those decisions. Even more crucially, operators should know when they need to question A/IS and when they need to overrule them. Creators of A/IS should take an active role in ensuring that operators of their technologies have the knowledge, experience, and skill necessary not only to use A/IS, but also to use it safely and appropriately, towards their intended ends. Creators should make provisions for the operators to override A/IS in appropriate circumstances. While standards for operator competence are necessary to ensure the effective, safe, and ethical application of A/IS, these standards are not the same for all forms of A/IS. The level of competence required for the safe and effective operation of A/IS will range from elementary, such as intuitive use guided by design, to advanced, such as fluency in statistics. Recommendations C reators of A/IS should specify the types and levels of knowledge necessary to understand and operate any given application of A/IS. In specifying the requisite types and levels of expertise, creators should do so for the individual components of A/IS and for the entire systems. C reators of A/IS should integrate safeguards against the incompetent operation of their systems. Safeguards could include issuing 33 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.notifications/warnings to operators in certain conditions, limiting functionalities for different levels of operators (e.g., novice vs. advanced), system shut-down in potentially risky conditions, etc. C reators of A/IS should provide the parties affected by the output of A/IS with information on the role of the operator, the competencies required, and the implications of operator error. Such documentation should be accessible and understandable to both experts and the general public. E ntities that operate A/IS should create documented policies to govern how A/IS should be operated. These policies should include the real-world applications for such A/IS, any preconditions for their effective use, who is qualified to operate them, what training is required for operators, how to measure the performance of the A/IS, and what should be expected from the A/IS. The policies should also include specification of circumstances in which it might be necessary for the operator to override the A/IS.O perators of A/IS should, before operating a system, make sure that they have access to the requisite competencies. The operator need not be an expert in all the pertinent domains but should have access to individuals with the requisite kinds of expertise. Further Resources S . Barocas and A.D. Selbst, The Intuitive Appeal of Explainable Machines , Fordham Law Review, W . Smart, C. Grimm, and W. Hartzog, An Education Theory of Fault for Autonomous Systems , 34 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Thanks to the Contributors We wish to acknowledge all of the people who contributed to this chapter. The General Principles Committee A lan Winfield (Founding Chair) Professor, Bristol Robotics Laboratory, University of the West of England; Visiting Professor, University of York M ark Halverson (Co-Chair) Founder and CEO at Precision Autonomy P eet van Biljon (Co-Chair) Founder and CEO at BMNP Strategies LLC, advisor on strategy, innovation, and business transformation; Adjunct professor at Georgetown University; Business ethics author S hahar Avin Research Associate, Centre for the Study of Existential Risk, University of Cambridge B ijilash Babu Senior Manager, Ernst and Young, EY Global Delivery Services India LLP R ichard Bartley Senior Director - Analyst, Security & Risk Management, Gartner, Toronto, Canada Security Principal Director, Accenture, Toronto, Canada. R . R. Brooks Professor, Holcombe Department of Electrical and Computer Engineering, Clemson University N icolas Economou Chief Executive Officer, H5; Chair, Science, Law and Society Initiative at The Future Society Chair, Law Committee, Global Governance of AI Roundtable; Member, Council on Extended Intelligence (CXI) H ugo Giordano Engineering Student at Texas A&M University A lexei Grinbaum Researcher at CEA (French Alternative Energies and Atomic Energy Commission) and Member of the French Commission on the Ethics of Digital Sciences and Technologies CERNA J ia He Independent Researcher, Graduate Delft University of Technology in Engineering and Public Policy, project member within United Nations, ICANN, and ITU Executive Director of Toutiao Research (Think Tank), Bytedance Inc. B ruce Hedin Principal Scientist, H5 C yrus Hodes Advisor AI Office, UAE Prime Minister s Office, Co-founder and Senior Advisor, AI Initiatives@The Future Society; Member, AI Expert Group at the OECD, Member, Global Council on Extended Intelligence; Co-founder and Senior Advisor, The AI Initiative @ The Future Society N athan F. Hutchins Applied Assistant Professor, Department of Electrical and Computer Engineering, The University of Tulsa N arayana GPL. Mandaleeka ( MGPL ) Vice President & Chief Scientist, Head, Business Systems & Cybernetics Centre, Tata Consultancy Services Ltd. V idushi Marda Programme Officer, ARTICLE 19 Ge orge T. Matthew Chief Medical Officer, North America, DXC Technology 35 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems General Principles This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. N icolas Miailhe Co-Founder & President, The Future Society; Member, AI Expert Group at the OECD; Member, Global Council on Extended Intelligence; Senior Visiting Research Fellow, Program on Science Technology and Society at Harvard Kennedy School. Lecturer, Paris School of International Affairs (Sciences Po); Visiting Professor, IE School of Global and Public Affairs R upak Rathore Principal Consultant at ATCS for Telematics, Connected Car and Internet of Things; Advisor on strategy, innovation and transformation journey management; Senior Member, IEEE P eter Teneriello Investment Analyst, Private Equity and Venture Capital, TMRS N iels ten Oever Head of Digital, Article 19, Co-chair Research Group on Human Rights Protocol Considerations in the Internet Research Taskforce (IRTF) A lan R. Wagner Assistant Professor, Department of Aerospace Engineering, Research Associate, The Rock Ethics Institute, The Pennsylvania State University. For a full listing of all IEEE Global Initiative Members, visit standards.ieee.org/content/dam/ ieee-standards/standards/web/documents/other/ec_bios.pdf . F or information on disclaimers associated with EAD1e, see How the Document Was Prepared.36 Classical Ethics in A/ISThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.We applied classical ethics methodologies to considerations of algorithmic design in autonomous and intelligent systems (A/IS) where machine learning may or may not reflect ethical outcomes that mimic human decision-making. To meet this goal, we drew from classical ethics theories and the disciplines of machine ethics, information ethics, and technology ethics. As direct control over tools becomes further removed, creators of autonomous systems must ask themselves how cultural and ethical presumptions bias artificially intelligent creations. Such introspection is more necessary than ever because the precise and deliberate design of algorithms in self-sustained digital systems will result in responses based on such design. By drawing from over two thousand years worth of classical ethics traditions, we explore established ethics systems, including both philosophical traditions (utilitarianism, virtue ethics, and deontological ethics) and religious and culture-based ethical systems (Buddhism, Confucianism, African Ubuntu traditions, and Japanese Shinto) and their stance on human morality in the digital age. 1 In doing so, we critique assumptions around concepts such as good and evil, right and wrong, virtue and vice, and we attempt to carry these inquiries into artificial systems decision-making processes. Through reviewing the philosophical foundations that define autonomy and ontology, we address the potential for autonomous capacity of artificially intelligent systems, posing questions of morality in amoral systems and asking whether decisions made by amoral systems can have moral consequences. Ultimately, we address notions of responsibility and accountability for the decisions made by autonomous systems and other artificially intelligent technologies.37 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISSection 1 Definitions for Classical Ethics in Autonomous and Intelligent Systems Research Issue: Assigning Foundations for Morality, Autonomy, and Intelligence Background Classical theories of economy in the Western tradition, starting with Plato and Aristotle, embrace three domains: the individual, the family, and the polis. The formation of the individual character ( ethos ) is intrinsically related to the others, as well as to the tasks of administration of work within the family ( oikos). Eventually, this all expands into the framework of the polis, or public space ( poleis ). When we discuss ethical issues of A/IS, it becomes crucial to consider these three traditional economic dimensions, since western classical ethics was developed from this foundation and has evolved in modernity into an individual morality disconnected from economics and politics. This disconnection has been questioned and explored by thinkers such as Adam Smith, Georg W. F. Hegel, Karl Marx, and others. In particular, Immanuel Kant s ethics located morality within the subject (see: categorical imperative ) and separated morality from the outside world and the consequences of being a part of it. The moral autonomous subject of modernity became thus a worldless isolated subject. This process is important to understand in terms of ethics for A/IS since it is, paradoxically, the kind of autonomy that is supposed to be achieved by intelligent machines as humans evolve into digitally networked beings. There lies a danger in uncritically attributing classical concepts of anthropomorphic autonomy to machines, including using the term artificial intelligence to describe them since, in the attempt to make them moral by programming moral rules into their behavior, we run the risk of assuming economic and political dimensions that do not exist, or that are not in line with contemporary human societies. While the concepts of artificial intelligence and autonomy are mainly used metaphorically as technical terms in computer science and technology, general and popular discourse may not share in the same nuanced understanding, and political and societal discourse may become distorted or 38 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISmisleading. The question of whether A /IS and the terminology used to describe them will have any kind of impact on our conception of autonomy depends on our policy toward it. For example, the commonly held fear that A/IS will relegate humanity to mere spectators or slaves, whether realistic or not, is informed by our view of, and terminology around, A/IS. Such attitudes are flexible and can be negotiated. A s noted above, present human societies are being redefined in terms of digital citizenship via online social networks. The present public debate about the replaceability of human work by intelligent machines is a symptom of this lack of awareness of the economic and political dimensions as defined by classical ethics, reducing ethical thinking to the morality of a worldless and isolated machine. There is still value that can be gained by considering how Western ethical traditions can be integrated into either A/IS public awareness campaigns or supplemented in engineering and science education programs, as noted under the issue Presenting ethics to the creators of A/IS . Below is a short overview of how four different traditions can add value. V irtue ethics: Aristotle argues, using the concept of telos , or goal, that the ultimate goal of humans is eudaimonia , roughly translated as flourishing . A moral agent achieves flourishing since it is an action, not a state by constantly balancing factors including social environment, material provisions, friends, family, and one's own self. One cultivates the self through habituation, practicing and strengthening virtuous action as the golden mean (a principle of rationality). Such cultivation requires an appropriate balance between extremes of excess and deficiency, which Aristotle identifies as vices. In the context of A/IS, virtue ethics has two immediate values. First, it provides a model for iterative learning and growth, and moral value informed by context and practice, not just as compliance with a given, static ruleset. Second, it provides to those who develop and implement A/IS a framework to counterbalance tendencies toward excess, which are common in economically-driven environments. D eontological ethics: As developed by 18th century German philosopher, Immanuel Kant, the basic premise of deontological ethics addresses the concept of duty. Humans have a rational capacity to create and abide by rules that allow for duty-based ethics to emerge. Rules that produce duties are said to have value in themselves without requiring a greater-good justification. Such rules are fundamental to our existence, self-worth, and to creating conditions that allow for peaceful coexistence and interaction, e.g., the duty not to harm others; the duty not to steal. To identify rules that can be universalized and made duties, Kant uses the categorical imperative: Act only on that maxim through which you can at the same time will that it should become a universal law. This means the rule must be inherently desirable, doable, valuable, and others must be able to understand and follow it. Rules based merely on personal choice without wider appeal are not capable of universalization. There is mutual reciprocity in rule-making and rule adherence; if you will that a rule should become universal law, you not only contribute 39 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISto rule creation but also agree to be bound by the same rule. The rule should be action-guiding, i.e., recommending, prescribing, limiting, or proscribing action. Kant also uses the humanity formulation of the categorical imperative: Act in such a way that you always treat humanity, whether in your own person or in the person of any other, never simply as a means, but always at the same time as an end. This produces duties to respect humanity and human dignity, and not to treat either as a means to an end. I n the context of A/IS, one consideration is to wonder if developers are acting with the best interests of humanity and human dignity in mind. This could possibly be extended to A/IS whereby they are assisting humanity as an instrument of action that has an impact on decision-making capabilities, despite being based on neural machine learning or set protocols. The humanity formulation of the categorical imperative has implications for various scenarios. The duty to respect human dignity may require some limitations on the functions and capability of A/IS so that they do not completely replace humans, human functions, and/or human central thinking activities such as judgment, discretion, and reasoning. Privacy and safeguarding issues around A/IS assisting humans, e.g., healthcare robots, may require programming certain values so that A/IS do not divulge personal information to third parties, or compromise a human s physical or mental well-being. It may also involve preventing A/IS from deceiving or manipulating humans. P otential benefits and financial incentives from exploiting A/IS may provide ends-means justifications for their use, while disregarding the treatment of humanity as an end in itself, e.g., cutting back on funding rigorous testing of A/IS before they reach the market and society. Maintaining human agency in human-machine interaction is a manifestation of the duty to respect human dignity. For example, a human has the right to know when they are interacting with A/IS, and may require consent for any A/IS interaction. U tilitarian ethics: Also called consequentialist ethics, this code of ethics refers to the consequences of one s decisions and actions. According to the utility principle, the right course of action is the one that maximizes the utility (utilitarianism) or pleasure (hedonism) for the greatest number of people. This ethics theory does, however, warn against superficial and short-term evaluations of utility or pleasure. Therefore, it is the responsibility of the A/IS developers to consider long-term effects. Social justice is paramount in this instance, thus it must be ascertained if the implementation of A/IS will contribute to humanity, or negatively impact employment or other capabilities. Indeed, where it is deemed A/IS can supplement humanity, it should be designed in such a way that the benefits are obvious to all the stakeholders. E thics of care: Generally viewed as an instance of feminist ethics, this approach emphasizes the importance of relationships which is context-bound. Relationships are ontologically basic to humanity, according to Nel Noddings, feminist and philosopher of education; to care for other human beings is one of our basic human attributes. For such 40 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISa theory to have relevance in this context, one needs to consider two criteria: the relationship with the other person, or entity, must already exist or must have the potential to exist, and the relationship should have the potential to grow into a caring relationship. Applied to A/IS, an interesting question comes to the foreground: Can one care for humans and their interests in tandem with non-human entities? If one expects A/IS to be beneficial to humanity, as in the instance of robots assisting with care of the elderly, then can one deduce the possibility of humans caring for A/IS? If that possibility exists, do principles of social justice become applicable to A/IS? Recommendations By returning to classical ethics foundations, expand the discussion on ethics in A/IS to include a critical assessment of anthropomorphic presumptions of ethics and moral rules for A/IS. Keep in mind that machines do not, in terms of classical autonomy, comprehend the moral or legal rules they follow. They move according to their programming, following rules that are designed by humans to be moral. Expand the discussion on ethics for A/IS to include an exploration of the classical foundations of economy, outlined above, as potentially influencing current views and assumptions around machines achieving isolated autonomy. Further Resources J . Bielby, Ed., Digital Global Citizenship , International Review of Information Ethics, vol. 23, pp. 2-3, Nov. O . Bendel, Towards Machine Ethics, in Technology Assessment and Policy Areas of Great Transitions: Proceedings from the PACITA 2013 Conference in Prague, PACITA 2013, Prague, March 13-15, 2013, T. Michalek, L. Heb kov , L. Hennen, C. Scherz, L. Nierling, J. Hahn, Eds. Prague: Technology Centre ASCR, pp. 321- O . Bendel, Considerations about the Relationship between Animal and Machine Ethics , AI & Society, vol. 31, no. 1, pp. 103-108, Feb. N . Berberich and K. Diepold, "" The Virtuous Machine - Old Ethics for New Technology? "" arXiv:10322 [cs.AI], June R . Capurro, M. Eldred, and D. Nagel, Digital Whoness: Identity, Privacy and Freedom in the Cyberworld. Berlin: Walter de Gruyter, D . Chalmers, The Singularity: A Philosophical Analysis , Journal of Consciousness Studies, vol. 17, pp. 7-65, D . Davidson, Representation and Interpretation, in Modelling the Mind, K. A. M. Said, W. H. Newton-Smith, R. Viale, and K. V. Wilkes, Eds. New York: Oxford University Press, 1990, pp. 13- N . Noddings, Caring: A Relational Approach to Ethics and Moral Education. Oakland, CA: University of California Press, O . Ulgen, Kantian Ethics in the Age of Artificial Intelligence and Robotics, QIL, vol. 43, pp. 59-83, Oct. 41 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/IS O . Ulgen, The Ethical Implications of Developing and Using Artificial Intelligence and Robotics in the Civilian and Military Spheres, House of Lords Select Committee, Sept. 6, 2017, UK. O . Ulgen, Human Dignity in an Age of Autonomous Weapons: Are We in Danger of Losing an Elementary Consideration of Humanity ? in How International Law Works in Times of Crisis, I. Ziemele and G. Ulrich, Eds. Oxford: Oxford University Press, Issue: The Distinction between Agents and Patients Background Of particular concern when understanding the relationship between human beings and A/IS is the uncritically applied anthropomorphic approach toward A/IS that many industry and policymakers are using today. This approach erroneously blurs the distinction between moral agents and moral patients, i.e., subjects, otherwise understood as a distinction between natural self-organizing systems and artificial, non-self-organizing devices. As noted above, A/IS cannot, by definition, become autonomous in the sense that humans or living beings are autonomous. With that said, autonomy in machines, when critically defined, designates how machines act and operate independently in certain contexts through a consideration of implemented order generated by laws and rules. In this sense, A/IS can, by definition, qualify as autonomous, especially in the case of genetic algorithms and evolutionary strategies. However, attempts to implant true morality and emotions, and thus accountability, i.e., autonomy, into A/IS blurs the distinction between agents and patients and may encourage anthropomorphic expectations of machines by human beings when designing and interacting with A/IS. Thus, an adequate assessment of expectations and language used to describe the human-A/IS relationship becomes critical in the early stages of its development, where analyzing subtleties is necessary. Definitions of autonomy need to be clearly drawn, both in terms of A/IS and human autonomy. On one hand, A/IS may in some cases manifest seemingly ethical and moral decisions, resulting for all intents and purposes in efficient and agreeable moral outcomes. Many human traditions, on the other hand, can and have manifested as fundamentalism under the guise of morality. Such is the case with many religious moral foundations, where established cultural mores are neither questioned nor assessed. In such scenarios, one must consider whether there is any functional difference between the level of autonomy in A/IS and that of assumed agency the ability to choose and act in humans via the blind adherence to religious, traditional, or habitual mores. The relationship between assumed moral customs, the ethical critique of those customs, and the law are important distinctions. The above misunderstanding in definitions of autonomy arises in part because of the tendency for humans to shape artificial creations in their own image, and our desire to lend our human 42 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISexperience to shaping a morphology of artificially intelligent systems. This is not to say that such terminology cannot be used metaphorically, but the difference must be maintained, especially as A/IS begin to resemble human beings more closely. It is possible for terms like artificial intelligence or morality of machines to be used as metaphors without resulting in misunderstanding. This is how language works and how humans try to understand their natural and artificial environment. However, the critical difference between human autonomy and autonomous systems involves questions of free will, predetermination, and being (ontology). The questions of critical ontology currently being applied to machines are not new questions to ethical discourse and philosophy; they have been thoroughly applied to the nature of human being a s well. John Stuart Mill, for example, is a determinist and claims that human actions are predicated on predetermined laws. He does, however, argue for a reconciliation of human free will with determinism through a theory of compatibility. Millian ethics provides a detailed and informed foundation for defining autonomy that could serve to help overcome general assumptions of anthropomorphism in A/IS and thereby address the uncertainty therein (Mill, . Recommendations When addressing the nature of autonomy in autonomous systems, it is recommended that the discussion first consider free will, civil liberty, and society from a Millian perspective in order to better grasp definitions of autonomy and to address general assumptions of anthropomorphism in A/IS.Further Resources R . Capurro, Toward a Comparative Theory of Agents . A I & Society, vol. 27, no. 4, pp. 479- 488, Nov. W . J. King and J. Ohya, The Representation of Agents: Anthropomorphism, Agency, and Intelligence, in Conference Companion on Human Factors in Computing Systems. Vancouver: ACM, 1996, pp. 289- W . Hofkirchner, Does Computing Embrace Self-Organisation? in Information and Computation: Essays on Scientific and Philosophical Understanding of Foundations of Information and Computation, G. Dodig- Crnkovic and M. Burgin, Eds. London: World Scientific, 201 1, pp. 185- I nternational Center for Information Ethics, 2018 . J . S. Mill, On Liberty . L ondon: Longman, Roberts & Green, P . P. Verbeek, What Things Do: Philosophical Reflections on Technology, Agency, and Design. University Park, PA: Pennsylvania State University Press, 43 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISIssue: The Need for an Accessible, Classical Ethics Vocabulary Background Philosophers and ethicists are trained in vocabulary relating to philosophical concepts and terminology. There is an intrinsic value placed on these concepts when discussing ethics and A/IS, since the layered meaning behind the terminology used is foundational to these discussions and is grounded in a subsequent entrenchment of values. Unfortunately, using philosophical terminology in cross-disciplinary instances, i.e., a conversation between technologists and policymakers, is often ineffective since not everyone has the education to be able to encompass the abstracted layers of meaning contained in philosophical terminology. However, not understanding a philosophical definition does not detract from the necessity of its utility. While ethical and philosophical theories should not be over-simplified for popular consumption, being able to adequately translate the essence of the rich history of ethics will go a long way in supporting a constructive dialogue on ethics and A/IS. With access and accessibility concerns intricately linked with education in communities, as well as secondary and tertiary institutions, society needs to take a vested interest in creating awareness for government officials, rural communities, and school teachers. Creating a more user-friendly vocabulary raises awareness on the necessity and application of classical ethics to digital societies.Identifying terms that will be intelligible to all relevant audiences is pragmatic, but care should be taken not to dilute or misrepresent concepts that are familiar to moral philosophy and ethics. One way around this is to engage in applied ethics; illustrate how a particular concept would work in the A/IS context or scenario. Another way is to understand whether terminology used across different disciplines actually has the same or similar meaning and effect which can be expressed accordingly. Recommendations Support and encourage the efforts of groups raising awareness for social and ethics committees, whose roles are to support ethics dialogue within their organizations, seeking approaches that are both aspirational and values-based. A/IS technologists should engage in cross-disciplinary exchanges whereby philosophy scholars and ethicists attend and present in non-philosophical courses. This will both raise awareness and sensitize non-philosophical scholars and practitioners to the vocabulary. Further Resources R . T. Ames, Confucian Role Ethics: A Vocabulary. Hong Kong: Chinese University Press, 201 R . Capurro, Towards an Ontological Foundation of Information Ethics , E thics and Information Technology, vol. 8, no. 4, pp. 175- 186, S . Mattingly-Jordan, R. Day, B. Donaldson, P. Gray, and L. M. Ingram, "" Ethically Aligned Design, First Edition Glossary ,"" Prepared for The IEEE Global Initiative for Ethically Aligned Design, Feb. 44 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/IS B . M. Lowe, Emerging Moral Vocabularies: The Creation and Establishment of New Forms of Moral and Ethical Meanings. Lanham, MD: Lexington Books, D . J. Flinders, In Search of Ethical Guidance: Constructing a Basis for Dialogue , I nternational Journal of Qualitative Studies in Education, vol. 5, no. 2, pp. 101-1 15, G . S. Saldanha, The Demon in the Gap of Language: Capurro, Ethics and Language in Divided Germany , in I nformation Cultures in the Digital Age. Wiesbaden, Germany: Springer Fachmedien, 2016, pp. 253- J . Van Den Hoven and G. J. Lokhorst, ""Deontic Logic and Computer Supported Computer Ethics,"" Metaphilosophy, vol. 33, no. 3, pp. 376-386, April Issue: Presenting Ethics to the Creators of Autonomous and Intelligent Systems Background The question arises as to whether or not classical ethics theories can be used to produce meta-level orientations to data collection and data use in decision-making. Keeping in mind that the task of philosophical ethics should be to examine good and evil, ethics should examine values, not prescribe them. Laws, which arise from ethics, are entrenched mores that have been critically assessed to prescribe. T he key is to embed ethics into engineering in a way that does not make ethics a servant, but instead a partner in the process. In addition to an ethics-in-practice approach, providing students and engineers with the tools necessary to build a similar orientation into their inventions further entrenches ethical design practices. In the abstract, this is not so difficult to describe, but is very difficult to encode into systems. This problem can be addressed by providing students with job aids such as checklists, flowcharts, and matrices that will help them select and use a principal ethical framework, and then exercise use of those devices with steadily more complex examples. In such an iterative process, students will start to determine for themselves what examples do not allow for perfectly clear decisions, and, in fact, require some interaction between frameworks. Produced outcomes such as videos, essays, and other formats such as project-based learning activities allow for a didactic strategy which proves effective in artificial intelligence ethics education. The goal is to provide students a means to use ethics in a manner analogous to how they are being taught to use engineering principles and tools. In other words, the goal is to help engineers tell the story of what they are doing. E thicists should use information flows and consider at a meta-level what information flows do and what they are supposed to do. E ngineers should then build a narrative that outlines the iterative process of ethical considerations in their design. Intentions are part of the narrative and provide a base to reflect back on those intentions.45 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/IS T he process then allows engineers to better understand their assumptions and adjust their intentions and design processes accordingly. They can only get to these by asking targeted questions. This process, one with which engineers are quite familiar, is basically Kantian and Millian ethics in play. The aim is to produce what is referred to in the computer programming lexicon as a macro . A macro is code that takes other code as its input(s) and produces unique outputs. This macro is built using the Western ethics tradition of virtue ethics. This further underscores the importance of education and training on ethical considerations relating to A/IS. Such courses should be developed and presented to students of engineering, A/IS, computer science, and other relevant fields. These courses do not add value a p osteriori, but should be embedded from the beginning to allow for absorption of the underlying ethical considerations as well as allowing for critical thinking to come to fruition once the students graduate. There are various approaches that can be considered on a tertiary level: P arallel (information) ethics program that is presented together with the science program during the course of undergraduate and postgraduate study; E mbedded (information) ethics modules within the science program, i.e., one module per semester; S hort (information) ethics courses specifically designed for the science program that can be attended by the current students, alumni, or professionals. These will function as either introductory, refresher, or specialized courses. C ourses can also be blended to include students and/or practitioners from diverse backgrounds rather than the more traditional practice of homogenous groups, such as engineering students, continuing education programs directed at a specific specialization, and the like. Recommendations Find ways to present ethics where the methodologies used are familiar to engineering students. As engineering is taught as a collection of techno-science, logic, and mathematics, embedding ethical sensitivity into these objective and non-objective processes is essential. Curricula development is crucial in each approach. In addition to research articles and best practices, it is recommended that engineers and practitioners come together with social scientists and philosophers to develop case studies, interactive virtual reality gaming, and additional course interventions that are relevant to students. Further Resources T . W. Bynum and S. Rogerson, Computer Ethics and Professional Responsibility . M alden, MA: Wiley-Blackwell, E . G. Seebauer and R. L. Barry, Fundamentals of Ethics for Scientists and Engineers . N ew York: Oxford University Press, 46 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/IS C . Whitbeck, Teaching Ethics to Scientists and Engineers: Moral Agents and Moral Problems , Science and Engineering Ethics, vol. 1, no. 3, pp. 299-308, Sept. B . Zevenbergen, et al. Philosophy Meets Internet Engineering: Ethics in Networked Systems Research , GTC Workshop Outcomes Paper. Oxford: Oxford Internet Institute, University of Oxford, M . Alvarez, Teaching Information Ethics , International Review of Information Ethics, vol. 14, pp. 23-28, Dec. P . P. Verbeek, Moralizing Technology: Understanding and Designing the Morality of Things . Chicago, IL: University of Chicago Press, 201 K . A. Joyce, K. Darfler, D. George, J. Ludwig, and K. Unsworth, Engaging STEM Ethics Education , Engaging Science, Technology, and Society, vol. 4, no. 1-7, Issue: Accessing Classical Ethics by Corporations and Companies Background Many companies, from startups to tech giants, understand that ethical considerations in tech design are increasingly important, but are not sure how to incorporate ethics into their tech design agenda. How can ethical considerations in tech design become an integrated part of the agenda of companies, public projects, and research consortia? Corporate workshops and exercises will need to go beyond opinion-gathering exercises to embed ethical considerations into structures, environments, training, and development. As it stands, classical ethics is not accessible enough to corporate endeavors in ethics, and as such, are not applicable to tech projects. There is often, but not always, a big discrepancy between the output of engineers, lawyers, and philosophers when dealing with computer science issues; there is also a large difference in how various disciplines approach these issues. While this is not true in all cases and there are now several interdisciplinary approaches in robotics and machine ethics as well as a growing number of scientists that hold double and interdisciplinary degrees there remains a vacuum for the wider understanding of classical ethics theories in the interdisciplinary setting. Such an understanding includes that of the philosophical language used in ethics and the translation of that language across disciplines. I f we take, for instance, the terminology and usage of the concept of trust in reference to technology, the term trust has specific philosophical, legal, and engineering connotations. It is not an abstract concept. It is attributable to humans, and relates to claims and actions people make. Machines, robots, and algorithms lack the ability to make claims and so cannot be attributed with trust. They cannot determine whether something is trustworthy or not. Software engineers may refer to trusting the data, but this relates to the data s authenticity and veracity to ensure software performance. In the context of A/IS, trust means functional reliability ; it means there is confidence in the technology s predictability, reliability, and security against hackers or impersonators of authentic users.47 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISRecommendations In order to achieve multicultural, multidisciplinary, and multi-sectoral dialogues between technologists, philosophers, and policymakers, a nuanced understanding in philosophical and technical language, which is critical to digital society from Internet of Things (IoT), privacy, and cybersecurity to issues of Internet governance, must be translated into norms and made available to technicians and policymakers who may not understand the nuances of the terminology in philosophical, legal, and engineering contexts. It is therefore recommended that the translation of the critical-thinking terminology of philosophers, policymakers, and other stakeholders on A/IS be translated into norms accessible to technicians. Further Resources A . Bhimani, Making Corporate Governance Count: The Fusion of Ethics and Economic Rationality , Journal of Management & Governance, vol. 12, no. 2, pp. 135-147, June A . B. Carroll, A History of Corporate Social Responsibility, in The Oxford Handbook of C orporate Social Responsibility, A. Chrisanthi, R. Mansell, D. Quah, and R. Silverstone, Eds. Oxford, U.K.: Oxford University Press, W . Lazonick, Globalization of the ICT Labor Force, in The Oxford Handbook of Information and Communication Technologies , A . Chrisanthi, R. Mansell, D. Quah, and R. Silverstone, Eds. Oxford, U.K.: Oxford University Press, I EEE P7000 , IEEE Standards Project for Model Process for Addressing Ethical Concerns During System Design will provide engineers and technologists with an implementable process aligning innovation management processes, IT system design approaches, and software engineering methods to minimize ethical risk for their organizations, stakeholders and end users. Issue: The Impact of Automated Systems on the Workplace Background The impact of A/IS on the workplace and the changing power relationships between workers and employers requires ethical guidance. Issues of data protection and privacy via big data in combination with the use of autonomous systems by employers are increasing, where decisions made via aggregate algorithms directly impact employment prospects. The uncritical use of A/IS in the workplace, and its impact on employee-employer relations, is of utmost concern due to the high chance of error and biased outcome. The concept of responsible research and innovation (RRI)is a growing area, particularly within the EU. It offers potential solutions to workplace bias and is being adopted by several research funders, such as the Engineering and Physical Sciences Research Council (EPSRC), who include RRI core principles in their mission statement. RRI is an umbrella concept that draws on classical ethics theory to provide tools to address ethical concerns from the outset of a project, from the design stage onwards.48 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISQuoting Rene Von Schomberg, science and technologies studies specialist and philosopher, Responsible Research and Innovation is a transparent, interactive process by which societal actors and innovators become mutually responsive to each other with a view to the (ethical) acceptability, sustainability and societal desirability of the innovation process and its marketable products (in order to allow a proper embedding of scientific and technological advances in our society). 2 When RRI methodologies are used in the ethical considerations of A/IS design, especially in response to the potential bias of A/IS in the workplace, theoretical deficiencies are then often exposed that would not otherwise have been exposed, allowing room for improvement in design at the development stage rather than from a retroactive perspective. RRI in design increases the chances of both relevance and strength in ethically aligned design. This emerging and exciting new concept aims to also push the boundaries to incorporate relevant stakeholders whose influence in responsible research is on a global stage. While this concept initially focuses on the workplace setting, success will only be achieved through the active involvement from private companies of industry, AI Institutes, and those who are at the forefront in A/IS design. Responsible research and innovation will be achieved through careful research and innovation governance that will ensure research purpose, process, and outcomes that are acceptable, sustainable, and even desirable. It will be incumbent on RRI experts to engage at a level where private companies will feel empowered and embrace this concept as both practical to implement and enact. Recommendations It is recommended, through the application of RRI as founded in classical ethics theory, that research in A/IS design utilize available tools and approaches to better understand the design process, addressing ethical concerns from the very beginning of the design stage of the project, thus maintaining a stronger, more efficient methodological accountability throughout. Further Resources M . Burget, E. Bardone, and M. Pedaste, Definitions and Conceptual Dimensions of Responsible Research and Innovation: A Literature Review, Science and Engineering Ethics, vol. 23, no. 1, pp. 1-9, E uropean Commission Communication, Artificial Intelligence for Europe , COM 237, April, R . Von Schomberg, Prospects for Technology Assessment in a Framework of Responsible Research and Innovation, in Technikfolgen Absch tzen Lehren: Bildungspotenziale Transdisziplin rer Methode. Wiesbaden, Germany: Springer VS, 201 1, pp. 39- B . C. Stahl, G. Eden, M. Jirotka, M. Coeckelbergh, From Computer Ethics to Responsible Research and Innovation in ICT: The Transition of Reference Discourses I nforming Ethics-Related Research in Information Systems, Information & Management, vol. 51, no. 6, pp. 810-818, September 49 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/IS B . C. Stahl, M. Obach, E. Yaghmaei, V. Ikonen, K. Chatfield, and A. Brem, The Responsible Research and Innovation (RRI) Maturity Model: Linking Theory and Practice , Sustainability, vol. 9, no. 6, June I EEE P7005 , Standards Project for Transparent Employer Data Governance is designed to provide organizations with a set of clear guidelines and certifications guaranteeing they are storing, protecting, and utilizing employee data in an ethical and transparent way. Section 2 Classical Ethics from Globally Diverse Traditions Issue: The Monopoly on Ethics by Western Ethical Traditions Background As human creators, our most fundamental values are imposed on the systems we design. It becomes incumbent on the global community to recognize which sets of values guide the design, and whether or not A/IS will generate problematic, i.e., discriminatory, consequences without consideration of non-Western values. There is an urgent need to broaden traditional ethics in its contemporary form of responsible innovation (RI) beyond the scope of Western ethical foundations, such as utilitarianism, deontology, and virtue ethics. There is also a need to include other traditions of ethics in RI, such as those inherent to Buddhism, Confucianism, and Ubuntu traditions.However, this venture poses problematic assumptions even before the issue above can be explored. In classifying Western values, we group together thousands of years of independent and disparate ideas originating from the Greco-Roman philosophical tradition with their Christian-infused cultural heritage and then the break from that heritage with the Enlightenment. What is it that one refers to by the term Western ethics ? Does one refer to philosophical ethics (ethics as a scientific discipline) or is the reference to Western morality? The West , however it may be defined, is an individualistic society, arguably more so than much of the rest of the world, and thus, in some aspects, should be even less collectively defined than Eastern ethical traditions. Suggest instead: If one is referring to Western values, one must designate which values and to whom they belong. Additionally, there is a danger in the field of intercultural information ethics, however 50 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISunconsciously or instinctively propagated, to not only group together all Western traditions under a s ingle banner, but to negatively designate any and all Western influence in global exchange to representing an abusive collective of colonial-influenced ideals. Just because there exists a monopoly of influence by one system over another does not mean that said monopoly is devoid of value, even for systems outside itself. In the same way that culturally diverse traditions have much to offer Western tradition(s), so, too, do they have much to gain from them. In order to establish mutually beneficial connections in addressing globally diverse traditions, it is of critical importance to first properly distinguish between subtleties in Western ethics as a discipline and morality as its object or subject matter. It is also important to differentiate between philosophical or scientific ethics and theological ethics. As noted above, the relationship between assumed moral customs, the ethical critique of those customs, and the law is an established methodology in scientific communities. Western and Eastern philosophy are very different, just like Western and Eastern ethics. Western philosophical ethics use scientific methods such as the logical, discursive, and dialectical approach (models of normative ethics) alongside the analytical and hermeneutical approaches. The Western tradition is not about education and teaching of social and moral values, but rather about the application of fundamentals, frameworks, and explanations. However, several contemporary globally relevant community mores are based in traditional and theological moral systems, requiring a conversation around how best to collaborate in the design and programming of ethics in A/IS amidst differing ethical traditions. While experts in Intercultural Information Ethics, such as Pak-Hang Wong, highlight the dangers of the dominance of Western ethics in A/IS design, noting specifically the appropriation of ethics by liberal democratic values to the exclusion of other value systems, it should be noted that those same liberal democratic values are put in place and specifically designed to accommodate such differences. However, while the accommodation of differences are, in theory, accounted for in dominant liberal value systems, the reality of the situation reveals a monopoly of, and a bias toward, established Western ethical value systems, especially when it comes to standardization. As Wong notes: Standardization is an inherently value-laden project, as it designates the normative criteria for inclusion to the global network. Here, one of the major adverse implications of the introduction of value-laden standard(s) of responsible innovation (RI) appears to be the delegitimization of the plausibility of RI based on local values, especially when those values come into conflict with the liberal democratic values, as the local values (or, the RI based on local values) do not enable scientists and technology developers to be recognized as members of the global network of research and innovation (Wong, . It does, however, become necessary for those who do not work within the parameters of accepted value monopolies to find alternative methods of accommodating different value systems. Liberal values arose out of conflicts 51 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISof cultural and subcultural differences and are designed to be accommodating enough to include a rather wide range of differences. RI enables policymakers, scientists, technology developers, and the public to better understand and respond to the social, ethical, and policy challenges raised by new and emerging technologies. Given the historical context from which RI emerges, it should not be surprising that the current discourse on RI is predominantly based on liberal democratic values. Yet, the bias toward liberal democratic values will inevitably limit the discussion of RI, especially in the cases where liberal democratic values are not taken for granted. Against this background, it is important to recognize the problematic consequences of RI solely grounded on, or justified by, liberal democratic values. In addition, many non-Western ethics traditions, including the Buddhist and Ubuntu traditions highlighted below, view relationship as a foundationally important concept to ethical discourse. One of the key parameters of intercultural information ethics and RI research must be to identify main commonalities of relationship approaches from different cultures and how to operationalize them for A/IS to complement classical methodologies of deontological and teleological ethics. Different cultural perceptions of time may influence relationship approaches and impact how A/IS are perceived and integrated, e.g., technology as part of linear progress in the West; inter-generational needs and principles of respect and benevolence in Chinese culture determining current and future use of technology. Recommendations In order to enable a cross-cultural dialogue of ethics in technology, discussions on ethics and A/IS must first return to normative foundations of RI to address the notion of responsible innovation from a range of value systems not predominant in Western classical ethics. Together with acknowledging differences, a special focus on commonalities in the intercultural understanding of the concept of relationship must complement the process. Further Resources J . Bielby, Comparative Philosophies in Intercultural Information Ethics, Confluence: Journal of World Philosophies, vol. 2, W . B. Carlin and K. C. Strong, ""A Critique of Western Philosophical Ethics: Multidisciplinary Alternatives for Framing Ethical Dilemmas,"" Journal of Business Ethics, vol. 14, no. 5, pp. 387-396, May C . Ess, Lost in translation ?: Intercultural dialogues on privacy and information ethics (introduction to special issue on privacy and data privacy protection in Asia) ,"" Ethics and Information Technology, vol. 7, no. 1, pp. 1-6, March S . Hongladarom, Intercultural Information Ethics: A Pragmatic Consideration, in Information Cultures in the Digital Age. Wiesbaden, Germany: Springer Fachmedien, 2016, pp. 191- L . G. Rodr guez and M. . P. lvarez, tica Multicultural y Sociedad en Red. Madrid: Fundaci n Telef nica, 52 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/IS P . H. Wong, What Should We Share?: Understanding the Aim of Intercultural Information Ethics , ACM SIGCAS Computers and Society, vol. 39, no. 3 pp. 50-58, Dec. S . A. Wilson, Conformity, Individuality, and the Nature of Virtue: A Classical Confucian Contribution to Contemporary Ethical Reflection , The Journal of Religious Ethics, vol. 23, no. 2, pp. 263-289, P . H. Wong, Responsible Innovation for Decent Nonliberal Peoples: A Dilemma? Journal of Responsible Innovation, vol. 3, no. 2, pp. 154-168, July R . B. Zeuschner, Classical Ethics, East and West: Ethics from a Comparative Perspective. Boston, MA: McGraw-Hill, S . Mattingly-Jordan, Becoming a Leader in Global Ethics , IEEE, Issue: The Application of Classical Buddhist Ethical Traditions to A/IS Design Background According to Buddhism, the field of ethics is concerned with behaving in such a way that the subject ultimately realizes the goal of liberation. The question, How should I act? is answered straightforwardly; one should act in such a way that one realizes liberation (nirvana) in the future, achieving what in Buddhism is understood as supreme happiness . Thus Buddhist ethics are clearly goal-oriented. In the Buddhist tradition, people attain liberation when they no longer endure any unsatisfactory conditions, when they have attained the state where they are completely free from any passions, including desire, anger, and delusion to name the traditional three, that ensnare one s self against freedom. In order to attain liberation, one engages oneself in mindful behavior (ethics), concentration (meditation), and what is deemed in Buddhism as wisdom , a term that remains ambiguous in Western scientific approaches to ethics. T hus ethics in Buddhism are concerned exclusively with how to attain the goal of liberation, or freedom. In contrast to Western ethics, Buddhist ethics are not concerned with theoretical questions on the source of normativity or what constitutes the good life. What makes an action a good action in Buddhism is always concerned with whether the action leads, eventually, to liberation or not. In Buddhism, there is no questioning why liberation is a good thing. It is simply assumed. Such an assumption places Buddhism, and ethical reflection from a Buddhist perspective, in the camp of mores rather than scientifically led ethical discourse, and it is approached as an ideology or a worldview. While it is critically important to consider, understand, and apply accepted ideologies such as Buddhism in A/IS, it is both necessary to differentiate the methodology from Western ethics, and respectful to Buddhist tradition, not to require that it be considered in a scientific 53 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/IScontext. Such assumptions put it at odds with the Western foundation of ethical reflection on mores. From a Buddhist perspective, one does not ask why supreme happiness is a good thing; one simply accepts it. The relevant question in Buddhism is not about methodological reflection, but about how to attain liberation from the necessity for such reflection. Thus, Buddhist ethics contain potential for conflict with Western ethical value systems which are founded on ideas of questioning moral and epistemological assumptions. Buddhist ethics are different from, for example, utilitarianism, which operates via critical analysis toward providing the best possible situation to the largest number of people, especially as it pertains to the good life. These fundamental differences between the traditions need to be, first and foremost, mutually understood and then addressed in one form or another when designing A/IS that span cultural contexts. The main difference between Buddhist and Western ethics is that Buddhism is based upon a metaphysics of relation. Buddhist ethics emphasizes how action l eads to achieving a g oal, or in the case of Buddhism, the final goal. In other words, an action is considered a good one when it contributes to the realization of the goal. It is relational when the value of an action is relative to whether or not it leads to the goal, the goal being the reduction and eventual cessation of suffering. In Buddhism, the self is constituted through the relationship between the synergy of bodily parts and mental activities. In Buddhist analysis, the self does not actually exist as a self-subsisting entity. Liberation, or nirvana, consists in realizing that what is known to be the self actually consists of nothing more than these connecting episodes and parts. To exemplify the above, one can draw from the concept of privacy as often explored via intercultural information ethics. The Buddhist perspective understands privacy as a protection, not of self-subsisting individuals, because such do not exist ultimately speaking, but of certain values that are found to be necessary for a well-functioning society to prosper in the globalized world. The secular formulation of the supreme happiness mentioned above is that of the reduction of the experience of suffering, or reduction of the metacognitive state of suffering. Such a state is the result of lifelong discipline and meditation aimed at achieving proper relationships with others and with the world. This notion of the reduction of suffering is something that can resonate well with certain Western traditions, such as epicureanism ataraxia, i.e., freedom from fear through reason and discipline, and versions of consequentialist ethics that are more focused on the reduction of harm. It also encompasses the concept of phronesis or practical wisdom from virtue ethics. Relational ethical boundaries promote ethical guidance that focuses on creativity and growth rather than solely on mitigation of consequence and avoidance of error. If the goal of the reduction of suffering can be formulated in a way that is not absolute, but collaboratively defined, this leaves room for many philosophies and related approaches as to how this goal can be accomplished. Intentionally making space for ethical pluralism is one potential antidote to dominance of the conversation by liberal thought, with its legacy of Western colonialism.54 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISRecommendations In considering the nature of interactions between human and autonomous systems, the above notion of proper relationships through Buddhist ethics can provide a useful platform that results in ethical statements formulated in a relational way, instead of an absolutist way. It is recommended as an additional methodology, along with Western-value methodologies, to address human/computer interactions. Further Resources R . Capurro, Intercultural Information Ethics: Foundations and Applications , Journal of Information, Communication & Ethics in Society, vol. 6, no. 2, pp. 1 16-126, C . Ess, Ethical Pluralism and Global Information Ethics , Ethics and Information Technology, vol. 8, no. 4, pp. 215-226, Nov. S . Hongladarom, Intercultural Information Ethics: A Pragmatic Consideration, in Information Cultures in the Digital Age, K. M. Bielby, Ed. Wiesbaden, Germany: Springer Fachmedien Wiesbaden, 2016, pp. 191- S . Hongladarom, J. Britz, Intercultural Information Ethics , International Review of Information Ethics, vol. 13, pp. 2-5, Oct. M . Nakada, Different Discussions on Roboethics and Information Ethics Based on Different Contexts (Ba). Discussions on Robots, Informatics and Life in the Information Era in Japanese Bulletin Board Forums and Mass Media, Proceedings Cultural Attitudes towards Communication and Technology, pp. 300-314, M . Mori, The Buddha in the Robot. Suginami- ku, Japan: Kosei Publishing, Issue: The Application of Ubuntu Ethical Traditions to A/IS Design Background In his article, African Ethics and Journalism Ethics: News and Opinion in Light of Ubuntu , Thaddeus Metz frames the following question: What does a sub-Saharan ethic focused on the good of community, interpreted philosophically as a moral theory, entail for the duties of various agents with respect to the news/opinion media? (Metz, 2015, . In applying that question to A/IS, it reads: If an ethic focused on the good of community, interpreted philosophically as a moral theory, is applied to A/IS, what would the implications be on the duties of various agents? Agents, in this regard, would therefore be the following: M embers of the A/IS research community A /IS programmers/computer scientists A /IS end-users A /IS themselves 55 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISUbuntu is a sub-Saharan philosophical tradition. Its basic tenet is that a person is a person through other persons. It develops further in the notions of caring and sharing as well as identity and belonging, whereby people experience their lives as bound up with their community. A person is defined in relation to the community since the sense of being is intricately linked with belonging. Therefore, community exists through shared experiences and values. It is a commonly held maxim in the Ubuntu tradition that, to be is to belong to a community and participate. As the saying goes, motho ke motho ka batho babang, or, a person is a person because of other people. Very little research, if any at all, has been conducted in light of Ubuntu ethics and A/IS, but its focus will be within the following moral domains: A mong the members of the A/IS research community B etween the A/IS community/programmers/ computer scientists and the end-users B etween the A/IS community/programmers/ computer scientists and A/IS B etween the end-users and A/IS B etween A/IS and A/IS Considering a future where A/IS will become more entrenched in our everyday lives, one must keep in mind that an attitude of sharing one s experiences with others and caring for their well-being will be impacted. Also, by trying to ensure solidarity within one s community, one must identify factors and devices that will form part of their lifeworld. If so, will the presence of A/IS inhibit the process of partaking in a community, or does it create more opportunities for doing so? One cannot classify A/IS as only a negative or disruptive force; it is here to stay and its presence will only increase. Ubuntu ethics must come to grips with, and contribute to, the body of knowledge by establishing a platform for mutual discussion and understanding. Ubuntu, as collective human dignity, may offer a way of understanding the impact of A/IS on humankind, e.g., the need for human moral and legal agency; human life and death decisions to be taken by humans rather than A/IS. Such analysis fleshes out the following suggestive comments of Desmond Tutu, renowned former chair of South Africa s Truth and Reconciliation Commission, when he says of Africans, (We say) a person is a person through other people... I am human because I belong (Tutu, . As Tutu notes, Harmony, friendliness, and community are great goods. Social harmony is for us the summum bonum the greatest good. Anything that subverts or undermines this sought-after good is to be avoided (. In considering the above, it is fair to state that community remains central to Ubuntu. In situating A/IS within this moral domain, they will have to adhere to the principles of community, identity, and solidarity with others. On the other hand, they will also need to be cognizant of, and sensitive toward, the potential for community-based ethics to exclude individuals on the basis that they do not belong or fail to meet communitarian standards. For example, 56 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISwould this mean the excluded individual lacks personhood and as a consequence would not be able to benefit from community-based A/IS initiatives? How would community-based A/IS programming avoid such biases against individuals? While virtue ethics question the goal or purpose of A/IS and deontological ethics question the duties, the fundamental question asked by Ubuntu would be, How does A/IS affect the community in which it is situated? This question links with the initial question concerning the duties of the various moral agents within the specific community. Motivation becomes very important, because if A/IS seek to detract from community, they will be detrimental to the identity of this community when it comes to job losses, poverty, lacks in education, and lacks in skills training. However, should A/IS seek to supplement the community by means of ease of access, support systems, and more, then it cannot be argued that they will be detrimental. In between these two motivators is a safeguarding issue about how to avoid excluding individuals from accessing community-based A/IS initiatives. It therefore becomes imperative that whoever designs the systems must work closely both with ethicists and the target community, audience, or end-user to ascertain whether their needs are identified and met. Recommendations It is recommended that a concerted effort be made toward the study and publication of literature addressing potential relationships between Ubuntu and other instances of African ethical traditions and A/IS value design. A/IS designers and programmers must work closely with the end-users and target communities to ensure their design objectives, products, and services are aligned with the needs of the end-users and target communities. Further Resources D . W. Lutz, African Ubuntu Philosophy and Global Management , J ournal of Business Ethics, vol. 84, pp. 313-328, Oct. T . Metz, African Ethics and Journalism Ethics: News and Opinion in Light of Ubuntu , Journal of Media Ethics: Exploring Questions of Media Morality , v ol. 30 no. 2, pp. 74-90, April T . Metz, "" Ubuntu as a moral theory and human rights in South Africa ,"" African Human Rights Law Journal, vol. 1 1, no. 2, pp. 532-559, R . Nicolson, Persons in Community: African Ethics in a Global Culture. Scottsville, South Africa: University of KwaZulu-Natal Press, A . Shutte, Ubuntu: An Ethic for a New South Africa. Dorpspruit, South Africa: Cluster Publications, D . Tutu, No Future without Forgiveness. London: Rider, O . Ulgen, Human Dignity in an Age of Autonomous Weapons: Are We in Danger of Losing an Elementary Consideration of Humanity ? in How International Law Works in Times of Crisis, I. Ziemele and G. Ulrich, Eds. Oxford: Oxford University Press, 2018, pp. 242-57 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISIssue: The Application of Shinto-Influenced Traditions to A/IS Design Background Alongside the burgeoning African Ubuntu reflections on A/IS, other indigenous techno-ethical reflections boast an extensive engagement. One such tradition is Japanese Shinto indigenous spirituality, or, Kami no michi , often cited as the catalyst for Japanese robot and autonomous systems culture, a culture that naturally stems from the traditional Japanese concept of karakuri ningyo (automata). Popular Japanese artificial intelligence, robot, and video-gaming culture can be directly connected to indigenous Shinto tradition, from the existence of k ami ( spirits) to puppets and automata. The relationship between A/IS and a human being is a personal relationship in Japanese culture and, one could argue, a very natural one. The phenomenon of relationship in Japan between humans and automata stands out as unique to technological relationships in world cultures, since the Shinto tradition is arguably the only animistic and naturalistic tradition that can be directly connected to contemporary digital culture and A/IS. From the Shinto perspective, the existence of A/IS, whether manifested through robots or other technological autonomous systems, is as natural to the world as rivers, forests, and thunderstorms. As noted by Spyros G. Tzafestas, author of Roboethics: A Navigating Overview , Japan s harmonious feeling for intelligent machines and robots, particularly for humanoid ones, (Tzafestas, 2015, colors and influences technological development in Japan, especially robot culture. The word Shinto can be traced to two Japanese concepts: Shin, meaning spirit, and to , the philosophical path. Along with the modern concept of the android, which can be traced back to three sources the first, to its Greek etymology that combines andras ( ), or man, and gynoids/gyni ( ), or woman; the second, via automatons and toys as per U.S. patent developers in the 1800s; and the third to Japan, where both historical and technological foundations for android development have dominated the market since the 1970s Japanese Shinto-influenced technology culture is perhaps the most authentic representation of the human- automaton interface. S hinto tradition is an animistic religious tradition, positing that everything is created with, and maintains, its own spirit ( kami) and is animated by that spirit an idea that goes a long way to defining autonomy in robots from a Japanese viewpoint. This includes, on one hand, everything that Western culture might deem natural, including rivers, trees, and rocks, and on the other hand, everything artificially (read: artfully ) created, including vehicles, homes, and automata (robots). Artifacts are as much a part of nature in Shinto as animals, and they are considered naturally beautiful rather than falsely artificial. A potential conflict between Western and Japanese concepts of nature and artifact arises when the two traditions are compared 58 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISand contrasted, especially in the exploration of artificial intelligence. While in Shinto, the artifact as artificial represents creation and authentic being, with implications for defining autonomy, the same artifact is designated as secondary and often times unnatural, false, and counterfeit in Western ethical philosophical tradition, dating back to Platonic and Christian ideas of separation of form and spirit. In both traditions, culturally presumed biases define our relationships with technology. While disparate in origin and foundation, both Western classical ethics traditions and Shinto ethical influences in modern A/IS have similar goals and outlooks for ethics in A/IS, goals that are centered in relationship . Recommendations Where Japanese culture leads the way in t he synthesis of traditional value systems and technology, we recommend that people involved with efforts in A/IS ethics explore the Shinto paradigm as representative, though not necessarily as directly applicable, to global efforts in understanding and applying traditional and classical ethics methodologies to A/IS. Further Resources R . M. Geraci, ""Spiritual Robots: Religion and Our Scientific View of the Natural World,"" Theology and Science, vol. 4, no. 3, pp. 229-246, D . F. Holland-Minkley, God in the Machine: Perceptions and Portrayals of Mechanical Kami in Japanese Anime . Ph.D. dissertation, University of Pittsburgh, Pittsburgh, PA, C . B. Jensen and A. Blok, Techno-Animism in Japan: Shinto Cosmograms, Actor-Network Theory, and the Enabling Powers of Non-Human Agencies , Theory, Culture & Society, vol. 30, no. 2, pp. 84-1 15, March F . Kaplan, "" Who Is Afraid of the Humanoid? Investigating Cultural Differences in the Acceptance of Robots ,"" International Journal of Humanoid Robotics, vol. 1, no. 3, pp. 465-480, S . G. Tzafestas, Roboethics: A Navigating Overview. Cham, Switzerland: Springer, G . Veruggio and K. Abney, ""22 Roboethics: The Applied Ethics for a New Science,"" in Robot Ethics: The Ethical and Social Implications of Robotics. Cambridge, MA: MIT Press, 201 1, p. 59 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISSection 3 Classical Ethics for a Technical World Issue: Maintaining Human Autonomy Background A/IS present the possibility for a digitally networked intellectual capacity that imitates, matches, and supersedes human intellectual capacity, including, among other things, general skills, discovery, and computing functions. In addition, A/IS can potentially acquire functionality in areas traditionally captured under the rubric of what we deem unique human and social ability. While the larger question of ethics and A/IS looks at the implications of the influence of autonomous systems in these areas, the pertinent issue is the possibility of autonomous systems imitating, influencing, and then determining the norms of human autonomy. This is done through the eventual negation of independent human thinking and decision-making, where algorithms begin to inform through targeted feedback loops what it is we a re an d what it is we should decide. Thus, how can the academic rigor of traditional ethics speak to the question of maintaining human autonomy in light of algorithmic decision-making?How will A/IS influence human autonomy in ways that may or may not be advantageous to the good life, and perhaps even if advantageous may be detrimental at the same time? How do these systems affect human autonomy and decision-making through the use of algorithms when said algorithms tend to inform ( in-form ) via targeted feedback loops? Consider, for example, Google s autocomplete tool, where algorithms attempt to determine one s search parameters via the user s initial keyword input, offering suggestions based on several criteria including search patterns. In this scenario, autocomplete suggestions influence, in real-time, the parameters the user phrases their search by, often reforming the user s perceived notions of what it was they were looking for in the first place, versus what they might have actually originally intended. Targeted algorithms also inform, as per emerging IoT, applications that monitor the user s routines and habits in the analog world. Consider for example that our bioinformation is, or soon will be, available for interpretation by autonomous systems. What happens when autonomous systems can inform the user in ways the user is not even aware of, using one s bioinformation in targeted advertising campaigns that seek to influence the user in real-time feedback loops based on the user s biological reactions such as 60 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISpupil dilation, body temperature, and emotional reaction, whether positive or negative, to that very same advertising, using information about our being to in-form and re-form our being? On the other hand, it becomes important not to adopt dystopian assumptions concerning autonomous machines threatening human autonomy. T he tendency to think only in negative terms presupposes a case for interactions between autonomous machines and human beings, a presumption not necessarily based in evidence. Ultimately, the behavior of algorithms rests solely in their design, and that design rests solely in the hands of those who designed them. Perhaps more importantly, however, is the matter of choice in terms of how the user chooses to interact with the algorithm. Users often don t know when an algorithm is interacting with them directly or their data which acts as a proxy for their identity. Should there be a precedent for the A/IS user to know when they are interacting with an algorithm? What about consent? T he responsibility for the behavior of algorithms remains with the designer, the user, and a set of well-designed guidelines that guarantee the importance of human autonomy in any interaction. As machine functions become more autonomous and begin to operate in a wider range of situations, any notion of those machines working for or against human beings becomes contested. Does the machine work for s omeone in particular, or for particular groups but not others? Who decides on the parameters? Is it the machine itself? Such questions become key factors in conversations around ethical standards. Recommendations A two-step process is recommended to maintain human autonomy in A/IS. The creation of an ethics-by-design methodology is the first step to addressing human autonomy in A/IS, where a critically applied ethical design of autonomous systems preemptively considers how and where autonomous systems may or may not dissolve human autonomy. The second step is the creation of a pointed and widely applied education curriculum that spans grade school through university, one based on a classical ethics foundation that focuses on providing choice and accountability toward digital being as a priority in information and knowledge societies. Further Resources B . van den Berg and J. de Mul, Remote Control. Human Autonomy in the Age of Computer-Mediated Agency, in Law, Human Agency and Autonomic Computing: The Philosophy of Law Meets the Philosophy of Technology, M. Hildebrandt and A. Rouvroy, Eds. London: Routledge, 201 1, pp. 46- L . Costa, A World of Ambient Intelligence , in Virtuality and Capabilities in a World of Ambient Intelligence. Cham, Switzerland: Springer International, 2016, pp. 15- P . P. Verbeek, Subject to Technology on Autonomic Computing and Human Autonomy, in The Philosophy of Law Meets the Philosophy of Technology: Autonomic Computing and Transformations of Human Agency, M. Hildebrandt and A. Rouvroy, Eds. New York: Routledge, 201 61 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/IS D . Reisman, J. Schultz, K. Crawford, and M. Whittaker, Algorithmic Impact Assessments: A practical Framework for Public Agency Accountability , AI NOW, April A . Chaudhuri, "" Philosophical Dimensions of Information and Ethics in the Internet of Things (IoT) Technology ,"" EDPACS, vol. 56, no. 4, pp. 7-18, Nov. Issue: Implications of Cultural Migration in A/IS Background In addition to developing an understanding of A/IS via different cultures, it is crucial to understand how A/IS are shaped and reshaped how they affect and are affected by human mobility and cultural diversity through active immigration. The effect of human mobility on state systems reliant on A/IS impacts the State structure itself, and thus the systems that the structure relies on, in the end influencing everything from democracy to citizenship. Where the State, through A/IS, invests in and gathers big data through mechanisms for registration and identification of people, mainly immigrants, human mobility becomes a foundational component in a system geared toward the preservation of human dignity. T raditional national concerns reflect two information foundations: information produced for human rights and information produced for national sovereignty. In the second foundation, State borders are considered the limits from which political governance is defined in terms of security. The preservation of national sovereignty depends on the production and domination of knowledge. In the realm of migratory policies, knowledge is created to measure people in transit: collecting, treating, and transferring information about territory and society. K nowledge organization has been the paramount pillar of scientific thought and scientific practice since the beginning of written civilization. Any scientific and technological development has only been possible through information policies that include the establishment of management processes to systematize them, and the codification of language. For the Greeks, this process was closely associated with the concept of a rete, or the excellence of one s self in politics as congregated in the polis. The notion of p olis i s as relevant as ever in the digital age with the development of digital technologies and the discussions around morality in A/IS. Where the systematization of knowledge is potentially freely created, the advent of the Internet and its flows are difficult to control. Ethical issues about the production of information are becoming paramount to our digital society. T he advancement of the fields of science and technology has not been followed by innovations in the political community, and the technical community has repeatedly tabled academic discussions about the hegemony of technocracy over policy issues, restricting the space of the policy arena and valorizing excessively technic solutions for human problems. This monopoly alters conceptions of morality, relocating the locus of the Kantian Categorical Imperative , causing the tension among different social and political contexts to become more pervasive.62 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISCurrent global migration dynamics have been met by unfavorable public opinion based in ideas of crisis and emergency, a response vastly disproportionate to what statistics have shown to be the reality. In response to these views, A/IS are currently designed and applied to measure, calculate, identify, register, systematize, normalize, and frame both human rights and security policies. This is largely no different of a process than what has been practiced since the period of colonialism. It includes the creation and implementation of a set of ancient and new technologies. Throughout history, mechanisms have been created firstly to identify and select individuals who share certain biological heritage, and secondly to individuals and social groups, including biological characteristics. Information is only possible when materialized as an infrastructure supported by ideas in action as a communicative act , which Habermas ( identifies in Hegel s work, converging three elements in human-in-the-world relationships: symbol, language, and labor. Information policies reveal the importance and the strength in which technologies influence economic, social, cultural, identity, and ethnic interactions. T raditional mechanisms used to control migration, such as the passport, are associated with globally established walls and fences. The more intense human mobility becomes, the more amplified are the discourses to discourage it, restricting human migrations, and deepening the need for an ethics related to conditions of citizenship. Together with the building of walls, other remote technologies are developed to monitor and surveil borders, buildings, and streets, also impacting ideas and moral presumptions of citizenship. Closed Circuit Television(CCTV), Unmanned Aerial Vehicles (UAVs), and satellites allow data transference in real time to databases, cementing the backbone that A/IS draws from, often with bias as per the expectations of developed countries. This centrality of data sources for A/IS expresses a divide between developed and underdeveloped countries, particularly as relevant to the refugee. Information is something that links languages, habits, customs, identification, and registration technologies. It provokes a reshaping of the immigrants and refugees citizenship and their value as people in terms of their citizenship, as they seek forms of surviving in, and against, the restrictions imposed by A/IS for surveillance and monitoring in an enlarged and more complex cosmopolis. An understanding of the impact of A/IS on migration and mobile populations, as used in state systems, is a critical first step to consider if systems are to become truly autonomous and intelligent, especially beyond the guidance of human deliberation. Digital technology systems used to register and identify human mobility, including refugees and other displaced populations, are not autonomous in the intelligent sense, and are dependent on the biases of worldviews around immigration. In this aspect, language is the locus where this dichotomy has to be considered to understand the diversity of morals when there are contacts among different cultures. 63 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISRecommendations Is it recommended that the State become a proactive player in the globalized processes of A/IS for migrant and mobile populations, introducing a series of mechanisms that limit the segregation of social spaces and groups, and consider the biases inherent in surveillance for control. Further Resources I . About and V. Denis, Histoire de l identification des personnes. Paris: La D couverte, I . About, J. Brown, G. Lonergan, Identification and Registration Practices in Transnational Perspective: People, Papers and Practices. London: Palgrave Macmillan, 2013, pp. 1- D . Bigo, Security and Immigration: Toward a Critique of the Governmentality of Unease, in Alternatives, Special Issue, no. pp. 63-92, R . Capurro, Citizenship in the Digital Age , in Information Ethics, Globalization and Citizenship, T. Samek and L. Schultz, Eds. Jefferson NC: McFarland, 2017, pp. 1 1- R . Capurro, Intercultural Information Ethics , in Localizing the Internet: Ethical Aspects in Intercultural Perspective, R. Capurro, J. Fr hbauer, and T. Hausmanninger, Eds. Munich: Fink, 2007, pp. 21- U N High Commissioner for Refugees (UNHCR), Policy on the Protection of Personal Data of Persons of Concern to UNHCR , May Issue: Applying Goal-Directed Behavior (Virtue Ethics) to Autonomous and Intelligent Systems Background Initial concerns regarding A/IS also include questions of function, purpose, identity, and agency, a continuum of goal-directed behavior with function being the most primitive expression. How can classical ethics act as a regulating force in autonomous technologies as goal-directed behavior transitions from being externally set by operators to being internally set? The question is important not just for safety reasons, but for mutual productivity. If autonomous systems are to be our trusted, creative partners, then we need to be confident that we possess mutual anticipation of goal-directed action in a wide variety of circumstances. A virtue ethics approach has merits for accomplishing this even without having to posit a character in an autonomous technology, since it places emphasis on habitual, iterative action focused on achieving excellence in a chosen domain or in accord with a guiding purpose. At points on the goal-directed continuum associated with greater sophistication, virtue ethics become even more useful by providing a framework for prudent decision-making that is in keeping with the autonomous system s purpose, but allows for creativity in how to achieve the purpose in a way that still allows for a degree of predictability. An ethics approach that does not rely on a decision 64 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISto refrain from transgressing, but instead to prudently pursue a sense of purpose informed by one s identity, might provide a greater degree of insight into the behavior of the system. Recommendations Program autonomous systems to be able to recognize user behavior for the purposes of predictability, traceability, and accountability and to hold expectations, as an operator and co-collaborator, whereby both user and system mutually recognize the decisions of the autonomous system as virtue ethics-based. Further Resources M . A. Boden, Ed. The Philosophy of Artificial Life. Oxford, U.K.: Oxford University Press, C . Castelfranchi, ""Modelling Social Action for AI Agents,"" Artificial Intelligence, vol. 103, no.1-2, pp. 157-182, W . D. Christensen and C. A. Hooker, ""Anticipation in Autonomous Systems: Foundations for a Theory of Embodied Agents,"" International Journal of Computing Anticipatory Systems, vol. 5, pp. 135-154, Dec. K . G. Coleman, Android Arete: Toward a Virtue Ethic for Computational Agents, Ethics and Information Technology, vol. 3, no. 4, pp. 247-265, J . G. Lennox, Aristotle on the Biological Roots of Virtue, Biology and the Foundations of Ethics , J. Maienschein and M. Ruse, Eds. Cambridge, U.K.: Cambridge University Press, 1999, pp. 405- L . Muehlhauser and L. Helm, ""The Singularity and Machine Ethics,"" in Singularity Hypotheses, A. H. Eden, J. H. Moor, J. H. Soraker, and E. Steinhart, Eds. Berlin: Springer, 2012, pp. 101- D . Vernon, G. Metta, and G. Sandini, "" A Survey of Artificial Cognitive Systems: Implications for the Autonomous Development of Mental Capabilities in Computational Agents ,"" IEEE Transactions on Evolutionary Computation, vol. 1 1, no. 2, pp. 151-180, April Issue: A Requirement for Rule-Based Ethics in Practical Programming Background Research in machine ethics focuses on simple moral machines. It is deontological ethics and t eleological ethics that are best suited to the kind of practical programming needed for such machines, as these ethical systems are abstractable enough to encompass ideas of non-human agency, whereas most modern ethics approaches are far too human-centered to properly accommodate the task. In the deontological model, duty is the point of departure. Duty can be translated into rules. It can be distinguished into rules and metarules. For example, a rule might take the form Don t lie! , whereas a metarule would take the form of Kant s categorical imperative: Act only according to that maxim whereby you can, at the same time, will that it should become a universal law. 65 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISA machine can follow simple rules. Rule-based systems can be implemented as formal systems, also referred to as axiomatic systems , and in the case of machine ethics, a set of rules is used to determine which actions are morally allowable and which are not. Since it is not possible to cover every situation by a rule, an inference engine is used to deduce new rules from a small set of simple rules called axioms by combining them. The morality of a machine comprises the set of rules that is deducible from the axioms. Formal systems have an advantage since properties such as decidability and consistency of a system can be effectively examined. If a formal system is decidable, every rule is either morally allowable or not, and the unknown is eliminated. If the formal system is consistent, one can be sure that no two rules can be deduced that contradict each other. In other words, the machine never has moral doubt about an action and never encounters a deadlock. The disadvantage of using formal systems is that many of them work only in closed worlds like computer games. In this case, what is not known is assumed to be false. This is in drastic conflict with real world situations, where rules can conflict and it is impossible to take into account the totality of the environment. In other words, consistent and decidable formal systems that rely on a closed world assumption can be used to implement an ideal moral framework for a machine, yet they are not viable for real world tasks. One approach to avoiding a closed world scenario is to utilize self-learning algorithms, such as case-based reasoning approaches. Here, the machine uses experience in the form of similar cases that it has encountered in the past or uses cases which are collected in databases. In the context of the teleological model, the consequences of an action are assessed. The machine must know the consequences of an action and what the action s consequences mean for humans, for animals, for things in the environment, and, finally, for the machine itself. It also must be able to assess whether these consequences are good or bad, or if they are acceptable or not, and this assessment is not absolute. While a decision may be good for one person, it may be bad for another; while it may be good for a group of people or for all of humanity, it may be bad for a minority of people. An implementation approach that allows for the consideration of potentially contradictory subjective interests may be realized by decentralized reasoning approaches such as agent-based systems. In contrast to this, centralized approaches may be used to assess the overall consequences for all involved parties. Recommendations By applying the classical methodologies of deontological and teleological ethics to machine learning, rules-based programming in A/IS can be supplemented with established praxis, providing both theory and a practicality toward consistent and determinable formal systems. 66 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/ISFurther Resources C . Allen, I. Smit, and W. Wallach, ""Artificial Morality: Top-Down, Bottom-Up, and Hybrid Approaches,"" Ethics and Information Technology, vol. 7, no. 3, pp. 149-155, O . Bendel, Die Moral in der Maschine: Beitr ge zu Roboter-und Maschinenethik . H eise Medien, O . Bendel, Oliver, Handbuch Maschinenethik. Wiesbaden, Germany: Springer VS, M . Fisher, L. Dennis, and M. Webster, Verifying Autonomous Systems , Communications of the ACM, vol. 56, no. 9, pp. 84-93, Sept. B . M. McLaren, Computational Models of Ethical Reasoning: Challenges, Initial Steps, and Future Directions , IEEE Intelligent Systems, vol. 21, no. 4, pp. 29-37, July M . A. Perez Alvarez, Tecnolog as de la Mente y Exocerebro o las Mediaciones del Aprendizaje , E . L. Rissland and D. B. Skalak, ""Combining Case-Based and Rule-Based Reasoning: A Heuristic Approach."" Proceedings of the 1 1th International Joint Conference on Artificial Intelligence, IJCAI 1989, Detroit, MI, August 20-25, 1989, San Francisco, CA: Morgan Kaufmann Publishers Inc., pp. 524-Thanks to the Contributors We wish to acknowledge all of the people who contributed to this chapter. The Classical Ethics in A/IS Committee J ared Bielby (Chair) President, Netizen Consulting Ltd; Chair, International Center for Information Ethics; editor, Information Cultures in the Digital Age S oraj Hongladarom (Co-chair) President at The Philosophy and Religion Society of Thailand M iguel . P rez lvarez Professor of Technology in Education, Colegio de Pedagog a, Facultad de Filosof a y Letras, Universidad Nacional Aut noma de M xico O liver Bendel Professor of Information Systems, Information Ethics and Machine Ethics, University of Applied Sciences and Arts Northwestern Switzerland FHNW D r. John T. F. Burgess Assistant Professor / Coordinator for Distance Education, School of Library and Information Studies, The University of Alabama R afael Capurro Founder, International Center for Information Ethics C orinne Cath-Speth PhD student at Oxford Internet Institute, The University of Oxford, Doctoral student at the Alan Turing Institute, Digital Consultant at ARTICLE 19 D r. Paola Di Maio Center for Technology Ethics, ISTCS.org UK and NCKU Taiwan R obert Donaldson Independent Computer Scientist, BMRILLC, Hershey, PA67 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Classical Ethics in A/IS R achel Fischer Research Officer: African Centre of Excellence for Information Ethics, Information Science Department, University of Pretoria, South Africa. D r. D. Michael Franklin Assistant Professor, Kennesaw State University, Marietta Campus, Marietta, GA W olfgang Hofkirchner Associate Professor, Institute for Design and Technology Assessment, Vienna University of Technology D r. Tae Wan Kim Associate Professor of Business Ethics, Tepper School of Business Carnegie Mellon University K ai Kimppa University Research Fellow, Information Systems, Turku School of Economics, University of Turku S ara R. Mattingly-Jordan Assistant Professor Center for Public Administration & Policy, Virginia Tech D r Neil McBride Reader in IT Management, School of Computer Science and Informatics, Centre for Computing and Social Responsibility, De Montfort University B runo Macedo Nathansohn Perspectivas Filos ficas em Informa o (Perfil-i); Brazilian Institute of Information in Science and Technology (IBICT) M arie-Therese Png PhD Student, Oxford Internet Institute, PhD Intern, DeepMind Ethics & Society D erek Poitras Independent Consultant, Object Oriented Software Development S amuel T. Segun PhD Candidate, Department of Philosophy, University of Johannesburg. Fellow, Philosophy Node of the Centre for Artificial Intelligence Research (CAIR) at the University of Pretoria and Research fellow at the Conversational School of Philosophy (CSP) D r. Ozlem Ulgen Reader in International Law and Ethics, School of Law, Birmingham City University K ristene Unsworth Assistant Professor, The College of Computing & Informatics, Drexel University D r. Xiaowei Wang Associate professor of Philosophy, Renmin University of China D r Sara Wilford Senior Lecturer, Research Fellow, School of Computer Science and Informatics, Centre for Computing and Social Responsibility, De Montfort University P ak-Hang Wong Research Associate, Department of Informatics, University of Hamburg B endert Zevenbergen Oxford Internet Institute, University of Oxford & Center for Information Technology Policy, Princeton University For a full listing of all IEEE Global Initiative Members, visit standards.ieee.org/content/dam/ ieee-standards/standards/web/documents/other/ec_bios.pdf . F or information on disclaimers associated with EAD1e, see How the Document Was Prepared. 1 This edition of Classical Ethics in A/IS does not (and could not) aspire to universal coverage of all of the world s traditions in the space available to us. Future editions will touch on several other traditions, including Judaism and Islam. 2 R. Von Schomberg, Prospects for Technology Assessment in a Framework of Responsible Research and Innovation in Technikfolgen Absch tzen Lehren: Bildungspotenziale Transdisziplin rer Methode. Wiesbaden, Germany: Springer VS, 2011, pp. 39-Endnotes68 Well-beingThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Prioritizing ethical and responsible artificial intelligence has become a widespread goal for society. Important issues of transparency, accountability, algorithmic bias, and value systems are being directly addressed in the design and implementation of autonomous and intelligent systems (A/IS). While this is an encouraging trend, a key question still facing technologists, manufacturers, and policymakers alike is how to assess, understand, measure, monitor, safeguard, and improve the well-being impacts of A/IS on humans. Finding the answer to this question is further complicated when A/IS are within a holistic and interconnected framework of well-being in which individual well-being is inseparable from societal, economic, and environmental systems. For A/IS to demonstrably advance well-being, we need consistent and multidimensional indicators that are easily implementable by the developers, engineers, and designers who are building our future. This chapter is intended for such developers, engineers, and designers referred to in this chapter as A/IS creators . Those affected by A/IS are referred to as A/IS stakeholders . A/IS technologies affect human agency, identity, emotion, and ecological systems in new and profound ways. Traditional metrics of success are not equipped to ensure A/IS creators can avoid unintended consequences or benefit from unexpected innovation in the algorithmic age. A/IS creators need expanded ways to evaluate the impact of their products, services, or systems on human well-being. These evaluations must also be done with an understanding that human well-being is deeply linked to the well-being of society, economies, and ecosystems. Today, A/IS creators largely measure success using metrics including profit, gross domestic product (GDP), consumption levels, and occupational safety. While important, these metrics fail to encompass the full spectrum of well-being impacts on individuals and society, such as psychological, social, and environmental factors. Where the priority given to these factors is not equal to that given to fiscal metrics of success, A/IS creators risk causing or contributing to negative and irreversible harms to our people and our planet. When A/IS creators are not aware that well-being indicators, in addition to traditional metrics, can provide guidance for their work, they are also missing out on innovation that can increase well-being and societal value. For instance, while it is commonly recognized that autonomous vehicles will save lives when safely deployed, a topic of less frequent discussion is how self-69 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.driving cars also have the potential to help the environment by reducing greenhouse gas emissions and increasing green space . Autonomous vehicles can also positively impact well- being by increasing work-life balance and enhancing the quality of time spent during commutes. Unless A/IS creators are made aware of the existence of alternative measures of progress, the value they provide, and the way they can be incorporated into A/IS work, technology and society will continue to rely upon traditional metrics of success. In an era where innovation is defined by holistic prosperity, alternative measures are needed more now than ever before. The 2009 Report by the Commission on the Measurement of Economic Performance and Social Progress which contributed substantially to the worldwide movement of governments using wider measures of well-being, states, What we measure affects what we do; and if our measurements are flawed, decisions may be distorted. We believe that A/IS creators can profoundly increase human and environmental flourishing by prioritizing well-being metrics as an outcome in all A/IS system designs now and for the future. The primary intended audience for this chapter is A/IS creators who are unfamiliar with the term well-being as it is used in the field of positive psychology and well-being studies. Our initial goal is to provide a broad introduction to qualitative and quantitative metrics and applications of well-being to educate and inspire A/IS creators. We do not prioritize or advocate for any specific indicator or methodology. For further elaboration on the definition of well-being, please see the first Issue listed in Section This chapter is divided into two main sections: T he Value of Well-being Metrics for A/IS Creators I mplementing Well-being Metrics for A/IS Creators The following resources are available online to provide readers with an introduction to existing well-being metrics and tools currently in use: T he State of Well-being Metrics T he Happiness Screening Tool for Business Product Decisions A dditional Resources: Standards Development Models and Frameworks70 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Section 1 The Value of Well-being Metrics for A/IS Creators Well-being metrics provide a broader perspective for A/IS creators than they normally might be familiar with in evaluating their products. This broader perspective unlocks greater opportunities to assure a positive impact of A/IS on human well-being, while minimizing the risk of unintended negative outcomes. This section defines well-being, discusses the value of well-being metrics to A/IS creators, and notes how similar frameworks like sustainability and human rights can be complemented by incorporating well-being metrics. Definition of Well-being For the purposes of Ethically Aligned Design , the term well-being refers to an evaluation of the general quality of life of an individual and the state of external circumstances. The conception of well-being encompasses the full spectrum of personal, social, and environmental factors that enhance human life and on which human life depend. The concept of well-being shall be considered distinct from moral or legal evaluation. Issue: There is ample and robust science behind well- being metrics and their use by international and national institutions. However, A/IS creators are often unaware that well-being metrics exist, or that they can be used to plan, develop, and evaluate technology. Background The concept of well-being refers to an evaluation of the general goodness of the state of an individual or community and is distinct from moral or legal evaluation. A well-being evaluation takes into account major aspects of a person s life, such as their happiness, success in their goals, and their overall positive functioning in their environment. There is now a thriving area of scientific research into the psychological, social, behavioral, economic, and environmental determinants of human well-being. 71 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.The term well-being is defined and used in various ways across different contexts and fields. For example: economists identifying economic welfare with levels of consumption and economic vitality, psychologists highlighting subjective experience, and sociologists emphasizing living, labor, political, social, and environmental conditions. We do not take a stand on any specific measure of well-being. The metrics listed below are an incomplete list and provided as a starting point for further inquiry. Among these are subjective well-being indicators, measures of quality of life, social progress and capabilities, and many more. There is now sufficient consensus among scientists that well-being can be reliably measured. Well-being measures differ in the number and the intricacy of indicators they employ. Short questionnaires of life satisfaction have emerged as particularly popular, although they do not reflect all aspects of well-being. While recognizing a scope for differences across well-being indicators, we note that the richest conception of well-being encompasses the full spectrum of personal, social, and environmental goods that enhance human life. We encourage A/IS creators to consider the wide range of available indicators and select those most relevant and revealing for particular stages of the A/IS technology s life cycle and the particular context for the technology s use and evaluation. That is, measures of well-being that may be well-suited to wealthy, industrialized nations may be less applicable in low- and middle-income countries, and vice versa. Among the most important and recognized aspects of well-being are (in alphabetical order): C ommunity: Belonging, Crime & Safety, Discrimination & Inclusion, Participation, Social Support C ulture: Identity, Values E conomy: Economic Policy, Equality & Environment, Innovation, Jobs, Sustainable Natural Resources & Consumption & Production, Standard of Living E ducation: Formal Education, Lifelong Learning, Teacher Training E nvironment: Air, Biodiversity, Climate Change, Soil, Water G overnment: Confidence, Engagement, Human Rights, Institutions Hu man Settlements: Energy, Food, Housing, Information & Communication Technology, Transportation P hysical Health: Health Status, Risk Factors, Service Coverage P sychological Health: Affect (feelings), Flourishing, Mental Illness & Health, Satisfaction with Life W ork: Governance, Time Balance, Workplace Environment72 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.In an effort to provide a basic orientation to well-being metrics, information about well-being indicators can be segmented into four categories: S ubjective or survey-based indicators Survey-based well-being indicators, subjective well-being (SWB) indicators, and multidimensional measurements of aspects of well-being, are being used by national institutions, international institutions, and governments to better understand levels of psychological well-being within countries and aspects of a country s population. These indicators are also being used to understand people s satisfaction in specific domains of life. Examples of surveys that include survey-based well-being indicators and SWB indicators include the European Social Survey , Bhutan s Gross National Happiness Indicators , well-being surveys created by The UK Office for National Statistics , and many more. Survey-based metrics are also employed in the field of positive psychology and in the World Happiness Report . The data are employed by researchers to understand the causes, consequences, and correlates of well-being. Data gathered from surveys tend to address concerns, such as day-to-day experience, overall satisfaction with life, and perceived flourishing. The findings of these researchers provide crucial and necessary guidance because they often diverge from and complement the understanding of traditional conditions, such as economic growth. O bjective indicators Objective indicators of quality of life have typically incorporated areas such as income, consumption, health, education, crime, housing, etc. These indicators have been used to understand conditions that support the well-being of countries and populations, and to measure the societal and environmental impact of companies. They are in use by organizations like the OECD with their Better Life Index , which also includes survey- based well-being indicators and SWB indicators, and the United Nations with their Sustainable Development Goals Indicators (formerly the Millennium Development Goals). For business, the Global Reporting Initiative , SDG Compass , and B-Corp provide broad indicator sets. C omposite indicators (indices that aggregate multiple metrics) Aggregate metrics combine subjective and/or objective metrics to produce one measure reflecting both objective aspects of quality of life and people s subjective evaluation of these. Examples of this are the UN s Human Development Index , the Social Progress Index , and the United Kingdom s Office of National Statistics Measures of National Well-being . Some subjective and objective indicators are also composite indicators, such as Bhutan s Gross National Happiness Index and the OECD s Better Life Index. S ocial media sourced data Social media can be used to measure the well- being of a geographic region or demographic group, based on sentiment analysis of publicly available data. Examples include the Hedonometer and the World Well-being Project . 73 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Recommendation A/IS creators should prioritize learning about well-being concepts, scientific learnings, research findings, and well-being metrics as potential determinants for how they create, deploy, market, and monitor their technologies, and ensuring their stakeholders learn the same. This process can be expedited if Standards Development Organizations (SDOs), such as the IEEE Standards Association, or other institutions such as the Global Reporting Initiative (GRI) or B-Corp, create certifications, guidelines, and standards that for the use of holistic, well-being metrics for A/IS in the public and private sectors. Further Resources T he IEEE P7010 Standards Project for Well- being Metric for Autonomous/Intelligent Systems , was formed with the aim of identifying well-being metrics for applicability to A/IS today and in the future. All are welcome to join the working group. O n 1 1 April 2017, IEEE hosted a dinner debate at the European Parliament in Brussels to discuss how the world s top metric of value, gross domestic product, must move Beyond GDP to holistically measure how intelligent and autonomous systems can hinder or improve human well-being. P rioritizing Human Well-being in the Age of Artificial Intelligence (Report) P rioritizing Human Well-being in the Age of Artificial Intelligence (Video) Issue: Increased awareness and application of well-being metrics by A/IS creators can create greater value, safety, and relevance to corporate communities and other organizations in the algorithmic age. Background While many organizations in the private and public sectors are increasingly aware of the need to incorporate well-being measures as part of their efforts, the reality is that bottom line, quarterly-driven shareholder growth remains a dominant goal and metric. Short term growth is often the priority in the private sector and public sector. As long as organizations exist in a larger societal system which prioritizes financial success, these companies will remain under pressure to deliver financial results that do not fully incorporate societal and environmental impacts, measurements, or priorities. Rather than focus solely on the negative aspects of how A/IS could harm humans and environments, we seek to explore how the implementation of well-being metrics can help A/IS to have a measurable, positive impact on human well-being as well as on systems and organizations. Incorporation of well-being goals and measures beyond what is strictly required can benefit both private sector organizations brands and public sector organizations stability and reputation, as well as help realize financial 74 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.savings, innovation, trust, and many other benefits. For instance, a companion robot outfitted to support seniors in assisted living situations might traditionally be launched with a technology development model that was popularized by Silicon Valley known as move fast and break things . The A/IS creator who rushed to bring the robot to market faster than the competition and who was unaware of well-being metrics, may have overlooked critical needs of the seniors. The robot might actually hurt the senior instead of helping by exacerbating isolation or feelings of loneliness and helplessness. While this is a hypothetical scenario, it is intended to demonstrate the value of linking A/IS design to well-being indicators. By prioritizing largely fiscal metrics of success, A/IS devices might fail in the market because of limited adoption and subpar reception. However, if during use of the A/IS product, success were measured in terms of relevant aspects of well-being, developers and researchers could be in a better position to attain funding and public support. Depending on the intended use of the A/IS product, well-being measures that could be used extend to emotional levels of calm or stress; psychological states of thriving or depression; behavioral patterns of engagement in community or isolation; eating, exercise and consumption habits; and many other aspects of human well-being. The A/IS product could significantly improve quality of life guided by metrics from trusted sources, such as the World Health Organization , European Social Survey , and Sustainable Development Goal Indicators . Thought leaders in the corporate arena have recognized the multifaceted need to utilize metrics beyond fiscal indicators. PricewaterhouseCoopers defines total impact as a holistic view of social, environmental, fiscal and economic dimensions the big picture . Other thought-leading organizations in the public sector, such as the OECD, demonstrate the desire for business leaders to incorporate metrics of success beyond fiscal indicators for their efforts, exemplified in their 2017 workshop, Measuring Business Impacts on People s Well-Being . The B-Corporation movement has created a new legal status for a new type of company that uses the power of business to solve social and environmental problems . Focusing on increasing stakeholder value versus shareholder returns alone, B-Corps are defining their brands by provably aligning their efforts with wider measures of well-being. Recommendations A/IS creators should work to better understand and apply well-being metrics in the algorithmic age. Specifically: A /IS creators should work directly with experts, researchers, and practitioners in well- being concepts and metrics to identify existing metrics and combinations of indicators that would bring support a triple bottom line , i.e., accounting for economic, social, and environmental impacts, approach to well-being. However, well-being metrics should only be used with consent, respect for privacy, and with strict standards for collection and use of these data. F or A/IS to promote human well-being, the well-being metrics should be chosen in collaboration with the populations most affected by those systems the A/IS 75 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.stakeholders including both the intended end-users or beneficiaries and those groups whose lives might be unintentionally transformed by them. This selection process should be iterative and through a learning and continually improving process. In addition, metrics of well-being should be treated as vehicles for learning and potential mid- course corrections. The effects of A/IS on human well-being should be monitored continuously throughout their life cycles, by A/IS creators and stakeholders, and both A/IS creators and stakeholders should be prepared to significantly modify, or even roll back, technology that is shown to reduce well-being, as defined by affected populations. A /IS creators in the business or academic, engineering, or policy arenas are advised to review the additional resources on standards development models and frameworks at the end of this chapter to familiarize themselves with existing indicators relevant to their work. Further Resources P ricewaterhouseCoopers (PwC). Managing and Measuring Total Impact: A New Language for Business Decisions , W orld Economic Forum. The Inclusive Growth and Development Report 2017, Geneva, Switzerland: World Economic Forum, January 16, O ECD Guidelines on Measuring Subjective Well-being , Na tional Research Council. Subjective Well- Being: Measuring Happiness, Suffering, and Other Dimensions of Experience. DC: The National Academies Press, Issue: A/IS creators have opportunities to safeguard human well-being by ensuring that A/IS does no harm to earth s natural systems or that A/IS contributes to realizing sustainable stewardship, preservation, and/or restoration of earth s natural systems. A/IS creators have opportunities to prevent A/IS from contributing to the degradation of earth s natural systems and hence losses to human well-being. Background It is unwise, and in truth impossible, to separate the well-being of the natural environment of the planet from the well-being of humanity. A range of studies, from the historic to more recent , prove that ecological collapse endangers human existence. Hence, the concept of well-being should encompass planetary well-being. Moreover, biodiversity and ecological integrity have intrinsic merit beyond simply their instrumental value to humans. Technology has a long history of contributing to ecological degradation through its role in expanding the scale of resource extraction and environmental pollution, for example, the immense power needs of network computing, which leads to climate change , water scarcity , soil degradation , species extinction , deforestation , 76 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.biodiversity loss , and destruction of ecosystems which in turn threatens humankind in the long run. These and other costs are often considered externalities and often do not figure into decisions or plans. At the same time, there are many examples, such as photovoltaics and smart grid technology that present potential ways to restore earth s ecosystems if undertaken within a systems approach aimed at sustainable economic and environmental development. Environmental justice research demonstrates that the negative environmental impacts of technology are commonly concentrated on the middle class and working poor, as well as those suffering from abject poverty, fleeing disaster zones, or otherwise lacking the resources to meet their needs. Ecological impact can thus exacerbate the economic and sociological effects of wealth disparities on human well-being by concentrating environmental injustice onto those who are less well off. Moreover, well-being research findings indicate that unfair economic and social inequality has a dampening effect on everyone's well-being, regardless of economic or social class. In these respects, A/IS are no exception; they can be used in ways that either help or harm the ecological integrity of the planet. It may be fair to say that ecological health and human well-being will, increasingly, depend upon A/IS creators. It is imperative that A/IS creators and stakeholders find ways to use A/IS to do no harm and to reduce the environmental degradation associated with economic growth while simultaneously identifying applications to restore the ecological health of the planet and thereby safeguarding the well-being of humans. For A/IS to reduce environmental degradation and promote well-being, it is required that not only A/IS creators act along such lines, but also that a systems approach is taken by all A/IS stakeholders to find solutions that safeguard human well-being with the understanding that human well-being is inextricable from healthy social, economic, and environmental systems. Recommendations A/IS creators need to recognize and prioritize the stewardship of the Earth s natural systems to promote human and ecological well-being. Specifically: Hu man well-being should be defined to encompass ecological health, access to nature, safe climate and natural environments, biosystem diversity, and other aspects of a healthy, sustainable natural environment. A /IS systems should be designed to use, support, and strengthen existing ecological sustainability standards with a certification or similar system, e.g., LEED , Energy Star , or Forest Stewardship Council . This directs automation and machine intelligence to follow the principle of doing no harm and to safeguard environmental, social, and economic systems. A /IS creators should prioritize doing no harm to the Earth s natural systems, both intended and unintended harm. A c ommittee should be convened to issue findings on ways in which A/IS can be used by business, NGOs, and governmental agencies to promote stewardship and restoration of natural systems while reducing the harmful impact of economic development on ecological sustainability and environmental justice.77 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Further Resources D . Austin and M. Macauley. "" Cutting Through Environmental Issues: Technology as a double-edged sword . The Brookings Institution, Dec. 2001 [Online]. Available: . [Accessed Dec. 1, 2018]. J. Newton, Well-being and the Natural Environment: An Overview of the Evidence . August 20, P . Dasgupta, Human Well-Being and the Natural Environment . Oxford, U.K.: Oxford University Press, R . Haines-Young and M. Potschin. The Links Between Biodiversity, Ecosystem Services and Human Well-Being , in Ecosystem Ecology: A New Synthesis , D. Raffaelli, and C. Frid, Eds. Cambridge, U.K.: Cambridge University Press, S . Hart, Capitalism at the Crossroads: Next Generation Business Strategies for a Post-Crisis World . Upper Saddle River, NJ: Pearson Education, U nited Nations Department of Economic and Social Affairs. Call for New Technologies to Avoid Ecological Destruction . Geneva, Switzerland, July 5, 201 P ope Francis. Encyclical Letter Laudato Si of the Holy Father Francis On the Care for Our Common Home . May 24, Env ironment , The 14th Dalai Lama. Accessed Dec. 9, . W hy Islam.org, Environment and Islam, Issue: Human rights law is related to, but distinct from, the pursuit of well-being. Incorporating a human-rights framework as an essential basis for A/IS creators means A/IS creators honor existing law as part of their well-being analysis and implementation. Background International human rights law has been firmly established for decades in order to protect various guarantees and freedoms as enshrined in charters such as the United Nations Universal Declaration of Human Rights and the Council of Europe s Convention on Human Rights . In 2018, the Toronto Declaration on machine learning standards was released, calling on both governments and technology companies to ensure that algorithms respect basic principles of equality and non-discrimination. The Toronto Declaration sets forth an obligation to prevent machine learning systems from discriminating, and in some cases violating, existing human rights law. Well-being initiatives are typically undertaken for the sake of public interest. However, any metric, including well-being metrics, can be misused to justify human rights violations. Encampment and mistreatment of refugees and ethnic cleansing undertaken to preserve a nation s culture (an aspect of well-being) is one example. Imprisonment or assassination of journalists or researchers to ensure the stability 78 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.of a government is another. The use of well- being metrics to justify human rights violations is an unconscionable perversion of the nature of any well-being metric. It should be noted that these same practices happen today in relation to GDP. For instance, in 2012, according to the International Labour Organization (ILO), approximately 21 million people are victims of forced labor (slavery), representing 9% to 56% of GDP income for various countries. These clear human rights violations, from sex trafficking and use of children in armies, to indentured farming or manufacturing labor, can increase a country s GDP while obviously harming human well-being. Well-being metrics are designed to measure the efficacy of efforts related to individual and societal flourishing. Well-being as a value complements justice, equality, and freedom. Well-designed application of well-being considerations by A/IS creators should not displace other issues of human rights or ethical methodologies, but rather complement them. Recommendation A human rights framework should represent the floor, and not the ceiling, for the standards to which A/IS creators must adhere. Developers and users of well-being metrics should be aware these metrics will not always adequately address human rights. Further Resources Uni ted Nations Universal Declaration of Human Rights , C ouncil of Europe s Convention on Human Rights , I nternational Labor Organization (ILO) Declaration on Fundamental Principles and Rights at Work , T he regularly updated University of Minnesota Human Rights Library provides a wealth of material on human rights laws, its history, and the organizations engaged in promoting them. T he Oxford Human Rights Hub reports on how and why technologies surrounding artificial intelligence raise human rights issues. 79 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Section 2 Implementing Well-being Metrics for A/IS Creators A key challenge for A/IS creators in realizing the benefits of well-being metrics is how to best incorporate them into their work. This section explores current best thinking on how to make this happen. Issue: How can A/IS creators incorporate well-being into their work? Background Without practical ways of incorporating well-being metrics to guide, measure, and monitor impact, A/IS will likely lack fall short of its potential to avoid harm and promote well-being. Incorporating well-being thinking into typical organizational processes of design, prototyping, marketing, etc., suggests a variety of adaptations. Organizations and A/IS creators should consider clearly defining the type of A/IS product or service that they are developing, including articulating its intended stakeholders and uses. By defining typical uses, possible uses, and finally unacceptable uses of the technology, creators will help to spell out the context of well-being. This can help to identify possible harms and risks given the different possible uses and end users, as well as intended and unintended positive consequences.Additionally, internal and external stakeholders should be extensively consulted to ensure that impacts are thoroughly considered through an iterative and learning stakeholder engagement process. After consultation, A/IS creators should select appropriate well-being indicators based on the possible scope and impact of their A/IS product or service. These well-being indicators can be drawn from mainstream sources and models and adapted as necessary. They can be used to engage in pre-assessment of the intended user population, projection of possible impacts, and post-assessment. Development of a well-being indicator measurement plan and relevant data infrastructure will support a robust integration of well-being. A/IS models can also be trained to explicitly include well-being indicators as subgoals. Data and discussions on well-being impacts can be used to suggest improvements and modifications to existing A/IS products and services throughout their lifecycle. For example, a team seeking to increase the well-being of people using wheelchairs found that when provided the opportunity to use a smart wheelchair, some users were delighted with the opportunity for more mobility, while others felt it would decrease their opportunities for social contact, increase their sense of isolation, and lead to an overall decrease in their well-being. Therefore, even though a product modification may increase well-being according to one indicator or set of 80 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.A/IS stakeholders, it does not mean that this modification should automatically be adopted. Finally, organizational processes can be modified to incorporate the above strategies. Appointment of an organizational lead person for well-being impacts, e.g., a well-being lead, ombudsman, or officer can help to facilitate this effort. Recommendation A/IS creators should adjust their existing development, marketing, and assessment cycles to incorporate well-being concerns throughout their processes. This includes identification of an A/IS lead ombudsperson or officer; identification of stakeholders and end users; determination of possible uses, harm and risk assessment; robust stakeholder engagement; selection of well-being indicators; development of a well-being indicator measurement plan; and ongoing improvement of A/IS products and services throughout the lifecycle. Further Resources P eter Senge and the Learning Organization - (synopsis) Purdue University S takeholder Engagement: A Good Practice Handbook for Companies Doing Business in Emerging Markets. International Finance Corporation, May Gl obal Reporting Initiative G NH Certification , Centre for Bhutan and GNH Studies, J . Helliwell, R. Layard, and J. Sachs, Eds., The Objective Benefits of Subjective Well-Being, in World Happiness Report New York: UN Sustainable Development Solutions Network, pp. 54-79, G lobal Happiness and Well-being Policy Report by the Global Happiness Council, Issue: How can A/IS creators influence A/IS goals to ensure well-being, and what can A/IS creators learn or borrow from existing models in the well-being and other arenas? Background Another way to incorporate considerations of well-being is to include well-being measures in the development, goal setting, and training of the A/IS systems themselves. Identified metrics of well-being could be formulated as auxiliary objectives of the A/IS. As these auxiliary well-being objectives will be only a subset of the intended goals of the system, the architecture will need to balance multiple objectives. Each of these auxiliary objectives may be expressed as a goal, set of rules, set of values, or as a set of preferences, which can be weighted and combined using established methodologies from intelligent systems engineering. 81 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.For example, an educational A/IS tool could not only optimize learning outcomes, but also incorporate measures of student social and emotional education, learning, and thriving. A/IS-related data relates both to the individual through personalized algorithms, in conjunction with affective sensors measuring and influencing emotion, and other aspects of individual well-being and to society as large data sets representing aggregate individual subjective and objective data. As the exchange of this data becomes more widely available via establishing tracking methodologies, the data can be aligned within A/IS products and services to increase human well-being. For example, robots like Pepper are equipped to share data regarding their usage and interaction with humans to the cloud. This allows almost instantaneous innovation, as once an action is validated as useful for one Pepper robot, all other Pepper units (and ostensibly their owners) benefit as well. As long as this data exchange happens with the predetermined consent of the robots owners, this innovation in real time model can be emulated for the large-scale aggregation of information relating to existing well-being metrics. A/IS creators can also help to operationalize well-being metrics by providing stakeholders with reports on the expected or actual outcomes of the A/IS and the values and objectives embedded in the systems. This transparency will help creators, users, and third parties assess the state of well-being produced by A/IS and make improvements in A/IS. In addition, A/IS creators should consider allowing end users to layer on their own preferences, such as allowing users to limit their use of an A/IS product if it leads to increased sustained stress levels, sustained isolation, development of unhealthy habits, or other decreases to well-being. Incorporating well-being goals and metrics into broader organizational values and processes would support the use of well-being metrics as there would be institutional support. A key factor in industrial, corporate, and societal progress is cross-dissemination of concepts and models from one industry or field to another. To date, a number of successful concepts and models exist in the fields of sustainability, economics, industrial design and manufacturing, architecture and urban development, and governmental policy. These concepts and models can provide a foundation for building a metrics standard and the use of well-being metrics by A/IS creators, from conception and design to marketing, product updates, and improvements to the user experience. Recommendation Create technical standards for representing goals, metrics, and evaluation guidelines for well-being metrics and their precursors and components within A/IS that include: O ntologies for representing technological requirements. A t esting framework for validating adherence to well-being metrics and ethical principles such as IEEE P7010 Standards Project for Well- being Metric for Autonomous and Intelligent Systems.82 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. T he exploration of models and concepts listed above as well as others as a basis for a well- being metrics standard for A/IS creators. (See page 191, Additional Resources: Additional Resources: Standards Development Models and Frameworks ) T he development of a well-being metrics standard for A/IS that encompasses an understanding of well-being as holistic and interlinked to social, economic, and ecological systems. Further Resources A .F.T Winfield, C. Blum, and W. Liu. Towards an Ethical Robot: Internal Models, Consequences and Ethical Action Selection , in Advances in Autonomous Robotics Systems. Springer, 2014, pp. 85 96 R . A. Calvo, and D. Peters. Positive Computing: Technology for Well-Being and Human Potential. Cambridge MA: MIT Press, Y . Collette, and P. Slarry. Multiobjective Optimization: Principles and Case Studies (Decision Engineering Series). Berlin, Germany: Springer, doi: 1007/978-3-662-08883- J . Greene, et al. Embedding Ethical Principles in Collective Decision Support Systems, in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence , 4147 Palo Alto, CA: AAAI Press, L . Li, I. Yevseyeva, V. Basto-Fernandes, H. Trautmann, N. Jing, and M. Emmerich, Building and Using an Ontology of Preference-Based Multiobjective Evolutionary Algorithms . In 9th International Conference on Evolutionary Multi-Criterion Optimization Volume 10173 (EMO , H. Trautmann, G. Rudolph, K. Klamroth, O. Sch tze, M. Wiecek, Y. Jin, and C. Grimme, Eds., Vol. Springer-Verlag, Berlin, Heidelberg, 406-421, P ositiveSocialImpact: Empowering people, organizations and planet with information and knowledge to make a positive impact to sustainable development, D .K. Ura, Bhutan s Gross National Happiness Policy Screening Tool. Issue: Decision processes for determining relevant well-being indicators through stakeholder deliberations need to be established. Background A/IS stakeholder involvement is necessary to determine relevant well-being indicators, for a number of reasons: Well-being will be defined differently by different groups affected by A/IS. The most relevant indicators of well-being may vary according to country, with concerns of wealthy nations being different than those of low- and middle-income countries. Indicators may vary based on geographical region or unique circumstances. The indicators may also be different across social groups, including gender, race, ethnicity, and disability status. C ommon indicators of well-being include satisfaction with life, healthy life expectancy, 83 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.economic standard of living, trust in government, social support, perceived freedom to make life decisions, income equality, access to education, and poverty rates. Applying them in particular settings necessarily requires judgment, to ensure that assessments of well-being are in fact meaningful in context and reflective of the life circumstances of the diverse groups in question. N ot all aspects of well-being are easily quantifiable. The importance of hard-to-quantify aspects of well-being is most likely to become apparent through interaction with those more directly affected by A/IS in specific settings. E ngineers and corporate employees frequently misunderstand stakeholders needs and expectations, especially when the stakeholders are very different from them in terms of educational and cultural background, social location, and/or economic status. The processes through which stakeholders become involved in determining relevant well-being indicators will affect the quality of the indicators selected and assessed. Stakeholders should be empowered to define well-being, assess the appropriateness of existing indicators and propose new ones, and highlight context-specific factors that bear on issues of well-being, whether or not the issues have been recognized previously or are amenable to measurement. Interactive, open-ended discussions or deliberations among a wide variety of stakeholders and system designers are more likely to yield robust, widely-shared understandings of well-being and how to measure it in context. Closed-ended or over-determined methods for soliciting stakeholder input are likely to miss relevant information that system designers have not anticipated.A process of stakeholder engagement and deliberation is one model for collective decision-making. Parties in such deliberation come together as equals. Their goal is to set aside their immediate, personal interests in order to think together about the common good. Participants in a stakeholder engagement and deliberation learn from one another s perspectives and experiences. In the real world, stakeholder engagement and deliberation may run into the following challenges: I ndividuals with more education, power, or higher social status may intentionally or unintentionally dominate the discussion, undermining their ability to learn from less powerful participants. T opics may be preemptively ruled out of bounds , to the detriment of collective problem-solving. An example would be if, in a deliberation on well-being and A/IS, participants were told that worries about the costs of health insurance were unrelated to A/IS and thus could not be discussed. E ngineers and scientists may claim authority over technical issues and be willing to deliberate only on social issues, obscuring the ways that technical and social issues are intertwined. L ess powerful groups may be unable to keep more powerful ones at the table when discussions get contentious, and vice versa. P articipants may not agree on who can legitimately be involved in the conversation. For example, the consensual spirit of deliberation is often used as a justification for excluding activists and others who already hold a position on the issue.84 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Stakeholder engagement and deliberative processes can be effective when: T heir design is guided by experts or practitioners who are experienced in deliberation models. D eliberations are facilitated by individuals sensitive to issues of power and are skilled in mediating deliberation sessions. L ess powerful actors participate with the help of allies who can amplify their voices. M ore powerful actors participate with an awareness of their own power and make a commitment to listen with humility, curiosity, and open-mindedness. D eliberations are convened by institutions or individuals who are trusted and respected by all parties and who hold all actors accountable for participating constructively. Ethically aligned design of A/IS would be furthered by thoughtfully constructed, context-specific deliberations on well-being and the best indicators for assessing it. Recommendation Appoint a lead team or person, leads , to facilitate stakeholder engagement and to serve as a resource for A/IS creators who use stakeholder-based processes to establish well-being indicators. Specifically: L eads should solicit and collect lessons learned from specific applications of stakeholder engagement and deliberation in order to continually refine its guidance. W hen determining well-being indicators, the leads should enlist the help of experts in public participation and deliberation. With expert guidance, facilitators can provide guidance for how to: take steps to mitigate the effects of unequal power in deliberative processes; incorporate appropriately trained facilitators and coaching participants in deliberations; recognize and curb disproportionate influence by more-powerful groups; use techniques to maximize the voices of less-powerful groups. L eads should use their convening power to bring together A/IS creators and stakeholders, including critics of A/IS, for deliberations on well-being indicators, impacts, and other considerations for specific contexts and settings. Leads involvement would help bring actors to the table with a balance of power and encourage all actors to remain in conversation until robust, mutually agreeable definitions are found. Further Resources D . E. Booher and J. E. Innes. Planning with Complexity: An Introduction to Collaborative Rationality for Public Policy. London: Routledge, J . A. Leydens and J. C. Lucena. Engineering Justice: Transforming Engineering Education and Practice . Wiley-IEEE Press, G . Ottinger. Assessing Community Advisory Panels: A Case Study from Louisiana s Industrial Corridor. Center for Contemporary History and Policy, E xpert and Citizen Assessment of Science and Technology (ECAST) Network 85 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Issue: There are insufficient mechanisms to foresee and measure negative impacts, and to promote and safeguard positive impacts of A/IS. Background A/IS technologies present great opportunity for positive change in every aspect of society. However, they can by design or unintentionally cause harm as well. While it is important to consider and make sense of possible benefits, harms, and trade-offs, it is extremely challenging to foresee all of the relevant, direct, and secondary impacts. However, it is prudent to review case studies of similar products and the impacts they have had on well-being, as well as to consider possible types of impacts that could apply. Issues to consider include: E conomic and labor impacts, including labor displacement, unemployment, and inequality, A ccountability, transparency, and explainability, S urveillance, privacy, and civil liberties, F airness, ethics, and human rights, P olitical manipulation, deception, nudging , and propaganda, Hu man physical and psychological health, E nvironmental impacts, Hu man dignity, autonomy, and human vs. A/IS roles, S ecurity, cybersecurity, and autonomous weapons, and E xistential risk and super intelligence.While this is a partial list, it is important to be aware of and reflect on possible and actual cases. For example: A pr ominent concern related to A/IS is of labor displacement and economic and social impacts at an individual and a systems level. A/IS technologies designed to replicate human tasks, behavior, or emotion have the potential to increase or decrease human well-being. These systems could complement human work and increase productivity, wages, and leisure time; or they could be used to supplement and displace human workers, leading to unemployment, inequality, and social strife. It is important for A/IS creators to think about possible uses of their technology and whether they want to encourage or design in restrictions in light of these impacts. A nother example relates to manipulation. Sophisticated manipulative technologies utilizing A/IS can restrict the fundamental freedom of human choice by manipulating humans who consume content without them recognizing the extent of the manipulation. Software platforms are moving from targeting and customizing content to much more powerful and potentially harmful persuasive computing that leverages psychological data and methods. While these approaches may be effective in encouraging use of a product, they may come at significant psychological and social costs. A /IS may deceive and harm humans by posing as humans. With the increased ability of artificial systems to meet the Turing test, an intelligence test for a computer that allows a human to distinguish human intelligence from artificial intelligence, there is a significant risk 86 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.that unscrupulous operators will abuse the technology for unethical commercial or outright criminal purposes. Without taking action to prevent it, it is highly conceivable that A/IS will be used to deceive humans by pretending to be another human being in a plethora of situations and via multiple mediums. A potential entry point for exploring these unintended consequences is computational sustainability. Computational-Sustainability.org defines the term as an interdisciplinary field that aims to apply techniques from computer science, information science, operations research, applied mathematics, and statistics for balancing environmental, economic, and societal needs for sustainable development . The Institute of Computational Sustainability states that the intent of computational sustainability is provide computational models for a sustainable environment, economy, and society . Examples of applied computational sustainability can be seen in the Stanford University Engineering Department s course in computational sustainability presentation . Computational sustainability technologies designed to increase social good could also be tied to existing well-being metrics. Recommendation T o avoid potential negative, unintended consequences, and secure and safeguard positive impacts, A/IS creators, end-users, and stakeholders should be aware of possible well-being impacts when designing, using, and monitoring A/IS systems. This includes being aware of existing cases and possible areas of impact, measuring impacts on well-being outcomes, and developing regulations to promote beneficent uses of A/IS. Specifically: A /IS creators should protect human dignity, autonomy, rights, and well-being of those directly and indirectly affected by the technology. As part of this effort, it is important to include multiple stakeholders, minorities, marginalized groups, and those often without power or a voice in consultation. P olicymakers, regulators, monitors, and researchers should consider issuing guidance on areas such as A/IS labor and the proper role of humans vs. A/IS in work transparency, trust, and explainability; manipulation and deception; and other areas that emerge. O ngoing literature review and analysis should be performed by research and other communities to curate and aggregate information on positive and negative A/IS impacts, along with demonstrated approaches to realize positive ones and ameliorate negative ones. A /IS creators working toward computational sustainability should integrate well-being concepts, scientific findings, and indicators into current computational sustainability models. They should work with well-being experts, researchers, and practitioners to conduct research and develop and apply models in A/IS development that prioritize and increase human well-being.87 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. C ross-pollination should be developed between computational sustainability and well-being professionals to ensure integration of well-being into computational sustainability frameworks, and vice versa. Where feasible and reasonable, do the same for conceptual models such as doughnut economics and systems thinking. Further Resources A I Safety Research by The Future of Life Institute D . Helbing, et al. Will Democracy Survive Big Data and Artificial Intelligence? Scientific American , February 25, J . L. Schenker, Can We Balance Human Ethics with Artificial Intelligence? Techonomy, January 23, 2017 . M . Bulman, EU to Vote on Declaring Robots To Be Electronic Persons . Independent, January 14, N . Nevejan, for the European Parliament. European Civil Law Rules in Robotics . October U niversity of Oxford. Social media manipulation rising globally, new report warns, social-media-globally.html . July 20, T he AI That Pretends To Be Human , LessWrong blog post, February 2, C . Chan, Monkeys Grieve When Their Robot Friend Dies . Gizmodo , January 1 1, P artnership on AI, AI, Labor, and the Economy Working Group launches in New York City, . April 25, C .Y. Johnson, Children can be swayed by robot peer pressure,study says , The Washington Post, August 15, [Online]. Available: . [Accessed 2018]. Further Resources for Computational Sustainability S tanford Engineering Department, Topics in Computational Sustainability Course Presentation , C omputational Sustainability, Computational Sustainability: Computational Methods for a Sustainable Environment, Economy, and Society Project Summary. C . P. Gomes, Computational Sustainability: Computational Methods for a Sustainable Environment, Economy, and Society in The Bridge: Linking Engineering and Society . Washington, DC: National Academy of Engineering of the National Academies, S .J. Gershman, E. J. Horvitz, and J. B. Tenenbaum. Computational rationality: A converging paradigm for intelligence in brains, minds, and machines , Science vol. 349, no. 6245, pp. 273 278, July A CM Fairness, Accountability and Transparency Conference 88 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Thanks to the Contributors We wish to acknowledge all of the people who contributed to this chapter. The Well-being Committee J ohn C. Havens (Co-Chair) Executive Director, The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems; Executive Director, The Council on Extended Intelligence; Author, Heartificial Intelligence: Embracing Our Humanity to Maximize Machines L aura Musikanski (Co-Chair) Executive Director at The Happiness Alliance home of The Happiness Initiative & Gross National Happiness Index L iz Alexander PhD Futurist A nna Alexandrova Senior Lecturer in Philosophy of Science at Cambridge University and Fellow of Kings College C hristina Berkley Executive Coach to leaders in exponential technologies, cutting-edge science, and aerospace C atalina Butnaru UK AI Ambassador for global community City.AI, and Founder of HAI, the first methodology for applications of AI in cognitive businesses C elina Beatriz Project Director at the Institute for Technology & Society of Rio de Janeiro (ITS Rio) P eet van Biljon Founder and CEO at BMNP Strategies LLC, advisor on strategy, innovation, and business transformation; Adjunct faculty at Georgetown University; Business ethics author A my Blankson Author of The Future of Happiness and Founder of TechWell, a research and consulting firm that aims to help organizations to create more positive digital cultures M arc B hlen Professor, University at Buffalo, Emerging Practices in Computational Media. R afael A. Calvo Professor and ARC Future Fellow at The University of Sydney. Co-author of Positive Computing: Technology for Well-Being and Human Potential R umman Chowdhury PhD Senior Principal, Artificial Intelligence, and Strategic Growth Initiative Responsible AI Lead, Accenture D r. Aymee Coget CEO and Founder of Happiness For HumanKind Da nny W. Devriendt Managing director of Mediabrands Dynamic (IPG) in Brussels, and the CEO of the Eye of Horus, a global think-tank for communication-technology related topics E imear Farrell Eimear Farrell, independent expert/consultant on technology and human rights (formerly at OHCHR) D anit Gal Project Assistant Professor, Keio University; Chair, IEEE Standard P7009 on the Fail-Safe Design of Autonomous and Semi-Autonomous Systems89 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. M arek Havrda PhD Strategy Advisor, GoodAI A ndra Keay Managing Director of Silicon Valley Robotics, cofounder of Robohub D r. Peggy Kern Senior Lecturer, Centre for Positive Psychology at the University of Melbourne's Graduate School of Education M ichael Lennon Senior Fellow, Center for Excellence in Public Leadership, George Washington University; Co-Founder, Govpreneur.org; Principal, CAIPP.org (Consortium for Action Intelligence and Positive Performance); Member, Well-being Metrics Standard for Ethical Artificial Intelligence and Autonomous Systems Committee A lan Mackworth Professor of Computer Science, University of British Columbia; Former President, AAAI; Co-author of Artificial Intelligence: Foundations of Computational Agents R ichard Mallah Director of AI Project, Future of Life Institute Fa brice Murtin Senior Economist, OECD Statistics and Data Directorate G wen Ottinger Associate Professor, Center for Science, Technology, and Society and Department of Politics, Drexel University; Director , Fair Tech Collective E leonore Pauwels Research Fellow on AI and Emerging Cybertechnologies, United Nations University (NY) and Director of the AI Lab, Woodrow Wilson International Center for Scholars (DC) V enerable Tenzin Priyadarshi MIT Media Lab, Director, Ethics Initiative G ideon Rosenblatt Writer, focused on work and the human experience in an era of machine intelligence, at The Vital Edge D aniel Schiff PhD Student, Georgia Institute of Technology; Chair, Sub-Group for Autonomous and Intelligent Systems Implementation, IEEE P7010 Standards Project for Well-being Metric for Autonomous and Intelligent Systems M adalena Sula Undergraduate student of Electrical and Computer Engineering, University of Thessaly, Greece, x-PR Manager of IEEE Student Branch of University of Thessaly, Data Scientist & Business Analyst in a multinational company V incent Siegerink Analyst, OECD Statistics and Data Directorate A ndy Townsend Emerging and Disruptive Technology, PwC UK A ndre Uhl Research Associate, Director's Office, MIT Media Lab R am n Villasante Founder of PositiveSocialImpact . Software designer, engineer, CTO & CPO in EdTech for sustainable development, social impact and innovation S arah Villeneuve Policy Analyst; Member, IEEE P7010 Standards Project for Well-being Metric for Autonomous and Intelligent Systems. For a full listing of all IEEE Global Initiative Members, visit standards.ieee.org/content/dam/ ieee-standards/standards/web/documents/other/ec_bios.pdf . F or information on disclaimers associated with EAD1e, see How the Document Was Prepared.Affective ComputingThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. 90Affect is a core aspect of intelligence. Drives and emotions, such as excitement and depression, are used to coordinate action throughout intelligent life, even in species that lack a nervous system. Emotions are one mechanism that humans evolved to accomplish what needs to be done in the time available with the information at hand to satisfice. Emotions are not an impediment to rationality; arguably they are integral to rationality in humans. Humans create and respond to both positive and negative emotional influence as they coordinate their actions with other individuals to create societies. Autonomous and intelligent systems (A/IS) are being designed to simulate emotions in their interactions with humans in ways that will alter our societies. A/IS should be used to help humanity to the greatest extent possible in as many contexts as are appropriate. While A/IS have tremendous potential to effect positive change, there is also potential that artifacts used in society could cause harm either by amplifying, altering, or even dampening human emotional experience. Even rudimentary versions of synthetic emotions, such as those already in use within nudging systems, have already altered the perception of A/IS by the general public and public policy makers. This chapter of Ethically Aligned Design addresses issues related to emotions and emotion- like control in interactions between humans and design of A/IS. We have put forward recommendations on a variety of topics: considering how affect varies across human cultures; the particular problems of artifacts designed for caring and private relationships; considerations of how intelligent artifacts may be used for nudging ; how systems can support human flourishing; and appropriate policy interventions for artifacts designed with inbuilt affective systems. Document Sections S ection 1 Systems Across Cultures S ection 2 When Systems Care S ection 3 System Manipulation/Nudging/Deception S ection 4 Systems Supporting Human Potential S ection 5 Systems with Synthetic Emotions91 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective ComputingSection 1 Systems Across Cultures Issue: Should affective systems interact using the norms for verbal and nonverbal communication consistent with the norms of the society in which they are embedded? Background Individuals around the world express intentions differently, including the ways that they make eye contact, use gestures, or interpret silence. These particularities are part of an individual s and a society's culture and are incorporated into their affective systems in order to convey the intended message. To ensure that the emotional systems of autonomous and intelligent systems foster effective communication within a specific culture, an understanding of the norms/values of the community where the affective system will be deployed is essential. Recommendations A well-designed affective system will have a set of essential norms, specific to its intended cultural context of use, in its knowledge base. Research has shown that A/I S technologies can use at least five types of cues to simulate social interactions.T hese include: physical cues such as simulated facial expressions, psychological cues such as simulated humor or other emotions, use of language, use of social dynamics like taking turns, and through social roles such as acting as a tutor or medical advisor. Further examples are listed below: a. W ell-designed affective systems will use language with affective content carefully and within the contemporaneous expectations of the culture. An example is small talk. Although small talk is useful for establishing a friendly rapport in many communities, some communities see people that use small talk as insincere and hypocritical. Other cultures may consider people that do not use small talk as unfriendly, uncooperative, rude, arrogant, or ignorant. Additionally, speaking with proper vocabulary, grammar, and sentence structure may contrast with the typical informal interactions between individuals. For example, the latest trend, TV show, or other media may significantly influence what is viewed as appropriate vocabulary and interaction style. b. W ell-designed affective systems will recognize that the amount of personal space (proxemics) given by individuals in an important part of culturally specific 192 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective Computinghuman interaction. People from varying cultures maintain, often unknowingly, different spatial distances between themselves to establish smooth communication. Crossing these limits may require explicit or implicit consent, which A/IS must learn to negotiate to avoid transmitting unintended messages. c. Eye contact is an essential component for culturally sensitive social interaction. F or some interactions, direct eye contact is needed but for others it is not essential and may even generate misunderstandings. It is important that A/IS be equipped to recognize the role of eye contact in the development of emotional interaction. d. Hand gestures and other non-verbal communication are very impor tant for social interaction. Communicative gestures are culturally specific and thus should be used with caution in cross-cultural situations. The specificity of physical communication techniques must be acknowledged in the design of functional affective systems. For instance, although a thumbs-up sign is commonly used to indicate approval, in some countries this gesture can be considered an insult. e. Humans use facial expressions to detect emotions and facilitate communication. F acial expressions may not be universal across cultures, however, and A/IS trained with a dataset from one culture may not be readily usable in another culture. Well-developed A/IS will be able to recognize, analyze, and even display facial expressions essential for culturally specific social interaction. Eng ineers should consider the need for cross-cultural use of affective systems. Well-designed systems will have options innate to facilitate flexibility in cultural programming. Mechanisms to enable and disable culturally specific add-ons should be considered an essential part of A/IS development. Further Resources G . Cotton, Gestures to Avoid in Cross-Cultural Business: In Other Words, Keep Your Fingers to Yourself! Huffington Post, June 13, P aralanguage Across Cultures , Sydney, Australia: Culture Plus Consulting, G . Cotton, Say Anything to Anyone, Say Anything to Anyone, Anywhere: 5 Keys to Successful Cross-Cultural Communication . Hoboken, NJ: Wiley, D . Elmer, Cross-Cultural Connections: Stepping Out and Fitting In Around the World . Westmont, IL: InterVarsity Press, B . J. Fogg, Persuasive Technology . Ubiquity , December 2, A . McStay, Emotional AI: The Rise of Empathic Media. London: Sage, M . Price, Facial Expressions Including Fear May Not Be as Universal as We Thought . Science, October 17, 93 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective ComputingIssue: It is presently unknown whether long-term interaction with affective artifacts that lack cultural sensitivity could alter human social interaction. Background Systems that do not have cultural knowledge incorporated into their knowledge base may or may not interact effectively with humans for whom emotion and culture are significant. Given that interaction with A/IS may affect individuals and societies, it is imperative that we carefully evaluate mechanisms to promote beneficial affective interaction between humans and A/IS. Humans often use mirroring in order to understand and develop their norms for behavior. Certain machine learning approaches also address improving A/IS interaction with humans through mirroring human behavior. Thus, we must remember that learning via mirroring can go in both directions and that interacting with machines has the potential to impact individuals norms, as well as societal and cultural norms. If affective artifacts with enhanced, different, or absent cultural sensitivity interact with impressionable humans this could alter their responses to social and cultural cues and values. The potential for A/IS to exert cultural influence in powerful ways, at scale, is an area of substantial concern. Recommendations C ollaborative research teams must research the effects of long-term interaction of people with affective systems. This should be done using multiple protocols, disciplinary approaches, and metrics to measure the modifications of habits, norms, and principles as well as careful evaluation of the downstream cultural and societal impacts. P arties responsible for deploying affective systems into the lives of individuals or communities should be trained to detect the influence of A/IS, and to utilize mitigation techniques if A/IS effects appear to be harmful. It should always be possible to shut down harmful A/IS. Further Resources T . Nishida and C. Faucher, Eds., Modelling Machine Emotions for Realizing Intelligence: Foundations and Applications . Berlin, Germany: Springer-Verlag, D . J. Pauleen, et al. Cultural Bias in Information Systems Research and Practice: Are You Coming from the Same Place I Am? Communications of the Association for Information Systems , vol. 17,) pp. 1 36, J. Bielby, Comparative Philosophies in Intercultural Information Ethics. Confluence: Online Journal of World Philosophies 2, no. 1, pp. 233 253, J. Bryson, Why Robot Nannies Probably Won t Do Much Psychological Damage. A commentary on an article by N. Sharkey 94 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective Computingand A. Sharkey, The Crying Shame of Robot Nannies. Interaction Studies , vol. 1 1, no. 2 pp. 161 190, July A . Sharkey, and N. Sharkey, Children, the Elderly, and Interactive Robots. IEEE Robotics & Automation Magazine, vol.18, no. 1, pp. 32 38, March 201 Issue: When affective systems are deployed across cultures, they could adversely affect the cultural, social, or religious values of the community in which they interact. Background Some philosophers argue that there are no universal ethical principles and that ethical norms vary from society to society. Regardless of whether universalism or some form of ethical relativism is true, affective systems need to respect the values of the cultures within which they are embedded. How systems should effectively reflect the values of the designers or the users of affective systems is not a settled discussion. There is general agreement that developers of affective systems should acknowledge that the systems should reflect the values of those with whom the systems are interacting. There is a high likelihood that when spanning different groups, the values imbued by the developer will be different from the operator or customer of that affective system, and that end-user values should be actively considered. Differences between affective systems and societal values may generate conflict situations producing undesirable results, e.g., gestures or eye contact being misunderstood as rude or threatening. Thus, affective systems should adapt to reflect the values of the community and individuals where they will operate in order to avoid misunderstanding. Recommendations Assuming that well-designed affective systems have a minimum subset of configurable norms incorporated in their knowledge base: A ffective systems should have capabilities to identify differences between the values they are designed with and the differing values of those with whom the systems are interacting. W here appropriate, affective systems will adapt accordingly over time to better fit the norms of their users. As societal values change, there needs to be a means to detect and accommodate such cultural change in affective systems. T hose actions undertaken by an affective system that are most likely to generate an emotional response should be designed to be easily changed in appropriate ways by the user without being easily hacked by actors with malicious intentions. Similar to how software today externalizes the language and vocabulary to be easily changeable based on location, affective systems should externalize some of the core aspects of their actions.95 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective ComputingFurther Resources J . Bielby, Comparative Philosophies in Intercultural Information Ethics. Confluence: Online Journal of World Philosophies 2, no. 1, pp. 233 253, M . Velasquez, C. Andre, T. Shanks, and M. J. Meyer. Ethical Relativism . Markkula Center for Applied Ethics, Santa Clara, CA: Santa Clara University, August 1, C ulture reflects the moral values and ethical norms governing how people should behave and interact with others. Ethics, an Overview . Boundless Management. T . Donaldson, Values in Tension: Ethics Away from Home Away from Home . Harvard Business Review. September October Section 2 When Systems Care Issue: Are moral and ethical boundaries crossed when the design of affective systems allows them to develop intimate relationships with their users? Background There are many robots in development or production designed to focus on intimate care of children, adults, and the elderly While robots capable of participating fully in intimate relationships are not currently available, the potential use of such robots routinely captures the attention of the media. It is important that professional communities, policy makers, and the general public participate in development of guidelines for appropriate use of A/IS in this area. Those guidelines should acknowledge fundamental human rights to highlight potential ethical benefits and risks that may emerge, if and when affective systems interact intimately with users. Among the many areas of concern are the representation of care, embodiment of caring A/IS, and the sensitivity of data generated through intimate and caring relationships with A/IS. The literature suggests that there are some potential benefits to individuals and to society from the incorporation of caring A/IS, along with duly cautionary notes concerning the possibility that these systems could negatively impact human-to-human intimate relations Recommendations As this technology develops, it is important to monitor research into the development of intimate relationships between A/IS and humans. Research should emphasize any technical and 96 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective Computingnormative developments that reflect use of A/IS in positive and therapeutic ways while also creating appropriate safeguards to mitigate against uses that contribute to problematic individual or social relationships: I ntimate systems must not be designed or deployed in ways that contribute to stereotypes, gender or racial inequality, or the exacerbation of human misery. I ntimate systems must not be designed to explicitly engage in the psychological manipulation of the users of these systems unless the user is made aware they are being manipulated and consents to this behavior. Any manipulation should be governed through an opt-in system. C aring A/IS should be designed to avoid contributing to user isolation from society. D esigners of affective robotics must publicly acknowledge, for example, within a notice associated with the product, that these systems can have side effects, such as interfering with the relationship dynamics between human partners, causing attachments between the user and the A/IS that are distinct from human partnership. C ommercially marketed A/IS for caring applications should not be presented to be a person in a legal sense, nor marketed as a person. Rather its artifactual, that is, authored, designed, and built deliberately, nature should always be made as transparent as possible, at least at point of sale and in available documentation, as noted in Section 4, Systems Supporting Human Potential.E xisting laws regarding personal imagery need to be reconsidered in light of caring A/IS. In addition to other ethical considerations, it will also be necessary t o establish conformance with local laws and mores in the context of caring A/IS systems. Further Resources M . Boden, J. Bryson, D. Caldwell, K. Dautenhahn, L. Edwards, S. Kember, P. Newman, V. Parry, G. Pegman, T. Rodden and T. Sorrell, Principles of robotics: regulating robots in the real world. Connection Science, vol. 29, no. 2, pp. 124-129, April J . J. Bryson, M. E. Diamantis, and T. D. Grant, Of, For, and By the People: The Legal Lacuna of Synthetic Persons. Artificial Intelligence & Law, vol. 25, no. 3, pp. 273 291, Sept. M . Scheutz, The Inherent Dangers of Unidirectional Emotional Bonds between Humans and Social Robots, in Robot Ethics: The Ethical and Social Implications of Robotics, P. Lin, K. Abney, and G. Bekey, Eds., pp. Cambridge, MA: MIT Press, 201 97 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective ComputingSection 3 System Manipulation/ Nudging/Deception Issue: Should affective systems be designed to nudge people for the user s personal benefit and/or for the benefit of others? Background Manipulation can be defined as an exercise of influence by one person or group, with the intention to attempt to control or modify the actions of another person or group. Thaler and Sunstein ( call the tactic of subtly modifying behavior a nudge 4 . Nudging mainly operates through the affective elements of a human rational system. Making use of a nudge might be considered appropriate in situations like teaching children, treating drug dependency, and in some healthcare settings. While nudges can be deployed to encourage individuals to express behaviors that have community benefits, a nudge could have unanticipated consequences for people whose backgrounds were not well considered in the development of the nudging system Likewise, nudges may encourage behaviors with unanticipated long-term effects, whether positive or negative, for the individual and/or society. The effect of A/IS nudging a person, such as potentially eroding or encouraging individual liberty, or expressing behaviors that are for the benefit others, should be well characterized in the design of A/IS.Recommendations S ystematic analyses are needed that examine the ethics and behavioral consequences of designing affective systems to nudge human beings prior to deployment. T he user should be empowered, through an explicit opt-in system and readily available, comprehensible information, to recognize different types of A/IS nudges, regardless of whether they seek to promote beneficial social manipulation or to enhance consumer acceptance of commercial goals. The user should be able to access and check facts behind the nudges and then make a conscious decision to accept or reject a nudge. Nudging systems must be transparent, with a clear chain of accountability that includes human agents: data logging is required so users can know how, why, and by whom they were nudged. A /IS nudging must not become coercive and should always have an opt-in system policy with explicit consent. A dditional protections against unwanted nudging must be put in place for vulnerable populations, such as children, or when informed consent cannot be obtained. Protections against unwanted nudging should be encouraged when nudges alter long-term behavior or when consent alone may not be a sufficient safeguard against coercion or exploitation. 98 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective Computing5. D ata gathered which could reveal an individual or groups susceptibility to a nudge or their emotional reaction to a nudge should not be collected or distributed without opt-in consent, and should only be retained transparently, with access restrictions in compliance with the highest requirements of data privacy and law. Further Resources R . Thaler, and C. R. Sunstein, Nudge: Improving Decision about Health, Wealth and Happiness , New Haven, CT: Yale University Press, L . Bovens, The Ethics of Nudge, in Preference change: Approaches from Philosophy, Economics and Psychology , T. Gr ne-Yanoff and S. O. Hansson, Eds., Berlin, Germany: Springer, 2008 pp. 207 S . D. Hunt and S. Vitell. ""A General Theory of Marketing Ethics."" Journal of Macromarketing, vol.6, no. 1, pp. 5-16, June A . McStay, Empathic Media and Advertising: Industry, Policy, Legal and Citizen Perspectives (the Case for Intimacy) , Big Data & Society, pp. 1-1 1, December J . de Quintana Medina and P. Hermida Justo, Not All Nudges Are Automatic: Freedom of Choice and Informative Nudges . Working paper presented to the European Consortium for Political Research, Joint Session of Workshops, 2016 Behavioral Change and Public Policy, Pisa, Italy, M . D. White, The Manipulation of Choice. Ethics and Libertarian Paternalism. New York: Palgrave Macmillan, 2013 C .R. Sunstein, The Ethics of Influence: Government in the Age of Behavioral Science. New York: Cambridge, 2016 M . Scheutz, The Affect Dilemma for Artificial Agents: Should We Develop Affective Artificial Agents? IEEE Transactions on Affective Computing, vol. 3, no. 4,pp. 424 433, Sept. A . Grinbaum, R. Chatila, L. Devillers, J.- G. Ganascia, C. Tessier and M. Dauchet. Ethics in Robotics Research: CERNA Recommendations , IEEE Robotics and Automation Magazine, vol. 24, no. 3,pp. 139 145, Sept. Designing Moral Technologies: Theoretical, Practical, and Ethical Issues Conference July 10 15, 2016, Monte Verit , Switzerland. 99 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective ComputingIssue: Governmental entities may potentially use nudging strategies, for example to promote the performance of charitable acts. Does the practice of nudging for the benefit of society, including nudges by affective systems, raise ethical concerns? Background A few scholars have noted a potentially controversial practice of the future: allowing a robot or another affective system to nudge a user for the good of society For instance, if it is possible that a well-designed robot could effectively encourage humans to perform charitable acts, would it be ethically appropriate for the robot to do so? This design possibility illustrates just one behavioral outcome that a robot could potentially elicit from a user. Given the persuasive power that an affective system may have over a user, ethical concerns related to nudging must be examined. This includes the significant potential for misuse. Recommendations A s more and more computing devices subtly and overtly influence human behavior, it is important to draw attention to whether it is ethically appropriate to pursue this type of design pathway in the context of governmental actions. T here needs to be transparency regarding who the intended beneficiaries are, and whether any form of deception or manipulation is going to be used to accomplish the intended goal. Further Resources J . Borenstein and R. Arkin, Robotic Nudges: Robotic Nudges: The Ethics of Engineering a More Socially Just Human Being Just Human Being . Science and Engineering Ethics, vol. 22, no. 1,pp. 31 46, Feb. J . Borenstein and R. Arkin. Nudging for Good: Robots and the Ethical Appropriateness of Nurturing Empathy and Charitable Behavior . AI and Society, vol. 32, no. 4, pp. 499 507, Nov. Issue: Will A/IS nudging systems that are not fully relevant to the sociotechnical context in which they are operating cause behaviors with adverse unintended consequences? Background A well-designed nudging or suggestion system will have sophisticated enough technical capabilities for recognizing the context in which it is applying nudging actions. Assessment of the context requires perception of the scope or impact of the actions to be taken, the consequences of incorrectly or incompletely 100 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective Computingapplied nudges, and acknowledgement of the uncertainties that may stem from long term consequences of a nudge Recommendations C onsideration should be given to the development of a system of technical licensing ( permits ) or other certification from governments or non-governmental organizations (NGOs) that can aid users to understand the nudges from A/IS in their lives. U ser autonomy is a key and essential consideration that must be taken into account when addressing whether affective systems should be permitted to nudge human beings. D esign features of an affective system that nudges human beings should include the ability to accurately distinguish between users, including detecting characteristics such as whether the user is an adult or a child. A ffective systems with nudging strategies should incorporate a design system of evaluation, monitoring, and control for unintended consequences. Further Resources J . Borenstein and R. Arkin, Robotic Nudges: Robotic Nudges: The Ethics of Engineering a More Socially Just Human Being Just Human Being . Science and Engineering Ethics, vol. 22, no. 1, pp. 31 46, R . C. Arkin, M. Fujita, T. Takagi, and R. Hasegawa, An Ethological and Emotional Basis for Human- Robot Interaction . Robotics and Autonomous Systems, vol. 42, no. 3 4 pp.191 201, March S . Omohundro Autonomous Technology and the Greater Human Good . Journal of Experimental and Theoretical Artificial Intelligence, vol. 26, no. 3, pp. 303 315, Issue: When, if ever, and under which circumstances, is deception performed by affective systems acceptable? Background Deception is commonplace in everyday human- human interaction. According to Kantian ethics, it is never ethically appropriate to lie, while utilitarian frameworks indicate that it can be acceptable when deception increases overall happiness. Given the diversity of views on ethics and the appropriateness of deception, should affective systems be designed to deceive? Does the non-consensual nature of deception restrict the use of A/IS in contexts in which deception may be required? 101 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective ComputingRecommendations It is necessary to develop recommendations regarding the acceptability of deception performed by A/IS, specifically with respect to when and under which circumstances, if any, it is appropriate. I n general, deception may be acceptable in an affective agent when it is used for the benefit of the person being deceived, not for the agent itself. For example, deception might be necessary in search and rescue operations or for elder- or child-care. F or deception to be used under any circumstance, a logical and reasonable justification must be provided by the designer, and this rationale should be certified by an external authority, such as a licensing body or regulatory agency. Further Resources R . C. Arkin, Robots That Need to Mislead: Biologically-inspired Machine Deception. IEEE Intelligent Systems 27, no. 6, pp. 60 75, J . Shim and R. C. Arkin, Other-Oriented Robot Deception: How Can a Robot s Deceptive Feedback Help Humans in HRI? Eighth International Conference on Social Robotics (ICSR , Kansas, MO., November J . Shim and R. C. Arkin, The Benefits of Robot Deception in Search and Rescue: Computational Approach for Deceptive Action Selection via Case-based Reasoning. 2015 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR , West Lafayette, IN, October J . Shim and R. C. Arkin, A Taxonomy of Robot Deception and its Benefits in HRI. Proceedings of IEEE Systems, Man and Cybernetics Conference, Manchester England, October 102 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective ComputingSection 4 Systems Supporting Human Potential Issue: Will extensive use of A/IS in society make our organizations more brittle by reducing human autonomy within organizations, and by replacing creative, affective, empathetic components of management chains? Background If human workers are replaced by A/IS, the possibility of corporations, governments, employees, and customers discovering new equilibria outside the scope of what the organizations past leadership originally foresaw may be unduly limited. A lack of empathy based on shared needs, abilities, and disadvantages between organizations and customers causes disequilibria between the individuals and corporations and governments that exist to serve them. Opportunities for useful innovation may therefore be lost through automation. Collaboration requires enough commonality of collaborating intelligences to create empathy the capacity to model the other s goals based on one s own. According to scientists within several fields, autonomy is a psychological need. Without it, humans fail to thrive, create, and innovate. Ethically aligned design should support, not hinder, human autonomy or its expression. Recommendations I t is important that human workers interaction with other workers not always be intermediated by affective systems (or other technology) which may filter out autonomy, innovation, and communication. Hu man points of contact should remain available to customers and other organizations when using A/IS. A ffective systems should be designed to support human autonomy, sense of competence, and meaningful relationships as these are necessary to support a flourishing life. E ven where A/IS are less expensive, more predictable, and easier to control than human employees, a core network of human employees should be maintained at every level of decision-making in order to ensure preservation of human autonomy, communication, and innovation. M anagement and organizational theorists should consider appropriate use of affective and autonomous systems to enhance their business models and the efficacy of their workforce within the limits of the preservation of human autonomy. 103 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective ComputingFurther Resources J . J. Bryson, Artificial Intelligence and Pro-Social Behavior, in Collective Agency and Cooperation in Natural and Artificial Systems, C. Misselhorn, Ed., pp. 281 306, Springer, D . Peters, R.A. Calvo, and R.M. Ryan, Designing for Motivation, Engagement and Wellbeing in Digital Experience , Frontiers in Psychology Human Media Interaction, vol. 9, pp 797, Issue: Does the increased access to personal information about other members of our society, facilitated by A/IS, alter the human affective experience? Does this access potentially lead to a change in human autonomy? Background Theoretical biology tells us that we should expect increased communication which A/IS facilitate to increase group-level investment Extensive use of A/IS could change the expression of individual autonomy and in its place increase group-based identities. Examples of this sort of social alteration may include: C hanges in the scope of monitoring and control of children s lives by parents. D ecreased willingness to express opinions for fear of surveillance or long-term consequences of past expressions being used in changed temporal contexts.U tilization of customers or other end users to perform basic corporate business processes such as data entry as a barter for lower prices or access, resulting potentially in reduced tax revenues. C hanges to the expression of individual autonomy could alter the diversity, creativity, and cohesiveness of a society. It may also alter perceptions of privacy and security, and social and legal liability for autonomous expressions. Recommendations O rganizations, including governments, must put a high value on individuals privacy and autonomy, including restricting the amount and age of data held about individuals specifically. E ducation in all forms should encourage individuation, the preservation of autonomy, and knowledge of the appropriate uses and limits to A/IS Further Resources J . J. Bryson, Artificial Intelligence and Pro-Social Behavior, in Collective Agency and Cooperation in Natural and Artificial Systems, C. Misselhorn, Ed., pp. 281 306, Springer, M . Cooke, A Space of One s Own: Autonomy, Privacy, Liberty, Philosophy & Social Criticism, Vol. 25, no. 1, pp. 22 53, D . Peters, R.A. Calvo, R.M. Ryan, Designing for Motivation, Engagement and Wellbeing in Digital Experience Frontiers in Psychology Human Media Interaction , vol. pp 797, 104 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective Computing J . Roughgarden, M. Oishi and E. Ak ay, Reproductive Social Behavior: Cooperative Games to Replace Sexual Selection. Science 31 1, no. 5763, pp. 965 969, Issue: Will use of A/IS adversely affect human psychological and emotional well-being in ways not otherwise foreseen? Background A/IS may be given unprecedented access to human culture and human spaces both physical and intellectual. A/IS may communicate via natural language, may move with humanlike form, and may express humanlike identity, but they are not, and should not be regarded as, human. Incorporation of A/IS into daily life may affect human well-being in ways not yet anticipated. Incorporation of A/IS may alter patterns of trust and capability assessment between humans, and between humans and A/IS. Recommendations V igilance and robust, interdisciplinary, on-going research on identifying situations where A/IS affect human well-being, both positively and negatively, is necessary. Evidence of correlations between the increased use of A/IS and positive or negative individual or social outcomes must be explored. D esign restrictions should be placed on the systems themselves to avoid machine decisions that may alter a person s life in unknown ways. Explanations should be available on demand in systems that may affect human well-being. Further Resources K . Kamewari, M. Kato, T. Kanda, H. Ishiguro and K. Hiraki. Six-and-a-Half-Month-Old Children Positively Attribute Goals to Human Action and to Humanoid-Robot Motion, Cognitive Development, vol. 20, no. 2, pp. 303 320, R .A. Calvo and D. Peters, Positive Computing: Technology for Wellbeing and Human Potential. Cambridge, MA: MIT Press, 105 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective ComputingSection 5 Systems with Synthetic Emotions Issue: Will deployment of synthetic emotions into affective systems increase the accessibility of A/IS? Will increased accessibility prompt unforeseen patterns of identification with A/IS? Background Deliberately constructed emotions are designed to create empathy between humans and artifacts, which may be useful or even essential for human-A/IS collaboration. Synthetic emotions are essential for humans to collaborate with the A/IS but can also lead to failure to recognize that synthetic emotions can be compartmentalized and even entirely removed. Potential consequences for humans include different patterns of bonding, guilt, and trust, whether between the human and A/IS or between other humans. There is no coherent sense in which A/IS can be made to suffer emotional loss, because any such affect, even if possible, could be avoided at the stage of engineering, or reengineered. As such, it is not possible to allocate moral agency or responsibility in the senses that have been developed for human emotional bonding and thus sociality. Recommendations C ommercially marketed A/IS should not be persons in a legal sense, nor marketed as persons. Rather their artifactual (authored, designed, and built deliberately) nature should always be made as transparent as possible, at least at point of sale and in available documentation. S ome systems will, due to their application, require opaqueness in some contexts, e.g., emotional therapy. Transparency in such systems should be available to inspection by responsible parties but may be withdrawn for operational needs. Further Resources R . C. Arkin, P. Ulam and A. R. Wagner, Moral Decision-making in Autonomous Systems: Enforcement, Moral Emotions, Dignity, Trust and Deception, Proceedings of the IEEE, vol. 100, no. 3, pp. 571 589, R . Arkin, M. Fujita, T. Takagi and R. Hasegawa. An Ethological and Emotional Basis for Human-Robot Interaction, Robotics and Autonomous Systems, vol.42, no. 3 4, pp.191 201, R . C. Arkin, Moving up the Food Chain: Motivation and Emotion in Behavior-based Robots, in Who Needs Emotions: The Brain Meets the Robot , J. Fellous and M. Arbib., Eds., New York: Oxford University Press, 106 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective Computing M . Boden, J. Bryson, D. Caldwell, et al. Principles of Robotics: Regulating Robots in the Real World. Connection Science, vol. 29, no. 2, pp. 124 129, J . J Bryson, M. E. Diamantis and T. D. Grant. Of, For, and By the People: The Legal Lacuna of Synthetic Persons, Artificial Intelligence & Law, vol. 25, no. 3, pp. 273 291, Sept. J . Novikova, and L. Watts, Towards Artificial Emotions to Assist Social Coordination in HRI, International Journal of Social Robotics, vol. 7, no. 1, pp. 77 88, M . Scheutz, The Affect Dilemma for Artificial Agents: Should We Develop Affective Artificial Agents? IEEE Transactions on Affective Computing, vol. 3, no. 4, pp. 424 433, A . Sharkey and N. Sharkey. Children, the Elderly, and Interactive Robots. IEEE Robotics & Automation Magazine, vol. 18, no. 1, pp. 32 38 , 107 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective ComputingThanks to the Contributors We wish to acknowledge all of the people who contributed to this chapter. The Affective Computing Committee R onald C. Arkin (Founding Co-Chair) Regents' Professor & Director of the Mobile Robot Laboratory; College of Computing Georgia Institute of Technology J oanna J. Bryson (Co-Chair) Reader (Associate Professor), University of Bath, Intelligent Systems Research Group, Department of Computer Science J ohn P. Sullins (Co-Chair) Professor of Philosophy, Chair of the Center for Ethics Law and Society (CELS), Sonoma State University G enevieve Bell Intel Senior Fellow Vice President, Corporate Strategy Office Corporate Sensing and Insights J ason Borenstein Director of Graduate Research Ethics Programs, School of Public Policy and Office of Graduate Studies, Georgia Institute of Technology C ynthia Breazeal Associate Professor of Media Arts and Sciences, MIT Media Lab; Founder & Chief Scientist of Jibo, Inc. J oost Broekens Assistant Professor Affective Computing, Interactive Intelligence group; Department of Intelligent Systems, Delft University of Technology Rafael Calvo Professor & ARC Future Fellow, School of Electrical and Information Engineering, The University of Sydney L aurence Devillers Professor of Computer Sciences, University Paris Sorbonne, LIMSI-CNRS 'Affective and social dimensions in spoken interactions' - member of the French Commission on the Ethics of Research in Digital Sciences and Technologies (CERNA) J onathan Gratch Research Professor of Computer Science and Psychology, Director for Virtual Human Research, USC Institute for Creative Technologie M ark Halverson Founder and CEO at Human Ecology Holdings and Precision Autonomy J ohn C. Havens Executive Director, The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems; Executive Director, The Council on Extended Intelligence; Author, Heartificial Intelligence: Embracing Our Humanity to Maximize Machines 108 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective Computing N oreen Herzfeld Reuter Professor of Science and Religion, St. John s University Ch ihyung Jeon Assistant Professor, Graduate School of Science and Technology Policy, Korea Advanced Institute of Science and Technology (KAIST) P reeti Mohan Software Engineer at Microsoft and Computational Linguistics Master s Student at the University of Washington B joern Niehaves Professor, Chair of Information Systems, University of Siegen R osalind Picard Rosalind Picard, (Sc.D, FIEEE) Professor, MIT Media Laboratory, Director of Affective Computing Research; Faculty Chair, MIT Mind+Hand+Heart; Co-founder & Chief Scientist, Empatica Inc.; Co-founder, Affectiva Inc. E dson Prestes Professor, Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Brazil; Head, Phi Robotics Research Group, UFRGS; CNPq Fellow Mat thias Scheutz Professor, Bernard M. Gordon Senior Faculty Fellow, Tufts University School of Engineering R obert Sparrow Professor, Monash University, Australian Research Council Future Fellow , 2010- C herry Tom Emerging Technologies Intelligence Manager, IEEE Standards Association For a full listing of all IEEE Global Initiative Members, visit standards.ieee.org/content/dam/ ieee-standards/standards/web/documents/other/ec_bios.pdf . F or information on disclaimers associated with EAD1e, see How the Document Was Prepared. 109 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Affective ComputingEndnotes 1 S ee B. J. Fogg, Persuasive technology . Ubiquity , December: 2, 2 S ee S. Turkle, W. Taggart , C.D. Kidd, and O. Daste, Relational artifacts with children and elders: the complexities of cybercompanionship, Connection Science, vol. 18, no. 4, 3 A d iscussion of intimate robots for therapeutic and personal use is outside of the scope of Ethically Aligned Design, First Edition . For further treatment, among others, see J. P. Sullins, Robots, Love, and Sex: The Ethics of Building a Love Machine. IEEE Transactions on Affective Computing 3, no. 4 (: 398 4 S ee R. Thaler, and C. R. Sunstein. Nudge: Improving Decision about Health, Wealth and Happiness , New Haven, CT: Yale University Press, 5 S ee J. de Quintana Medina and P. Hermida Justo. Not All Nudges Are Automatic: Freedom of Choice and Informative Nudges. Working paper presented to the European Consortium for Political Research, Joint Session of Workshops, 2016 Behavioral Change and Public Policy, Pisa, Italy, 2016; and M. D. White, The Manipulation of Choice. Ethics and Libertarian Paternalism. New York: Palgrave Macmillan, 6 S ee, for example, J. Borenstein and R. Arkin. Robotic Nudges: The Ethics of Engineering a More Socially Just Human Being . Science and Engineering Ethics, vol. 22, no. 1 (: 31 7 S ee S. Omohundro, Autonomous Technology and the Greater Human Good . Journal of Experimental and Theoretical Artificial Intelligence 26, no. 3 (: 303 8 S ee J. Roughgarden, M. Oishi, and E. Ak ay. Reproductive Social Behavior: Cooperative Games to Replace Sexual Selection. Science 31 1, no. 5763 (: 965 9 S ee the Well-being chapter of this Ethically Aligned Design, First Edition .110 Personal Data and Individual AgencyThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Regulations like the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) of 2018 are helping to improve personal data protection. But legal compliance is not enough to mitigate the ethical implications and core challenges to human agency embodied by algorithmically driven behavioral tracking or persuasive computing. The core of the issue is one of parity. Humans cannot respond on an individual basis to every algorithm tracking their behavior without technological tools supported by policy allowing them to do so. Individuals may provide consent without fully understanding specific terms and conditions agreements. But they are also not equipped to fully recognize how the nuanced use of their data to inform personalized algorithms affects their choices at the risk of eroding their agency. Here we understand agency as an individual s ability to influence and shape their life trajectory as determined by their cultural and social contexts. Agency in the digital arena enables an individual to make informed decisions where their own terms and conditions can be recognized and honored at an algorithmic level. To strengthen individual agency, governments and organizations must test and implement technologies and policies that let individuals create, curate, and control their online agency as associated with their identity. Data transactions should be moderated and case-by-case authorization decisions from the individual as to who can process what personal data for what purpose. Specifically, we recommend governments and organizations: Cr eate: Provide every individual with the means to create and project their own terms and conditions regarding their personal data that can be read and agreed to at a machine- readable level. Cu rate: Provide every individual with a personal data or algorithmic agent which they curate to represent their terms and conditions in any real, digital, or virtual environment. C ontrol : Provide every individual access to services allowing them to create a trusted identity to control the safe, specific, and finite exchange of their data. Three sections of this chapter reflect these core ideals regarding human age ncy. A fourth section addresses issues surrounding personal data and individual agency relating to children.111 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Section 1 Create To retain agency in the algorithmic era, each individual must have the means to create and project their own terms and conditions regarding their personal data. These must be readable and usable by both humans and machines. Issue : What would it mean for a person to have individually controlled terms and conditions for their personal data? Background Part of providing individually controlled terms and conditions for personal data is to help each person consider what their preferences are regarding their data versus dictating how they need to share it. While questions along these lines are framed in light of a person s privacy, their preferences also reveal larger values for individuals. The ethical issue is whether A/IS act in accordance with these values. This process of investigating one s values to identify these preferences is a powerful step towards regaining data agency. The point is not only that a person s data are protected, but also that by curating these answers they become educated about how important their information is in the context of how it is shared. Most individuals also believe controlling their personal data only happens on the sites or social networks to which they belong and have no idea of the consequences of how that data may be used by others in the future. Agreeing to most standard terms and conditions on these sites largely means users consent to give up control of their personal data rather than play a meaningful role in defining and curating its downstream use. The scope of how long one should or could control the downstream use of their data can be difficult to calculate as consent-based models of personal data have trained users to release rights on any claims for use of their data which are entirely provided to the service, manufacturer, and their partners. However, models like YouTube s Content ID provide a form of precedent for thinking about how an individual s data could be technically protected where it is considered as an asset they could control and copyright. Here is language from YouTube s site about the service : Copyright owners can use a system called Content ID to easily identify and manage their content on YouTube. Videos uploaded to YouTube are scanned against a database of files that have been submitted to us by content owners. In this sense, the question of how long or how far downstream one s personal data should be protected takes on the same logic of how long a corporation s intellectual property or copyrights could be protected based on initial legal terms set. 112 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.One challenge is how to define use of data that can affect the individual directly, versus use of aggregated data. For example, an individual subway user s travel card, tracking their individual movements, should be protected from uses that identify or profile that individual to make inferences about his/her likes or location generally. But data provided by a user could be included in an overall travel system s management database, aggregated into patterns for scheduling and maintenance as long as the individual-level data are deleted. Where users have predetermined via their terms and conditions that they are willing to share their data for these travel systems, they can meaningfully articulate how to share their information. Under current business models, it is common for people to consent to the sharing of discrete data like credit card transaction data, answers to test questions, or how many steps they walk. However, once aggregated these data and the associated insights may lead to complex and sensitive conclusions being drawn about individuals. This end use of the individual s data may not have been part of the initial sharing agreement. This is why models for terms and conditions created for user control typically alert people via onscreen or other warning methods when their predetermined preferences are not being honored. Recommendation Individuals should be provided tools that produce machine-readable terms and conditions that are dynamic in nature and serve to protect their data and honor their preferences for its use. Specifically: P ersonal data access and consent should be managed by the individual using their curated terms and conditions that provide notification and an opportunity for consent at the time data are exchanged, versus outside actors being able to access personal data without an individual s awareness or control. T erms should be presented in a way that allows a user to easily read, interpret, understand, and choose to engage with any A/IS. Consent should be both conditional and dynamic, where dynamic means downstream uses of a person s data must be explicitly called out, allowing them to cancel a service and potentially rescind or kill any data they have shared with a service to date via the use of a Smart Contract or specific conditions as described in mutual terms and conditions between two parties at the time of exchange. F or further information on these issues, please see the following section in regard to algorithmic agents and their application. Further Resources I EEE P7012 - IEEE Standards Project for Machine Readable Personal Privacy Terms . This approved standardization project (currently in development) directly honors the goals laid out in Section One of this document. T he Personalized Privacy Assistant Project Carnegie Mellon University. 2019 . 113 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. M . Orcutt, Personal AI Privacy Watchdog Could Help You Regain Control of Your Data MIT Technology Review, May 1 1, M . Hintze, Privacy Statements: Purposes, Requirements, and Best Practices. Cambridge, U.K.: Cambridge University Press, D . J. Solove, Privacy self-management and the consent dilemma, Harvard Law Review, vol. 126, no. 7, pp. 1880 1903, May N . Sadeh, M. Degeling, A. Das, A. S. Zhang, A. Acquisti, L. Bauer, L. Cranor, A. Datta, and D. Smullen, A Privacy Assistant for the Internet of Things: files/soups17_poster_sadeh.pdf H . Lee, R. Chow, M. R. Haghighat, H. M. Patterson and A. Kobsa, IoT Service Store: A Web-based System for Privacy-aware IoT Service Discovery and Interaction, 2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops), Athens, pp. 107-1 12, L . Cranor, M. Langheinrich, M. Marchiori, M. Presler-Marshall, and J. Reagle, The Platform for Privacy Preferences 0 (P3P1. Specification, W3C Recommendation, [Online]. Available: Apr. L . F. Cranor, Personal Privacy Assistants in the Age of the Internet of Things, in World Economic Forum Annual Meeting, Section 2 Curate To retain agency in the algorithmic era, we must provide every individual with a personal data or algorithmic agent they curate to represent their terms and conditions in any real, digital, or virtual environment. This agent would be empowered to act as an individual s legal proxy in the digital and virtual arena. Oftentimes, the functionality of this agent will be automated, operating along the lines of current ad blockers which do not permit prespecified algorithms to access a user s data. For other situations that might be unique or new to this agent, a user could specify that notices or updates be sent on a case-by-case basis to determine where there could be a concern. Issue: What would it mean for a person to have an algorithmic agent helping them actively represent and curate their terms and conditions at all times? Background While it s essential to create your own terms and conditions to broadcast your preferences, it s also important to recognize that humans do not operate at an algorithmic speed or level. A significant part of retaining your agency in this 114 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.way involves identifying trusted services that can essentially act on your behalf when making decisions about your data. Part of this logic entails putting you at the center of your data . One of the greatest challenges to user agency is that once you give your data away, you do not know how it is being used or by whom. But when all transactions about your data go through your A/IS agent honoring your preferences, you have better opportunities to control how your information is shared. As an example, with medical data while it is assumed most would share all their medical data with their spouse most would also not wish to share that same amount of data with their local gym. This is an issue that extends beyond privacy, meaning one s cultural or individual preferences about what personal information to share, to utility and clarity. This type of sharing also benefits users or organizations on the receiving end of data from these exchanges. For instance, the local gym in the previous example may only need basic heart or general health information and would actually not wish to handle or store sensitive cancer or other personal health data for reasons of liability. A precedent for this type of patient- or user- centric model comes from Gliimpse, a service described by Jordan Crook from TechCrunch in his article, Apple acquired Gliimpse, a personal health data startup : Gliimpse works by letting users pull their own medical info into a single virtual space, with the ability to add documents and pictures to fill out the profile. From there, users can share that data (as a comprehensive picture) to whomever they wish. The fact that Apple acquired the startup points to the potential for the successful business model of user-centric data exchange and putting individuals at the center of their data. A person s A/IS agent is a proactive algorithmic tool honoring their terms and conditions in the digital, virtual, and physical worlds. Any public space where a user may not be aware they are under surveillance by facial recognition, biometric, or other tools that could track, store, and utilize their data can now provide overt opportunity for consent via an A/IS agent platform. Even where an individual is not sure they are being tracked, by broadcasting their terms and conditions via digital means, they can demonstrate their preferences in the public arena. Via Bluetooth or similar technologies, individuals could offer their terms and conditions in a ubiquitous and always-on manner. This means even when an individual s terms and conditions are not honored, people would have the ability to demonstrate their desire not to be tracked which could provide a methodology for the democratic right to protest in a peaceful manner. And where those terms and conditions are recognized meaning technically recognized even if they are not honored one s opinions could be formally logged via GPS and timestamp data. The A/IS agent could serve as an educator and negotiator on behalf of its user by suggesting how requested data could be combined with other data that has already been provided, inform the user if data are being used in a way that was not authorized, or make recommendations to the user based on a personal profile. As a negotiator, the agent could broker conditions for sharing data and could include payment to the user as a 115 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.term, or even retract consent for the use of data previously authorized, for instance, if a breach of conditions was detected. Recommendations Algorithmic agents should be developed for individuals to curate and share their personal data. Specifically: F or purposes of privacy, a person must be able to set up complex permissions that reflect a variety of wishes. T he agent should help a person foresee and mitigate potential ethical implications of specific machine learning data exchanges. A u ser should be able to override his/her personal agents should he/she decide that the service offered is worth the conditions imposed. A n agent should enable machine-to-machine processing of information to compare, recommend, and assess offers and services. I nstitutional systems should ensure support for and respect the ability of individuals to bring their own agent to the relationship without constraints that would make some guardians inherently incompatible or subject to censorship. V ulnerable parts of the population will need protection in the process of granting access. Further Resources I EEE P7006 - IEEE Standards Project on Personal Data AI Agent Working Group. Designed as a tool to allow any individual to create their own personal terms and conditions for their data, the AI Agent will also provide a technological tool for individuals to manage and control their identity in the digital and virtual world. T ools allowing an individual to create a form of an algorithmic guardian are often labeled as PIMS, or Personal Information Management Services. Nesta in the United Kingdom was one of the funders of early research about PIMS conducted by CtrlShift. 116 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Section 3 Control To retain agency in the algorithmic era, we must provide every individual access to services allowing them to create a trusted identity to control the safe, specific, and finite exchange of their data. Issue: How can we increase agency by providing individuals access to services allowing them to create a trusted identity to control the safe, specific, and finite exchange of their data? Background Pervasive behavior-tracking adversely affects human agency by recognizing our identity in every action we take on and offline. This is why identity as it relates to individual data is emerging at the forefront of the risks and opportunities related to use of personal information for A/IS. Across the identity landscape there is increasing tension between the requirement for federated identities versus a range of identities. In federated identities, all data are linked to a natural and identified person. When one has a range of identities, or personas, these can be context specific and determined by the use case. New movements, such as Self-Sovereign Identity defined as the right of a person to determine his or her own identity are emerging alongside legal identities, e.g., those issued by governments, banks, and regulatory authorities, to help put individuals at the center of their data in the algorithmic age.Personas, identities that act as proxies, and pseudonymity are also critical requirements for privacy management and agency. These help individuals select an identity that is appropriate for the context they are in or wish to join. In these settings, trust transactions can still be enabled without giving up the root identity of the user. For example, it is possible to validate that a user is over eighteen or is eligible for a service. Attribute verification will play a significant role in enabling individuals to select the identity that provides access without compromising agency. This type of access is especially important in dealing with the myriad of algorithms interacting with narrow segments of our identity data. In these situations, individuals typically are not aware of the context for how their data will be used. Recommendation Individuals should have access to trusted identity verification services to validate, prove, and support the context-specific use of their identity. Further Resources S ovrin Foundation, The Inevitable Rise of Self- Sovereign Identity , Sept. 29, T . Ruff, Three Models of Digital Identity Relationships, Evernym , Apr. 24, C . Pettey, The Beginner s Guide to Decentralized Identity. Gartner, C . Allen, The Path to Self-Sovereign Identity. GitHub, 117 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Section 4 Children s Data Issues While the focus of this chapter is to provide all individuals with agency regarding their personal data, some sectors of society have little or no control. For some elderly individuals or the mentally ill, it is because they have been found to not have mental capacity , and for prisoners in the criminal justice system, society has taken control away as punishment. In the case of children, this is because they are considered human beings in development with evolving capacities. We examine the issues of children as an example case and recommend either regulation or a technical architecture that provides a veil and buffer from harm until a child is at an age where they can claim personal responsibility for their decisions. In many parts of the world, children are viewed by the law as being primarily charges of their parents who make choices on their behalf. In Europe, however, the state has a role in ensuring the best interests of the child 1 In schools, the two interests operate side-by-side, with parents being given some control over their child s education but with many decisions being made by the schools. Many of the issues described above concern choices around personal data and the future impacts of how the data are gathered and shared. Children are at the forefront of technological developments with future educational and recreational technology gathering data from them all day at school and intelligent toys throughout their time at home. As children post, click, search, and share information, their data are linked to various profiles, grouped into segmented audiences, and fed into machine learning algorithms. Some of these may be designed to target campaigns that increase sales, influence sentiment, encourage online games, impact social networks, or influence religious and political views. Data fed into algorithmic advertising is not only gathered from children s online actions but also from their devices. An example of device data is browser fingerprinting. 3 It includes a set of data about a child s browser or operating system. Fingerprinting vastly increases privacy risks because it is used to link to an individual. Increasingly, children s beliefs and social norms are established by what they see and experience online. Their actions reflect what they believe is possible and expected. The report, Digital Deceit: Technologies Behind Precision Propaganda on the Internet 4, explains how companies collect, process, and then monetize personal preferences, socioeconomic status, fears, political and religious beliefs, location, and patterns of internet use. Companies, governments, political parties, and philosophical and religious organizations use data available about students and children to influence how they spend their time, money, and the people or institutions they trust and with whom they spend time and build relationships. Many aspects of a child s life can be digitized. Their behavioral, device, and network data are combined and used by machine learning 118 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.algorithms to determine the information and content that best achieve the educational goals of the schools and the economic goals of the advertisers and platform companies. Issue: Mass personalization of instruction Background The mass personalization of education offers better education for all at very low cost through A/IS-enabled computer-based instruction that promises to free up teachers to work with kids individually to pursue their passions. These applications will rely on the continuous gathering of personal data regarding mood, thought processes, private stories, physiological data, and more. The data will be used to construct a computational model of each child s interests, understanding, strengths, and weaknesses. The model provides an intimate understanding of how they think, what they understand, how they process information, or react to new information; all of which can be used to drive instructional content and feedback. Sharing of this data between classes, enabling it to follow students through their schooling, will make the models more effective and beneficial to children, but it also exposes children and their families to social control. If performance data are correlated with social data on a family, it could be used by social authorities in decision-making about the family. For example, since 2015-2018, well-being digital tests were performed in schools in Denmark. Children were asked about everything from bullying, loneliness, and stomachaches. Recently it was disclosed that although the collected data was presented as anonymous, they were not. Data were stored with social security numbers, correlated with other test data, and even used in case management by some Danish municipalities. 5 Commercial profiling and correlation of different sets of personal data may further affect these children in future job or educational situations. Recommendation Educational data offer a unique opportunity to model individuals thought processes and could be used to predict or change individuals behavior in many situations. Governments and organizations should classify educational data as being sensitive and implement special protective standards. Children s data should be held in escrow and not used for any commercial purposes until a child reaches the age of majority and is able to authorize use as they choose. Further Resources T he journal of the International Artificial Intelligence in Education Society: D eeper discussion and bibliography of future trends of AI-based education with utopian and dystopian case scenarios: N. Pinkwart, Another 25 Years of AIED? Challenges and Opportunities for Intelligent Educational Technologies of the Future, International Journal of Artificial Intelligence in Education, vol. 26, no. 2, pp. 771 783, [Online]. 119 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Available: 016-0099-7 [Accessed Dec. 2018]. I nformation Commissioners Office (ico.), What if we want to profile children or make automated decisions about them? K . Firth-Butterfield, What happens when your child s friend is an AI toy that talks back? in World Economic Forum: Generation AI, May 22, 2018 . Issue: Technology choice-making in schools Background Children, as minors, have no standing to give or deny consent, or to control the use of their personal data. Parents only have limited choices in what are often school-wide implementations of educational technology. Examples include the use of Google applications, face recognition in security systems, and computer driven instruction as described above. In many cases, parents only choice would be to send their children to a different school, but that choice is seldom available. How should schools make these choices? How much input should parents have? Should parents be able to demand technology-free teaching?There are many gaps in current student data regulation. In June 2018, CLIP, The Center on Law and Information Policy at Fordham Law School published, Transparency and the Marketplace for Student Data . 6 This study concluded that student lists are commercially available for purchase on the basis of ethnicity, affluence, religion, lifestyle, awkwardness, and even a perceived or predicted need for family planning services . Fordham found that the data market is becoming one of the largest and most profitable marketplaces in the United States. Data brokers have databases that store billions of data elements on nearly every United States consumer. However, information from students in the pursuit of an education should not be exploited and commercialized without restraint. Fordham researchers found at least 14 data brokers who advertise the sale of student information. One sold lists of students as young as two years old. Another sold lists of student profiles on the basis of ethnicity, religion, economic factors, and even gawkiness. Recommendation Local and national educational authorities must work to develop policies surrounding students personal data with all stakeholders: administrators, teachers, technology providers, students, and parents in order to balance the best educational interests of each child with the best practices to ensure safety of their personal data. Such efforts will raise awareness among all stakeholders of the promise and the compromises inherent in new educational technologies. 120 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Further Resources C ommon Sense Media privacy evaluation project: education/privacy D . T. Ritvo, L. Plunkett, and P. Haduong, Privacy and Student Data: Companion Learning Tools. Berkman Klein Center for Internet and Society at Harvard University, [Online]. Available: [Accessed Dec. 2018] . F . Alim, N. Cardozo, G. Gebhart, K. Gullo, and A. Kalia, Spying on Students: School-Issued Devices and Student Privacy, Electronic Frontier Foundation, April 13, N . C. Russell, J. R. Reidenberg, E. Martin, and T. Norton, Transparency and the Marketplace for Student Data, Virginia Journal of Law and Technology , Forthcoming. Available at SSRN: , June 6, Issue: Intelligent toys Background Children will not only be exposed to A/IS at school but also at home, while they play and while they sleep. Toys are already being sold that offer interactive, intelligent opportunities for play. Many of them collect video and audio data which is stored on company servers and either is or could be mined for profiling or marketing data. There is currently little regulatory oversight. In the United States COPPA 7 offers some protection for the data of children under Germany has outlawed such toys using legislation banning spying equipment enacted in Corporate A/IS are being embodied in toys and given to children to play with, to talk to, tell stories to, and to explore all the personal development issues that we learn about in private play as children. Recommendations Child data should be held in escrow and not used for any commercial purposes until a child reaches the age of majority and is able to authorize use as they choose. Governments and organizations need to educate and inform parents of the mechanisms of A/IS and data collection in toys and the possible impact on children in the future. Further Resources K . Firth-Butterfield, What happens when your child s friend is an AI toy that talks back? in World Economic Forum: Generation AI, , May 22, 2018 . D . Basulto, How artificial intelligence is moving from the lab to your kid s playroom, Washington Post, Oct. 15, [Online]. Available: com/news/innovations/wp/2015/10/15/how-artificial-intelligence-is-moving-from-the-lab-to-your-kids-playroom/?utm_term = .89a1431a05a7 [Accessed Dec. 1, 2018].121 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. S . Chaudron, R. Di Gioia, M. Gemo, D. Holloway, J. Marsh, G. Mascheroni J. Peter, and D. Yamada-Rice , , S . Chaudron, R. Di Gioia, M. Gemo, D. Holloway, J. Marsh, G. Mascheroni, J. Peter, D. Yamada-Rice Kaleidoscope on the Internet of Toys - Safety, security, privacy and societal insights , EUR 28397 EN, doi:2788/05383, Luxembourg: Publications Office of the European Union, Z . Kleinman, Alexa, are you friends with our kids? BBC News, July 16, [Online]. Available: technology-%5b. [Accessed Dec. 1, 2018]. J . Wakefield, Germany bans children s smartwatches. BBC News, Nov. 17 [Online]. Available: Dec. 2018]. Thanks to the Contributors We wish to acknowledge all of the people who contributed to this chapter. The Personal Data and Individual Agency Committee K atryna Dow (Co-Chair) CEO & Founder at Meeco J ohn C. Havens (Co-Chair) Executive Director, The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems; Executive Director, The Council on Extended Intelligence; Author, Heartificial Intelligence: Embracing Our Humanity to Maximize Machines M ads Schaarup Andersen Senior Usable Security Expert in the Alexandra Institute s Security Lab A jay Bawa Technology Innovation Lead, Avanade Inc. A riel H. Brio Privacy and Data Counsel at Sony Interactive Entertainment W alter Burrough Co-Founder, Augmented Choice; PhD Candidate (Computer Science) Serious Games Institute Da nny W. Devriendt Managing director of Mediabrands Dynamic (IPG) in Brussels, and the CEO of the Eye of Horus, a global think-tank for communication-technology related topics D r. D. Michael Franklin Assistant Professor, Kennesaw State University, Marietta Campus, Marietta, GA J ean-Gabriel Ganascia Professor, University Pierre et Marie Curie; LIP6 Laboratory ACASA Group Leader122 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. B ryant Joseph Gilot, MD CM DPhil MSc Center for Personalised Medicine, University of Tuebingen Medical Center, Germany & Chief Medical Officer, Blockchain Health Co., San Francisco Dav id Goldstein Seton Hall University Adr ian Gropper, M.D. CTO, Patient Privacy Rights Foundation; HIE of One Project M arsali S. Hancock Chair, IEEE Standards for Child and Student Data governance, CEO and Co-Foundation EP3 Foundation.F G ry Hasselbalch Founder DataEthics, Author, Data Ethics - The New Competitive Advantage Y anqing Hong Graduate, University of Utrecht Researcher at Tsinghua University P rofessor Meg Leta Jones Assistant Professor in the Communication, Culture & Technology program at Georgetown University M ahsa Kiani Chair of Student Activities, IEEE Canada; Vice Editor, IEEE Canada Newsletter (ICN); PhD Candidate, Faculty of Computer Science, University of New Brunswick B renda Leong Senior Counsel, Director of Operations, The Future of Privacy Forum E mma Lindley Founder, Innovate Identity E wa Luger Chancellor s Fellow at the University of Edinburgh, within the Design Informatics Group S ean Martin McDonald CEO of FrontlineSMS, Fellow at Stanford s Digital Civil Society Lab, Principal at Digital Public H iroshi Nakagawa Professor, The University of Tokyo, and AI in Society Research Group Director at RIKEN Center for Advanced Intelligence Project (AIP) S ofia C. Olhede Professor of Statistics and an Honorary Professor of Computer Science at University College London, London, U.K; Member of the Programme Committee of the International Centre for Mathematical Sciences. U go Pagallo University of Turin Law School; Center for Transnational Legal Studies, London; NEXA Center for Internet & Society, Politecnico of Turin D r. Juuso Parkkinen Senior Data Scientist, Nightingale Health; Programme Team Member, MyData 2017 conference E leonore Pauwels Research Fellow on AI and Emerging Cybertechnologies, United Nations University (NY) and Director of the AI Lab, Woodrow Wilson International Center for Scholars (DC) D r. Deborah C. Peel Founder, Patient Privacy Rights & Creator, the International Summits on the Future of Health Privacy W alter Pienciak Principal Architect, Advanced Cognitive Architectures, Ltd. P rofessor Serena Quattrocolo University of Turin Law School C arolyn Robson Group Data Privacy Manager at Etihad Aviation Group G ilad Rosner Internet of Things Privacy Forum; Horizon Digital Economy Research Institute, UK; UC Berkeley Information School123 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Personal Data and Individual Agency This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. P rof. Dr.-Ing. Ahmad-Reza Sadeghi Director System Security Lab, Technische Universit t Darmstadt / Director Intel Collaborative Research Institute for Secure Computing R ose Shuman Partner at BrightFront Group & Founder, Question Box D r. Zolt n Szl vik Lead/Researcher, IBM Center for Advanced Studies Benelux U dbhav Tiwari Centre for Internet and Society, IndiaFor a full listing of all IEEE Global Initiative Members, visit standards.ieee.org/content/dam/ ieee-standards/standards/web/documents/other/ec_bios.pdf . F or information on disclaimers associated with EAD1e, see How the Document Was Prepared. Endnotes 1 Europ ische Union, Europ ischer Gerichtshof f r Menschenrechte, & Europarat (Eds.). (. Handbook on European law relating to the rights of the child. Luxembourg: Publications Office of the European Union. - ments/Handbook_rights_child_ENG.PDF 2 Children Act (. Retrieved from legislation.gov.uk/ukpga/1989/41/section/1 3 Browser fingerprints, and why they are so hard to erase | Network World. 17 Feb. 2015, . Accessed 25 July. 4 D. Gosh and B. Scott, Digital Deceit: The Tech- nologies behind Precision Propaganda on the Internet 23 Jan. 2018, - aldeceit/ . Accessed 10 Nov 5 Case described in Danish here eu/trivsel-enhver-pris/ 6 Russell, N. Cameron, Reidenberg, Joel R., Mar- tin, Elizabeth, and Norton, Thomas, Transparency and the Marketplace for Student Data (June 6, . Virginia Journal of Law and Technology, Forthcoming. Available at SSRN: 7 Children s Online Privacy Protection Act (COPPA) - to Guide Ethical Research and DesignThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. 124Autonomous and intelligent systems (A/IS) research and design must be developed against the backdrop that technology is not neutral. A/IS embody values and biases that can influence important social processes like voting, policing, and banking. To ensure that A/IS benefit humanity, A/IS research and design must be underpinned by ethical and legal norms. These should be instantiated through values-based research and design methods. Such methods put human well-being at the core of A/IS development. To help achieve these goals, researchers, product developers, and technologists across all sectors need to embrace research and development methods that evaluate their processes, products, values, and design practices in light of the concerns and considerations raised in this chapter. This chapter is split up into three sections: Section 1 Interdisciplinary Education and Research Section 2 Corporate Practices on A/IS Section 3 Responsibility and Assessment Each of the sections highlights various areas of concern (issues) as well as recommendations and further resources. Overall, we address both structural and individual approaches. We discuss how to improve the ethical research and business practices surrounding the development of A/IS and attend to the responsibility of the technology sector vis- -vis the public interest. We also look at that what can be done at the level of educational institutions, among others, informing engineering students about ethics, social justice, and human rights. The values-based research and design method will require a change of current system development approaches for organizations. This includes a commitment of research institutions to strong ethical guidelines for research and of businesses to values that transcend narrow economic incentives.125 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignSection 1 Interdisciplinary Education and Research Integrating applied ethics into education and research to address the issues of A/IS requires an interdisciplinary approach, bringing together humanities, social sciences, physical sciences, engineering, and other disciplines. Issue: Integration of ethics in A/IS-related degree programs Background A/IS engineers and design teams do not always thoroughly explore the ethical considerations implicit in their technical work and design choices. Moreover, the overall science, technology, engineering, and mathematics (STEM) field struggles with the complexity of ethical considerations, which cannot be readily articulated and translated into the formal languages of mathematics and computer programming associated with algorithms and machine learning. Ethical issues can easily be rendered invisible or inappropriately reduced and simplified in the context of technical practice. For the dangers of this approach see for instance, Lipton and Steinhardt (, listed under Further Resources . This problem is further compounded by the fact that many STEM programs do not sufficiently integrate applied ethics throughout their curricula. When they do, often ethics is relegated to a stand-alone course or module that gives students little or no direct experience in ethical decision-making. Ethics education should be meaningful, applicable, and incorporate best practices from the broader field. The aim of these recommendations is to prepare students for the technical training and engineering development methods that incorporate ethics as essential so that ethics, and relevant principles, like human rights, become naturally a part of the design process. Recommendations E thics training needs to be a core subject for all those in the STEM field, beginning at the earliest appropriate level and for all advanced degrees. E ffective STEM ethics curricula should be informed by experts outside the STEM community from a variety of cultural and educational backgrounds to ensure that students acquire sensitivity to a diversity of robust perspectives on ethics and design. S uch curricula should teach aspiring engineers, computer scientists, and statisticians about the relevance and impact of their decisions in designing A/IS technologies. Effective 126 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and Designethics education in STEM contexts and beyond should span primary, secondary, and postsecondary education, and include both universities and vocational training schools. R elevant accreditation bodies should reinforce this integrated approach as outlined above. Further Resources I EEE P7000TM Standards Project for a Model Process for Addressing Ethical Concerns During System Design . IEEE P7000 aims to enhance corporate IT innovation practices by providing processes for embedding a values- and virtue-based thinking, culture, and practice into them. Z . Lipton and J. Steinhardt, Troubling Trends in Machine Learning Scholarship. ICML conference paper, July J . Holdren, and M. Smith. Preparing for the Future of Artificial Intelligence. Washington, DC: Executive Office of the President, National Science and Technology Council, C omparing the UK, EU, and US approaches to AI and ethics: C. Cath, S. Wachter, B. Mittelstadt, et al., Artificial Intelligence and the Good Society : The US, EU, and UK Approach. Science and Engineering Ethics, vol. 24, pp. 505-528, Issue : Interdisciplinary collaborations Background More institutional resources and incentive structures are necessary to bring A/IS engineers and designers into sustained and constructive contact with ethicists, legal scholars, and social scientists, both in academia and industry. This contact is necessary as it can enable meaningful interdisciplinary collaboration and shape the future of technological innovation. More could be done to develop methods, shared knowledge, and lexicons that would facilitate such collaboration. This issue relates, among other things, to funding models as well as the lack of diversity of backgrounds and perspectives in A/IS-related institutions and companies, which limit cross-pollination between disciplines. To help bridge this gap, additional translation work and resource sharing, including websites and Massive Open Online Courses (MOOCs), need to happen among technologists and other relevant experts, e.g., in medicine, architecture, law, philosophy, psychology, and cognitive science. Furthermore, there is a need for more cross-disciplinary conversation and multi-disciplinary research, as is being done, for instance, at the annual ACM Fairness, Accountability, and Transparency (FAT*) conference or the work done by the Canadian Institute For Advanced Research (CIFAR), which is developing Canada s AI strategy. 127 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignRecommendations Funding models and institutional incentive structures should be reviewed and revised to prioritize projects with interdisciplinary ethics components to encourage integration of ethics into projects at all levels. Further Resources S . Barocas, Course Material for Ethics and Policy in Data Science, Cornell University, L . Floridi, and M. Taddeo. What Is Data Ethics? Philosophical Transactions of the Royal Society, vol. 374, no. 2083, 1 DOI 1098/ rsta.0360 , S . Spiekermann, Ethical IT Innovation: A Value- Based System Design Approach. Boca Raton, FL: Auerbach Publications, K . Crawford, Artificial Intelligence s White Guy Problem , New York Times , July 25, [Online]. Available: October 28, 2018]. Issue: A/IS culture and context Background A responsible approach to embedding values into A/IS requires that algorithms and systems are created in a way that is sensitive to the variation of ethical practices and beliefs across cultures. The designers of A/IS need to be mindful of cross-cultural ethical variations while also respecting widely held international legal norms. Recommendation Establish a leading role for intercultural information ethics (IIE) practitioners in ethics committees informing technologists, policy makers, and engineers. Clearly demonstrate through examples how cultural variation informs not only information flows and information systems, but also algorithmic decision-making and value by design. Further Resources D . J. Pauleen, et al. Cultural Bias in Information Systems Research and Practice: Are You Coming From the Same Place I Am? Communications of the Association for Information Systems, vol. 17, no. 17, J . Bielby, Comparative Philosophies in Intercultural Information Ethics , Confluence: Online Journal of World Philosophies 2, no. 1, pp. 233 253, 128 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignIssue: Institutional ethics committees in the A/IS fields Background It is unclear how research on the interface of humans and A/IS, animals and A/IS, and biological hazards will impact research ethical review boards. Norms, institutional controls, and risk metrics appropriate to the technology are not well established in the relevant literature and research governance infrastructure. Additionally, national and international regulations governing review of human-subjects research may explicitly or implicitly exclude A/IS research from their purview on the basis of legal technicalities or medical ethical concerns, regardless of the potential harms posed by the research. Research on A/IS human-machine interaction, when it involves intervention or interaction with identifiable human participants or their data, typically falls to the governance of research ethics boards, e.g., institutional review boards. The national level and institutional resources, e.g., hospitals and universities, necessary to govern ethical conduct of Human-Computer Interaction (HCI), particularly within the disciplines pertinent to A/IS research, are underdeveloped. First, there is limited international or national guidance to govern this form of research. Sections of IEEE standards governing research on A/IS in medical devices address some of the issues related to the security of A/IS-enabled devices. However, the ethics of testing those devices for the purpose of bringing them to market are not developed into policies or guidance documents from recognized national and international bodies, e.g., U.S. Food and Drug Administration (FDA) and EU European Medicines Agency (EMA). Second, the bodies that typically train individuals to be gatekeepers for the research ethics bodies are under-resourced in terms of expertise for A/IS development, e.g., Public Responsibility in Medicine and Research (PRIM&R) and the Society of Clinical Research Associates (SoCRA). Third, it is not clear whether there is sufficient attention paid to A/IS ethics by research ethics board members or by researchers whose projects involve the use of human participants or their identifiable data. For example, research pertinent to the ethics- governing research at the interface of animals and A/IS research is underdeveloped with respect to systematization for implementation by the Institutional Animal Care and Use Committee (IACUC) or other relevant committees. In institutions without a veterinary school, it is unclear that the organization would have the relevant resources necessary to conduct an ethical review of such research. Similarly, research pertinent to the intersection of radiological, biological, and toxicological research ordinarily governed under institutional biosafety committees and A/IS research is not often found in the literature pertinent to research ethics or research governance. 129 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignRecommendation The IEEE and other standards-setting bodies should draw upon existing standards, empirical research, and expertise to identify priorities and develop standards for the governance of A/IS research and partner with relevant national agencies, and international organizations, when possible. Further Resources S . R. Jordan, The Innovation Imperative. Public Management Review 16, no. 1, pp. 67 89, B . Schneiderman, The Dangers of Faulty, Biased, or Malicious Algorithms Requires Independent Oversight . Proceedings of the National Academy of Sciences of the United States of America 1 13, no. 48, 13538 13540, J . Metcalf and K. Crawford, Where are Human Subjects in Big Data Research? The Emerging Ethics Divide . Big Data & Society, May 14, [Online]. Available: SSRN: com/abstract=2779647 . [Accessed Nov. 1, 2018]. R . Calo, Consumer Subject Review Boards: A Thought Experiment . Stanford Law Review Online 66 97, Sept. 130 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignSection 2 Corporate Practices on A/IS Corporations are eager to develop, deploy, and monetize A/IS, but there are insufficient structures in place for creating and supporting ethical systems and practices around A/IS funding, development, and use. Issue: Values-based ethical culture and practices for industry Background Corporations are built to create profit while competing for market share. This can lead corporations to focus on growth at the expense of avoiding negative ethical consequences. Given the deep ethical implications of widespread deployment of A/IS, in addition to laws and regulations, there is a need to create values-based ethical culture and practices for the development and deployment of those systems. To do so, we need to further identify and refine corporate processes that facilitate values-based design. Recommendations The building blocks of such practices include top-down leadership, bottom-up empowerment, ownership, and responsibility, along with the need to consider system deployment contexts and/or ecosystems. Corporations should identify stages in their processes in which ethical considerations, ethics filters , are in place before products are further developed and deployed. For instance, if an ethics review board comes in at the right time during the A/IS creation process, it would help mitigate the likelihood of creating ethically problematic designs. The institution of an ethical A/IS corporate culture would accelerate the adoption of the other recommendations within this section focused on business practices. Further Resources A CM Code of Ethics and Professional Ethics , which includes various references to human well-being and human rights, R eport of UN Special Rapporteur on Freedom of Expression. AI and Freedom of Expression . T he website of the Benefit corporations (B-corporations) provides a good overview of a range of companies that personify this type of culture. R . Sisodia, J. N. Sheth and D. Wolfe, Firms of Endearment , 2nd edition. Upper Saddle River, NJ: FT Press, This book showcases how companies embracing values and a stakeholder approach outperform their competitors in the long run. 131 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignIssue: Values-based leadership Background Technology leadership should give innovation teams and engineers direction regarding which human values and legal norms should be promoted in the design of A/IS. Cultivating an ethical corporate culture is an essential component of successful leadership in the A/IS domain. Recommendations Companies should create roles for senior-level marketers, engineers, and lawyers who can collectively and pragmatically implement ethically aligned design. There is also a need for more in-house ethicists, or positions that fulfill similar roles. One potential way to ensure values are on the agenda in A/IS development is to have a Chief Values Officer (CVO), a role first suggested by Kay Firth-Butterfield, see Further Resources . However, ethical responsibility should not be delegated solely to CVOs. They can support the creation of ethical knowledge in companies, but in the end, all members of an organization will need to act responsibly throughout the design process. Companies need to ensure that their understanding of values-based system innovation is based on de jure and de facto international human rights standards. Further Resources K . Firth-Butterfield, How IEEE Aims to Instill Ethics in Artificial Intelligence Design, The Institute. Jan. 19, 2017 . [Online]. Available: . [Accessed October 28, 2018]. Uni ted Nations, Guiding Principles on Business and Human Rights: Implementing the United Nations Protect, Respect and Remedy Framework , New York and Geneva: UN, 201 I nstitute for Human Rights and Business (IHRB), and Shift, ICT Sector Guide on Implementing the UN Guiding Principles on Business and Human Rights, C . Cath, and L. Floridi, The Design of the Internet s Architecture by the Internet Engineering Task Force (IETF) and Human Rights. Science and Engineering Ethics, vol. 23, no. 2, pp. 449 468, Apr. Issue: Empowerment to raise ethical concerns Background Engineers and design teams may encounter obstacles to raising ethical concerns regarding their designs or design specifications within their organizations. Corporate culture should incentivize technical staff to voice the full range of ethical questions to relevant corporate actors throughout the full product lifecycle, including the design, development, and deployment 132 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and Designphases. Because raising ethical concerns can be perceived as slowing or halting a design project, organizations need to consider how they can recognize and incentivize values-based design as an integral component of product development. Recommendations Employees should be empowered and encouraged to raise ethical concerns in day-to-day professional practice. To be effective in ensuring adoption of ethical considerations during product development or internal implementation of A/IS, organizations should create a company culture and set of norms that encourage incorporating ethical considerations in the design and implementation processes. New categories of considerations around these issues need to be accommodated, along with updated Codes of Conduct, company value-statements, and other management principles so individuals are empowered to share their insights and concerns in an atmosphere of trust. Additionally, bottom-up approaches like company town hall meetings should be explored that reward, rather than punish, those who bring up ethical concerns. Further Resources T he British Computer Society (BCS) , Code of Conduct, C . Cath, and L. Floridi, The Design of the Internet s Architecture by the Internet Engineering Task Force (IETF) and Human Rights , Science and Engineering Ethics, vol. 23, no. 2, pp. 449 468, Apr. Issue: Ownership and responsibility Background There is variance within the technology community on how it sees its responsibility regarding A/IS. The difference in values and behaviors are not necessarily aligned with the broader set of social concerns raised by public, legal, and professional communities. The current makeup of most organizations has clear delineations among engineering, legal, and marketing functions. Thus, technologists will often be incentivized in terms of meeting functional requirements, deadline, and financial constraints, but for larger social issues may say, Legal will handle that. In addition, in employment and management technology or work contexts, ethics typically refers to a code of conduct regarding professional behavior versus a values-driven design process mentality. As such, ethics regarding professional conduct often implies moral issues such as integrity or the lack thereof, in the case of whistleblowing, for instance. However, ethics in A/IS design include broader considerations about the consequences of technologies. Recommendations Organizations should clarify the relationship between professional ethics and applied A/IS ethics by helping or enabling designers, engineers, and other company representatives to discern the differences between these kinds of ethics and where they complement each other.133 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignCorporate ethical review boards, or comparable mechanisms, should be formed to address ethical and behavioral concerns in relation to A/IS design, development and deployment. Such boards should seek an appropriately diverse composition and use relevant criteria, including both research ethics and product ethics, at the appropriate levels of advancement of research and development. These boards should examine justifications of research or industrial projects. Further Resources H H van der Kloot Meijberg and RHJ ter Meulen, Developing Standards for Institutional Ethics Committees: Lessons from the Netherlands, Journal of Medical Ethics 27 i36-i40, Issue: Stakeholder inclusion Background The interface between A/IS and practitioners, as well as other stakeholders, is gaining broader attention in domains such as healthcare diagnostics, and there are many other contexts where there may be different levels of involvement with the technology. We should recognize that, for example, occupational therapists and their assistants may have on-the-ground expertise in working with a patient, who might be the end user of a robot or social A/IS technology. In order to develop a product that is ethically aligned, stakeholders feedback is crucial to design a system that takes ethical and social issues into account. There are successful user experience (UX) design concepts, such as accessibility, that consider human physical disabilities, which should be incorporated into A/IS as they are more widely deployed. It is important to continuously consider the impact of A/IS through unanticipated use and on unforeseen interests. Recommendations To ensure representation of stakeholders, organizations should enact a planned and controlled set of activities to account for the interests of the full range of stakeholders or practitioners who will be working alongside A/IS and incorporating their insights to build upon, rather than circumvent or ignore, the social and practical wisdom of involved practitioners and other stakeholders. Further Resources C . Schroeter, et al., Realization and User Evaluation of a Companion Robot for People with Mild Cognitive Impairments , Proceedings of IEEE International Conference on Robotics and Automation (ICRA , Karlsruhe, Germany pp. 1 145 1 T . L. Chen, et al. Robots for Humanity: Using Assistive Robotics to Empower People with Disabilities , IEEE Robotics and Automation Magazine, vol. 20, no. 1, pp. 30 39, R . Hartson, and P. S. Pyla. The UX Book: Process and Guidelines for Ensuring a Quality User Experience . Waltham, MA: Elsevier, 134 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignIssue: Values-based design Background Ethics are often treated as an impediment to innovation, even among those who ostensibly support ethical design practices. In industries that reward rapid innovation in particular, it is necessary to develop ethical design practices that integrate effectively with existing engineering workflows. Those who advocate for ethical design within a company should be seen as innovators seeking the best outcomes for the company, end users, and society. Leaders can facilitate that mindset by promoting an organizational structure that supports the integration of dialogue about ethics throughout product life cycles. A/IS design processes often present moments where ethical consequences can be highlighted. There are no universally prescribed models for this because organizations vary significantly in structure and culture. In some organizations, design team meetings may be brief and informal. In others, the meetings may be lengthy and structured. The transition points between discovery, prototyping, release, and revisions are natural contexts for conducting such reviews. Iterative review processes are also advisable, in part because changes to risk profiles over time can illustrate needs or opportunities for improving the final product. Recommendations Companies should study design processes to identify situations where engineers and researchers can be encouraged to raise and resolve questions of ethics and foster a proactive environment to realize ethically aligned design. Achieving a distributed responsibility for ethics requires that all people involved in product design are encouraged to notice and respond to ethical concerns. Organizations should consider how they can best encourage and facilitate deliberations among peers. Organizations should identify points for formal review during product development. These reviews can focus on red flags that have been identified in advance as indicators of risk. For example, if the datasets involve minors or focus on users from protected classes, then it may require additional justification or alterations to the research or development protocols. Further Resources A . Sinclair, Approaches to Organizational Culture and Ethics , Journal of Business Ethics, vol. 12, no. 1, pp. 63 73, A l Y. S. Chen, R. B. Sawyers, and P. F. Williams. Reinforcing Ethical Decision Making Through Corporate Culture , Journal of Business Ethics 16, no. 8, pp. 855 865, K . Crawford and R. Calo, There Is a Blind Spot in AI Research , Nature 538, pp. 311 313, 135 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignSection 3 Responsibility and Assessment Lack of accountability of the A/IS design and development process presents a challenge to ethical implementation and oversight. This section presents four issues, moving from macro oversight to micro documentation practices. Issue: Oversight for algorithms The algorithms behind A/IS are not subject to consistent oversight. This lack of assessment causes concern because end users have no account of how a certain algorithm or system came to its conclusions. These recommendations are similar to those made in the General Principles and Embedding Values into Autonomous and Intelligent Systems chapters of Ethically Aligned Design , but here the recommendations are used as they apply to the narrow scope of this chapter . Recommendations Accountability: As touched on in the General Principles chapter of Ethically Aligned Design , algorithmic transparency is an issue of concern. It is understood that specifics relating to algorithms or systems contain intellectual property that cannot, or will not, be released to the general public. Nonetheless, standards providing oversight of the manufacturing process of A/IS technologies need to be created to avoid harm and negative consequences. We can look to other technical domains, such as biomedical, civil, and aerospace engineering, where commercial protections for proprietary technology are routinely and effectively balanced with the need for appropriate oversight standards and mechanisms to safeguard the public. Human rights and algorithmic impact assessments should be explored as a meaningful way to improve the accountability of A/IS. These need to be paired with public consultations, and the final impact assessments must be made public. Further Resources F . Pasquale, The Black Box Society: The Secret Algorithms That Control Money and Information. Cambridge, MA: Harvard University Press, R . Calo, Artificial Intelligence Policy: A Primer and Roadmap, UC Davis Law Review, pp. 399 435, A RTICLE Privacy and Freedom of Expression in the Age of Artificial Intelligence, Privacy International, April [Online]. Available: content/uploads/2018/04/Privacy-and-Freedom-of-Expression-In-the-Age-of-Artificial-Intelligence-pdf . [Accessed October 28, 2018]. 136 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignIssue: Independent review organization Background We need independent, expert opinions that provide guidance to the general public regarding A/IS. Currently, there is a gap between how A/IS are marketed and their actual performance or application. We need to ensure that A/IS technology is accompanied by best-use recommendations and associated warnings. Additionally, we need to develop a certification scheme for A/IS which ensures that the technologies have been independently assessed as being safe and ethically sound. For example, today it is possible for systems to download new self-parking functionality to cars, and no independent reviewer establishes or characterizes boundaries or use. Or, when a companion robot promises to watch your children, there is no organization that can issue an independent seal of approval or limitation on these devices. We need a ratings and approval system ready to serve social/automation technologies that will come online as soon as possible. We also need further government funding for research into how A/IS technologies can best be subjected to review, and how review organizations can consider both traditional health and safety issues, as well as ethical considerations. Recommendations An independent, internationally coordinated body akin to ISO should be formed to oversee whether A/IS products actually meet ethical criteria, both when designed, developed, deployed, and when considering their evolution after deployment and during interaction with other products. It should also include a certification process. Further Resources A . Tutt, An FDA for Algorithms, Administrative Law Review 69, 83 123, M . U. Scherer, Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies , Harvard Journal of Law and Technology vol. 29, no. 2, 354 400, D . R. Desai and J. A. Kroll, Trust But Verify: A Guide to Algorithms and the Law . Harvard Journal of Law and Technology , Forthcoming; Georgia Tech Scheller College of Business Research Paper No. 17-19, Issue: Use of black-box components Background Software developers regularly use black box components in their software, the functioning of which they often do not fully understand. Deep machine learning processes, which are driving many advancements in autonomous and intelligent systems, are a growing source of black box software. At least for the foreseeable future, A/IS developers will likely be unable to build systems that are guaranteed to operate as intended.137 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignRecommendations When systems are built that could impact the safety or well-being of humans, it is not enough to just presume that a system works. Engineers must acknowledge and assess the ethical risks involved with black box software and implement mitigation strategies. Technologists should be able to characterize what their algorithms or systems are going to do via documentation, audits, and transparent and traceable standards. To the degree possible, these characterizations should be predictive, but given the nature of A/IS, they might need to be more retrospective and mitigation-oriented. As such, it is also important to ensure access to remedy adverse impacts. Technologists and corporations must do their ethical due diligence before deploying A/IS technology. Standards for what constitutes ethical due diligence would ideally be generated by an international body such as IEEE or ISO, and barring that, each corporation should work to generate a set of ethical standards by which their processes are evaluated and modified. Similar to a flight data recorder in the field of aviation, algorithmic traceability can provide insights on what computations led to questionable or dangerous behaviors. Even where such processes remain somewhat opaque, technologists should seek indirect means of validating results and detecting harms. Further Resources M . Ananny and K. Crawford, Seeing without Knowing: Limitations of the Transparency Ideal and Its Application to Algorithmic Accountability , New Media & Society , vol. 20, no. 3, pp. 973-989, Dec. 13, D . Reisman, J. Schultz, K. Crawford, and M. Whittaker, Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability, AI NOW [Online]. Available: aiareport2018.pdf . [Accessed October 28, 2018]. J . A. Kroll The Fallacy of Inscrutability . Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences , C. Cath, S. Wachter, B. Mittelstadt and L. Floridi, Eds., October 15, 2018 DOI: 1098/rsta.Issue: Need for better technical documentation Background A/IS are often construed as fundamentally opaque and inscrutable. However, lack of transparency is often the result of human decision. The problem can be traced to a variety of sources, including poor documentation that excludes vital information about the limitations and assumptions of a system. Better documentation combined with internal and external auditing are crucial to understanding a system s ethical impact.138 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignRecommendation Engineers should be required to thoroughly document the end product and related data flows, performance, limitations, and risks of A/IS. Behaviors and practices that have been prominent in the engineering processes should also be explicitly presented, as well as empirical evidence of compliance and methodology used, such as training data used in predictive systems, algorithms and components used, and results of behavior monitoring. Criteria for such documentation could be: auditability, accessibility, meaningfulness, and readability. Companies should make their systems auditable and should explore novel methods for external and internal auditing. Further Resources S . Wachter, B. Mittelstadt, and L. Floridi. Transparent, Explainable, and Accountable AI for Robotics . Science Robotics, vol. 2 , no. 6, May 31, [Online]. Available: DOI: 1 126/scirobotics.aan6080. [Accessed Nov. S . Barocas, and A. D. Selbst, Big Data s Disparate Impact . California Law Review 104, 671-732, J . A. Kroll, J. Huey, S. Barocas, E. W. Felten, J. R. Reidenberg, D. G. Robinson, and H. Yu. Accountable Algorithms . University of Pennsylvania Law Review 165, no. 1, 633 705, J . M. Balkin, Free Speech in the Algorithmic Society: Big Data, Private Governance, and New School Speech Regulation . UC Davis Law Review , 139 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Methods to Guide Ethical Research and DesignThanks to the Contributors We wish to acknowledge all of the people who contributed to this chapter. The Methods to Guide Ethical Research and Design Committee C orinne Cath-Speth (Co-Chair) PhD student at Oxford Internet Institute, The University of Oxford, Doctoral student at the Alan Turing Institute, Digital Consultant at ARTICLE 19 R aja Chatila (Co-Chair) CNRS-Sorbonne Institute of Intelligent Systems and Robotics, Paris, France; Member of the French Commission on the Ethics of Digital Sciences and Technologies CERNA; Past President of IEEE Robotics and Automation Society T homas Arnold Research Associate at Tufts University Human-Robot Interaction Laboratory J ared Bielby President, Netizen Consulting Ltd; Chair, International Center for Information Ethics; editor, Information Cultures in the Digital Age R eid Blackman, PhD Founder & CEO Virtue Consultants, Assistant Professor of Philosophy Colgate University T om Guarriello, PhD Founding Faculty member in the Master s in Branding program at New York City s School of Visual Arts, Host of RoboPsyc Podcast and author of RoboPsych Newsletter J ohn C. Havens Executive Director, The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems; Executive Director, The Council on Extended Intelligence; Author, Heartificial Intelligence: Embracing Our Humanity to Maximize Machines S ara Jordan Assistant Professor of Public Administration in the Center for Public Administration & Policy at Virginia Tech J ason Millar Professor, robot ethics at Carleton University S arah Spiekermann Chair of the Institute for Information Systems & Society at Vienna University of Economics and Business; Author of the textbook Ethical IT-Innovation , the popular book Digitale Ethik Ein Wertesystem f r das Jahrhundert and Blogger on The Ethical Machine Sh annon Vallor William J. Rewak Professor in the Department of Philosophy at Santa Clara University in Silicon Valley and Executive Board member of the Foundation for Responsible Robotics K lein, Wilhelm E. J., PhD Senior Research Associate & Lecturer in Technology Ethics, City University of Hong Kong For a full listing of all IEEE Global Initiative Members, visit standards.ieee.org/content/dam/ ieee-standards/standards/web/documents/other/ec_bios.pdf . F or information on disclaimers associated with EAD1e, see How the Document Was Prepared.A/IS for Sustainable Development The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. 140Autonomous and intelligent systems (A/IS) offer unique and impactful opportunities as well as risks both to people living in high-income countries (HIC) and in low-and middle-income countries (LMIC). The scaling and use of A/IS represent a genuine opportunity across the globe to provide individuals and communities be they rural, semi-urban, or urban with the means to satisfy their needs and develop their full potential, with greater autonomy and choice. A/IS will potentially disrupt economic, social, and political relationships and interactions at many levels. Those disruptions could provide an historical opportunity to reset those relationships in order to distribute power and wealth more equitably and thus promote social justice. 1 They could also leverage quality and better standards of life and protect people s dignity, while maintaining cultural diversity and protecting the environment. One possible vehicle that can be used to agree on priorities and prioritize resources and actions is the United Nations Agenda for Sustainable Development, which was adopted by the UN General Assembly in 2015; 193 nations voted in favor of the Agenda, which also includes 17 Sustainable Development Goals (SDGs) for the world to achieve by The Agenda challenges all member states to make concerted efforts toward the above mentioned goals, and thus toward a sustainable, prosperous, and resilient future for people and the planet. These universally applicable goals should be reached by 2 The value of A/IS is significantly associated with the generation of various types of superior and unique insights, many of which could help achieve positive socioeconomic outcomes for both HIC and LMIC societies, in keeping with the SDGs. The ethical imperative driving this chapter is that A/IS must be harnessed to benefit humanity, promote equality, and realize the world community s vision of a sustainable future and the SDGs: .of universal respect for human rights and human dignity, the rule of law, justice, equality and nondiscrimination; of respect for race, ethnicity and cultural diversity; and of equal opportunity permitting the full realization of human potential and contributing to shared prosperity. A world which invests in its children and in which every child grows up free from violence and exploitation. A world in which every woman and girl enjoys full gender equality and all legal, social and economic barriers to their empowerment have been removed. A just, equitable, tolerant, open and socially inclusive world in which the needs of the most vulnerable are met. 3141 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development We recognize that how A/IS are deployed globally will be a determining factor in whether, in fact, no-one gets left behind , whether human rights and dignity of all people are respected, whether children are protected, and whether the gap between rich and poor, within and between nations, narrows or widens. A/IS can advance the Sustainable Development Agenda s transformative vision, but at the same time, A/IS can undermine it if risks reviewed in this chapter are not managed properly. For example, A/IS create the risk of accelerating inequality within and among nations, if their development and marketing are controlled by a few select companies, primarily in HIC. The benefits would largely accrue to the highly educated and wealthier segment of the population, while displacing the less educated workforce, both by automation and by the absence of educational or retraining systems capable of imparting skills and knowledge needed to work productively alongside A/IS. These risks, although differentiated by IT infrastructure, educational attainment, economic, and cultural contexts, exist in HIC and LMIC alike. The inequality in accessing and using the internet, both within and among countries, raises questions on how to spread A/IS benefits across humanity. Ensuring A/IS for the common good is an ethical imperative and at the core of Ethically Aligned Design, First Edition ; the key elements of this common good are that it is human- centered, accountable, and ensure outcomes that are fair and inclusive. This chapter explores the imperative for A/IS to serve humanity by improving the quality and standard of life for all people everywhere. It makes recommendations for advancing equal access to this transformative technology, so that it drives the well-being of all people, rather than further concentrating wealth, resources, and decision-making power in the hands of a few countries, companies, or citizens. The recommendations further reflect policies and collaborative public, private, and people programs which, if implemented, will respect the ethical imperative embedded in the Sustainable Development Agenda s transformative vision. The respect of human rights and dignity, and the advancement of common good with equal benefit to both HIC and LMIC, are central to every recommendation within this chapter. 142 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Section 1 A/IS in Service to Sustainable Development for All A/IS have the potential to contribute to the resolution of some of the world s most pressing problems, including: violation of fundamental rights, poverty, exploitation, climate change, lack of high-quality services to excluded populations, increased violence, and the achievement of the SDGs. Issue: Current roadmaps for development and deployment of A/IS are not aligned with or guided by their impact in the most important challenges of humanity, defined in the seventeen United Nations Sustainable Development Goals (SDGs), which collectively aspire to create a more equal world of prosperity, peace, planet protection, and human dignity for all people. 4 Background SDGs promoting prosperity, peace, planet protection, human dignity, and respect for human rights of all, apply to HIC and LMIC alike. Yet ensuring that the benefits of A/IS will accrue to humanity as a whole, leaving no one behind , requires an ethical commitment to global citizenship and well-being, and a conscious effort to counter the nature of the tech economy, with its tendency to concentrate wealth within high income populations. Implementation of the SDGs should benefit excluded sectors of society in every country, regardless of A/IS infrastructure. The Road to Dignity by 2030 document of the UN Secretary General reports on resources and methods for implementing the 2030 Agenda for Sustainable Development and emphasizes the importance of science, technology, and innovation for a sustainable future. 5 The UN Secretary General posits that: A sustainable future will require that we act now to phase out unsustainable technologies and to invest in innovation and in the development of clean and sound technologies for sustainable development. We must ensure that they are fairly priced, broadly disseminated and fairly absorbed, including to and by developing countries. (para. A/IS are among the technologies that can play an important role in the solution of the deep social problems plaguing our global civilization, contributing to the transformation of society away from an unsustainable, unequal socioeconomic system, towards one that realizes the vision of universal human dignity, peace, and prosperity. However, with all the potential benefits of A/IS, there are also risks. For example, given A/IS technology s immense power needs, without 143 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development new sources of sustainable energy harnessed to power A/IS in the future, there is a risk that it will increase fossil fuel use and have a negative impact on the environment and the climate. While 45% of the world s population is not connected to the internet, they are not necessarily excluded from A/IS potential benefits: in LMIC mobile networks can provide data for A/IS applications. However, only those connected are likely to benefit from the income-producing potential of internet technologies. In 2017, internet penetration in HIC left behind certain portions of the population often in rural or remote areas; 12% of U.S. residents and 20% of residents across Europe were unable to access the internet. In Asia with its concentration of LMIC, 52% of the population, on average, had no access, a statistic skewed by the large population of China, where internet penetration reached 45% of the population. In numerous other countries in the region, 99% of residents had no access. This nearly total exclusion also exists in several countries in Africa, where the overall internet penetration is only 35%: 2 of every 3 residents in Africa have no access. 6 Those with no internet access also do not generate data needed to train A/IS, and are thereby excluded from benefits of the technology, the development of which risks systematic discriminatory bias, particularly against people from minority populations, and those living in rural areas, or in low-income countries. As a comparison, one study estimated that in the US, just one home automation product can generate a data point every six seconds. 7 In Mozambique, where about 90% of the population lack internet access, the average household generates zero digital . 8 With mobile phones generating much of the data needed for developing A/IS applications in LMIC, unequal phone ownership may build in bias. For example, there is a risk of discrimination against women, who across LMIC are 14% less likely than men to own a mobile phone, and in South Asia where 38% are less likely to own a mobile phone. 9 Recommendations The current range of A/IS applications in sectors crucial to the SDGs, and to excluded populations everywhere, should be studied, with the strengths, weaknesses, and potential of the most significant recent applications analyzed, and the best ones developed at scale. Specific objectives to consider include: I dentifying and experimenting with A/IS technologies relevant to the SDGs, such as: big data for development relevant to, for example, agriculture and medical tele-diagnosis; geographic information systems needed in public service planning, disaster prevention, emergency planning, and disease monitoring; control systems used in, for example, naturalizing intelligent cities through energy and traffic control and management of urban agriculture; applications that promote human empathy focused on diminishing violence and exclusion and increasing well-being. P romoting the potential role of A/IS in sustainable development by collaboration between national and international government agencies and nongovernmental organizations (NGOs) in technology sectors.144 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development A nalyzing the cost of and proposing strategies for publicly providing internet access for all, as a means of diminishing the gap in A/IS potential benefit to humanity, particularly between urban and rural populations in HIC and LMIC alike. I nvesting in the documentation and dissemination of innovative applications of A/IS that advance the resolution of identified societal issues and the SDGs. R esearching sustainable energy to power A/IS computational capacity. I nvesting in the development of transparent monitoring frameworks to track the concrete results of donations by international organizations, corporations, independent agencies, and the State, to ensure efficiency and accountability in applied A/IS. D eveloping national legal, policy, and fiscal measures to encourage competition in the A/IS domestic markets and the flourishing of scalable A/IS applications. I ntegrating the SDGs into the core of private sector business strategies and adding SDG indicators to companies key performance indicators, going beyond corporate social responsibility (CSR). A pplying the well-being indicators10 to evaluate A/IS impact from multiple perspectives in HIC and LMIC alike. Further Resources R . Van Est and J.B.A. Gerritsen, with assistance of L. Kool, Human Rights in the Robot Age: Challenges arising from the use of Robots, Artificial Intelligence and Augmented Reality Expert Report written for the Committee on Culture, Science, Education and Media of the Parliamentary Assembly of the Council of Europe (PACE), The Hague: Rathenau Instituut W orld Economic Forum Global Future Council on Human Rights 2016-18, White Paper: How to Prevent Discriminatory Outcomes in Machine Learning, World Economic Forum, March Uni ted Nations General Assembly, Transforming Our World: The 2030 Agenda for Sustainable Development (A/RES/70/21 October Preamble. A_RES_70_1_E.pdf . U nited Nations Global Pulse, Big Data for Development: Challenges and Opportunities, 145 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Issue: A/IS are often viewed only as having impact in market contexts, yet these technologies also have an impact on social relations and culture. Background A/IS are expected to have an impact beyond market domains and business models, diffusing throughout the global society. For instance, A/IS have and will impact social relationships in a way similar to how mobile phones changed our daily lives, reflecting directly on our culture, customs, and language. The extent and direction of this impact is not yet clear, but documented experience in HIC and high internet-penetration environments of trolls, fake news, and cyberbullying on social media offer a cautionary tale. 11 Depression, social isolation, aggression, and the dissemination of violent behavior with damage to human relations, so extreme that, in some cases, it has resulted in suicide, are all correlated with the internet. 12 As an example, the technology for smart homes has been used for inflicting domestic violence by remotely locking doors, turning off heat/AC, and otherwise harassing a partner. This problem could be easily extended to include elder and child abuse. 13 Measures need to be developed to prevent A/IS from contributing to the emergence or amplification of social disorders. Recommendations To understand the impact of A/IS on society, it is necessary to consider product and process innovation, as well as wider sociocultural and ethical implications, from a global perspective, including the following: E xploring the development of algorithms capable of detecting and reporting discrimination, cyberbullying, deceptive content and identities, etc., and of notifying competent authorities; recognizing that the use of such algorithms must address ethical concerns related to algorithm explainability as well as take into account the risk to certain aspects of human rights, notably to privacy and freedom from oppression. D eveloping a globally recognized professional Code of Ethics with and for technology companies. I dentifying social disorders, such as depression, anxiety, psychological violence, political manipulation, etc., correlated with the use of A/IS-based technologies as a world health problem; monitoring and measuring their impact. E laborating metrics measuring how, where and on whom there is a cultural impact of new A/IS-based technologies. 146 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Further Resources T . Luong, Thermostats, Locks and Lights: Digital Tools of Domestic Abuse, The New York Times , June 23, 2018, nytimes.com/2018/06/23/technology/smart-home-devices-domestic-abuse.html . J . Naughton, The internet of things has opened up a new frontier of domestic abuse. The Guardian, July M . Pianta, Innovation and Employment, Handbook of Innovation . Oxford, U.K.: Oxford University Press, M .J. Salganik, Bit by Bit. Princeton, NJ: Princeton University Press J . Torresen, A Review of Future and Ethical Perspectives of Robotics and AI Frontiers in Robotics and AI, Jan. 15, [Online]. Available: Nov. 1, 2018]. Issue: The right to truthful information is key to a democratic society and to achieving sustainable development and a more equal world, but A/IS poses risks to this right that must be managed. Background Social media have become the dominant technological infrastructure for the dissemination of information such as news, opinion, advertising, etc., and are currently in the vanguard of the movement toward customized/targeted information based on user profiling that involves significant use of A/IS techniques. Analysis of opinion polls and trends in social networks, blogs, etc., and of the emotional response to news items can be used for the purposes of manipulation, facilitating both the selection of news that guides public opinion in the desired direction and the practice of sensationalism. The ""personalization of the consumer experience"", that is, the adaptation of articles to the interests, political vision, cultural level, education, and geographic location of the reader, is a new challenge for the journalism profession that expands the possibilities of manipulation. The information infrastructure is currently lacking in transparency, such that it is difficult or impossible to know (except perhaps for the infrastructure operator): w hat private information is being collected for user profiling and by whom, w hich groups are targeted and by whom, w hat information has been received by any given targeted group, w ho financed the creation and dissemination of this information, t he percentage of the information being disseminated by bots, and w ho is financing these bots. Many actors have found this opaque infrastructure ideal for spreading politically motivated disinformation, which has a negative 147 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development effect on the creation of a more equal world, democracy, and the respect for fundamental rights. This disinformation can have tragic consequences. For instance, human rights groups have unearthed evidence that the military authorities of Myanmar used Facebook for inciting hatred against the Rohingya Muslim minority, hatred which facilitated an ethnic cleansing campaign and the murder of up to 50,000 people. 14 The UN determined that these actions constituted genocide, crimes against humanity, and war crimes. 15 Recommendations To protect democracy, respect fundamental rights, and promote sustainable development, governments should implement a legislative agenda which prevents the spread of misinformation and hate speech, by: E nsuring more control and transparency in the use of A/IS techniques for user profiling in order to protect privacy and prevent user manipulation. U sing A/IS techniques to detect untruthful information circulating in the infrastructures, overseen by a democratic body to prevent potential censorship. O bliging companies owning A/IS infrastructures to provide more transparency regarding their algorithms, sources of funding, services, and clients. D efining a new legal status somewhere between ""platforms"" and ""content providers"" for A/IS infrastructures. R eformulating the deontological codes of the journalistic profession to take into account the intensive use of A/IS techniques foreseen in the future. P romoting the right to information in official documents, and developing A/IS techniques to automate journalistic tasks such as verification of sources and checking the accuracy of the information in official documents, or in the selection, hierarchy, assessment, and development of news, thereby contributing to objectivity and reliability. Further Resources M . Broussard, Artificial Iintelligence for Investigative Reporting: Using an expert system to enhance journalists ability to discover original public affairs stories. Digital Journalism, vol. 3, no. 6, pp. 814-831, M . Carlson, The robotic reporter: Automated journalism and the redefinition of labor, compositional forms, and journalistic authority. Digital Journalism, vol. 3, no. 3, pp. 416-431, A . L pez Barriuso, F. de la Prieta Pintado, . Lozano Murciego, , D. Hern ndez de la Iglesia and J. Revuelta Herrero, JOUR-MAS: A Multi-agent System Approach to Help Journalism Management, vol. 4, no. 4, P . Mozur, A Genocide Incited on Facebook with Posts from Myanmar s Military, The New York Times, Oct. 15 .k-genocide.html UK P arliament, House of Commons, Digital, Culture, Media and Sport Committee Disinformation and fake news : Interim Report, Fifth Report of Session 2017 19UK Parliament, Published on July 29, 148 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Section 2 Equal Availability Issue: Vastly different power structures among and within countries create risk that A/IS deployment accelerates, rather than reduces, inequality in the pursuit of a sustainable future. It is unclear how LMIC can best implement A/IS via existing resources and take full advantage of the technology s potential to achieve a sustainable future. Background The potential use of A/IS to create sustainable economic growth for LMIC is uniquely powerful. Yet, many of the debates surrounding A/IS take place within HIC, among highly educated and financially secure individuals. It is imperative that all humans, in any condition around the world, are considered in the general development and application of these systems to avoid the risk of bias, excessive inequality, classism, and general rejection of these technologies. With much of the financial and technical resources for A/IS development and deployment residing in HIC, not only are A/IS benefits more difficult to access for LMIC populations, but those A/IS applications that are deployed outside of HIC realities may not be appropriate. This is for reasons of cultural/ethnic bias, language difficulties, or simply an inability to adapt to local internet infrastructure constraints. Furthermore, technological innovation in LMIC comes up against many potential obstacles, which could be considered when undertaking initiatives aimed at enhancing LMIC access: R eluctance to provide open source licensing of technological development innovations, L ack of the human capital and knowledge required to adapt HIC-developed technologies to resolving problems in the LMIC context, or to develop local technological solutions to these problems, R etention of A/IS capacity in LMIC due to globally uncompetitive salaries, L ack of infrastructure for deployment, and difficulties in taking technological solutions to where they are needed, L ack of organizational and business models for adapting technologies to the specific needs of different regions, L ack of active participation of the target population, L ack of political will to allow people to have access to technological resources, E xistence of oligopolies that hinder new technological development, L ack of inclusive and high-quality education at all levels, and B ureaucratic policies ill-adapted to highly dynamic scenarios.149 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development For A/IS capacities and benefits to become equally available worldwide, training, education, and opportunities should be provided particularly for LMIC. Currently, access to products that facilitate A/IS research of timely topics is quite limited for researchers in LMIC, due to cost considerations. If A/IS capacity and governance problems, such as relevant laws, policies, regulations, and anti-corruption safeguards, are addressed, LMIC could have the ability to use A/IS to transform their economies and leapfrog into a new era of inclusive growth. Indeed, A/IS itself can contribute to good governance when applied to the detection of corruption in state and banking institutions, one of the most serious recognized constraints to investment in LMIC. Particular attention, however, must be paid to ensure that the use of A/IS is for the common good especially in the context of LMIC and does not reinforce existing socioeconomic inequities through systematic discriminatory bias in both design and application, or undermine fundamental rights through, among other issues, lax data privacy laws and practice. Recommendations A/IS benefits should be equally available to populations in HIC and LMIC, in the interest of universal human dignity, peace, prosperity, and planet protection. Specific measures for LMIC should include: D eploying A/IS to detect fraud and corruption, to increase the transparency of power structures, to contribute to a favorable investment, governance, and innovation environment. S upporting LMIC in the development of their own A/IS strategies, and in the retention or return of their A/IS talent to prevent brain drain . E ncouraging global standardization/ harmonization and open source A/IS software. P romoting distribution of knowledge and wealth generated by the latest A/IS, including through formal public policy and financial mechanisms to advance equity worldwide. D eveloping public datasets to facilitate the access of people from LMIC to data resources to facilitate their applied research, while ensuring the protection of personal data. C reating A/IS international research centers in every continent, that promote culturally appropriate research, and allow the remote access of LMIC's communities to high-end technology. 16 F acilitating A/IS access in LMIC through online courses in local languages. E nsuring that, along with the use of A/IS, discussions related to identity, platforms, and blockchain are conducted, such that core enabling technologies are designed to meet the economic, social, and cultural needs of LMIC. D iminishing the barriers and increase LMIC access to technological products, including the formation of collaborative networks between developers in HIC and LMIC, supporting the latter in attending global A/IS conferences. 17 P romoting research into A/IS-based technologies, for example, mobile lightweight A/IS applications, that are readily available in LMIC. F acilitating A/IS research and development in LMIC through investment incentives, public-150 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development private partnerships, and/or joint grants, and collaboration between international organizations, government bodies, universities, and research institutes. P rioritizing A/IS infrastructure in international development assistance, as necessary to improve the quality and standard of living and advance progress towards the SDGs in LMIC. R ecognizing data issues that may be particular to LMIC contexts, i.e., insufficient sample size for machine learning which sometimes results in de facto discrimination, and inadequate laws for, and the practice of, data protection. S upporting research on the adaptation of A/IS methods to scarce data environments and other remedies that facilitate an optimal A/IS enabling environment in LMIC. Further Resources A . Akubue, Appropriate Technology for Socioeconomic Development in Third World Countries. The Journal of Technology Studies 26, no. 1, pp. 33 43, O . Ajakaiye and M. S. Kimenyi. Higher Education and Economic Development in Africa: Introduction and Overview. Journal of African Economies 20, no. 3, iii3 iii13, 201 D . Allison-Hope and M. Hodge, ""Artificial Intelligence: A Rights-Based Blueprint for Business, San Francisco: BSF, Aug. 28, 2018 D . E. Bloom, D. Canning, and K. Chan. Higher Education and Economic Development in Africa (Vol. . Washington, DC: World Bank, N . Bloom, Corporations in the Age of Inequality. Harvard Business Review, April 21, C . Dahlman, Technology, Globalization, and Competitiveness: Challenges for Developing Countries. Industrialization in the 21st Century . New York: United Nations, M . Fong, Technology Leapfrogging for Developing Countries. Encyclopedia of Information Science and Technology , 2nd ed. Hershey, PA: IGI Global, 2009 (pp. 3707 . C . B. Frey and M. A. Osborne. The Future of Employment: How Susceptible Are Jobs to Computerisation? (working paper). Oxford, U.K.: Oxford University, B . Hazeltine and C. Bull. Appropriate Technology: Tools, Choices, and Implications. New York: Academic Press, M cKinsey Global Institute. Disruptive Technologies: Advances That Will Transform Life, Business, and the Global Economy (report), May D . Rotman, How Technology Is Destroying Jobs. MIT Technology Review , June 12, R . Sauter and J. Watson. Technology Leapfrogging: A Review of the Evidence, A Report for DFID. Brighton, England: University of Sussex. October 3, The Rich and the Rest. The Economist. October 13, Wealth without Workers, Workers without Wealth. The Economist . October 4, W orld Bank. Global Economic Prospects Technology Diffusion in the Developing World. Washington, DC: World Bank, W orld Development Report Digital Dividends . Washington, DC: World Bank. doi:1596/978-1-4648-0671- W orld Wide Web Foundation Artificial Intelligence: The Road ahead in Low and Middle-income Countries, webfoundation.org, June 151 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Section 3 A/IS and Employment Issue: A/IS are changing the nature of work, disrupting employment, while technological change is happening too fast for existing methods of (re)training the workforce. Background The current pace of technological development will heavily influence changes in employment structure. In order to properly prepare the workforce for such evolution, actions should be proactive and not only reactive. The wave of automation caused by the A/IS revolution will displace a very large share of jobs across domains and value chains. The U.S. automated vehicle case study analyzed in the White House 2016 report Artificial Intelligence, Automation, and the Economy is emblematic of what is at stake: 2 to 1 million existing part- and full-time U.S. jobs are exposed over the next two decades, although the timeline remains uncertain. 18 The risk of unemployment for LMIC is more serious than for developed countries. The industry of most LMIC is labor intensive. While labor may be cheap(er) in LMIC economies, the ripple effects of A/IS and automation will be felt much more than in the HIC economies. The 2016 World Bank Development Report stated that the share of occupations susceptible to automation and A/IS is higher in LMIC than in HIC, where such jobs have already disappeared. In addition, the qualities which made certain jobs easy to outsource to LMIC where wages are lower are those that may make them easy to automate. 19 An offsetting factor is the reality that many LMIC lack the communication, energy, and IT infrastructure required to support highly automated industries. 20 Notwithstanding this reality, the World Bank estimated the automatable share of employment, unadjusted for adoption time lag, for LMIC ranges from 85% in Ethiopia to 62% in Argentina, compared to the OECD average of 57%. 21 In the coming decades, the automation wave calls for higher investment and the transformation of labor market capacity development programs. Innovative and fair ways of funding such an investment are required; the solutions should be designed in cooperation with the companies benefiting from the increase of profitability, thanks to automation. This should be done in a responsible way so that the innovation cycle is not broken, and yet workforce capacity does not fall behind the needs of 21st century employment. At the same time, A/IS and other digital technologies offer real potential to innovate new approaches to job-search assistance, placement, and hiring processes in the age of personalized services. The efficiency of matching labor supply and demand can be tremendously enhanced by the rise of multisided platforms and predictive analytics, provided they do not entrench discrimination. 22 The case of platforms, such as LinkedIn, for instance, with its 470 million 152 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development registered users, and online job consolidators such as indeed.com and Simply Hired, are interesting as an evolution in hiring practices, at least for those able to access the internet. Tailored counseling and integrated retraining programs also represent promising grounds for innovation. In addition, much will have to be done to create fair and effective lifelong skill development/training, infrastructures, and mechanisms capable of empowering millions of people to viably transition jobs, sectors, and potentially locations, and to address differential geographic impacts that exacerbate income and wealth disparities. Effectively enabling the workforce to be more mobile physically, legally, and virtually will be crucial. This implies systemic policy approaches which encompass housing, transportation, licensing, tax incentives, and crucially in the age of A/IS, universal broadband access, especially in rural areas of both HIC and LMIC. Recommendations To thrive in the A/IS age, workers must be provided training in skills that improve their adaptability to rapid technological changes; programs should be available to any worker, with special attention to the low-skilled workforce. Those programs can be private, that is, sponsored by the employer, or publicly and freely offered through specific public channels and government policies, and should be available regardless of whether the worker is in between jobs or still employed. Specific measures include: O ffering new technical programs, possibly earlier than high school, to increase the workforce capacity to close the skills gap and thrive in employment alongside A/IS. C reating opportunities for apprenticeships, pilot programs, and scaling up data-driven evidence-based solutions that increase employment and earnings. S upporting new forms of public-private partnerships involving civil society, as well as new outcome-oriented financial mechanisms, e.g., social impact bonds, that help scale up successful innovations. S upporting partnerships between universities, innovation labs in corporations, and governments to research and incubate startups for A/IS graduates. 23 D eveloping regulations to hold corporations responsible for employee retraining necessary due to increased automation and other technological applications having impact on the workforce. F acilitating private sector initiatives by public policy for co-investment in training and retraining programs through tax incentives. E stablishing and resourcing public policies that assure the survival and well-being of workers, displaced by A/IS and automation, who cannot be retrained. R esearching complementary areas, to lay solid foundations for the transformation outlined above. R equiring more policy research on the dynamics of professional transitions in different labor market conditions. 153 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development R esearching the fairest and most efficient public-private options for financing labor force transformation due to A/IS. D eveloping national and regional future of work strategies based on sound research and strategic foresight. Further Resources V . Cerf and D. Norfors, The People-centered Economy: The New Ecosystem for Work. California: IIIJ Foundation, E xecutive Office of the President. Artificial Intelligence, Automation, and the Economy. December 20, S . Kilcarr, Defining the American Dream for Trucking ... and the Nation, Too, FleetOwner , April 26, M . Mason, Millions of Californians Jobs could be Affected by Automation a Scenario the next Governor has to Address, Los Angeles Times , October 14, O ECD, Labor Market Programs: Expenditure and Participants, OECD Employment and Labor Market Statistics (database), M . Vivarelli, Innovation and Employment: A Survey, Institute for the Study of Labor (IZA) Discussion Paper No. 2621, February Issue: Analysis of the A/IS impact on employment is too focused on the number and category of jobs affected, whereas more attention should be addressed to the complexities of changing the task content of jobs. Background Current attention on automation and employment tends to focus on the sheer number of jobs lost or gained. It is important to focus the analysis on how employment structures will be changed by A/IS, rather than solely dwelling on the number of jobs that might be impacted. For example, rather than carrying out a task themselves, workers will need to shift to supervision of robots performing that task. Other concerns include changes in traditional employment structures, with an increase in flexible, contract-based temporary jobs, without employee protection, and a shift in task composition away from routine/repetitive and toward complex decision-making. This is in addition to the enormous need for the aforementioned retraining. Given the extent of disruption, workforce trends will need to measure time spent unemployed or underemployed, labor force participation rates, and other factors beyond simple unemployment numbers. 154 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development The Future of Jobs 2018 report of the World Economic Forum highlights: ...the potential of new technologies to create as well as disrupt jobs and to improve the quality and productivity of the existing work of human employees. Our findings indicate that, by 2022, augmentation of existing jobs through technology may free up workers from the majority of data processing and information search tasks and may also increasingly support them in high-value tasks such as reasoning and decision-making as augmentation becomes increasingly common over the coming years as a way to supplement and complement human labour. 24 The report predicts the shift in skill demand between today and 2022 will be significant and that proactive, strategic and targeted efforts will be needed to map and incentivize workforce redeployment [and therefore]... investment decisions [on] whether to prioritize automation or augmentation and the question of whether or not to invest in workforce reskilling. 25 Comparing Skills Demand, 2018 Versus 2022, Top Ten Source: Future of Jobs Survey 2018, World Economic Forum, Table 4 TODAY, 2018 TRENDING, 2022 DECLINING, 2022 A nalytical thinking and innovation 2 . C omplex problem- solving C ritical thinking and analysis 4 . A ctive learning and learning strategies 5 . C reativity, originality, and initiative 6 . A ttention to detail, trustworthiness E motional Intelligence R easoning, problem- solving, and ideation L eadership and social influence 10 . C oordination and time management1. A nalytical thinking and innovation 2 . A ctive learning and learning strategies C reativity, originality, and initiative 4 . T echnology design and programming 5 . C ritical thinking and analysis 6 . C omplex problem- solving L eadership and social influence 8 . E motional intelligence R easoning, problem- solving, and ideation S ystems analysis and evaluation1. M anual dexterity, endurance, and precision M emory, verbal, auditory, and spatial abilities M anagement of financial and material resources T echnology installation and maintenance R eading, writing, math, and active listening M anagement of personnel Q uality control and safety awareness C oordination and time-management V isual, auditory, and speech abilities 10 . T echnology use, monitoring, and control155 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Recommendations While there is evidence that robots and automation are taking jobs away in various sectors, a more balanced, granular, analytical, and objective treatment of A/IS impact on the workforce is needed to effectively inform policy making and essential workforce reskilling. Specifics to accomplish this include: C reating an international and independent agency able to properly disseminate objective statistics and inform the media, as well as the general public, about the impact of robotics and A/IS on jobs, tax revenue, growth, 26 and well-being. A nalyzing and disseminating data on how current task content of jobs have changed, based on a clear assessment of the automatability of the occupational description of such jobs. P romoting automation with augmentation, as recommended in the Future of Jobs Report 2018 (see chart on page 154 ), to maximize the benefit of A/IS to employment and meaningful work. I ntegrating more granulated dynamic mapping of the future jobs, tasks, activities, workplace-structures, associated work-habits, and skills base spurred by the A/IS revolution, in order to innovate, align, and synchronize skill development and training programs with future requirements. This workforce mapping is needed at the macro, but also crucially at the micro, levels where labor market programs are deployed. C onsidering both product and process innovation, and looking at them from a global perspective in order to understand properly the global impact of A/IS on employment. P roposing mechanisms for redistribution of productivity increases and developing an adaptation plan for the evolving labor market. Further Resources E . Brynjolfsson and A. McAfee. The Second Age of Machine Intelligence: Work Progress and Prosperity in a Time of Brilliant Technologies. New York, NY: W. W. Norton & Company, P .R. Daugherty, and H.J. Wilson, Human + Machine: Reimagining Work in the Age of AI . Watertown, MA: Harvard Business Review Press, I nternational Federation of Robotics. The Impact of Robots on Productivity, Employment and Jobs, A positioning paper by the International Federation of Robotics, April R ockEU. Robotics Coordination Action for Europe Report on Robotics and Employment, Deliverable D3.1, June 30, W orld Economic Forum, Centre for the New Economy and Society, The Future of Jobs 2018, Geneva: WEF 156 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Section 4 Education for the A/IS Age Issue: Education to prepare the future workforce, in both HIC and LMIC, to design ethical A/IS applications or to have a comparative advantage in working alongside A/IS, is either lacking or unevenly available, risking inequality perpetuated across generations, within and between countries, constraining equitable growth, supporting a sustainable future, and achievement of the SDGs. Background Multiple international institutions, in particular educational engineering organizations,27 have called on universities to play an active role, both locally and globally, in the resolution of the enormous problems that the world faces in securing peace, prosperity, planet protection, and universal human dignity: armed conflict, social injustice, rapid climate change, abuse of human rights, etc. Addressing global social problems is one of the central objectives of many universities, transversal to their other functions, including research in A/IS. UNESCO points out that universities preparation of future scientists and engineers for social responsibility is presently very limited, in view of the enormous ethical and social problems associated with technology. 28 Enhancing the global dimension of engineering in undergraduate and postgraduate A/IS education is necessary, so that students can be prepared as technical professionals, aware of the opportunities and risks that A/IS present, and ready for work anywhere in the world in any sector. Engineering studies at the university and postgraduate levels is just one dimension of the A/IS education challenge. For instance, business, law, public policy, and medical students will also need to be prepared for professions where A/IS are a partner, and to have internalized ethical principles to guide the deployment of such technologies. LMIC need financial and academic support to incorporate global A/IS professional curricula in their own universities, and all countries need to develop the pipeline by preparing elementary and secondary school students to access such professional programs. While the need for curriculum reform is recognized, the impact of A/IS on various professions and socioeconomic contexts is, at this time, both evolving and largely undocumented. Thus, the overhaul of education systems at all levels should be preceded by A/IS research. Much of LMIC education is not globally competitive today, so there is a risk that the global advent of A/IS could negatively affect the chances of young people in LMIC finding 157 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development productive employment, further fueling global inequality. Education systems worldwide have to be reformed and transformed to fit the new demands of the information age, in view of the changing mix of skills demanded from the workforce. 29 In 21st century education, it has been observed that children need less rote knowledge, given so much is instantly accessible on the web and more tools to network and innovate are available; less memory and more imagination should be developed; and fewer physical books and more internet access is required. Young people everywhere need to develop their capacities for creativity, human empathy, ethics, and systems thinking in order to work productively alongside robots and A/IS technologies. Science, Technology, Engineering, Art/design, and Math (STEAM) subjects need to be more extensive and more creatively taught. 30 In addition, research is needed to establish ways that a new subject, empathy, can be added to these crucial 21st century subjects in order to educate the future A/IS workforce in social skills. Instead, in rich and poor countries alike, children are continuing to be educated for an industrial age which has disappeared or never even arrived. LMIC education systems, being less entrenched in many countries, may have the potential to be more flexible than those in HIC. Perhaps A/IS can be harnessed to help educational systems to leapfrog into the 21st century, just as mobile phone technology enabled LMIC leapfrog over the phase of wired communication infrastructure. Recommendations Education with respect to A/IS must be targeted to three sets of students: the general public, present and future professionals in A/IS, and present and future policy makers. To prepare the future workforce to develop culturally appropriate A/IS, to work productively and ethically alongside such technologies, and to advance the UN SDGs, the curricula in HIC and LMIC universities and professional schools require innovation. Equally importantly, preuniversity education systems, starting with early childhood education, need to be reformed to prepare society for the risks and opportunities of the A/IS age, rather than the current system which prepares society for work in an industrial age that ended with the 20th century. Specific recommendations include: P reparing future managers, lawyers, engineers, civil servants, and entrepreneurs to work productively and ethically as global citizens alongside A/IS, through reform of undergraduate and graduate curricula as well as of preschool, primary, and secondary school curricula. This will require: F omenting interaction between universities and other actors such as companies, governments, NGOs, etc., with respect to A/IS research through definition of research priorities and joint projects, subcontracts to universities, participation in observatories, and co-creation of curricula, cooperative teaching, internships/service learning, and conferences/seminars/courses. E stablishing and supporting more multidisciplinary degrees that include 158 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development A/IS, and adapting university curricula to provide a broad, integrated perspective which allows students to understand the impact of A/IS in the global, economic, environmental, and sociocultural domains and trains them as future policy makers in A/IS fields. I ntegrating the teaching of ethics and A/IS across the education spectrum, from preschool to postgraduate curricula, instead of relegating ethics to a standalone module with little direct practical application. P romoting service learning opportunities that allow A/IS undergraduate and graduate students to apply their knowledge to meet the needs of a community. C reating international exchange programs, through both private and public institutions, which expose students to different cultural contexts for A/IS applications in both HIC and LMIC. C reating experimental curricula to prepare people for information-based work in the 21st century, from preschool through postgraduate education. T aking into account transversal competencies students need to acquire to become ethical global citizens, i.e., critical thinking, empathy, sociocultural awareness, flexibility, and deontological reasoning in the planning and assessment of A/IS curricula. T raining teachers in teaching methodologies suited to addressing challenges imposed in the age of A/IS. S timulating STEAM courses in preuniversity education. E ncouraging high-quality HIC-LMIC collaborative A/IS research in both private and public universities. C onducting research to support innovation in education and business for the A/IS world, which could include: R esearching the impact of A/IS on the governance and macro/micro strategies of companies and organizations, together with those companies, in an interdisciplinary manner which harnesses expertise of both social scientists and technology experts. R esearching the impact of A/IS on the business model for the development of new products and services through the collaborative efforts of management, operations, and the technical research and development function. R esearching how empathy can be taught and integrated into curricula, starting at the preschool level. R esearching how schools and education systems in low-income settings of both HIC and LMIC can leverage their less-entrenched interests to leapfrog into a 21st century-ready education system. 159 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development E stablishing ethics observatories in universities with the purpose of fostering an informed public opinion capable of participating in policy decisions regarding the ethics and social impact of A/IS applications. C reating professional continuing education and employment opportunities in A/IS for current professionals, including through online and executive education courses. C reating educative mass media campaigns to elevate society s ongoing baseline level of understanding of A/IS systems, including what it is, if and how it can be trusted in various contexts, and what are its limitations.Further Resources A BET Computing and Engineering Accreditation Criteria Available at: accreditation-criteria/ A BET, 2017 ABET Impact Report, Working Together for a Sustainable Future , e mlyon business school, Artificial Intelligence in Management (AIM) Institute U NESCO, The UN Decade of Education for Sustainable Development, Shaping the Education of Tomorrow . UNESCO 160 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Section 5 A/IS and Humanitarian Action Issue: A/IS are contributing to humanitarian action to save lives, alleviate suffering, and maintain human dignity both during and in the aftermath of man-made crises and natural disasters, as well as to prevent and strengthen preparedness for the occurrence of such situations. However, there are ethical concerns with both the collection and use of data during humanitarian emergencies. Background There have been a number of promising A/IS applications that relieve suffering in humanitarian crises, such as extending the reach of the health system by using drones to deliver blood to remote parts of Rwanda, 31 locating and removing landmines,32 efforts to use A/IS to track movements and population survival needs following a natural disaster, and to meet the multiple management requirements of refugee camps. 33 There are also promising developments using A/IS and robotics to assist people with disabilities to recover mobility, and robots to rescue people trapped in collapsed buildings. 34 A/IS are also being used to monitor conflict zones and to enable early warning systems. 35 For example, Microsoft has partnered with the UN Human Rights Office of the High Commissioner (OHCHR) to use big data in order to track and analyze human rights violations in conflict zones. 36 Machine learning is being used for improved decision-making regarding asylum adjudication and refugee resettlement, with a view to increasing successful integration between refugees and host communities. 37 In addition, there is evidence that a recent growth in human empathy has increased well-being while diminishing psychological and physical violence, 38 inspiring some researchers to look for ways of harnessing the power of A/IS to introduce more empathy and less violence into society. T he design and ethical deployment of these technologies in crisis settings are both essential and challenging. Large volumes of both personally identifiable and demographically identifiable data are collected in fragile environments, where tracking of individuals or groups may compromise their security if data privacy cannot be assured. Consent to data use is also impractical in such environments, yet crucial for the respect of human rights. 161 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Recommendations The potential for A/IS to contribute to humanitarian action to save and improve lives should be prioritized for research and development, including by organizing global research challenges, while also building in safeguards to protect the creation, collection, processing, sharing, use, and disposal of information, including data from and about individuals and populations. Specific recommendations include: P romoting awareness of the vulnerable condition of certain communities around the globe and the need to develop and use A/IS applications for humanitarian purposes. E laborating competitions and challenges in high impact conferences and university hackathons to engage both technical and nontechnical communities in the development of A/IS for humanitarian purposes and to address social issues. S upport civil society groups who organize themselves for the purpose of A/IS research and advocacy to develop applications to benefit humanitarian causes. 39 D eveloping and applying ethical standards for the collection, use, sharing, and disposal of data in fragile settings. F ollowing privacy protection frameworks for pressing humanitarian situations that ensure the most vulnerable are protected. 40 S etting up clear ethical frameworks for exceptional use of A/IS technologies in life-saving humanitarian situations, compared to ""normal"" situations. 41 S timulating the development of low-cost and open source solutions based on A/IS to address specific humanitarian problems. T raining A/IS experts in humanitarian action and norms, and humanitarian practitioners to catalyze collaboration in designing, piloting, developing, and implementing A/IS technologies for humanitarian purposes. Forging public-private A/IS participant alliances that develop crisis scenarios in advance. W orking on cultural and contextual acceptance of any A/IS introduced during emergencies. D ocumenting and developing quantifiable metrics for evaluating the outcomes of humanitarian digital projects, and educating the humanitarian ecosystem on the same. 162 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Further Resources E . Prestes et al., ""The 2016 Humanitarian Robotics and Automation Technology Challenge [Competitions],"" in IEEE Robotics & Automa-tion Magazine , vol. 23, no. 3, pp. 23-24, Sept. - ber=7565655 L . Marques et al., ""Automation of humanitarian demining: The 2016 Humanitarian Robotics and Automation Technology Challenge,"" 2016 International Conference on Robotics and Automation for Humanitarian Applications (RAHA) , Kollam, 2016, pp. 1- - plore.ieee.org/stamp/stamp.jsp?tp=&arnum - ber=7931893&isnumber=7931858 C YBATHLON 2020 Preliminary Race Task Descriptions ch/cybathlon-2020/preliminary-race-task-descriptions.html C YBATHLON Scientific Publications I mmigration Policy Lab (IPL), Harnessing Big Data to Improve Refugee Resettlement H arvard Humanitarian Initiative, The Signal Code , J .A. Quinn, et al., Humanitarian applications of machine learning with remote-sensing data: review and case study in refugee settlement mapping Philosophical Transactions of the Royal Society A, 376 20170363; DOI: 1098/rsta.Aug. 6, Hu manitarian Innovation Guide: https:// higuide.elrha.org/ , P . Meier, Digital Humanitarians: How Big Data is Changing the Face of Humanitarian Response. Florida: CRC Press, Technology for human rights: UN Human Rights Office announces landmark partnership with Microsoft M . Luengo-Oroz, 10 big data science challenges facing humanitarian organizations, UNHCR, Nov. 22, O ptic Technologies, Press Release, Vatican Hack 2018 Results, 18 March 2018, which announced winning AI applications to benefit migrants and refugees as well as social inclusion and interfaith dialogue, 163 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Thanks to the Contributors We wish to acknowledge all of the people who contributed to this chapter. The A/IS for Sustainable Development Committee E lizabeth D. Gibbons (Chair) Senior Fellow and Director of the Child Protection Certificate Program, FXB Center for Health and Human Rights, Harvard T.H. Chan School of Public Health K ay Firth-Butterfield (Founding Co-Chair) Project Head, AI and Machine Learning at the World Economic Forum. Founding Advocate of AI-Global; Senior Fellow and Distinguished Scholar, Robert S. Strauss Center for International Security and Law, University of Texas, Austin; Co-Founder, Consortium for Law and Ethics of Artificial Intelligence and Robotics, University of Texas, Austin; Partner, Cognitive Finance Group, London, U.K. R aj Madhavan (Founding Co-Chair) Founder & CEO of Humanitarian Robotics Technologies, LLC, Maryland, U.S.A. R onald C. Arkin Regents' Professor & Director of the Mobile Robot Laboratory; Associate Dean for Research & Space Planning, College of Computing, Georgia Institute of Technology J oanna J. Bryson Reader (Associate Professor), University of Bath, Intelligent Systems Research Group, Department of Computer Science R enaud Champion Director of Emerging Intelligences, emlyon business school; Founder of Robolution Capital & CEO of PRIMNEXT C handramauli Chaudhuri Senior Data Scientist; Fractal Analytics R ozita Dara Assistant Professor, Principal Investigator of Data Management and Data Governance program, School of Computer Science, University of Guelph, Canada S cott L. David Director of Policy at University of Washington Center for Data Management and Privacy Governance LabInformation Assurance and Cybersecurity J ia He Executive Director of Toutiao Research (Think Tank), Bytedance Inc. W illiam Hoffman Associate director and head of Data-Driven Development, The World Economic Forum M ichael Lennon Senior Fellow, Center for Excellence in Public Leadership, George Washington University; Co-Founder, Govpreneur.org; Principal, CAIPP.org (Consortium for Action Intelligence and Positive Performance); Member, Wellbeing Metrics Standard for Ethical Artificial Intelligence and Autonomous Systems Committee M iguel Luengo-Oroz Chief Data Scientist, United Nations Global Pulse. 164 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development A ngeles Manjarr s Professor of the Department of Artificial Intelligence of the Spanish National Distance-Learning University N icolas Miailhe Co-Founder & President, The Future Society; Member, AI Expert Group at the OECD; Member, Global Council on Extended Intelligence; Senior Visiting Research Fellow, Program on Science Technology and Society at Harvard Kennedy School. Lecturer, Paris School of International Affairs (Sciences Po). Visiting Professor, IE School of Global and Public Affairs R oya Pakzad Research Associate and Project Leader in Technology and Human Rights, Global Digital Policy Incubator (GDPi), Stanford University E dson Prestes Professor, Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Brazil; Head, Phi Robotics Research Group, UFRGS; CNPq Fellow S imon Pickin Professor, Dpto. de Sistemas Inform ticos y Computaci n, Facultad de Inform tica, Universidad Complutense de Madrid, Spain R ose Shuman Partner at BrightFront Group & Founder, Question Box H ruy Tsegaye One of the founders of iCog Labs; a pioneer company in East Africa to work on Research and Development of Artificial General Intelligence, Ethiopia For a full listing of all IEEE Global Initiative Members, visit standards.ieee.org/content/dam/ ieee-standards/standards/web/documents/other/ec_bios.pdf . F or information on disclaimers associated with EAD1e, see How the Document Was Prepared.165 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development 1 See, for example, the writing of T. Piketty, Capital in the Twenty-First Century (Cambridge: Belknap Press . 2 S ee preamble of the United Nations General Assembly, Transforming our world: the 2030 Agenda for Sustainable Development (A/RES/70/21 October : This Agenda is a plan of action for people, planet and prosperity. It also seeks to strengthen universal peace in larger freedom. We recognize that eradicating poverty in all its forms and dimensions, including extreme poverty, is the greatest global challenge and an indispensable requirement for sustainable development. All countries and all stakeholders, acting in collaborative partnership, will implement this plan. We are resolved to free the human race from the tyranny of poverty and want and to heal and secure our planet. We are determined to take the bold and transformative steps which are urgently needed to shift the world on to a sustainable and resilient path. As we embark on this collective journey, we pledge that no one will be left behind. The 17 Sustainable Development Goals and 169 targets which we are announcing today demonstrate the scale and ambition of this new universal Agenda. 3 I bid, paragraph 4 A /IS has the potential to advance positive change toward all seventeen 2030 Sustainable Development Goals, which are: Goal End poverty in all its forms everywhereGoal End hunger, achieve food security and improved nutrition and promote sustainable agriculture Goal Ensure healthy lives and promote well-being for all at all ages Goal Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all Goal Achieve gender equality and empower all women and girls Goal Ensure availability and sustainable management of water and sanitation for all Goal Ensure access to affordable, reliable, sustainable and modern energy for all Goal Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all Goal Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation Goal Reduce inequality within and among countries Goal Make cities and human settlements inclusive, safe, resilient and sustainable Goal Ensure sustainable consumption and production patterns Goal Take urgent action to combat climate change and its impactsEndnotes166 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development Goal Conserve and sustainably use the oceans, seas and marine resources for sustainable development Goal Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss Goal Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels Goal Strengthen the means of implementation and revitalize the global partnership for sustainable development Source: United Nations General Assembly, Transforming our world: the 2030 Agenda for Sustainable Development (A/RES/70/21 October p. 14 5 U nited Nations Secretary General The road to dignity by ending poverty, transforming all lives and protecting the planet United Nations, A/69/700, 4 December 2014, pp. 25-27 6 I nternet World Stats internetworldstats.com/stats.htm , accessed 17 May 7 ( Internet of Things, Privacy and Security in a Connected World, FTC, https:// les/documents/reports/federal-trade-commission-staff- report-november-2013-workshop-entitled-internet-things-privacy/150127iotrpt.pdf )8 W orld Economic Forum Global Future Council on Human Rights 2016-18 White Paper: How to Prevent Discriminatory Outcomes in Machine Learning (WEF: March . 9 World Wide Web Foundation Artificial Intelligence: the Road ahead in Low and Middle-income Countries (June webfoundation.org) p.13 10 S ee the Well-being chapter of Ethically Aligned Design , First Edition 11 S ee, for example, S. Vosougi, D. Roy, and S. Aral, The spread of true and false news online Science 09 Mar Vol. 359, Issue 6380, pp. 1 146-1 151 and M. Fox, Fake News:Lies spread faster on social media than Truth does NBC Health News, 8 March 2018 ; Cyberbullying Research Center: Summary of Cyberbullying Research 2004-2016 and TeenSafe Cyberbullying Facts and Statistics TeenSafe October 4, 2016, A. Hutchison, Social Media Still Has a Fake News Problem and Digital Literacy is Largely to Blame Social Media Today, October 5, 2018 D.D. Luxton , J.D. June, and J. M. Fairall, Social Media and Suicide: A Public Health Perspective , Am J Public Health . 2012 May; 102(Suppl : S195 S200. J. Twege, T. E. Joiner, M.L. Rogers, Increases in Depressive Symptoms, Suicide-Related Outcomes, and Suicide Rates Among U.S. Adolescents After 2010 and Links 167 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development to Increased New Media Screen Time Clinical Psychological Science, November 14, 2017 177/2167702617723376 12 D .D. Luxton, J.D. June, and J. M. Fairall, Social Media and Suicide: A Public Health Perspective , Am J Public Health. 2012 May; 102(Suppl : S195 S200. J. Twege, T. E. Joiner, M.L. Rogers, Increases in Depressive Symptoms, Suicide-Related Outcomes, and Suicide Rates Among U.S. Adolescents After 2010 and Links to Increased New Media Screen Time Clinical Psychological Science, November 14, 2017 177/2167702617723376 13 T . Luong, Thermostats, Locks and Lights: Digital Tools of Domestic Abuse. The New York Times , June 23, 2018, com/2018/06/23/technology/smart-home-devices-domestic-abuse.html 14 P . Mozur, A Genocide incited on Facebook with posts from Myanmar s Military , The New York Times , October 15, com/2018/10/15/technology/myanmar-facebook-genocide.html 15 Uni ted Nations Human Rights Council Human rights situations that require the Council s attention Report of the independent international fact-finding mission on Myanmar* ( A/HRC/39/64, 12 September 16 See for example Google AI in Ghana https:// 17 S ee Artificial Intelligence: the Road ahead in Low and Middle-income Countries 18 E xecutive Office of the President of the United States. Artificial Intelligence, Automation, and the Economy. December 20, page 19 F rom World Wide Web Foundation Artificial Intelligence: The Road ahead in Low and Middle-income Countries (June webfoundation.org) page 20 Ibid. 21 W orld Bank, World Development Report Digital Dividends. Washington, DC: World Bank. doi:1596/978-1-4648-0671-1 page 22 S ee for example: J. Dasten, Amazon scraps secret AI recruiting tool that showed bias against women Reuters Business News October 9, 2018, 23 F or example, The Vector Institute, CIFAR and the Legal Innovation Group at Ryerson University. See and legalinnovationzone.ca . 24 W orld Economic Forum, Centre for the New Economy and Society the Future of Jobs 2018 (Geneva: WEF p. 25 I bid, page 9 26 I t must be noted that the OECD is already engaged in this work as well as are some government bodies. See 168 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. A/IS for Sustainable Development 27 UNESCO, WHO, ABET, Bologna Follow-Up Group Secretariat for the European Higher Education Area 28 U NESCO, The UN Decade of Education for Sustainable Development, Shaping the Education of Tomorrow. (UNESCO: Paris . 29 See F uture of Jobs Report 2018 Survey table, p. 30 N ational Math and Science Initiative, STEM Education and Workforce, 2014 31 a rticles/2018-08-16/this-27-year-old-launches- drones-that-deliver-blood-to-rwanda-s-hospitals 32 h ttps:// business/2015/may/25/robots-rescue-lethal-rehabilitation-landmines-drones 33 S ee for example, C. Fey, Tech can improve lives in refugee camps Cambridge Network, 10 May 2018 34 h ttps:// searching-survivors-mexico-earthquake-snake-robots rescue-robot-algorithm.html 35 h ttp://focus.barcelonagse.eu/can-machine- learning-help-policymakers-detect-conflict/ 36 "" United Nations Human Rights Office of the High Commissioner, press release, ""Technology for human rights: UN Human Rights Office announces landmark partnership with Microsoft 16 May "" 37 F or example, researchers at Stanford University are running a pilot project to develop machine learning algorithms for a better resettlement program. To train their algorithm, the Immigration Policy Lab (IPL) at Stanford University and ETH Zurich gathered data from refugee resettlement agencies in the US and Switzerland. The model is optimized based on refugees background and skill sets to match them to a host city in which the individual has a higher chance of finding employment. 38 S ee for example S. Pinker, The Better Angels of Our Nature: Why Violence has Declined (Penguin and R. Krznaric, Empathy: How it matters and how to get it. (Perigee . 39 S ee for example TechToronto: techtoronto.org and #AI and Big Data 40 S ee for example Harvard Humanitarian Initiative Signal Code 41 S ee Humanitarian Innovation Guide: Values into Autonomous and Intelligent SystemsThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. 169Society has not established universal standards or guiding principles for embedding human values and norms into autonomous and intelligent systems (A/IS) today. But as these systems are instilled with increasing autonomy in making decisions and manipulating their environment, it is essential that they are designed to adopt, learn, and follow the norms and values of the community they serve. Moreover, their actions should be transparent in signaling their norm compliance and, if needed, they must be able to explain their actions. This is essential if humans are to develop appropriate levels of trust in A/IS in the specific contexts and roles in which A/IS function. At the present time, the conceptual complexities surrounding what values are (Hitlin and Piliavin 2004 1; Malle and Dickert 20072; Rohan 20003; Sommer make it difficult to envision A/IS that have computational structures directly corresponding to social or cultural values such as security, autonomy, or fairness . It may be a more realistic goal to embed explicit norms into such systems. Since norms are observable in human behavior, they can therefore be represented as instructions to act in defined ways in defined contexts, for a specific community from family to town to country and beyond. A community s network of social and moral norms is likely to reflect the community s values, and A/IS equipped with such a network would, therefore, also reflect the community s values. For discussion of specific values that are critical for ethical considerations of A/IS, see the chapters of Ethically Aligned Design, Personal Data and Individual Agency and Well-being . Norms are typically expressed in terms of obligations and prohibitions, and these can be expressed computationally (Malle, Scheutz, and Austerweil 2017 5; V zquez-Salceda, Aldewereld and Dignum . They are typically qualitative in nature, e.g., do not stand too close to people. However, the implementation of norms also has a quantitative component the measurement of the physical distance we mean by too close , and the possible instantiations of the quantitative component technically enable the qualitative norm. 170 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsTo address the broad objective of embedding norms and, by implication, values into A/IS, this chapter addresses three more concrete goals: I dentifying the norms of the specific community in which the A/IS operate, C omputationally implementing the norms of that community within the A/IS, and E valuating whether the implementation of the identified norms in the A/IS are indeed conforming to the norms reflective of that community. Pursuing these three goals represents an iterative process that is sensitive to the purpose of the A/IS and to its users within a specific community. It is understood that there may be conflicts of values and norms when identifying, implementing, and evaluating these systems. Such conflicts are a natural part of the dynamically changing and renegotiated norm systems of any community. As a result, we advocate for an approach in which systems are designed to provide transparent signals describing the specific nature of their behavior to the individuals in the community they serve. Such signals may include explanations or offers for inspection and must be in a language or form that is meaningful to the community. Further Resources S . Hitlin and J. A. Piliavin, Values: Reviving a Dormant Concept. Annual Review of Sociology 30, pp.359 393, B . F. Malle, and S. Dickert. Values, in Encyclopedia of Social Psychology , edited by R. F. Baumeister and K. D. Vohs. Thousand Oaks, CA: Sage, B . F. Malle, M. Scheutz, and J. L. Austerweil. Networks of Social and Moral Norms in Human and Robot Agents, in A World with Robots: International Conference on Robot Ethics : ICRE 2015, edited by M. I. Aldinhas Ferreira, J. Silva Sequeira, M. O. Tokhi, E. E. Kadar, and G. S. Virk, 3 Cham, Switzerland: Springer International Publishing, M . J. Rohan, A Rose by Any Name? The Values Construct. Personality and Social Psychology Review 4, pp. 255 277, U . Sommer, Werte: Warum Man Sie Braucht, Obwohl es Sie Nicht Gibt. [Values. Why We Need Them Even Though They Don t Exist.] Stuttgart, Germany: J. B. Metzler, J . V zquez-Salceda, H. Aldewereld, and F. Dignum. Implementing Norms in Multiagent Systems, in Multiagent System Technologies. MATES 2004, edited by G. Lindemann, Denzinger, I. J. Timm, and R. Unland. (Lecture Notes in Computer Science, vol. ) Berlin: Springer, 171 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsSection 1 Identifying Norms for Autonomous and Intelligent Systems We identify three issues that must be addressed in the attempt to identify norms and corresponding values for A/IS. The first issue asks which norms should be identified and with which properties. Here we highlight context specificity as a fundamental property of norms. Second, we emphasize another important property of norms: their dynamically changing nature (Mack 2018 , which requires A/IS to have the capacity to update their norms and learn new ones. Third, we address the challenge of norm conflicts that naturally arise in a complex social world. Resolving such conflicts requires priority structures among norms, which help determine whether, in a given context, adhering to one norm is more important than adhering to another norm, often in light of overarching standards, e.g., laws and international humanitarian principles. Issue Which norms should be identified? Background If machines engage in human communities, then those agents will be expected to follow the community s social and moral norms. A necessary step in enabling machines to do so is to identify these norms. But which norms should be identified? Laws are publicly documented and therefore easy to identify, so they can be incorporated into A/IS as long as they do not violate humanitarian or community moral principles. Social and moral norms are more difficult to ascertain, as they are expressed through behavior, language, customs, cultural symbols, and artifacts. Most important, communities ranging from families to whole nations differ to various degrees in the norms they follow. Therefore, generating a universal set of norms that applies to all A/IS in all contexts is not realistic, but neither is it advisable to completely tailor the A/IS to individual preferences. We suggest that it is feasible to identify broadly observed norms of communities in which a technology is deployed. Furthermore, the difficulty of generating a universal set of norms is not inconsistent with the goal of seeking agreement over Universal Human Rights (see the General Principles chapter of Ethically Aligned Design ). However, these universal rights are not sufficient for devising A/IS that conform to the specific norms of its community. Universal Human Rights must, however, constrain the kinds of norms that are implemented in the A/IS (cf. van de Poel 2016 . Embedding norms in A/IS requires a careful understanding of the communities in which the A/IS are to be deployed. Further, even within a particular community, different types of A/IS will demand different sets of norms. The relevant 172 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systemsnorms for self-driving vehicles, for example, may differ greatly from those for robots used in healthcare. Thus, we recommend that to develop A/IS capable of following legal, social, and moral norms, the first step is to identify the norms of the specific community in which the A/IS are to be deployed and, in particular, norms relevant to the kinds of tasks and roles for which the A/IS are designed. Even when designating a narrowly defined community, e.g., a nursing home, an apartment complex, or a company, there will be variations in the norms that apply, or in their relative weighting. The norm identification process must heed such variation and ensure that the identified norms are representative, not only of the dominant subgroup in the community but also of vulnerable and underrepresented groups. The most narrowly defined community is a single person, and A/IS may well have to adapt to the unique expectations and needs of a given individual, such as the arrangement of a disabled person s living accommodations. However, unique individual expectations must not violate norms in the larger community. Whereas the arrangement of someone s kitchen or the frequency with which a care robot checks in with a patient can be personalized without violating any community norms, encouraging the robot to use derogatory language to talk about certain social groups does violate such norms. In the next section, we discuss how A/IS might handle such norm conflicts. Innovation projects and development efforts for A/IS should always rely on empirical research, involving multiple disciplines and multiple methods; to investigate and document both context- and task-specific norms, spoken and unspoken, that typically apply in a particular community. Such a set of empirically identified norms should then guide system design. This process of norm identification and implementation must be iterative and revisable. A/IS with an initial set of implemented norms may betray biases of original assessments (Misra, Zitnick, Mitchell, and Girshick 2016 that can be revealed by interactions with, and feedback from, the relevant community. This leads to a process of norm updating, which is described next in Issue Recommendation To develop A/IS capable of following social and moral norms, the first step is to identify the norms of the specific community in which the A/IS are to be deployed and, in particular, norms relevant to the kinds of tasks and roles that the A/IS are designed for. This norm identification process must use appropriate scientific methods and continue through the system's life cycle. Further Resources M ack, Ed., Changing social norms. Social Research: An International Quarterly, 85, no.1, 1 271, I . Misra, C. L. Zitnick, M. Mitchell, and R. Girshick, (. Seeing through the human reporting bias: Visual Classifiers from Noisy Human-Centric Labels. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2930 doi :1109/CVPR.320 I. van de Poel, An Ethical Framework for Evaluating Experimental Technology , Science and Engineering Ethics , 22, no. 3,pp. 667- 686, 173 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsIssue The need for norm updating Background Norms are not static. They change over time, in response to social progress, political change, new legal measures, or novel opportunities (Mack 2018 . Norms can fade away when, for whatever reasons, fewer and fewer people adhere to them. And new norms emerge when technological inno-vation invites novel behaviors and novel standards, e.g., cell phone use in public. A/IS should be equipped with a starting set of social and legal norms before they are deployed in their intended community (see Issue , but this will not suffice for A/IS to behave appropriately over time. A/IS or the designers of A/IS, must be adept at identifying and adding new norms to its starting set, because the initial norm identification process in the community will undoubtedly have missed some norms and because the community s norms change. Humans rely on numerous capacities to update their knowledge of norms and learn new ones. They observe other community members behavior and are sensitive to collective norm change; they explicitly ask about new norms when joining new communities, e.g., entering college or a job in a new town; and they respond to feedback from others when they exhibit uncertainty about norms or have violated a norm. Likewise, A/IS need multiple capacities to improve their own norm knowledge and to adapt to a community s dynamically changing norms. These capacities include: P rocessing behavioral trends by members of the target community and comparing them to trends predicted by the baseline norm system, A sking for guidance from the community when uncertainty about applicable norms exceeds a critical threshold, R esponding to instruction from the community members who introduce a robot to a previously unknown context or who notice the A/IS uncertainty in a familiar context, and R esponding to formal or informal feedback from the community when the A/IS violate a norm. The modification of a normative system can occur at any level of the system: it could involve altering the priority weightings between individual norms, changing the qualitative expression of a norm, or altering the quantitative parameters that enable the norm. We recommend that the system s norm changes be transparent. That is, the system or its designer should consult with users, designers, and community representatives when adding new norms to its norm system or adjusting the priority or content of existing norms. Allowing a system to learn new norms without public or expert review has detrimental consequences (Green and Hu 2018 . The form of consultation 174 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systemsand the specific review process will vary by machine sophistication e.g., linguistic capacity and function/role, or a flexible social companion versus a task-defined medical robot and best practices will have to be established. In some cases, the system may document its dynamic change, and the user can consult this documentation as desired. In other cases, explicit announcements and requests for discussion with the designer may be appropriate. In yet other cases, the A/IS may propose changes, and the relevant human community, e.g., drawn from a representative crowdsourced panel, will decide whether such changes should be implemented in the system. Recommendation To respond to the dynamic change of norms in society A/IS or their designers must be able to amend their norms or add new ones, while being transparent about these changes to users, designers, broader community representatives, and other stakeholders. Further Resources B . Green and L. Hu. The Myth in the Methodology: Towards a Recontextualization of Fairness in ML. Paper presented at the Debates workshop at the 35th International Conference on Machine Learning, Stockholm, Sweden M ack, Ed., Changing social norms, Social Research: An International Quarterly , 85 (1, Special Issue), 1-271, Issue A/IS will face norm conflicts and need methods to resolve them. Background Often, even within a well-specified context, no action is available that fulfills all obligations and prohibitions. Such situations often described as moral dilemmas or moral overload (Van den Hoven 2012 must be computationally tractable by A/IS; they cannot simply stop in their tracks and end on a logical contradiction. Humans resolve such situations by accepting trade-offs between conflicting norms, which constitute priorities of one norm or value over another in a given context. Such priorities may be represented in the norm system as hierarchical relations. Along with identifying the norms within a specific community and task domain, empirical research must identify the ways in which people prioritize competing norms and resolve norm conflicts, and the ways in which people expect A/IS to resolve similar norm conflicts. These more local conflict resolutions will be further constrained by some general principles, such as the Common Good Principle (Andre and Velasquez 1992 or local and national laws. For example, a self-driving vehicle s prioritization of one factor over another in its decision-making will need to reflect the laws and norms of the population in which the A/IS are deployed, e.g., the traffic laws of a U.S. state and the United States as a whole.175 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsSome priority orders can be built into a given norm network as hierarchical relations, e.g., more general prohibitions against harm to humans typically override more specific norms against lying. Other priority orders can stem from the override that norms in the larger community exert on norms and preferences of an individual user. In the earlier example discussing personalization (see Issue , the A/IS of a racist user who demands the A/IS use derogatory language for certain social groups will have to resist such demands because community norms hierarchically override an individual user s preferences. In many cases, priority orders are not built in as fixed hierarchies because the priorities are themselves context-specific or may arise from net moral costs and benefits of the particular case at hand. A/IS must have learning capacities to track such variations and incorporate user and community input, e.g., about the subtle differences between contexts, so as to refine the system s norm network (see Issue . Tension may sometimes arise between a community s social and legal norms and the normative considerations of designers or manufacturers. Democratic processes may need to be developed that resolve this tension processes that cannot be presented in detail in this chapter. Often such resolution will favor the local laws and norms, but in some cases the community may have to be persuaded to accept A/IS favoring international law or broader humanitarian principles over, say, racist or sexist local practices. In general, we recommend that the system s resolution of norm conflicts be transparent that is, documented by the system and ready to be made available to users, the relevant community of deployment, and third-party evaluators. Just like people explain to each other why they made decisions, they will expect any A/IS to be able to explain their decisions and be sensitive to user feedback about the appropriateness of the decisions. To do so, design and development of A/IS should specifically identify the relevant groups of humans who may request explanations and evaluate the systems behaviors. In the case of a system detecting a norm conflict, the system should consult and offer explanations to representatives from the community, e.g., randomly sampled crowdsourced members or elected officials, as well as to third-party evaluators, with the goal of discussing and resolving the norm conflict. Recommendation A/IS developers should identify the ways in which people resolve norm conflicts and the ways in which they expect A/IS to resolve similar norm conflicts. A system s resolution of norm conflicts must be transparent that is, documented by the system and ready to be made available to users, the relevant community of deployment, and third-party evaluators. 176 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsFurther Resources M . Velasquez, C. Andre, T. Shanks, S.J., and M. J. Meyer, The Common Good. Issues in Ethics , vol. 5, no. 1, J . Van den Hoven, Engineering and the Problem of Moral Overload. Science and Engineering Ethics, vol. 18, no. 1, pp. 143 155, D . Abel, J. MacGlashan, and M. L. Littman. Reinforcement Learning as a Framework for Ethical Decision Making. AAAI Workshop AI, Ethics, and Society, Volume WS-16-02 of 13th AAAI Workshops . Palo Alto, CA: AAAI Press, O . Bendel, Die Moral in der Maschine: Beitr ge zu Roboter- und Maschinenethik. Hannover, Germany: Heise Medien, A ccessible popular-science contributions to philosophical issues and technical implementations of machine ethics S . V. Burks, and E. L. Krupka. A Multimethod Approach to Identifying Norms and Normative Expectations within a Corporate Hierarchy: Evidence from the Financial Services Industry. Management Science, vol. 58, pp. 203 217, I llustrates surveys and incentivized coordination games as methods to elicit norms in a large financial services firm F . Cushman, V. Kumar, and P. Railton, Moral Learning, Cognition , vol. 167, pp. 1 282, M . Flanagan, D. C. Howe, and H. Nissenbaum, Embodying Values in Technology: Theory and Practice. Information Technology and Moral Philosophy , J. van den Hoven and J. Weckert, Eds., Cambridge University Press, 2008, pp. 322 Cambridge Core, Cambridge University Press. Preprint available at B . Friedman, P. H. Kahn, A. Borning, and A. Huldtgren. Value Sensitive Design and Information Systems, in Early Engagement and New Technologies: Opening up the Laboratory, N. Doorn, Schuurbiers, I. van de Poel, and M. Gorman, Eds., vol. 16, pp. 55 Dordrecht: Springer, A c omprehensive introduction into Value Sensitive Design and three sample applications G . Mackie, F. Moneti, E. Denny, and H. Shakya. What Are Social Norms? How Are They Measured? UNICEF Working Paper. University of California at San Diego: UNICEF, Sept. A b road survey of conceptual and measurement questions regarding social norms. J . A. Leydens and J. C. Lucena. Engineering Justice: Transforming Engineering Education and Practice. Hoboken, NJ: John Wiley & Sons, I dentifies principles of engineering for social justice.177 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systems B . F. Malle, Integrating Robot Ethics and Machine Morality: The Study and Design of Moral Competence in Robots. Ethics and Information Technology, vol. 18, no. 4, pp. 243 256, D iscusses how a robot s norm capacity fits in the larger vision of a robot with moral competence. K . W. Miller, M. J. Wolf, and F. Grodzinsky, This Ethical Trap Is for Roboticists, Not Robots: On the Issue of Artificial Agent Ethical Decision-Making. Science and Engineering Ethics, vol. 23, pp. 389 401, T his article raises doubts about the possibility of imbuing artificial agents with morality, or of claiming to have done so. O pen Roboethics Initiative: www. openroboethics.org . A series of poll results on differences in human moral decision-making and changes in priority order of values for autonomous systems (e.g., on care robots ), A . Rizzo and L. L. Swisher, Comparing the Stewart Sprinthall Management Survey and the Defining Issues Test-2 as Measures of Moral Reasoning in Public Administration. Journal of Public Administration Research and Theory, vol. 14, pp. 335 348, D escribes two assessment instruments of moral reasoning (including norm maintenance) based on Kohlberg s theory of moral development. S . H. Schwartz, An Overview of the Schwartz Theory of Basic Values. Online Readings in Psychology and Culture 2, C omprehensive overview of a specific theory of values, understood as motivational orientations toward abstract outcomes (e.g., self-direction, power, security). S . H. Schwartz and K. Boehnke. Evaluating the Structure of Human Values with Confirmatory Factor Analysis. Journal of Research in Personality, vol. 38, pp. 230 255, D escribes an older method of subjective judgments of relations among valued outcomes and a newer, formal method of analyzing these relations. W . Wallach and C. Allen. Moral Machines: Teaching Robots Right from Wrong . New York: Oxford University Press, T his book describes some of the challenges of having a one-size-fits-all approach to embedding human values in autonomous systems. 178 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsSection 2 Implementing Norms in Autonomous and Intelligent Systems Once the norms relevant to A/IS role in a specific community have been identified, including their properties and priority structure, we must link these norms to the functionalities of the underlying computational system. We discuss three issues that arise in this process of norm implementation. First, computational approaches to enable a system to represent, learn, and execute norms are only slowly emerging. However, the diversity of approaches may soon lead to substantial advances. Second, for A/IS that operate in human communities, there is a particular need for transparency ranging from the technical process of implementation to the ethical decisions that A/IS will make in human-machine interactions, which will require a high level of explainability. Third, failures of normative reasoning can be considered inevitable and mitigation strategies should therefore be put in place to handle such failures when they occur. As a general guideline, we recommend that, through the entire process of implementation of norms, designers should consider various forms and metrics of evaluation, and they should define and incorporate central criteria for assessing the A/IS norm conformity, e.g., human-machine agreement on moral decisions, verifiability of A/IS decisions, or justified trust. In this way, implementation already prepares for the critical third phase of evaluation (discussed in Section .Issue Many approaches to norm implementation are currently available, and it is not yet settled which ones are most suitable. Background The prospect of developing A/IS that are sensitive to human norms and factor them into morally or legally significant decisions has intrigued science fiction writers, philosophers, and computer scientists alike. Modest efforts to realize this worthy goal in limited or bounded contexts are already underway. This emerging field of research appears under many names, including: machine morality, machine ethics, moral machines, value alignment, computational ethics, artificial morality, safe AI, and friendly AI. There are a number of different implementation routes for implementing ethics into autonomous and intelligent systems. Following Wallach and Allen ( 14, we might begin to categorize these as either: A. T op-down approaches, where the system, e.g., a software agent, has some symbolic representation of its activity, and so can identify specific states, plans, or actions as ethical or unethical with respect to particular ethical requirements (Dennis, 179 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsFisher, Slavkovik, Webster 201615; Pereira and Saptawijaya 201616; R tzer, 201617; Scheutz, Malle, and Briggs ; or B. B ottom-up approaches, where the system, e.g., a learning component, builds up, through experience of what is to be considered ethical and unethical in certain situations, an implicit notion of ethical behavior (Anderson and Anderson 2014 19; Riedl and Harrison . Relevant examples of these two are: (A) symbolic agents that have explicit representations of plans, actions, goals, etc.; and (B) machine learning systems that train subsymbolic mechanisms with acceptable ethical behavior. For more detailed discussion, see Charisi et al. 2017 Many of the existing experimental approaches to building moral machines are top-down, in the sense that norms, rules, principles, or procedures are used by the system to evaluate the acceptability of differing courses of action, or as moral standards or goals to be realized. Increasingly, however, A/IS will encounter situations that initially programmed norms do not clearly address, requiring algorithmic procedures to select the better of two or more novel courses of action. Recent breakthroughs in machine learning and perception enable researchers to explore bottom-up approaches in which the A/IS learn about their context and about human norms, similar to the manner in which a child slowly learns which forms of behavior are safe and acceptable. Of course, unlike current A/IS, children can feel pain and pleasure, and empathize with others. Still, A/IS can learn to detect and take into account others pain and pleasure, thus at least achieving some of the positive effects of empathy. As research on A/IS progresses, engineers will explore new ways to improve these capabilities. Each of the first two options has obvious limitations, such as option A s inability to learn and adapt and option B s unconstrained learning behavior. A third option tries to address these limitations: C. Hybrid approaches, combining (A) and (B). For example, the selection of action might be carried out by a subsymbolic system, but this action must be checked by a symbolic gateway agent before being invoked. This is a typical approach for Ethical Governors (Arkin, 2008 22; Winfield, Blum, and Liu or Guardians (Etzioni that monitor, restrict, and even adapt certain unacceptable behaviors proposed by the system (see Issue . Alternatively, action selection in light of norms could be done in a verifiable logical format, while many of the norms constraining those actions can be learned through bottom-up learning mechanisms (Arnold, Kasenberg, and Scheutz 2017 . These three architectures do not cover all possible techniques for implementing norms into A/IS. For example, some contributors to the multi-agent systems literature have integrated norms into their agent specifications (Andrighetto et al. 2013 , and even though these agents live in societal simulations and are too underspecified to be translated into individual A/IS such as robots, the emerging work can inform cognitive architectures of such A/IS that fully integrate norms. Of course, none of these experimental systems should be deployed outside of the laboratory before testing or before certain criteria are met, which we outline in the remainder of this section and in Section 180 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsRecommendation In light of the multiple possible approaches to computationally implement norms, diverse research efforts should be pursued, especially collaborative research between scientists from different schools of thought and different disciplines. Further Resources M . Anderson, and S. L. Anderson, GenEth: A General Ethical Dilemma Analyzer, Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence , Qu bec City, Qu bec, Canada, July 27 31, 2014, pp. 253 261, Palo Alto, CA, The AAAI Press, G . Andrighetto, G. Governatori, P. Noriega, and L. W. N. van der Torre, eds. Normative Multi-Agent Systems. Saarbr cken/Wadern, Germany: Dagstuhl Publishing, R . Arkin, Governing Lethal Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot Architecture. Proceedings of the 2008 3 rd ACM/IEEE International Conference on Human-Robot Interaction (HRI) , Amsterdam, Netherlands, March 12 -15, 2008, IEEE, pp. 121 128, T . Arnold, D. Kasenberg, and M. Scheutz. Value Alignment or Misalignment What Will Keep Systems Accountable ? The Workshops of the Thirty-First AAAI Conference on Artificial Intelligence: Technical Reports , WS-17-AI, Ethics, and Society, pp. 81 Palo Alto, CA: The AAAI Press, V . Charisi, L. Dennis, M. Fisher, et al. Towards Moral Autonomous Systems, A . Conn, How Do We Align Artificial Intelligence with Human Values? Future of Life Institute , Feb. 3, L . Dennis, M. Fisher, M. Slavkovik, and M. Webster, Formal Verification of Ethical Choices in Autonomous Systems. Robotics and Autonomous Systems, vol. 77, pp. 1 14, A . Etzioni and O. Etzioni, Designing AI Systems That Obey Our Laws and Values. Communications of the ACM , vol. 59, no. 9, pp. 29 31, Sept. L . M. Pereira and A. Saptawijaya, Programming Machine Ethics. Cham, Switzerland: Springer International, M . O. Riedl and B. Harrison. Using Stories to Teach Human Values to Artificial Agents. AAAI Workshops 2016 . Phoenix, Arizona, February 12 13, F . R tzer, ed. Programmierte Ethik: Brauchen Roboter Regeln oder Moral? Hannover, Germany: Heise Medien, M . Scheutz, B. F. Malle, and G. Briggs. Towards Morally Sensitive Action Selection for Autonomous Social Robots. Proceedings of the 24th International Symposium on Robot and Human Interactive Communication, RO-MAN 2015 (: 492 U . Sommer, Werte: Warum Man Sie Braucht, Obwohl es Sie Nicht Gibt. [Values. Why we need them even though they don t exist.] Stuttgart, Germany: J. B. Metzler, I . Sommerville, Software Engineering . Harlow, U.K.: Pearson Studium, W . Wallach and C. Allen. Moral Machines: Teaching Robots Right from Wrong . New York: Oxford University Press, F . T. Winfield, C. Blum, and W. Liu. Towards an Ethical Robot: Internal Models, Consequences and Ethical Action Selection in Advances in Autonomous Robotics Systems, Lecture Notes in Computer Science Volume , M. Mistry, A. Leonardis, Witkowski, and C. Melhuish, eds. pp. 85 Springer, 181 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsIssue The need for transparency from implementation to deployment Background When A/IS become part of social communities and behave according to the norms of their communities, people will want to understand the A/IS decisions and actions, just as they want to understand each other s decisions and actions. This is particularly true for morally significant actions or omissions: an ethical reasoning system should be able to explain its own reasoning to a user on request. Thus, transparency, or explainability , of A/IS is paramount (Chaudhuri 2017 27; Wachter, Mittelstadt, and Floridi , and it will allow a community to understand, predict, and modify the A/IS (see Section 1, Issue 2; for a nuanced discussion see Selbst and Barocas . Moreover, as the norms embedded in A/IS are continuously updated and refined (see Section 1, Issue , transparency allows for appropriate trust to be developed (Grodzinsky, Miller, and Wolf 201 1 , and, where necessary, allows the community to modify a system s norms, reasoning, and behavior. Transparency can occur at multiple levels, e.g., ordinary language or coder verification, and for multiple stakeholders, e.g., user, engineer, and attorney. (See IEEE P7001 , IEEE Standards Project for Transparency of Autonomous Systems). It should be noted that transparency to all parties may not always be advisable, such as in the case of security programs that prevent a system from being hacked (Kroll et al. 2016 . Here we briefly illustrate the broad range of transparency by reference to four ways in which systems can be transparent traceability, verifiability, honest design, and intelligibility and apply these considerations to the implementation of norms in A/IS. Transparency as traceability Most relevant for the topic of implementation is the transparency of the software engineering process during implementation (Cleland-Huang, Gotel, and Zisman2012 . It allows for the originally identified norms (Section 1, Issue to be traced through to the final system. This allows technical inspection of which norms have been implemented, for which contexts, and how norm conflicts are resolved, e.g., priority weights given to different norms. Transparency in the implementation process may also reveal biases that were inadvertently built into systems, such as racism and sexism, in search engine algorithms (Noble 2013 . (See Section 3, Issue ) Such traceability in turn calibrates a community s trust about whether A/IS are conforming to the norms and values relevant in their use contexts (Fleischmann and Wallace 2005 . Transparency as verifiability Transparency concerning how normative reasoning is approached in the implementation is important as we wish to verify that the normative decisions the system makes match the required norms and values. Explicit and exact representations of these normative decisions can then provide the basis for a range of strong mathematical techniques, such as formal verification (Fisher, Dennis, and Webster 2013 . Even if a system cannot explain every single reasoning step in understandable human terms, a log of ethical reasoning should be available for inspection of later evaluation purposes (Hind et al. 2018 .182 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsTransparency as honest design German designer Dieter Rams coined the term honest design to refer to design that does not make a product more innovative, powerful or valuable than it really is (Vitsoe 2018 37; see also Donelli 201538; Jong . Honest design of A/IS is one aspect of their transparency, because it allows the user to see through the outward appearance and accurately infer the A/IS actual capacities. At times, however, the physical appearance of a system does not accurately represent what the system is capable of doing e.g., the agent displays signs of a certain human-like emotion but its internal state does not represent that human emotion. Humans are quick to make strong inferences from outward appearances of human-likeness to the mental and social capacities the A/IS might have. Demands for transparency in design therefore put a responsibility on the designer to not attempt to manipulate the consumer with promises that cannot be kept (Vitsoe 2018 . Transparency as intelligibility As mentioned above, humans will want to understand the A/IS decisions and actions, especially the morally significant ones. A clear requirement for an ethical A/IS is that the system be able to explain its own reasoning to a user, when asked or, ideally, also when suspecting the user s confusion, and the system should do so at a level of ordinary human reasoning, not with incomprehensible technical detail (Tintarev and Kutlak 2014 . Furthermore, when the system cannot explain some of its actions, technicians or designers should be available to make those actions intelligible. Along these lines, the European Union s General Data Protection Regulation (GDPR), in effect since May 2018, states that, for automated decisions based on personal data, individuals have a right to an explanation of the [algorithmic] decision reached after such assessment and to challenge the decision . (See boyd [sic] 2016 42, for a critical discussion of this regulation.) Recommendation A/IS, especially those with embedded norms, must have a high level of transparency, shown as traceability in the implementation process, mathematical verifiability of their reasoning, honesty in appearance-based signals, and intelligibility of the systems operation and decisions. Further Resources d . boyd, Transparency Accountability. Data & Society: Points , November 29, A . Chaudhuri, Philosophical Dimensions of Information and Ethics in the Internet of Things (IoT) Technology, The EDP Audit, Control, and Security Newsletter, vol. 56, no. 4, pp. 7-18, DOI: 1080/1380474, J . Cleland-Huang, O. Gotel, and A. Zisman, eds. Software and Systems Traceability . London: Springer, doi:1007/978- 1-4471-2239-5 G . Donelli, Good design is honest. (blog). March 13, Accessed Oct 22, M . Fisher, L. A. Dennis, and M. P. Webster. Verifying Autonomous Systems. Communications of the ACM , vol. 56, no. 9, pp. 84 93, 183 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systems K . R. Fleischmann and W. A. Wallace. A Covenant with Transparency: Opening the Black Box of Models. Communications of the ACM, vol. 48, no. 5, pp. 93 97, F . S. Grodzinsky, K. W. Miller, and M. J. Wolf. Developing Artificial Agents Worthy of Trust: Would You Buy a Used Car from This Artificial Agent? Ethics and Information Technology , vol. 13, pp. 17 27, 201 M . Hind, et al. Increasing Trust in AI Services through Supplier s Declarations of Conformity. ArXiv E-Prints , Aug. [Online] Available: . [Accessed October 28, 2018]. C . W. De Jong, ed., Dieter Rams: Ten Principles for Good Design . New York, NY: Prestel Publishing, J . A. Kroll, J. Huey, S. Barocas et al. Accountable Algorithms. University of Pennsylvania Law Review 165 S . U. Noble, Google Search: Hyper-Visibility as a Means of Rendering Black Women and Girls Invisible. InVisible Culture 19, D . Selbst and S. Barocas, The Intuitive Appeal of Explainable Machines, 87 Fordham Law Review 1085 , Available at SSRN: https:// ssrn.com/abstract=3126971 or org/2139/ssrn.3126971 , Feb. 19, N . Tintarev and R. Kutlak. Demo: Making Plans Scrutable with Argumentation and Natural Language Generation. Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces, pp. 29 32, V itsoe. The Power of Good Design. Vitsoe , Retrieved Oct 22, 2018 from . S .Wachter, B. Mittelstadt, and L. Floridi, Transparent, Explainable, and Accountable AI for Robotics. Science Robotics, vol. 2, no. 6, eaan6080. doi:1 126/scirobotics. aan6080, 184 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsIssue Failures will occur. Background Operational failures and, in particular, violations of a system s embedded community norms, are unavoidable, both during system testing and during deployment. Not only are implementations never perfect, but A/IS with embedded norms will update or expand their norms over time (see Section 1, Issue and interactions in the social world are particularly complex and uncertain. Thus, prevention and mitigation strategies must be adopted, and we sample four possible ones. First, anticipating the process of evaluation during the implementation phase requires defining criteria and metrics for such evaluation, which in turn better allows the detection and mitigation of failures. Metrics will include: T echnical variables, such as traceability and verifiability, U ser-level variables such as reliability, understandable explanations, and responsiveness to feedback, and C ommunity-level variables such as justified trust (see Issue and the collective belief that A/IS are generally creating social benefits rather than, for example, technological unemployment. Second, a systematic risk analysis and management approach can be useful (Oetzel and Spiekermann 2014 for an application to privacy norms. This approach tries to anticipate potential points of failure, e.g., norm violations, and, where possible, develops some ways to reduce or remove the effects of failures. Successful behavior, and occasional failures, can then iteratively improve predictions and mitigation attempts. Third, because not all risks and failures are predictable (Brundage et al 2018 44; Vanderelst and Winfield , especially in complex human-machine interactions in social contexts, additional mitigation mechanisms must be made available. Designers are strongly encouraged to augment the architectures of their systems with components that handle unanticipated norm violations with a fail-safe, such as the symbolic gateway agents discussed in Section 2, Issue Designers should identify a number of strict laws, that is, task- and community-specific norms that should never be violated, and the fail-safe components should continuously monitor operations against possible violations of these laws. In case of violations, the higher-order gateway agent should take appropriate actions, such as safely disabling the system s operation, or greatly limiting its scope of operation, until the source of failure is identified. The fail-safe components need to be understandable, extremely reliable, and protected against security breaches, which can be achieved, for example, by validating them carefully and not letting them adapt their parameters during execution. Fourth, once failures have occurred, responsible entities, e.g., corporate, government, science, and engineering, shall create a publicly accessible 185 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systemsdatabase with undesired outcomes caused by specific A/IS systems. The database would include descriptions of the problem, background information on how the problem was detected, which context it occurred in, and how it was addressed. In summary, we offer the following recommendation. Recommendation Because designers and developers cannot anticipate all possible operating conditions and potential failures of A/IS, multiple strategies to mitigate the chance and magnitude of harm must be in place. Further Resources M . Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garfunkel, A. Dafoe, P. Scharre, T. Zeitzo, et al. "" The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation, CoRR abs/07228 [cs.AI]. M . C. Oetzel and S. Spiekermann, A Systematic Methodology for Privacy Impact Assessments: A Design Science Approach. European Journal of Information Systems , vol. 23, pp. 126 150, D . Vanderelst and A.F. Winfield, 2018 The Dark Side of Ethical Robots, In Proc. The First AAAI/ACM Conf. on Artificial Intelligence, Ethics and Society, New Orleans, LA, Feb. 1 -3, 186 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsSection 3 Evaluating the Implementation of A/IS The success of implementing appropriate norms in A/IS must be rigorously evaluated. This evaluation process must be anticipated during design and incorporated into the implementation process and continue throughout the life cycle of the system s deployment. Assessment before full-scale deployment would best take place in systematic test beds that allow human users from the defined community and representing all demographic groups to engage safely with the A/IS in intended tasks. Multiple disciplines and methods should contribute to developing and conducting such evaluations. Evaluation criteria must capture, among others, the quality of human-machine interactions, human approval and appreciation of the A/IS, appropriate trust in the A/IS, adaptability of the A/IS to human users, and benefits to human well-being in the presence or under the influence of the A/IS. A range of normative aspects to be considered can be found in British Standard BS 861 2016 on Robot Ethics (British Standards Institution 2016 . These are important general evaluation criteria, but they do not yet fully capture evaluation of a system that has norm capacities . To evaluate a system s norm-conforming behavior, one must describe and ideally, formally specify criterion behaviors that reflect the previously identified norms, describe what the user expects the system to do, verify that the system really does this, and validate that the specification actually matches the criteria. Many different evaluation techniques are available in the field of software engineering (Sommerville 2015 , ranging from formal mathematical proof, through rigorous empirical testing against criteria of normatively correct behavior, to informal analysis of user interactions and responses to the machine s norm awareness and compliance. All these approaches can, in principle, be applied to the full range of A/IS including robots (Fisher, Dennis, and Webster 2013 . More general principles from system quality management may also be integrated into the evaluation process, such as the Plan-Do-Check-Act (PDCA) cycle that underlies standards like ISO 9001 (International Organization for Standardization 2015 . Evaluation may be done by first parties, e.g., designers, manufacturers, and users, as well as third parties, e.g., regulators, independent testing agencies, and certification bodies. In either case, the results of evaluations should be made available to all parties, with strong encouragement to resolve discovered system limitations and resolve potential discrepancies among multiple evaluations. As a general guideline, we recommend that evaluation of A/IS implementations must be anticipated during a system s design, incorporated 187 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systemsinto the implementation process, and continue throughout the system s deployment (cf . ITIL principles, BMC . Evaluation must include multiple methods, be made available to all parties from designers and users to regulators, and should include procedures to resolve conflicting evaluation results. Specific issues that need to be addressed in this process are discussed next. Further Resources B ritish Standards Institution. BS861 2016, Robots and Robotic Devices. Guide to the Ethical Design and Application of Robots and Robotic Systems, B MC Software. ITIL: The Beginner s Guide to Processes & Best Practices . com/guides/itil-introduction.html , Dec. 6, M . Fisher, L. A. Dennis, and M. P. Webster. Verifying Autonomous Systems. Communications of the ACM , vol. 56, no. 9, pp. 84 93, I nternational Organization for Standardization (. ISO 2015, Quality management systems Requirements. Retrieved July 12, 2018 from . I . Sommerville, Software Engineering. 10th ed. Harlow, U.K.: Pearson Studium, Issue Not all norms of a target community apply equally to human and artificial agents Background An intuitive criterion for evaluations of norms embedded in A/IS would be that the A/IS norms should mirror the community s norms that is, the A/IS should be disposed to behave the same way that people expect each other to behave. However, for a given community and a given A/IS use context, A/IS and humans are unlikely to have identical sets of norms. People will have some unique expectations for humans than they do not for machines, e.g., norms governing the regulation of negative emotions, assuming that machines do not have such emotions. People may in some cases have unique expectations of A/IS that they do not have for humans, e.g., a robot worker, but not a human worker, is expected to work without regular breaks. Recommendation The norm identification process must document the similarities and differences between the norms that humans apply to other humans and the norms they apply to A/IS. Norm implementations should be evaluated specifically against the norms that the community expects the A/IS to follow. 188 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsIssue A/IS can have biases that disadvantage specific groups Background Even when reflecting the full system of community norms that was identified, A/IS may show operation biases that disadvantage specific groups in the community or instill biases in users by reinforcing group stereotypes. A system s bias can emerge in perception. For example, a passport application AI rejected an Asian man s photo because it insisted his eyes were closed (Griffiths 2016 . Bias can emerge in information processing. For instance, speech recognition systems are notoriously less accurate for female speakers than for male speakers (Tatman 2016 . System bias can affect decisions, such as a criminal risk assessment device which overpredicts recidivism by African Americans (Angwin et al. 2016 . The system s bias can present itself even in its own appearance and presentation: the vast majority of humanoid robots have white skin color and use female voices (Riek and Howard 2014 . The norm identification process detailed in Section 1 is intended to minimize individual designers biases because the community norms are assessed empirically. The identification process also seeks to incorporate norms against prejudice and discrimination. However, biases may still emerge from imperfections in the norm identification process itself, from unrepresentative training sets for machine learning systems, and from programmers and designers unconscious assumptions. Therefore, unanticipated or undetected biases should be further reduced by including members of diverse social groups in both the planning and evaluation of A/IS and integrating community outreach into the evaluation process, e.g., DO-IT program and RRI framework. Behavioral scientists and members of the target populations will be particularly valuable when devising criterion tasks for system evaluation and assessing the success of evaluating the A/IS performance on those tasks. Such tasks would assess, for example, whether the A/IS apply norms in discriminatory ways to different races, ethnicities, genders, ages, body shapes, or to people who use wheelchairs or prosthetics, and so on. Recommendation Evaluation of A/IS must carefully assess potential biases in the systems performance that disadvantage specific social and demographic groups. The evaluation process should integrate members of potentially disadvantaged groups in efforts to diagnose and correct such biases. Further Resources J . Angwin, J. Larson, S. Mattu, and L. Kirchner, Machine Bias: There s Software Used Across the Country to Predict Future Criminals. And It s Biased Against Blacks. ProPublica, May 23, J . Griffiths, New Zealand Passport Robot Thinks This Asian Man s Eyes Are Closed. CNN.com, December 9, 189 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systems L . D. Riek and D. Howard,. A Code of Ethics for the Human-Robot Interaction Profession. Proceedings of We Robot, April 4, R . Tatman, Google s Speech Recognition Has a Gender Bias. Making Noise and Hearing Things , July 12, Issue Challenges to evaluation by third parties Background A/IS should have sufficient transparency to allow evaluation by third parties, including regulators, consumer advocates, ethicists, post-accident investigators, or society at large. However, transparency can be severely limited in some systems, especially in those that rely on machine learning algorithms trained on large data sets. The data sets may not be accessible to evaluators; the algorithms may be proprietary information or mathematically so complex that they defy common-sense explanation; and even fellow software experts may be unable to verify reliability and efficacy of the final system because the system s specifications are opaque. For less inscrutable systems, numerous techniques are available to evaluate the implementation of the A/IS norm conformity. On one side there is formal verification, which provides a mathematical proof that the A/IS will always match specific normative and ethical requirements, typically devised in a top-down approach (see Section 2, Issue . This approach requires access to the decision-making process and the reasons for each decision (Fisher, Dennis, and Webster 2013 . A simpler alternative, sometimes suitable even for machine learning systems, is to test the A/IS against a set of scenarios and assess how well they matches their normative requirements, e.g., acting in accordance with relevant norms and recognizing other agents norm violations. A red team may also devise scenarios that try to get the A/IS to break norms so that its vulnerabilities can be revealed. These different evaluation techniques can be assigned different levels of strength : strong ones demonstrate the exhaustive set of the A/IS allowable behaviors for a range of criterion scenarios; weaker ones sample from criterion scenarios and illustrate the systems behavior for that subsample. In the latter case, confidence in the A/IS ability to meet normative requirements is more limited. An evaluation s concluding judgment must therefore acknowledge the strength of the verification technique used, and the expressed confidence in the evaluation an d in the A/IS themselves must be qualified by this level of strength. Transparency is only a necessary requirement for a more important long-term goal: having systems be accountable to their users and community members. However, this goal raises many questions such as to whom the A/IS are accountable, who has the right to correct the systems, and which kind of A/IS should be subject to accountability requirements.190 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsRecommendation To maximize effective evaluation by third parties, e.g., regulators and accident investigators, A/IS should be designed, specified, and documented so as to permit the use of strong verification and validation techniques for assessing the system s safety and norm compliance, in order to achieve accountability to the relevant communities. Further Resources M . Fisher, L. A. Dennis, and M. P. Webster. Verifying Autonomous Systems. Communications of the ACM, vol. 56, pp. 84 93, K . Abney, G. A. Bekey, and P. Lin. Robot Ethics: The Ethical and Social Implications of Robotics . Cambridge, MA: The MIT Press, 201 M . Anderson and S. L. Anderson, eds. Machine Ethics. New York: Cambridge University Press, 201 M . Boden, J. Bryson, et al. Principles of Robotics: Regulating Robots in the Real World. Connection Science 29, no. 2, pp. 124 129, M . Coeckelbergh, Can We Trust Robots? Ethics and Information Technology, vol.14, pp. 53 60, L . A. Dennis, M. Fisher, N. Lincoln, A. Lisitsa, and S. M. Veres, Practical Verification of Decision-Making in Agent-Based Autonomous Systems. Automated Software Engineering, vol. 23, no. 3, pp. 305 359, M . Fisher, C. List, M. Slavkovik, and A. F. T. Winfield. Engineering Moral Agents From Human Morality to Artificial Morality (Dagstuhl Seminar . Dagstuhl Reports 6, no. 5, pp. 1 14 137, K . R. Fleischmann, Information and Human Values . San Rafael, CA: Morgan and Claypool, G . Governatori and A. Rotolo. How Do Agents Comply with Norms? in Normative Multi-Agent Systems , G. Boella, P. Noriega, G. Pigozzi, and H. Verhagen, eds., Dagstuhl Seminar Proceedings . Dagstuhl, Germany: Schloss Dagstuhl Leibniz- Zentrum f r Informatik, B . Higgins, New York City Task Force to Consider Algorithmic Harm. Artificial Intelligence Technology and the Law Blog , Feb. 7, [Online]. Available: . [Accessed Nov. 1, 2018]. S . L. Jarvenpaa, N. Tractinsky, and L. Saarinen. Consumer Trust in an Internet Store: A Cross- Cultural Validation Journal of Computer- Mediated Communication, vol. 5, no. 2, pp. 1 37, E . H. Leet and W. A. Wallace. Society s Role and the Ethics of Modeling, in Ethics in Modeling , W. A. Wallace, ed., Tarrytown, NY: Elsevier, 1994, pp. 242 M . A. Mahmoud, M. S. Ahmad, M. Z. M. Yusoff, and A. Mustapha. A Review of Norms and Normative Multiagent Systems, The Scientific World Journal , vol. 2014, Article ID 684587, 191 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent SystemsThanks to the Contributors We wish to acknowledge all of the people who contributed to this chapter. The Embedding Values into Autonomous Intelligent Systems Committee A Jung Moon (Founding Chair) Director of Open Roboethics Institute B ertram F. Malle (Co-Chair) Professor, Department of Cognitive, Linguistic, and Psychological Sciences, Co-Director of the Humanity-Centered Robotics Initiative, Brown University F rancesca Rossi (Co-Chair) Full Professor, computer science at the University of Padova, Italy, currently at the IBM Research Center at Yorktown Heights, NY S tefano Albrecht Postdoctoral Fellow in the Department of Computer Science at The University of Texas at Austin B ijilash Babu Senior Manager, Ernst and Young, EY Global Delivery Services India LLP J an Carlo Barca Senior Lecturer in Software Engineering and Internet of Things (IoT), School of Info Technology, Deakin University, Australia C atherine Berger IEEE Standards Senior Program Manager, IEEE M alo Bourgon COO, Machine Intelligence Research Institute R ichard S. Bowyer Adjunct Senior Lecturer and Research Fellow, College of Science and Engineering, Centre for Maritime Engineering, Control and Imaging (cmeci), Flinders University, South Australia S tephen Cave Executive Director of the Leverhulme Centre for the Future of Intelligence, University of Cambridge R aja Chatila CNRS-Sorbonne Institute of Intelligent Systems and Robotics, Paris, France; Member of the French Commission on the Ethics of Digital Sciences and Technologies CERNA; Past President of IEEE Robotics and Automation Society M ark Coeckelbergh Professor, Philosophy of Media and Technology, the University of Vienna Lo uise Dennis Lecturer, Autonomy and Verification Laboratory, University of Liverpool L aurence Devillers Professor of Computer Sciences, University Paris Sorbonne, LIMSI-CNRS 'Affective and social dimensions in spoken interactions'; member of the French Commission on the Ethics of Research in Digital Sciences and Technologies (CERNA) V irginia Dignum Associate Professor, Faculty of Technology Policy and Management, TU Delft192 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systems E bru Dogan Research Engineer, VEDECOM T akashi Egawa Cloud Infrastructure Laboratory, NEC Corporation, Tokyo V anessa Evers Professor, Human-Machine Interaction, and Science Director, DesignLab, University of Twente M ichael Fisher Professor of Computer Science, University of Liverpool, and Director of the UK Network on the Verification and Validation of Autonomous Systems, vavas.org K en Fleischmann Associate Professor in the School of Information at The University of Texas at Austin Ed ith Pulido Herrera Bioengineering group, Antonio Nari o University, Bogot , Colombia R yan Integlia assistant professor, Electrical and Computer Engineering, Florida Polytechnic University; Co-Founder of the em[POWER] Energy Group C atholijn Jonker Full professor of Interactive Intelligence at the Faculty of Electrical Engineering, Mathematics and Computer Science of the Delft University of Technology. Part-time full professor at Leiden Institute of Advanced Computer Science of the Leiden University S ara Jordan Assistant Professor of Public Administration in the Center for Public Administration & Policy at Virginia Tech J ong-Wook Kim Professor, AI.Robotics Lab, Department of Electronic Engineering, Dong-A University, Busan, Korea S ven Koenig Professor, Computer Science Department, University of Southern California B renda Leong Senior Counsel, Director of Operations, The Future of Privacy Forum A lan Mackworth Professor of Computer Science, University of British Columbia; Former President, AAAI; Co-author of Artificial Intelligence: Foundations of Computational Agents . P ablo Noriega Scientist, Artificial Intelligence Research Institute of the Spanish National Research Council (IIIA-CSIC), Barcelona. R ajendran Parthiban Professor, School of Engineering, Monash University, Bandar Sunway, Malaysia H eather M. Patterson Senior Research Scientist, Anticipatory Computing Lab, Intel Corp. E dson Prestes Professor, Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Brazil; Head, Phi Robotics Research Group, UFRGS; CNPq Fellow. L aurel Riek Associate Professor, Computer Science and Engineering, University of California San Diego L eanne Seeto Co-Founder and Strategy and Operations Precision Autonomy S arah Spiekermann Chair of the Institute for Information Systems & Society at Vienna University of Economics and Business; Author of the textbook Ethical IT-Innovation , the popular book Digitale Ethik Ein Wertesystem f r das Jahrhundert and Blogger on The Ethical Machine 193 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systems J ohn P. Sullins Professor of Philosophy, Chair of the Center for Ethics Law and Society (CELS), Sonoma State University J aan Tallinn Founding engineer of Skype and Kazaa; co-founder of the Future of Life Institute M ike Van der Loos Associate Prof., Dept. of Mechanical Engineering, Director of Robotics for Rehabilitation, Exercise and Assessment in Collaborative Healthcare (RREACH) Lab, and Associate Director of CARIS Lab, University of British Columbia W endell Wallach Consultant, ethicist, and scholar, Yale University's Interdisciplinary Center for Bioethics Ne ll Watson CFBCS, FICS, FIAP, FIKE, FRSA, FRSS, FLS Co-Founder and Chairman, EthicsNet, AI & Robotics Faculty Singularity University, Foresight Machine Ethics Fellow K arolina Zawieska Postdoctoral Research Fellow in Ethics and Cultural Learning of Robotics at DeMontfort University, UK and Researcher at Industrial Research Institute for Automation and Measurements PIAP, Poland For a full listing of all IEEE Global Initiative Members, visit standards.ieee.org/content/dam/ ieee-standards/standards/web/documents/other/ec_bios.pdf . F or information on disclaimers associated with EAD1e, see How the Document Was Prepared .194 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systems1 S . Hitlin and J. A. Piliavin. Values: Reviving a Dormant Concept. Annual Review of Sociology 30 (: 359 2 B . F. Malle, and S. Dickert. Values, The Encyclo - pedia of Social Psychology , edited by R. F. Baumeis- ter and K. D. Vohs. Thousand Oaks, CA: Sage, 3 M . J. Rohan, A Rose by Any Name? The Values Construct. Personality and Social Psychology Re- view 4 (: 255 4 A . U. Sommer, Werte: Warum Man Sie Braucht, Obwohl es Sie Nicht Gibt. [Values. Why We Need Them Even Though They Don t Exist.] Stuttgart, Germany: J. B. Metzler, 5 B . F. Malle, M. Scheutz, and J. L. Austerweil. Net- works of Social and Moral Norms in Human and Robot Agents, in A World with Robots: International Conference on Robot Ethics : ICRE 2015, edited by M. I. Aldinhas Ferreira, J. Silva Sequeira, M. O. Tokhi, E. E. Kadar, and G. S. Virk, 3 Cham, Switzerland: Springer International Publishing, 6 J . V zquez-Salceda, H. Aldewereld, and F. Dig- num. Implementing Norms in Multiagent Systems, in Multiagent System Technologies. MATES 2004, edited by G. Lindemann, Denzinger, I. J. Timm, and R. Unland. ( Lecture Notes in Computer Science, vol. 3187 .) Berlin: Springer, 7 A . Mack, (Ed.). Changing social norms. Social Research: An International Quarterly, 85, no.1 (: 1 8 I . van de Poel, An Ethical Framework for Evaluat - ing Experimental Technology , Science and Engi - neering Ethics , 22, no. 3 (: 667-9 I . Misra, C. L. Zitnick, M. Mitchell, and R. Girshick, (. Seeing through the human reporting bias: Visual Classifiers from Noisy Human-Centric Labels. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 2930 . doi: 1109/CVPR.320 10 A . Mack, (Ed.). (. Changing social norms. Social Research: An International Quarterly , 85(1, Special Issue), 1 11 B . Green and L. Hu. The Myth in the Method- ology: Towards a Recontextualization of Fairness in ML. Paper presented at the Debates workshop at the 35th International Conference on Machine Learning, Stockholm, Sweden 12 J . Van den Hoven, Engineering and the Problem of Moral Overload. Science and Engineering Ethics 18, no. 1 (: 143 13 C . Andre and M. Velasquez. The Common Good . Issues in Ethics 5, no. 1 (. 14 W . Wallach and C. Allen. Moral Machines: Teach - ing Robots Right from Wrong . New York: Oxford University Press, 15 L . Dennis, M. Fisher, M. Slavkovik, and M. Webster. Formal Verification of Ethical Choices in Autonomous Systems. Robotics and Autonomous Systems 77 (: 1 Endnotes195 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systems16 L . M. Pereira and A. Saptawijaya. Programming Machine Ethics . Cham, Switzerland: Springer Inter- national, 17 F . R tzer, ed. Programmierte Ethik: Brauchen Roboter Regeln oder Moral? Hannover, Germany: Heise Medien, 18 M . Scheutz, B. F. Malle, and G. Briggs. Towards Morally Sensitive Action Selection for Autonomous Social Robots. Proceedings of the 24th Interna-tional Symposium on Robot and Human Interactive Communication, RO-MAN 2015 (: 492 19 M . Anderson and S. L. Anderson. GenEth: A General Ethical Dilemma Analyzer. Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence (: 253 20 M . O. Riedl and B. Harrison. Using Stories to Teach Human Values to Artificial Agents. Proceed - ings of the 2nd International Workshop on AI, Ethics and Society , Phoenix, Arizona, 21 V . Charisi, L. Dennis, M. Fisher et al. Towards Moral Autonomous Systems , 22 R . Arkin, Governing Lethal Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot Ar-chitecture. Proceedings of the 2008 3 rd ACM/IEEE International Conference on Human-Robot Interac - tion (: 121 23 A . F. T. Winfield, C. Blum, and W. Liu. Towards an Ethical Robot: Internal Models, Consequences and Ethical Action Selection in Advances in Autono - mous Robotics Systems, Lecture Notes in Computer Science Volume , edited by M. Mistry, A. Leonardis, Witkowski, and C. Melhuish, 85 Springer, 24 A . Etzioni, Designing AI Systems That Obey Our Laws and Values. Communications of the ACM 59, no. 9 (: 29 25 T . Arnold, D. Kasenberg, and M. Scheutz. Value Alignment or Misalignment What Will Keep Sys-tems Accountable? The Workshops of the Thir-ty-First AAAI Conference on Artificial Intelligence: Technical Reports, WS-17-AI, Ethics, and Society, 81 Palo Alto, CA: The AAAI Press, 26 G . Andrighetto, G. Governatori, P. Noriega, and L. W. N. van der Torre, eds. Normative Multi-Agent Systems . Saarbr cken/Wadern, Germany: Dagstuhl Publishing, 27 A . Chaudhuri, ( Philosophical Dimen- sions of Information and Ethics in the Internet of Things (IoT) Technology. The EDP Audit, Con-trol, and Security Newsletter, 4, 7-18, DOI: 1080/1380474 28 S .Wachter, B. Mittelstadt, and L. Floridi, Trans- parent, Explainable, and Accountable AI for Robot-ics. Science Robotics 2, no. 6 (: eaan6080. doi:1 126/scirobotics. aan6080 29 A . D. Selbst and S. Barocas, The Intuitive Ap- peal of Explainable Machines (February 19, . Fordham Law Review. Available at SSRN: or 30 F . S. Grodzinsky, K. W. Miller, and M. J. Wolf. De- veloping Artificial Agents Worthy of Trust: Would You Buy a Used Car from This Artificial Agent? Ethics and Information Technology 13, (201 : 17 196 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systems31 J . A. Kroll, J. Huey, S. Barocas et al. Accountable Algorithms. University of Pennsylvania Law Review 165 (. 32 J . Cleland-Huang, O. Gotel, and A. Zisman, eds. Software and Systems Traceability. London: Springer, doi:1007/978- 1-4471-2239-5 33 S . U. Noble, Google Search: Hyper-Visibility as a Means of Rendering Black Women and Girls Invisible. InVisible Culture 19 (. 34 K . R. Fleischmann and W. A. Wallace. A Covenant with Transparency: Opening the Black Box of Models. Communications of the ACM 48, no. 5 (: 93 35 M . Fisher, L. A. Dennis, and M. P. Webster. Verifying Autonomous Systems. Communications of the ACM 56, no. 9 (: 84 36 M . Hind, et al. Increasing Trust in AI Services through Supplier s Declarations of Conformity. ArXiv E-Prints , Aug. Retrieved October 28, 2018 from V itsoe. The Power of Good Design. Vitsoe , Retrieved Oct 22, 2018 from . 38 G . Donelli, (2015, March . Good design is honest (Blogpost). Retrieved Oct 22, 2018 from 39 C . de Jong Ed., Ten principles for good design: Dieter Rams. New York, NY: Prestel Publishing, 40 Ibid.41 N . Tintarev and R. Kutlak. Demo: Making Plans Scrutable with Argumentation and Natural Language Generation. Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces (: 29 42 d . boyd, Transparency Accountability . Data & Society: Points , November 29, 43 C . Oetzel and S. Spiekermann, A Systematic Methodology for Privacy Impact Assessments: A Design Science Approach. European Journal of Information Systems 23, (: 126 44 M . Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garfunkel, A. Dafoe, P. Scharre, T. Zeitzo, et al. The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation. CoRR abs/07228 (. . 45 D . Vanderelst and A.F. Winfield, 2018 The Dark Side of Ethical Robots. In Proc. AAAI/ACM Conf. on Artificial Intelligence, Ethics and Society, New Orleans. 46 B ritish Standards Institution. BS861 2016, Robots and Robotic Devices. Guide to the Ethical Design and Application of Robots and Robotic Systems , 47 I . Sommerville, Software Engineering (10th edition). Harlow, U.K.: Pearson Studium, 48 M . Fisher, L. A. Dennis, and M. P. Webster. Verifying Autonomous Systems. Communications of the ACM 56, no. 9 (: 84 197 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Embedding Values into Autonomous and Intelligent Systems49 I nternational Organization for Standardization (. ISO 2015, Quality management systems Requirements. Retrieved July 12, 2018 from . 50 B MC Software. ITIL: The Beginner s Guide to Processes & Best Practices . 6 Dec. 2016, http:// 51 J . Griffiths, New Zealand Passport Robot Thinks This Asian Man s Eyes Are -Closed . CNN.com, December 9, 52 R . Tatman, Google s Speech Recognition Has a Gender Bias . Making Noise and Hearing Things, July 12, 53 J . Angwin, J. Larson, S. Mattu, L. Kirchner. Machine Bias: There s Software Used Across the Country to Predict Future Criminals. And It s Biased Against Blacks . ProPublica, May 23, 54 L . D. Riek and D. Howard. A Code of Ethics for the Human-Robot Interaction Profession . Proceedings of We Robot, April 4, 55 M . Fisher, L. A. Dennis, and M. P. Webster. Verifying Autonomous Systems. Communications of the ACM 56 (: 84 PolicyThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. 198Introduction Autonomous and intelligent systems (A/IS) are a part of our society. The use of these powerful technologies promotes a range of social benefits. They may spur development across economies and society through numerous applications, including in commerce, finance, employment, health care, agriculture, education, transportation, politics, privacy, public safety, national security, civil liberties, and human rights. To encourage the development of socially beneficial applications of A/IS, and to protect the public from adverse consequences of A/IS, intended or otherwise, effective policies and government regulations are needed. Effective A/IS policies serve the public interest in several important respects. A/IS policies and regulations, at both the national level and as developed by professional organizations and governing institutions, protect and promote safety, privacy, human rights, and cybersecurity, as well as enhance the public s understanding of the potential impacts of A/IS on society. Without policies designed with these considerations in mind, there may be critical technology failures, loss of life, and high-profile social controversies. Such events could engender policies that unnecessarily hinder innovation, or regulations that do not effectively advance public interest and protect human rights. We believe that effective A/IS policies should embody a rights-based approach 1 that addresses five issues:E nsure that A/IS support, promote, and enable internationally recognized legal norms. Establish policies for A/IS using the internationally recognized legal framework for human rights standards that is directed at accounting for the impact of technology on individuals. 199 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Policy2. De velop government expertise in A/IS. Facilitate skill development, technical and otherwise, to further boost the ability of policy makers, regulators, and elected officials to make informed proposals and decisions about the various facets of these new technologies. E nsure governance and ethics are core components in A/IS research, development, acquisition, and use. Require support for A/IS research and development (R&D) efforts with a focus on the ethical impact of A/IS. To benefit from these new technologies while also ensuring they meet societal needs and values, governments should be actively involved in supporting relevant R&D efforts. C reate policies for A/IS to ensure public safety and responsible A/IS design. Governments must ensure consistent and locally adaptable policies and regulations for A/IS. Effective regulation should address transparency, explainability, predictability, bias, and accountability for A/IS algorithms, as well as risk management, privacy, data protection measures, safety, and security considerations. Certification of systems involving A/IS is a key technical, societal, and industrial issue. E ducate the public on the ethical and societal impacts of A/IS. Industry, academia, the media, and governments must establish strategies for informing and engaging the public on benefits and challenges posed by A/IS. Communicating accurately both the positive potential of A/IS and the areas that require caution and further development is critical to effective decision-making environments. As A/IS comprise a greater part of our daily lives, managing the associated risks and rewards becomes increasingly important. Technology leaders and policy makers have much to contribute to the debate on how to build trust, promote safety and reliability, and integrate ethical and legal considerations into the design of A/IS technologies. This chapter provides a principled foundation for these discussions.200 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. PolicyIssue Ensure that A/IS support, promote, and enable internationally recognized legal norms Background A/IS technologies have the potential to impact internationally recognized economic, social, cultural, and political rights through unintended outcomes and outright design decisions. Important examples of this issue have occurred with certain unmanned aircraft systems (Bowcott , use of A/IS in predictive policing (Shapiro , banking (Garcia , judicial sentencing (Osoba and Welser , and job hunting and hiring practices (Datta, Tschantz, and Datta . Even service delivery of goods (Ingold and Soper can impact human rights by automating discrimination (Eubanks and inhibiting the right of assembly, freedom of expression, and access to information. To ensure A/IS are used as a force for social benefit, nations must develop policies that safeguard human rights. A/IS regulation, development, and deployment should, therefore, be based on international human rights standards and standards of international humanitarian laws. When put into practice, both states and private actors will consider their responsibilities to protect and respect internationally recognized political, social, economic, and cultural rights. Similarly, business actors will consider their obligations to respect international human rights, as described in the United Nations Guiding Principles on Business and Human Rights (OHCHR 201 , also known as the Ruggie principles.The Ruggie principles have been widely referenced and endorsed by corporations and have led to the adoption of several corporate social responsibility (CSR) policies in various companies. With broadened support, the Ruggie principles will strengthen the role of businesses in protecting and promoting human rights and ensuring that the most crucial human values and legal standards of human rights are respected by A/IS technologists. Recommendations National policies and business regulations for A/IS should be founded on a rights-based approach. The Ruggie principles provide the internationally recognized legal framework for human rights standards that accounts for the impact of technology on individuals while also addressing inequalities, discriminatory practices, and the unjust distribution of resources. These six considerations for a rights-based approach to A/IS flow from the recommendation above: R esponsibility: Identify the right holders and the duty bearers and ensure that duty bearers have an obligation to fulfill all human rights. Ac countability: Oblige states, as duty bearers, to behave responsibly, to seek to represent the greater public interest, and to be open to public scrutiny of their A/IS policies. Pa rticipation: Encourage and support a high degree of participation of duty bearers, right holders, and other interested parties. 201 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Policy N ondiscrimination: Underlie the practice of A/IS with principles of nondiscrimination, equality, and inclusiveness. Particular attention must be given to vulnerable groups, to be determined locally, such as minorities, indigenous peoples, or persons with disabilities. Em powerment: Empower right holders to claim and exercise their rights. C orporate responsibility: Ensure that companies developments of A/IS comply with the rights-based approach. Companies must not willingly provide A/IS to actors that will use them in ways that lead to human rights violations. Further Resources Hu man rights-based approaches have been applied to development, education and reproductive health. See the UN Practitioners Portal on Human Rights Based Programming . O . Bowcott, Drone Strikes by US May Violate International Law, Says UN , The Guardian , October 18, A . Shapiro, Reform Predictive Policing , Nature News, vol. 541, no. 7638, pp. 458 460, Jan. 25, M . Garcia, How to Keep Your AI from Turning Into a Racist Monster , Wired , April 21, O . A. Osoba, and W. Welser IV, An Intelligence in Our Image: The Risks of Bias and Errors in Artificial Intelligence , (Research Report . Santa Monica, CA: RAND Corporation, A . Datta, M. C. Tschantz, and A. Datta. Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice, and Discrimination, arXiv:6491 [Cs] , D . Ingold, and S. Soper, Amazon Doesn t Consider the Race of Its Customers. Should It? Bloomberg, April 21, U nited Nations. Office of the High Commissioner of Human Rights. Guiding Principles on Business and Human Rights: Implementing the United Nations Protect, Respect and Remedy Framework . United Nations Office of the High Commissioner of Human Rights. New York and Geneva: UN, M apping Regulatory Proposals for Artificial Intelligence in Europe . Access Now , November V . Eubanks, Automating Inequality. How High- Tech Tools Profile, Police, and Punish the Poor . St. Martin s Press, January 202 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. PolicyIssue Develop government expertise in A/IS Background There is a consensus among private sector and academic stakeholders that effectively governing A/IS and related technologies requires a level of technical expertise that governments currently do not possess. Effective governance requires experts who understand and can analyze the interactions between A/IS technologies, policy objectives, and overall societal values. Sufficient depth and breadth of technical expertise will help ensure policies and regulations successfully support innovation, adhere to national principles, and protect public safety. Effective governance also requires an A/IS workforce that has adequate training in ethics and access to other resources on human rights standards and obligations, along with guidance on how to apply them in practice. Recommendations Policy makers should support the development of expertise required to create a public policy, legal, and regulatory environment that allows innovation to flourish while protecting the public and gaining public trust. 2 Example strategies include the following: E xpertise can be furthered through technical fellowships, or rotation schemes, where technologists spend an extended time in political offices, or policy makers work with organizations 3 that operate at the intersection of technology policy, technical engineering, and advocacy. This will enhance the technical knowledge of policy makers, strengthen ties between political and technical communities, and contribute to the formulation of effective A/IS policy. E xpertise can also be developed through cross-border sharing of best practices around A/IS legislation, consumer protection, workforce transformation, and economic displacement stemming from A/IS-based automation. This can be done through governmental cooperation, knowledge exchanges, and by building A/IS components into venues and efforts surrounding existing regulation, e.g., the General Data Protection Regulation (GDPR). B ecause A/IS involve rapidly evolving technologies, both workforce training in A/IS areas and long-term science, technology, engineering, and math (STEM) educational strategies, along with ethics courses, are needed beginning in primary school and extending into university or vocational courses. These strategies will foster A/IS expertise in the next generation of many groups, e.g., supervisors of critical systems, scientists, and policy makers. 203 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. PolicyFurther Resources J . Holdren, and M. Smith, Preparing for the Future of Artificial Intelligence. Washington, DC: Executive Office of the President, National Science and Technology Council, P . Stone, R. Brooks, E. Brynjolfsson, R. Calo, O. Etzioni, G. Hager, J. Hirschberg, S. Kalyanakrishnan, E. Kamar, S. Kraus, K. Leyton-Brown, D. Parkes, W. Press, A. Saxenian, J. Shah, M. Tambe, and A. Teller. 'Artificial Intelligence and Life in 2030': One Hundred Year Study on Artificial Intelligence . (Report of the 2015-2016 Study Panel). Stanford, CA: Stanford University, J apan Industrial Policy Spotlights AI, Foreign Labor . Nikkei Asian Review, May 20, Y .H. Weng, A European Perspective on Robot Law: Interview with Mady Delvaux-Stehres . Robohub , July 15, Issue Ensure governance and ethics are core components in A/IS research, development, acquisition, and use. Background Greater national investment in ethical A/IS research and development would stimulate the economy, create high-value jobs, improve governmental services to society, and encourage international innovation and collaboration (U.S. OSTP report on the Future of AI . A/IS have the potential to improve our societies through technologies such as intelligent robots and self-driving cars that will revolutionize automobile transportation and logistics systems and reduce traffic fatalities. A/IS can improve quality of life through smart cities and decision support in health care, social services, criminal justice, and the environment. To ensure such a positive effect on individuals, societies, and businesses, nations must increase A/IS R&D investments, with particular focus on the ethical development and deployment of A/IS. International collaboration involving governments, private industry, and non-governmental organizations (NGOs) would promote the development of standards, data sharing, and norms that guide ethically aligned A/IS R&D. Recommendations Develop national and international standards for A/IS to enable efficient and effective public and private sector investments. Important aspects for international standards include measures of societal benefits derived from A/IS, the use of ethical considerations in A/IS investments, and risks increased or decreased by A/IS. Nations should consider their own ethical principles and develop a framework for ethics that each country could use to reflect local systems of values and laws. This will encourage actors to think both locally and globally regarding ethics. Therefore, we recommend governments to: E stablish priorities for funding A/IS research that identify approaches and challenges for A/IS governance. This research will identify models for national and global A/IS governance and assess their benefits and adequacy to address A/IS societal needs.204 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Policy E ncourage the participation of a diverse set of stakeholders in the standards development process. Standards should address A/IS issues such as fairness, security, transparency, understandability, privacy, and societal impacts of A/IS. A global framework for identification and sharing of these and other issues should be developed. Standards should incorporate independent mechanisms to properly vet, certify, audit, and assign accountability for the A/IS applications. E ncourage and establish national and international research groups that provide incentives for A/IS research that is publicly beneficial but may not be commercially viable. Further Resources E . T. Kim, How an Old Hacking Law Hampers the Fight Against Online Discrimination. The New Yorker , October 1, N ational Research Council. Developments in Artificial Intelligence, Funding a Revolution: Government Support for Computing Research. Washington, DC: The National Academies Press, N . Chen, L. Christensen, K. Gallagher, R. Mate, and G. Rafert, Global Economic Impacts Associated with Artificial Intelligence. Analysis Group, February 25, T he Networking and Information Technology Research and Development Program, Supplement to the President s Budget, FY2017. NITRD National Coordination Office, April S . B. Furber, F. Galluppi, S. Temple, and L. A. Plana, The SpiNNaker Project. Proceedings of the IEEE, vol. 102, no. 5, pp. 652 665, H . Markram, The Human Brain Project, Scientific American, vol. 306, no. 2, pp. 50 55, June L . Yuan, China Gears Up in Artificial- Intelligence Race. Wall Street Journal , August 24, Issue Create policies for A/IS to ensure public safety and responsible A/IS design Background Effective governance encourages innovation and cooperation, helps synchronize policies globally, and reduces barriers to trade. Governments must ensure consistent and appropriate policies and regulations for A/IS that address transparency, explainability, predictability, and accountability of A/IS algorithms, risk management, 4 data protection, safety, and certification of A/IS. Appropriate regulatory responses are context- dependent and should be developed through an approach that is based on human rights 5 and has human well-being as a key goal.205 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. PolicyRecommendations Nations should develop and harmonize their policies and regulations for A/IS using a process that is based on informed input from a range of expert stakeholders, including academia, industry, NGOs, and government officials, that addresses questions related to the governance and safe deployment of A/IS. We recommend: P olicy makers should consider similar work from around the world. Due to the transnational nature of A/IS, globally synchronized policies can benefit public safety, technological innovation, and access to A/IS. P olicies should foster the development of economies able to absorb A/IS. Additional focus is needed to address the effect of A/IS on employment and income and how to ameliorate certain societal conditions. New models of public-private partnerships should be studied. P olicies for A/IS should remain founded on a rights-based approach. P olicy makers should be prepared to address issues that will arise when innovative and new practices enabled by A/IS are not consistent with current law. In A/IS, where there is often a different system developer, integrator, user, and ultimate customer, application of traditional legal concepts of agency, strict liability, and parental liability will require legal research and deliberation. Challenges from A/IS that must be considered include increasing complexity of and interactions between systems, and the potential for reduced predictability due to the nature of machine learning systems.Further Resources P . Stone, R. Brooks, E. Brynjolfsson, R. Calo, O. Etzioni, G. Hager, J. Hirschberg, S. Kalyanakrishnan, E. Kamar, S. Kraus, K. Leyton-Brown, D. Parkes, W. Press, A. Saxenian, J. Shah, M. Tambe, and A. Teller. 'Artificial Intelligence and Life in 2030': One Hundred Year Study on Artificial Intelligence . (Report of the 2015-2016 Study Panel). Stanford, CA: Stanford University, R . Calo, The Case for a Federal Robotics Commission , The Brookings Institution, O . Groth, and Mark Nitzberg, Solomon s Code: Humanity in a World of Thinking Machines (chapter 8 on governance) , New York: Pegasus Books, A . Mannes, Institutional Options for Robot Governance , 1 40, in We Robot 2016 , Miami, FL, April 1 2, G . E. Marchant, K. W. Abbott, and B. Allenby, Innovative Governance Models for Emerging Technologies . Cheltenham, U.K.: Edward Elgar Publishing, Y . H. Weng, Y. Sugahara, K. Hashimoto, and A. Takanishi. Intersection of Tokku Special Zone, Robots, and the Law: A Case Study on Legal Impacts to Humanoid Robots, International Journal of Social Robotics 7, no. 5, pp. 841 857, 206 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. PolicyIssue Educate the public on the ethical and societal impacts of A/IS Background It is imperative for industry, academia, and government to communicate accurately to the public both the positive and negative potential of A/IS and the areas that require caution. 6 Strategies for informing and engaging the public on A/IS benefits and challenges are critical to creating an environment conducive to effective decision-making. Educating users of A/IS will help influence the nature of A/IS development. Educating policy makers and regulators on the technical and legal aspects of A/IS will help enable the creation of well-defined policies that promote human rights, safety, and economic benefits. Educating corporations, researchers, and developers of A/IS on the benefits and risks to individuals and societies will enhance the creation of A/IS that better serve human well-being. 7 Another key requirement is that A/IS are sufficiently transparent regarding implicit and explicit values and algorithmic processes. This is necessary for the public understanding of A/IS accountability, predictions, decisions, biases, and mistakes. Recommendations Establish an international multi-stakeholder forum, to include commercial, governmental, and other civil society groups, to determine the best practices for using and developing A/IS. Codify the deliberations into international norms and standards. Many industries in particular, system industries (automotive, air and space, defense, energy, medical systems, manufacturing) will be changed by the growing use of A/IS. Therefore, we recommend governments to: I ncrease funding for interdisciplinary research and communication on topics ranging from basic research on intelligence to principles of ethics, safety, privacy, fairness, liability, and trustworthiness of A/IS. Societal aspects should be addressed both at an academic level and through the engagement of business, civil society, public authorities, and policy makers. E mpower and enable independent journalists and media outlets to report on A/IS by providing access to technical expertise. C onduct educational outreach to inform the public on A/IS research, development, applications, risks and rewards, along with the policies, regulations, and testing that are designed to safeguard human rights and public safety. 207 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. PolicyDevelop a broad range of A/IS educational programs. Undergraduate, professional degree, advanced degree, and executive education programs should offer instruction that ensures lawyers, legislators, and A/IS workers are well informed about issues arising from A/IS, including the need for measurable standards of A/IS performance, effects, and ethics, and the need to mature the still nascent capabilities to measure these elements of A/IS. Further Resources N etworking and Information Technology Research and Development (NITRD) Program, The National Artificial Intelligence Research and Development Strategic Plan , Washington, DC: Office of Science and Technology Policy, J . Saunders, P. Hunt, and J. S. Hollywood, Predictions Put into Practice: A Quasi- Experimental Evaluation of Chicago s Predictive Policing Pilot , Journal of Experimental Criminology, vol. 12, no. 347, pp. 347 371, [Online] Available: doi:1007/s1 1292-019272-[Accessed Nov. 10, 2018]. B . Edelman and M. Luca, Digital Discrimination: The Case of Airbnb.com . Harvard Business School Working Paper 14-054, Jan. 28, C . Garvie, A. Bedoya, and J. Frankle. The Perpetual Line-Up: Unregulated Police Face Recognition in America. Washington, DC: Georgetown Law, Center on Privacy & Technology, M . Chui, and J. Manyika, Automation, Jobs, and the Future of Work . Seattle, WA: McKinsey Global Institute, R . C. Arkin, Ethics and Autonomous Systems: Perils and Promises [Point of View] . Proceedings of the IEEE 104, no. 10, pp. 1779 1781, Sept. 19, E uropean Commission, Eurobarometer Survey on Autonomous Systems (DG Connect, June , looks at Europeans attitudes toward robots, driverless vehicles, and autonomous drones. The survey shows that those who have more experience with robots (at home, at work or elsewhere) are more positive toward their use. 208 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. PolicyThanks to the Contributors We wish to acknowledge all of the people who contributed to this chapter. The Policy Committee K ay Firth-Butterfield (Founding Co-Chair) Project Head, AI and Machine Learning at the World Economic Forum. Founding Advocate of AI-Global; Senior Fellow and Distinguished Scholar, Robert S. Strauss Center for International Security and Law, University of Texas, Austin; Co-Founder, Consortium for Law and Ethics of Artificial Intelligence and Robotics, University of Texas, Austin; Partner, Cognitive Finance Group, London, U.K. D r. Peter S. Brooks (Co-Chair) Institute for Defense Analyses M ina Hanna (Co-Chair) Chair IEEE-USA Artificial Intelligence and Autonomous Systems Policy Committee, Vice Chair IEEE-USA Research and Development Policy Committee, Member of the Editorial Board of IEEE Computer Magazine C hloe Autio Government & Policy Group, Intel Corporation St an Byers Frontier Markets Specialist C orinne Cath-Speth PhD student at Oxford Internet Institute, The University of Oxford, Doctoral student at the Alan Turing Institute, Digital Consultant at ARTICLE 19 M ichelle Finneran Dennedy Vice President, Chief Privacy Officer, Cisco; Author, The Privacy Engineer s Manifesto: Getting from Policy to Code to QA to Value E ileen Donahoe Executive Director of Stanford Global Digital Policy Incubator D anit Gal Project Assistant Professor, Keio University; Chair, IEEE Standard P7009 on the Fail-Safe Design of Autonomous and Semi-Autonomous Systems O laf J. Groth Professor of Strategy, Innovation, Economics & Program Director for Disruption Futures, HULT International Business School; Visiting Scholar, UC Berkeley BRIE/CITRIS; CEO, Cambrian.ai P hilip Hall (Founding Co-Chair) Co- Founder & CEO, RelmaTech; Member (and Immediate Past Chair), IEEE-USA Committee on Transportation & Aerospace Policy (CTAP); and Member, IEEE Society on Social Implications of Technology J ohn C. Havens Executive Director, The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems; Executive Director, The Council on Extended Intelligence; Author, Heartificial Intelligence: Embracing Our Humanity to Maximize Machines C yrus Hodes Senior Advisor, AI Office, UAE Prime Minister s Office; Co-Founded at Harvard Kennedy School the AI Initiative; Member, AI Expert Group at the OECD; Member, Global Council on Extended Intelligence.209 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Policy Ch ihyung Jeon Assistant Professor, Graduate School of Science and Technology Policy (STP), Korea Advanced Institute of Science and Technology (KAIST) A nja Kaspersen Former Head of International Security, World Economic Forum and head of strategic engagement and new technologies at the international committee of Red Cross (ICRC) N icolas Miailhe Co-Founder & President, The Future Society; Member, AI Expert Group at the OECD; Member, Global Council on Extended Intelligence; Senior Visiting Research Fellow, Program on Science Technology and Society at Harvard Kennedy School. Lecturer, Paris School of International Affairs (Sciences Po); Visiting Professor, IE School of Global and Public Affairs. Si mon Mueller Executive Director, The AI Initiative; Vice President, The Future Society C arolyn Nguyen Director, Microsoft's Technology Policy Group, responsible for policy initiatives related to data governance and personal data M ark J. Nitzberg Executive Director, Center for Human-Compatible Artificial Intelligence at UC Berkeley; co-author, Solomon s Code: Humanity in a World of Thinking Machines D aniel Schiff PhD Student, Georgia Institute of Technology; Chair, Sub-Group for Autonomous and Intelligent Systems Implementation, IEEE P7010: Well-being Metric for Autonomous and Intelligent Systems Eva ngelos Simoudis Co-Founder and Managing Director, Synapse Partners. Author, The Big Data Opportunity in our Driverless Future B rian W. Tang Founder and Managing Director, Asia Capital Markets Institute (ACMI); Founding executive director, LITE Lab@HKU at Hong Kong University Faculty of Law M artin Tisn Managing Director, Luminate S arah Villeneuve Policy Analyst; Member, IEEE P7010: Well-being Metric for Autonomous and Intelligent Systems Adr ian Weller Senior Research Fellow, University of Cambridge; Programme Director for AI, The Alan Turing Institute Y ueh-Hsuan Weng Assistant Professor, Frontier Research Institute for Interdisciplinary Sciences (FRIS), Tohoku University; Fellow, Transatlantic Technology Law Forum (TTLF), Stanford Law School D arrell M. West Vice President and Director, Governance Studies | Founding Director, Center for Technology Innovation | The Douglas Dillon Chair, Brookings Institution A ndreas Wolkenstein Researcher on neurotechnologies, AI, and political philosophy at LMU Munich (Germany) For a full listing of all IEEE Global Initiative Members, visit standards.ieee.org/content/dam/ ieee-standards/standards/web/documents/other/ec_bios.pdf . F or information on disclaimers associated with EAD1e, see How the Document Was Prepared.210 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Policy1 T his approach is rooted in internationally recognized economic, social, cultural, and political rights. 2 T his recommendation concurs with the multiple recommendations of the United States National Science and Technology Council, One Hundred Year Study of Artificial Intelligence, Japan s Cabinet Office Council, European Parliament s Committee on Legal Affairs, and others. 3 F or example, American Civil Liberties Union, Article 19, the Center for Democracy & Technology, Canada.AI, or Privacy International. United Nations committees may also be useful in fostering knowledge exchanges. 4 T his includes consideration regarding application of the precautionary principle, as used in environmental and health policy-making, where the possibility of widespread harm is high and extensive scientific knowledge or understanding on the matter is lacking.5 Hu man rights based approaches have been applied to development, education, and reproductive health. See the UN Practitioners Portal on Human Rights Based Programming. 6 ( AI100), Stanford University., August 7 P rivate sector initiatives are already emerging, such as the Partnership on AI; the AI for Good Foundation; and the Ethics and Governance of Artificial Intelligence Initiative, launched by Harvard s Berkman Klein Center for Internet & Society and the MIT Media Lab.Endnotes211 LawThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.The law affects and is affected by the development and deployment of autonomous and intelligent systems (A/IS) in contemporary life. Science, technological development, law, public policy, and ethics are not independent fields of activity that occasionally overlap. Instead, they are disciplines that are fundamentally tied to each other and collectively interact in the creation of a social order. Accordingly, in studying A/IS and the law, we focus not only on how the law responds to the technological innovation represented by A/IS, but also on how the law guides and sets the conditions for that innovation. This interactive process is complex, and its desired outcomes can rest on particular legal and cultural traditions. While acknowledging this complexity and uncertainty, as well as the acute risk that A/IS may intentionally or unintentionally be misused or abused, we seek to identify principles that will steer this interactive process in a manner that leads to the improvement, prosperity, and well-being of everyone. The fact that the law has a unique role to play in achieving this outcome is observed by Sheila Jasanoff, a preeminent scholar of science and technology studies: Part of the answer is to recognize that science and technology for all their power to create, preserve, and destroy are not the only engines of innovation in the world. Other social institutions also innovate, and they may play an invaluable part in realigning the aims of science and technology with those of culturally disparate human societies. Foremost among these is the law. 1 The law can play its part in ensuring that A/IS, in both design and operation, are aligned with principles of ethics and human well-being. 2 Comprehensive coverage of all issues within our scope of study is not feasible in a single chapter of Ethically Aligned Design (EAD) . Accordingly, aggregate coverage will expand as issues not yet studied are selected for treatment in future versions of EAD .Law212 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawEAD, First Edition includes commentary about how the law should respond to a number of specific ethical and legal challenges raised by the development and deployment of A/IS in contemporary life. It also focuses on the impact of A/IS on the practice of law itself. More specifically, we study both the potential benefits and the potential risks resulting from the incorporation of A/IS into a society s legal system specifically, in law making, civil justice, criminal justice, and law enforcement. Considering the results of those inquiries, we endeavor to identify norms for the adoption of A/IS in a legal system that will enable the realization of the benefits while mitigating the risks. 3 In this chapter of EAD, we include the following: Section Norms for the Trustworthy Adoption of A/IS in Legal Systems. This section addresses issues raised by the potential adoption of A/IS in legal systems for the purpose of performing, or assisting in performing, tasks traditionally carried out by humans with specialized legal training or expertise. The section begins with the question of how A/IS, if properly incorporated into a legal system, can improve the functions of that legal system and thus enhance its ability to contribute to human well-being. The section then discusses challenges to the safe and effective incorporation of A/IS into a legal system and identifies the chief challenge as an absence of informed trust. The remainder of the section examines how societies can fill the trust gap by enacting policies and promoting practices that advance publicly accessible standards of effectiveness , competence , accountability , and transparency . Section Legal Status of A/IS. This section addresses issues raised by the legal status of A/IS, including the potential assignment of certain legal rights and obligations to such systems. The section provides background on the issue and outlines some of the potential advantages and disadvantages of assigning some form of legal personhood to A/IS. Based on these considerations, the section concludes that extending legal personhood to A/IS is not appropriate at this time. It then considers alternatives and outlines certain future conditions that might warrant reconsideration of the section s central recommendation. 213 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawSection Norms for the Trustworthy Adoption of A/IS in Legal Systems4 It s a day that is here. John G. Roberts , Chief Justice of the Supreme Court of the United States, when asked in 2017 whether he could foresee a day when intelligent machines would assist with courtroom fact-finding or judicial decision-making. 5 A/IS hold the potential to improve the functioning of a legal system and, thereby, to contribute to human well-being. That potential will be realized, however, only if both the use of A/IS and the avoidance of their use are grounded in solid information about the capabilities and limitations of A/IS, the competencies and conditions required for their safe and effective operation (including data requirements), and the lines along which responsibility for the outcomes generated by A/IS can be assigned. Absent that information, society risks both uninformed adoption of A/IS and uninformed avoidance of adoption of A/IS, risks that are particularly acute when A/IS are applied in an integral component of the social order, such as the law. U ninformed adoption poses the risk that A/IS will be applied to inform or replace the judgments of legal actors (legislators, judges, lawyers, law enforcement officers, and jurors) without controls to ensure their safe and effective operation. They may even be used for purposes other than those for which the systems have been validated and vetted for legal use. In addition to actual harm to individuals, the result will be distrust, not only of the effectiveness of A/IS, but also of the fairness and effectiveness of the legal system itself. U ninformed avoidance of adoption poses the risk that a lack of understanding of what is required for the safe and effective operation of A/IS will result in blanket distrust of all forms and applications of A/IS, even those that are, when properly applied, safe and effective. The result will be a failure to realize the significant improvements in the legal system that A/IS can offer and a continuation of systems that are, even with the best of safeguards, still subject to human bias, inconsistency, and error. 6 In this section, we consider how society can address these risks by developing norms for the adoption of A/IS in legal systems. The specific issues discussed follow. The first and second issues reflect the potential benefits of, and challenges to, trustworthy adoption of A/IS in the world s legal systems. The remaining issues discuss four principles, 7 which, if adhered to, will enable trustworthy adoption.8 9214 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law I ssue Well-being, Legal Systems, and A/IS How can A/IS improve the functioning of a legal system and, thereby, enhance human well-being? I ssue Impediments to Informed Trust What are the challenges to adopting A/IS in legal systems and how can those impediments be overcome? I ssue Effectiveness How can the collection and disclosure of evidence of effectiveness of A/IS foster informed trust in the suitability of A/IS for adoption in legal systems? I ssue Competence How can specification of the knowledge and skills required of the human operator(s) of A/IS foster informed trust in the suitability of A/IS for adoption in legal systems? I ssue Accountability How can the ability to apportion responsibility for the outcome of the application of A/IS foster informed trust in the suitability of A/IS for adoption in legal systems? I ssue Transparency How can sharing information that explains how A/IS reach given decisions or outcomes foster informed trust in the suitability of A/IS for adoption in legal systems? Issue Well-Being, Legal Systems, and A/IS How can A/IS improve the functioning of a legal system and, thereby, enhance human well-being? Background An effective legal system contributes to human well-being. The law is an integral component of social order; the nature of a legal system informs, in fundamental ways, the nature of a society, its potential for economic growth and technological innovation, and its capacity for advancing the well-being of its members. If the law is a constitutive element of social order, it is not surprising that it also plays a key role in setting the conditions for well-being and economic growth. In part, this flows from the fact that a well-functioning legal system is an element of good governance. Good governance and a well-functioning legal system can help society and its members flourish, as measured by indicators of both economic prosperity 10 and human well-being.11 The attributes of good governance can be defined in several ways. Good governance can mean democracy; the observance of norms of human rights enshrined in conventions such as the Universal Declaration of Human Rights 12 and the Convention of the Rights of the Child;13 and constitutional constraints on government power. It can also 215 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawmean bureaucratic competence, law and order, property rights, and contract enforcement. The United Nations (UN) defines the rule of law as: a principle of governance in which all persons, institutions and entities, public and private, including the State itself, are accountable to laws that are publicly promulgated, equally enforced and independently adjudicated. . . . It requires, as well, measures to ensure adherence to the principles of supremacy of law, equality before the law, accountability to the law, fairness in the application of the law, separation of powers, participation in decision-making, legal certainty, avoidance of arbitrariness and procedural and legal transparency. 14 Orderly systems of legal rules and institutions generally correlate positively with economic prosperity, social stability, and human well-being, including the protection of childhood. 15 Studies from the World Bank suggest that legal reforms can lead to increased foreign investment, higher incomes, and greater wealth. 16 Wealth, in turn, can enable policies that support improved education, health, environmental protection, equal opportunity, and, in democratic societies, greater individual freedom. Law, moreover, can contribute to prosperity not only through its functional attributes, but also through its substantive content. Patent laws, for example, if well-designed, can encourage technological innovation, leading to increases in productivity and the economic growth that follows. Poorly designed patent laws, on the other hand, may foster monopolistic markets and decrease competition, resulting in a decreased pace of technological innovation, fewer gains in productivity, and slower economic growth. 17 While economic growth is a valuable benefit of a well-designed and well-functioning legal system, it is not the only benefit. Such a system can bring benefits to society and its members that, beyond economic prosperity, extend to mental and physical well-being. Specific benefits include the protection and advancement of an individual s dignity, 18 human rights,19 liberty, stability, security, equality of treatment under the law, and ability to provide for the future.20 In fact, recent thinking on the relationship between law and economic development has come to hold that a well-functioning legal system is not simply a means to development but is development, insofar as such a system is a constitutive element of a social order that protects and advances human dignity, rights, and well-being. As this position has been characterized by David Kennedy: the focal point for development policy was increasingly provided less by economics than from ideas about the nature of the good state themselves provided by literatures of political science, political economy, ethics, social theory, and law. In particular, human rights and the rule of law 21 became substantive definitions of development. One should promote human rights not to facilitate development but as development. The rule of law was not a development tool it was itself a development 216 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawobjective. Increasingly, law understood as a combination of human rights, courts, property rights, formalization of entitlements, prosecution of corruption, and public order came to define development. 22 While this shift from considering law as a means to an end to considering law as an end in itself has been criticized on the grounds that it takes the focus off the difficult political choices that are inherent in any development policy, 23 it remains true that a well-functioning legal system is essential to the realization of a social order that protects and advances human dignity, rights, and well-being. A/IS can contribute to the proper functioning of a legal system. A properly functioning legal system, one that is conducive to both economic prosperity and human well-being, will have a number of attributes. It should be: S peedy: enable quick resolution of civil and criminal cases; Fa ir: produce results that are just and proportionate to circumstance;24 F ree from undesirable bias: operate without prejudice; C onsistent: arrive at outcomes in a principled, consistent, and nonarbitrary manner; T ransparent: be open to appropriate public examination and oversight;25 A ccessible: be equally open to all citizens and residents in resolving disputes; E ffective: achieve the ends intended by its laws and rules without negative collateral consequences; 26 A ccurate: achieve accurate results, minimizing both false positives (persons unjustly or incorrectly targeted, investigated, or sentenced for crimes) and false negatives (persons incorrectly not targeted, investigated, or sentenced for crimes); Ada ptable: have the flexibility to adapt to changes in societal circumstances. A/IS have the potential to alter the overall functioning of a legal system. A/IS, applied responsibly and appropriately, could improve the legislative process, enhance access to justice, accelerate judicial decision-making, provide transparent and readily accessible information on why and how decisions were reached, reduce bias, support uniformity in judicial outcomes, help society identify (and potentially correct) judicial errors, and improve public confidence in the legal system. By way of example: A /IS can make legislation and regulation more effective and adaptable. For lawmaking, A/IS could help legislators analyze data to craft more finely tuned, responsive, evidence- based laws and regulations. This could, potentially, offer self-correcting suggestions to legislators (and to the general public) to help inform dialogue on how to meet defined public policy objectives. A /IS can make the practice of law more effective and efficient. For example, A/IS can enhance the speed , accuracy , and accessibility of the process of fact-finding in legal proceedings. When used appropriately in legal fact-finding, particularly in jurisdictions that allow extensive discovery or disclosure, A/IS already make litigation and investigations more accessible by analyzing vast data 217 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawcollections faster, more efficiently, and potentially more effectively27 than document analysis conducted solely by human attorneys. By making fact-finding in an era of big data progressively easier, faster, and cheaper, A/IS may facilitate access to justice for parties that otherwise may find using the legal system to resolve disputes cost-prohibitive. A/IS can also help ensure that justice is rendered based on better accounting of the facts, thus serving the central purpose of any legal system. I n both civil and criminal proceedings, A/IS can be used to improve the accuracy, fairness, and consistency of decisions rendered during proceedings. A/IS could serve as an auditing function for both the civil and criminal justice systems, helping to identify and correct judicial and law enforcement errors. 28 A /IS can increase the speed , accuracy , fairness , freedom from bias , and general effectiveness with which law enforcement resources are deployed to combat crime. A/IS could be used to reduce or prevent crime, respond more quickly to crimes in progress, and improve collaboration among different law enforcement agencies. 29 A /IS can help ensure that determinations about the arrest, detention, and incarceration of individuals suspected of, or convicted of, violations of the law are fair , free from bias , consistent , and accurate . Automated risk assessment tools have the potential to address issues of systemic racial bias in sentencing, parole, and bail determination while also safely reducing incarceration and recidivism rates by identifying individuals who are less likely to commit crimes if released. A /IS can help to ensure that the tools, procedures, and resources of the legal system are more transparent and accessible to citizens. For the ordinary citizen, A/IS can democratize access to legal expertise, especially in smaller matters, where they may provide effective, prompt, and low-cost initial guidance to an aggrieved party; for example, in landlord-tenant, product purchase, employment, or other contractual contexts where the individual often tends to find access to legal information and legal advice prohibitive, or where asymmetry of resources between the parties renders recourse to the legal system inequitable. 30 A/IS have the potential to improve how a legal system functions in fundamental ways. As is the case with all powerful tools, there are some risks. A/IS should not be adopted in a legal system without due care and scrutiny; they should be adopted after a society s careful reflection and proper examination of evidence that their deployment and operation can be trusted to advance human dignity, rights, and well-being (see Issues 2 . Recommendations31 P olicymakers should, in the interest of improving the function of their legal systems and bringing about improvements to human well-being, explore, through a broad consultative dialogue with all stakeholders, how A/IS can be adopted for use in their legal systems. They should do 218 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawso, however, only in accordance with norms for adoption that mitigate the risks attendant on such adoption (see Issues 2 6 in this section). G overnments, non-governmental organizations, and professional associations should support educational initiatives designed to create greater awareness among all stakeholders of the potential benefits and risks of adopting A/IS in the legal system, and of the ways of mitigating such risks. A particular focus of these initiatives should be the ordinary citizen who interacts with the legal system as a victim or criminal defendant. Further Resources A . Brunetti, G. Kisunko, and B. Weder, Credibility of Rules and Economic Growth: Evidence from a Worldwide Survey of the Private Sector , The World Bank Economic Review, vol. 12, no. 3, pp. 353-384, Sep. S . Jasanoff, Governing Innovation: The Social Contract and the Democratic Imagination, Seminar, vol. 597, pp. 16-25, May D . Kennedy, The Rule of Law, Political Choices and Development Common Sense, in The New Law and Economic Development: A Critical Appraisal , D. M. Trubek and A. Santos, eds., Cambridge: Cambridge University Press, 2006, pp. 95- A rtificial Intelligence , National Institute of Standards and Technology. K . Schwab, The Global Competitiveness Report: 2018 , The World Economic Forum, A . Sen, Development as Freedom. New York, NY: Alfred A. Knopf, Uni ted Nations General Assembly, Universal Declaration of Human Rights , Dec. 10, U NICEF, Convention on the Rights of the Child , Nov. 4, U nited Nations Office of the High Commissioner: Human Rights, The Vienna Declaration and Programme of Action , June 25, W orld Bank, World Development Report Governance and the Law , Jan. W orld Justice Project, Rule of Law Index , June 219 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawIssue Impediments to Informed Trust What are the challenges to adopting A/IS in legal systems and how can those impediments be overcome? Background Although the benefits to be gained by adopting A/IS in legal systems are potentially numerous (see the discussion of Issue , there are also significant risks that must be addressed in order for the A/IS to be adopted in a manner that will realize those benefits. The risks sometimes mirror expected benefits: t he potential for opaque decision-making; t he intentional or unintentional biases and abuses of power; t he emergence of nontraditional bad actors; t he perpetuation of inequality; t he depletion of public trust in a legal system; t he lack of human capital active in judicial systems to manage and operate A/IS; t he sacrifice of the spirit of the law in order to achieve the expediency that the letter of the law allows; t he unanticipated consequences of the surrender of human agency to nonethical agents; t he loss of privacy and dignity; an d the erosion of democratic institutions.32 By way of example: C urrently, A/IS used in justice systems are not subject to uniform rules and norms and are often adopted piecemeal at the local or regional level, thereby creating a highly variable landscape of tools and adoption practices. Critics argue that, far from improving fact-finding in civil and criminal matters or eliminating bias in law enforcement, these tools have unproven accuracy, are error-prone, and may serve to entrench existing social inequalities. These tools potential must be weighed against their pitfalls. These include unclear efficacy; incompetent operation; and potential impairment of a legal system s ability to adhere to principles of socioeconomic, racial, or religious equality, government transparency, and individual due process, to render justice in an informed, consistent, and fair manner. I n the case of State v. Loomis , an important but not widely known case, the Wisconsin Supreme Court held that a trial court s use of an algorithmic risk assessment tool in sentencing did not violate the defendant s due process rights, despite the fact that the methodology used to obtain the automated assessment was not disclosed to either the court or the defendant. 33 A man received a lengthy sentence based in part on what an opaque algorithm thought of him. While the court considered many factors, and sought to balance competing societal values, this 220 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawis just one case in a growing set of cases illustrating how criminal justice systems are being impacted by proprietary claims of trade secrets, opaque operation of A/IS, a lack of evidence of the effectiveness of A/IS, and a lack of norms for the adoption of A/IS in the extended legal system. M ore generally, humans tend to be subject to the cognitive bias known as anchoring , which can be described as the excessive reliance on an initial piece of information. This may lead to the progressive, unwitting, and detrimental reliance of judges and legal practitioners on assessments produced by A/IS. This risk is compounded by the fact that A/IS are (and shall remain in the foreseeable future) nonethical agents, incapable of empathy, and thus at risk of being unable to produce decisions aligned with not just the letter of the law, but also the spirit of the law and reasonable regard for the circumstances of each defendant. T he required technical and scientific knowledge to procure, deploy, and effectively operate A/IS, as well as that required to measure the ability of A/IS to achieve a given purpose without adverse collateral consequences, represent significant hurdles to the beneficial long-term adoption of A/IS in a legal system. This is especially the case when as is the case presently actors in the civil and criminal justice systems and in law enforcement may lack the requisite specialized technological or scientific expertise. 34 Such risks must be addressed in order to ensure sustainable management and public oversight of what will foreseeably become an increasingly automated justice system. 35 The view expressed by the Organisation for Economic Co-operation and Development (OECD) in the domain of digital security that robust strategies to [manage risk] are essential to establish the trust needed for economic and social activities to fully benefit from digital innovation 36 applies equally to the adoption of A/IS in the world s legal systems. Informed trust. If we are to realize the benefits of A/IS, we must trust that they are safe and effective. People board airplanes, take medicine, and allow their children on amusement park rides because they trust that the tools, methods, and people powering those technologies meet certain safety and effectiveness standards that reduce the risks to an acceptable level given the objectives and benefits to be achieved. This need for trust is especially important in the case of A/IS used in a legal system. The black box nature and lack of trust in A/IS deployed in the service of a legal system could quickly translate into a lack of trust in the legal system itself. This, in turn, may lead to an undermining of the social order. Therefore, if we are to improve the functioning of our legal systems through the adoption of A/IS, we must enact policies and promote practices that allow those technologies to be adopted on the basis of informed trust . Informed trust rests on a reasoned evaluation of clear and accurate information about the effectiveness of A/IS and the competence of their operators. 37 221 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawTo formulate policies and standards of practice intended to foster informed trust, it is helpful, first, to identify principles applicable over the entire supply chain for the delivery of A/IS-enabled decisions and guidance, including design, development, procurement, deployment, operation, and validation of effectiveness, that, if adhered to, will foster trust. Once those general principles have been identified, specific policies and standards of practice can be formulated that encourage adherence to the principles in every aspect of a legal system, including lawmaking, civil and criminal justice, and law enforcement. Such principles, if they are to serve their intended purpose of informing effective policies and practices, must meet certain design criteria. Specifically, the principles should be (a) individually necessary and collectively sufficient, (b) globally applicable but culturally flexible, and (c) capable of being operationalized in applicable functions of the legal system . A set of principles that meets these criteria will provide an effective framework for the development of policies and practices that foster trust, while leaving considerable flexibility in the specific policies and standards of practice that a society chooses to implement in furthering adherence to the principles. A set of four principles that we believe meets the design criteria just described are the following: E ffectiveness : Adoption of A/IS in a legal system should be based on sound empirical evidence that they are fit for their intended purpose. C ompetence : A/IS should be adopted in a legal system only if their creators specify the skills and knowledge required for their effective operation and if their operators adhere to those competency requirements. A ccountability : A/IS should be adopted in a legal system only if all those engaged in their design, development, procurement, deployment, operation, and validation of effectiveness maintain clear and transparent lines of responsibility for their outcomes and are open to inquiries as may be appropriate. T ransparency : A/IS should be adopted in a legal system only if the stakeholders in the results of A/IS have access to pertinent and appropriate information about their design, development, procurement, deployment, operation, and validation of effectiveness. In the remainder of Section 1, we elaborate on each of these principles. Before turning to a specific discussion of each, we add two further considerations that should be kept in mind when applying them collectively. Differences in emphasis. While all four of the aforementioned principles will contribute to the fostering of trust, each principle will not contribute equally in every circumstance. For example, in many applications of A/IS, a well-established measure of effectiveness, obtained by proven and accepted methods, may go a considerable way to creating conditions for trust in the given application. In such a case, the other principles may add to trust, but they may not be necessary to establish trust. Or, to take another example, in some applications the role of the human operator may be minimal, while in other applications there will be extensive scope for 222 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawhuman agency where competence has a greater role to play. In finding the right emphasis and balance among the four principles, policymakers and practitioners will have to consider the specific circumstances of A/IS. Flexibility in implementation. It should be noted that we have addressed the four principles above at a rather high level and have not offered specific prescriptions of how adherence to the principles should be implemented. This is by design. Although adherence to all four principles is important, it is also important that, at the operational level, flexibility be allowed for the selection and implementation of policies and practices that (a) are in harmony with a given society s traditions, norms, and values; (b) conform with the laws and regulations operative in a given jurisdiction; and (c) are consistent with the ethical obligations of legal practitioners. Recommendations G overnments should set procurement and contracting requirements that encourage parties seeking to use A/IS in the conduct of business with or for the government, particularly with or for the court system and law enforcement agencies, to adhere to the principles of effectiveness, competence, accountability, and transparency as described in this chapter. This can be achieved through legislation or administrative regulation. All government efforts in this regard should be transparent and open to public scrutiny. P rofessionals engaged in the practice, interpretation, and enforcement of the law (such as lawyers, judges, and law enforcement officers), when engaging with or relying on providers of A/IS technology or services, should require, at a minimum, that those providers adhere to, and be able to demonstrate adherence to, the principles of effectiveness, competence, accountability, and transparency as described in this chapter. Likewise, those professionals, when operating A/IS themselves, should adhere to, and be able to demonstrate adherence to, the principles of effectiveness, competence, accountability, and transparency. Demonstrations of adherence to the requirements should be publicly accessible. R egulators should permit insurers to issue professional liability and other insurance policies that consider whether the insured (either a provider or operator of A/IS in a legal system) adheres to the principles of effectiveness, competence, accountability, and transparency (as they are articulated in this chapter). Further Resources C riminal Law Sentencing Guidelines Wisconsin Supreme Court Requires Warning Before Use of Algorithmic Risk Assessments in Sentencing State v. Loomis, 881 N.W.2d 749 (Wis. , Harvard Law Review, vol. 130, no. 5, pp. 1530-1537, K . Freeman, Algorithmic Injustice: How the Wisconsin Supreme Court Failed to Protect Due Process Rights in State v. Loomis , North Carolina Journal of Law and Technology, vol. 18, no. 5, pp. 75-76, 223 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law M anaging Digital Security and Privacy Risk: Background Report for Ministerial Panel 2 , Organisation for Economic Co-operation and Development (OECD) Directorate for Science, Technology, and Innovation: Committee on Digital Economy Policy, June 1, S tate v Loomis , 881 N.W.2d 749 (Wis. , cert. denied (. G lobal Governance of AI Roundtable: Summary Report 2018, World Government Summit, Issue Effectiveness How can the collection and disclosure of evidence of effectiveness of A/IS foster informed trust in the suitability for adoption in legal systems? Background An essential component of trust in a technology is trust that it works and meets the purpose for which it is intended. We now turn to a discussion of the role that evidence of effectiveness, chiefly in the form of the results of a measurement exercise, can play in fostering informed trust in A/IS as applied in legal systems. 38 We begin with a general characterization of what we mean by evidence of effectiveness : what we are measuring, how we are measuring, what form our results take, and who the intended consumers of the evidence are. We then identify the specific features of the practice of measuring effectiveness that will enable it to contribute to informed trust in A/IS as applied in a legal system. What constitutes evidence of effectiveness? What we are measuring. In gathering evidence of effectiveness, we are seeking to gather empirical data that will tell us whether a given technology or its application will serve as an effective solution to the problem it is intended to address. Serving as an effective solution means more than meeting narrow specifications or requirements; it means that the A/IS are capable of addressing their target problems in the real world, which, in the case of A/IS applied in a legal system, are problems in the making, administration, adjudication, or enforcement of the law. It also means remaining practically feasible once collateral concerns and potential unintended consequences are taken into account. 39 To take a non-A/IS example, under the definition of effectiveness we are considering, for an herbicide to be considered effective, it must be shown not only to kill the target weeds, but also to do so without causing harm to nontarget plants, to the person applying the agent, and to the environment in general. Under the definition above, assessing the effectiveness of A/IS in accomplishing the target task (narrowly defined) is not sufficient; it may also be necessary to assess the extent to which the A/IS are aligned with applicable 224 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawlaws, regulations, and standards,40 and whether (and to what extent) they impinge on values such as privacy, fairness, or freedom from bias.41 Whether such collateral concerns are salient will depend on the nature of the A/IS and on the particular circumstances in which they are to be applied.42 However, it is only from such a complete view of the impact of A/IS that a balanced judgment can be made of the appropriateness of their adoption. 43 Although the scope of an evaluation of effectiveness is broader than a narrowly focused verification that a specific requirement is met, it has its limits. There are measures of aspects of A/IS that one might find useful but that are outside the scope of effectiveness. For example, given frequently expressed concerns that A/IS will one day cross the limits of their intended purpose and overwhelm their creators and users, one might seek to define and obtain general measures of the autonomy of a system or of a system s capacity for artificial general intelligence (AGI). Although such measures could be useful assuming they could be defined they are beyond the scope of evaluations of effectiveness. Effectiveness is always tied to a target purpose, even if it includes consideration of the collateral effects of the manner of meeting that purpose. What we are measuring is therefore a general fitness for purpose . How we measure. Evidence of effectiveness is typically gathered in one of two types of exercises: 44 A single-system validation exercise measures and reports on the effectiveness of a single system on a given task. In such an exercise, the system to be validated will typically have already carried out the target task on a given data set. The purpose of the validation is to provide empirical evidence of how successful the system has been in carrying out the task on that data set. Measurements are obtained by independent sampling and review of the data to which the system was applied. Once obtained, those metrics serve to corroborate or refute the hypothesis that the system operated as intended in the instance under consideration. An example of validation as applied to legal fact-finding would be a test of the effectiveness of A/IS that had been used to retrieve material relevant (as defined by the humans deploying the system) to a given legal inquiry from a collection of emails. A m ulti-system (or benchmarking) evaluation involves conducting a comparative study of the effectiveness of several systems designed to meet the same objective. Typically, in such a study, a test data set is identified, a task to be performed is defined (ideally, a task that models the real-world objectives and conditions for which the systems under evaluation have been designed , the systems to be evaluated are used to carry out the task, and the success of each system in carrying out the task is measured and reported. An example of this sort of evaluation applied to a specific 225 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawreal-world challenge in the justice system is the series of evaluations of the effectiveness of information retrieval systems in civil discovery, including A/IS, conducted as part of the US National Institute of Standards and Technology (NIST) Text REtrieval Conference (TREC) Legal Track initiative. 46 The measurements obtained by both types of evaluation exercises are valuable. The results of a single-system validation exercise are typically more specific, answering the question of whether a system was effective in a specific instance. The results of a multi-system evaluation are typically more generic, answering the question of whether a system can be effective in real-world circumstances. Both questions are important, hence both types of evaluations are valuable. 47 The form of results. The results of an evaluation typically take the form of a number a quantitative gauge of effectiveness. This can be, for example, the decreased likelihood of developing a given medical condition; safety ratings for automobiles; recall measures for retrieving responsive documents; and so on. Certainly, qualitative considerations are not (and should not) be ignored; they often provide context crucial to interpreting the quantitative results. 48 Nevertheless, at the heart of the results of an evaluation exercise is a number, a metric that serves as a telling indicator of effectiveness. 49 In some cases, the research community engaged in developing any new system will have reached consensus on salient effectiveness metrics. In other cases, the research community may not have reached a consensus, requiring further study. In the case of A/IS, given both their accelerating development and the fact that they are often applied to tasks for which the effectiveness of their human counterparts is seldom precisely gauged, we are often still at the stage of defining metrics. An example of an application of A/IS for which there is a general consensus around measures of effectiveness is legal electronic discovery, 50 where there is a working consensus around the use of the evaluation metrics referred to as recall and precision . 51 Conversely, in the case of A/IS applied in support of sentencing decisions, a consensus on the operative effectiveness metrics does not yet exist. 52 The consumers of the results. In defining metrics, it is important to keep in mind the consumers of the results of an evaluation of effectiveness. Broadly speaking, it is helpful to distinguish between two categories of stakeholders who will be interested in measurements of effectiveness: E xperts are the researchers, designers, operators, and advanced users with appropriate scientific or professional credentials who have a technical understanding of the way in which a system works and are well-versed in evaluation methods and the results they generate. N onexperts are the legislators, judges, lawyers, prosecutors, litigants, communities, victims, defendants, and system advocates whose work or legal outcomes may, even if only indirectly, be affected by the results 226 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawof a given system. These individuals, however, may not have a technical understanding of the way in which a system operates. Furthermore, they may have little experience in conducting scientific evaluations and interpreting their results. Effectiveness metrics must meet the needs of both expert and nonexpert consumers. W ith respect to experts, the purpose of an effectiveness metric is to advance both long-term research and more immediate product development, maintenance, and oversight . To achieve that purpose, it is appropriate to define a fine-grained metric that may not be within the grasp of the nonexpert. Researchers and developers will be acting on the information provided by such a metric, so it should be tailored to their needs. W ith respect to nonexperts, including the general public, the purpose of an effectiveness metric is to advance informed trust, meaning trust that is based on sound evidence that the A/IS have met, or will meet, their intended objectives, taking into account both the immediate purpose and the contextual purpose of preserving and fostering important values such as human rights, dignity, and well-being. For this purpose, it will be necessary to define a metric that can serve as a readily understood summary measure of effectiveness. This metric must provide a simple, direct answer to the question of how effective a given system is. Automobile safety ratings are an example of this sort of metric. For automobile designers and engineers, the summary metrics are not sufficiently fine-grained to give immediately actionable information; for consumers, however, the metrics, insofar as they are accurate, empower them to make better-informed buying decisions. For the purpose of fostering informed trust in A/IS adopted in the legal system, the most important goal is to establish a clear measure of effectiveness that can be understood by nonexperts. However, significant obstacles to achieving this goal include (a) developer incentives that prioritize research and development, along with the metrics that support such efforts, and (b) market forces that inhibit, or do not encourage, consumer-facing metrics. For those reasons, it is important that the selection and definition of the operative metrics draw on input not only from the A/IS creators but from other stakeholders as well; only under these conditions will a consensus form around the meaningfulness of the metrics. What measurement practices foster informed trust? By equipping both experts and nonexperts with accurate information regarding the capabilities and limitations of a given system, measurements of effectiveness can provide society with information needed to adopt and apply A/IS in a thoughtful, carefully considered, beneficial manner. 53 In order for the practice of measuring effectiveness to realize its full potential for fostering trust and mitigating the risks of uninformed adoption and uninformed avoidance of adoption, it must have certain features:227 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law M eaningful metrics: As noted above, an essential element of a measurement practice is a metric that provides an accurate and readily understood gauge of effectiveness. The metric should provide clear and actionable information as to the extent to which a given application has, or has not, met its objective so that potential users of the results of the application can respond accordingly. For example, in legal discovery, both recall and precision have done this well and have contributed to the acceptance of the use of A/IS for this purpose. 54 S ound methods: Measures of effectiveness must be obtained by scientifically sound methods. If, for example, measures are obtained by sampling, those sample-based estimates must be the result of sound statistical procedures that hold up to objective scrutiny. V alid data: Data on which evaluations of effectiveness are conducted should accurately represent the actual data to which the given A/IS would be applied and should be vetted for potential bias. Any data sets used for benchmarking or testing should be collected, maintained, and used in accordance with principles for the protection of individual privacy and agency. 55 Aw areness and consensus: Measurement practices must not only be technically sound in terms of metrics, methods, and data, but they must also be widely understood and accepted as evidence of effectiveness. I mplementation: Measurement practices must be both practically feasible and actually implemented, i.e., widely adopted by practitioners T ransparency. Measurement methods and results must be open to scrutiny by experts and the general public. 57 Without such scrutiny, the measurements will not be trusted and will be incapable of fulfilling their intended purpose. 58 In seeking to advance informed trust in A/IS, policymakers should formulate policies and promote standards that encourage sound measurement practices, especially those that incorporate the key features. Additional note. While in all circumstances all four principles discussed in this chapter (Effectiveness, Competence, Accountability, Transparency) will have something to contribute to the fostering of informed trust, it is not the case that in every circumstance all four principles will contribute equally to the fostering of trust. In some circumstances, a well-established measure of effectiveness, obtained by proven and accepted methods, may go a considerable way, on its own, in fostering trust in a given application or distrust, if that is what the measurements indicate. In such circumstances, the challenges presented by the other principles, e.g., the challenge of adhering to the principle of transparency while respecting intellectual property considerations, may become of secondary importance.228 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawIllustration Effectiveness The search for factual evidence in large document collections in US civil or criminal proceedings has traditionally involved page-by-page manual review by attorneys. Starting in the 1990s, the proliferation of electronic data, such as email, rendered manual review prohibitively costly and time-consuming. By 2008, A/IS designed to substantially automate review of electronic data (a task known as e-discovery ) were available. Yet, adoption remained limited. Chief among the obstacles to adoption was a concern about the effectiveness, and hence defensibility in court, of A/IS in e-discovery. Simply put, practitioners and courts needed a sound answer to a simple question: Does it work? Starting in 2006, the US NIST 59 conducted studies to assess that question.60 The studies focused on, among others, two sound statistical metrics, both expressed as easy-to-understand percentages: 61,62 Re call, which is a gauge of the extent to which all the relevant documents were retrieved. For example, if there were 1,000 relevant documents to be found in the collection, and the review process identified 700 of them, then it achieved 70% recall. P recision , which is a gauge of the extent to which the documents identified as relevant by a process were actually relevant. For example, if for every two relevant documents the system captured, it also captured a nonrelevant one (i.e., a false positive), then it achieved 67% precision. The studies provided empirical evidence that some systems could achieve high scores (80%) according to both metrics. 63 In a seminal follow-up study, Maura R. Grossman and Gordon V. Cormack found that two automated systems did, in fact, conclusively outperform human reviewers. 64 Drawing on the results of that study, Magistrate Judge Andrew Peck, in an opinion with far-reaching consequences, gave court approval for the use of A/IS to conduct legal discovery. 65 The story of the TREC Legal Track s role in facilitating the adoption of A/IS for legal fact-finding contains a few lessons: M etrics: By focusing on recall and precision, the TREC studies quantified the effectiveness of the systems evaluated in a way that legal practitioners could readily understand. B enchmarks: The TREC studies filled an important gap: independent, scientifically sound evaluations of the effectiveness of A/IS applied to the real-world challenge of legal e-discovery. C ollaboration: The founders of the TREC studies and the most successful participants came from both scientific and legal backgrounds, demonstrating the importance of multidisciplinary collaboration. The TREC studies are a shining example of how the truth-seeking protocols of science can be used to advance the truth-seeking protocols of the law. They can serve as a conceptual basis for future benchmarking efforts, as well as the development of standards and certification programs to support informed trust when it comes to effectiveness of A/IS deployed in legal systems. 66229 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawRecommendations G overnments should fund and support the establishment of ongoing benchmarking exercises designed to provide valid, publicly accessible measurements of the effectiveness of A/IS deployed, or potentially deployed, in the legal system. That support could take a number of forms, ranging from direct sponsorship and oversight for example, by nonregulatory measurement laboratories such as the US NIST to indirect support by the recognition of the results of a credible third-party benchmarking exercise for the purposes of meeting procurement and contracting requirements. All government efforts in this regard should be transparent and open to public scrutiny. G overnments should facilitate the creation of data sets that can be used for purposes of evaluating the effectiveness of A/IS as applied in the legal system. In assisting in the creation of such data sets, governments and administrative agencies will have to take into consideration potentially competing societal values, such as the protection of personal data, and arrive at solutions that maintain those values while enabling the creation of usable, real-world data sets. All government efforts in this regard should be transparent and open to public scrutiny. C reators of A/IS to be applied to legal matters should pursue valid measures of the effectiveness of their systems, whether through participation in benchmarking exercises or through conducting single-system validation exercises. Creators should describe the procedures and results of the testing in clear language that is understandable to both experts and nonexperts, and should do so without disclosing intellectual property. Further, the descriptions should be open to examination by all stakeholders, including, when appropriate, the general public. R esearchers engaged in the study and development of A/IS for use in the legal system should seek to define meaningful metrics that gauge the effectiveness of the systems they study. In selecting and defining metrics, researchers should seek input from all stakeholders in the outcome of the given application of A/IS in the legal system. The metrics should be readily understandable by experts and nonexperts alike. G overnments and industry associations should undertake educational efforts to inform both those engaged in the operation of A/IS deployed in the legal system and those affected by the results of their operation of the salient measures of effectiveness and what they can indicate about the capabilities and limitations of the A/IS in question. C reators of A/IS for use in the legal system should ensure that the effectiveness metrics defined by the research community are readily obtainable and accessible to all stakeholders, including, when appropriate, the general public. Creators should provide guidance on how to interpret and respond to the metrics generated by the system. O perators of A/IS applied to a legal task should follow the guidance on the measurement of effectiveness provided for 230 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawthe A/IS being used. This includes guidance about which metrics to obtain, how and when to obtain them, how to respond to given results, when it may be appropriate to follow alternative methods of gauging effectiveness, and so on. I n interpreting and responding to measurements of the effectiveness of A/IS applied to legal problems or questions, allowance should be made by those interpreting the results for variation in the specific objectives and circumstances of a given deployment of A/IS. Quantitative results should be supplemented by qualitative evaluation of the practical significance of a given outcome and whether it indicates a need for remediation. This evaluation should be done by an individual with the technical expertise and pragmatic experience needed to make a sound judgment. I ndustry associations or other organizations should collaborate on developing standards for measuring and reporting on the effectiveness of A/IS. These standards should be developed with input from both the scientific and legal communities. R ecommendation 1 under Issue 2, with respect to effectiveness. R ecommendation 2 under Issue 2, with respect to effectiveness. Further Resources D a Silva Moore v. Publicis Groupe , 2012 WL 607412 (S.D.N.Y. Feb. 24, . C . Garvie, A. M. Bedoya, and J. Frankle, The Perpetual Line-Up: Unregulated Police Face Recognition in America , Georgetown Law, Center on Privacy & Technology, Oct. M . R. Grossman and G. V. Cormack, Technology-Assisted Review in E-Discovery Can Be More Effective and More Efficient Than Exhaustive Manual Review , Richmond Journal of Law and Technology, vol. 17, no. 3, 201 B . Hedin, D. Brassil, and A. Jones, On the Place of Measurement in E-Discovery, in Perspectives on Predictive Coding and Other Advanced Search Methods for the Legal Practitioner, J. R. Baron, R. C. Losey, and M. D. Berman, Eds. Chicago: American Bar Association, J . A. Kroll, The fallacy of inscrutability , Philosophical Transactions of the Royal Society A: Mathematical, Physical, and Engineering Sciences, vol. 376, no. 2133, Oct. D . W. Oard, J. R. Baron, B. Hedin, D. Lewis, and S. Tomlinson, Evaluation of Information Retrieval for E-Discovery , Artificial Intelligence and Law, vol. 18, no. 4, pp. 347-386, Aug. T he Sedona Conference, The Sedona Conference Commentary on Achieving Quality in the E-Discovery Process, The Sedona Conference Journal, vol. 15, pp. 265-304, M . T. Stevenson, Assessing Risk Assessment in Action , Minnesota Law Review, vol. 103, June 231 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law G lobal Governance of AI Roundtable: Summary Report 2018, World Government Summit, H igh-Level Expert Group on Artificial Intelligence, DRAFT Ethics Guidelines for Trustworthy AI: Working Document for Stakeholders Consultation, The European Commission. Brussels, Belgium: Dec. 18, Issue Competence How can specification of the knowledge and skills required of the human operator(s) of A/IS foster informed in the suitability of A/IS for adoption in legal systems? Background An essential component of informed trust in a technological system, especially one that may affect us in profound ways, is confidence in the competence of the operator(s) of the technology. We trust surgeons or pilots with our lives because we have confidence that they have the knowledge, skills, and experience to apply the tools and methods needed to carry out their tasks effectively. We have that confidence because we know that these operators have met rigorous professional and scientific accreditation standards before being allowed to step into the operating room or cockpit. This informed trust in operator competence is what gives us confidence that surgery or air travel will result in the desired outcome. No such standards of operator competence currently exist with respect to A/IS applied in legal systems, where the life, liberty, and rights of citizens can be at stake. That absence of standards hinders the trustworthy adoption of A/IS in the legal domain. The human operator is an integral component of A/IS Almost all current applications of A/IS in legal systems, like those in most other fields, require human mediation and likely will continue to do so for the near future. This human mediation, post design and post development, will take a number of forms, including decisions about (a) whether or not to use A/IS for a given purpose, 67 (b) the data used to train the systems, (c) settings for system parameters to be used in generating results, (d) methods of validating results, (e) interpretation and application of the results, and so on. Because these systems outcomes are a function of all their components, including the human operator(s), their effectiveness, and by extension trustworthiness, will depend on their human operator(s). Despite this, there are few standards that specify how humans should mediate applications of A/IS in legal systems, or what knowledge qualifies a person to apply A/IS and interpret their results. 68 This reality is especially troubling for the instances in which the life, rights, or liberty of humans are at stake. Today, while professional codes of ethics for lawyers are beginning to include among their 232 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawrequirements an awareness and understanding of technologies with legal application,69 the operators of A/IS in legal systems are essentially deemed to be capable of determining their own competence: lawyers or IT professionals operating in civil discovery, correctional officers using risk assessment algorithms, and law enforcement agencies engaging in predictive policing or using automated surveillance technologies. All are mostly able to use A/IS without demonstrating that they understand the operation of the system they are using or that they have any particular set of consensus competencies. 70 The lack of competency requirements or standards undermines the establishment of informed trust in the use of A/IS in legal systems. If courts, legal practitioners, law enforcement agencies, and the general public are to rely on the results of A/IS when applied to tasks traditionally carried out by legal professionals, they must have grounds for believing that those operating A/IS will possess the requisite knowledge and skill to understand the conditions and methods for operating the systems effectively, including evaluating the data on which the A/IS trained, the data to which they are applied, the results they produce, and the methods and results of measuring the effectiveness the systems. Applied incompetently, A/IS could produce the opposite intended effect. Instead of improving a legal system and bringing about the gains in well-being that follow from such improvements they may undermine both the fairness and effectiveness of a legal system and trust in its fairness and effectiveness, creating conditions for social disorder and the deterioration of human well-being that would follow from that disorder. By way of illustration: A c ity council might misallocate funds for policing across city neighborhoods because it relies on the output of an algorithm that directs attention to neighborhoods based on arrest rates rather than actual crime rates. 71 I n civil justice, A/IS applied in a search of documents to uncover relevant facts may fail to do so because an operator without sufficient competence in statistics may materially overestimate the accuracy of the system, thus ceasing vital fact-finding activities. 72 I n the money bail system, reliance on A/IS to reduce bias may instead perpetuate it. For example, if a judge does not understand whether an algorithm makes sufficient contextual distinctions between gradations of offenses, 73 that judge would not able to probe the output of the A/IS and make a well-informed use of it. I n the criminal justice system, an operator using A/IS in sentencing decision-support may fail to identify bias, or to assess the risk of bias, in the results generated by the A/IS, 74 unfairly depriving a citizen of his or her liberty or prematurely granting an offender s release, increasing the risk of recidivism. More generally, without the confidence that A/IS operators will apply the technology as intended and supervise it appropriately, the general public will harbor fear, uncertainty, and doubt about the use of A/IS in legal systems and potentially about the legal systems themselves.233 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawFostering informed trust in the competence of human operators If negative outcomes such as those just described are to be avoided, it will be necessary to include among norms for the adoption of A/IS in a legal system a provision for building informed trust in the operators of A/IS . Building trust will require articulating standards and best practices for two groups of agents involved in the deployment of A/IS: creators and operators. On the one hand, those engaged in the design, development, and marketing of A/IS must commit to specifying the knowledge, skills, and conditions required for the safe, ethical, and effective deployment and operation of the systems. 75 On the other hand, those engaged in actually operating the systems, including both legal professionals and experts acting in the service of legal professionals, must commit to adhering to these requirements in a manner consistent with other operative legal, ethical, and professional requirements. The precise nature of the competency requirements will vary with the nature and purpose of the A/IS and what is at stake in their effective operation. The requirements for the operation of A/IS designed to assist in the creation of contracts, for example, might be less stringent than those for the operation of A/IS designed to assess flight risk, which could affect the liberty of individual citizens. A corollary of these provisions is that education and training in the requisite skills should be available and accessible to those who would operate A/IS, whether that training is provided through professional schools, such as law school; through institutions providing ongoing professional training, such as, for federal judges in the United States, the Federal Judicial Center; through professional and industry associations, such as the American Bar Association; or through resources accessible by the general public. 76 Making sure such training is available and accessible will be essential to ensuring that the resources needed for the competent operation of A/IS are widely and equitably distributed. 77 It will take a combined effort of both creators and operators to ensure both that A/IS designed for use in legal systems are properly applied and that those with a stake in the effective functioning of legal systems including legal professionals, of course, but also decision subjects, victims of crime, communities, and the general public will have informed trust, or, for that matter, informed distrust (if that is what a competence assessment finds) in the competence of the operators of A/IS as applied to legal problems and questions. 78 Illustration Competence Included among the offerings of Amazon Web Services is an image and video analysis service known as Amazon Rekognition. 79 The service is designed to enable the recognition of text, objects, people, and actions in images and videos. The technology also enables the search and comparison of faces, a feature with potential law enforcement and national security applications, such as comparing faces identified in video taken by a security camera with those in a database of jail booking photos. Attracted by 234 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawthe latter feature, police departments in Oregon and Florida have undertaken pilots of Rekognition as a tool in their law enforcement efforts. 80 In 2018, the American Civil Liberties Union (ACLU), a frequent critic of the use of facial recognition technologies by law enforcement agencies, 81 conducted a test of Rekognition. The test consisted of first constructing a database of 25,000 booking photos ( mugshots ) then comparing publicly available photos of all then-current members of the US Congress against the images in the database. The test found that Rekognition incorrectly matched the faces of 28 members of Congress with faces of individuals who had been arrested for a crime. 82 The ACLU argues that the high number of false positives generated by the technology shows that police use of facial recognition technologies generally (and of Rekognition in particular) poses a risk to the privacy and liberty of law-abiding citizens. The ACLU has used the results of its test of Rekognition to support its proposal that Congress enact a moratorium on the use of facial recognition technologies by law enforcement agencies until stronger safeguards against their misuse, and potential abuse, can be put in place. 83 In response to the ACLU report, Amazon noted that the ACLU researchers, in conducting their study, had applied the technology utilizing a similarity threshold (a gauge of the likelihood of a true match) of 80%, a threshold that casts a fairly wide net for potential matches (and hence generates a high number of false positives). For applications in which there are greater costs associated with false positives (e.g., policing), Amazon recommends utilizing a similarity threshold value of 99% or above to reduce accidental misidenti fic ation.84 Amazon also noted that, in all law enforcement use cases, it would be expected that the results of the technology would be reviewed by a human before any actual police action would be undertaken. The story of the ACLU s testing of Rekognition and Amazon s response to the test highlights the importance of specifying and adhering to guidelines for competent use. 85 Had a law enforcement agency used the technology in the way it was used in the ACLU test, it would, in most legitimate use cases, be guilty of incompetent use. At the same time, Amazon is not free of blame insofar as it did not specify prominently and clearly the competency guidelines for effective use of the technology in support of law enforcement efforts, as well as the risks that might be incurred if those guidelines are not followed. Competent use 86 follows both from the A/IS creator s specification of well-grounded 87 competency guidelines and from the A/IS operator s adherence to those guidelines.88 Recommendations C reators of A/IS for application in legal systems should provide clear and accessible guidance for the knowledge, skills, and experience required of the human operators of the A/IS if the systems are to achieve expected levels of effectiveness. Included in that guidance should be a delineation of the risks involved if those requirements are not met. Such guidance should be 235 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawdocumented in a form that is accessible and understandable by both experts and the general public. C reators and developers of A/IS for application in legal systems should create written policies that govern how the A/IS should be operated. In creating these policies, creators and developers should draw on input from the legal professionals who will be using the A/IS they are creating. The policies should include: t he specification of the real-world applications for the A/IS; t he preconditions for their effective use; t he training and skills that are required for operators of the systems; t he procedures for gauging the effectiveness of the A/IS; t he considerations to take into account in interpreting the results of the A/IS; t he outcomes that can be expected by both operators and other affected parties when the A/IS are operated properly; and t he specific risks that follow from improper use. The policies should also specify circumstances in which it might be necessary for the operator to override the A/IS. All such policies should be publicly accessible. C reators and developers of A/IS to be applied in legal systems should integrate safeguards against the incompetent operation of their systems. Safeguards could include issuing notifications and warnings to operators in certain conditions, requiring, as appropriate, acknowledgment of receipt; limiting access to A/IS functionality based on the operator s level of expertise; enabling system shut-down in potentially high-risk conditions; and more. These safeguards should be flexible and governed by context-sensitive policies set by competent personnel of the entity (e.g., the judiciary), utilizing the A/IS to address a legal problem. G overnments should provide that any individual whose legal outcome is affected by the application of A/IS should be notified of the role played by A/IS in that outcome. Further, the affected party should have recourse to appeal to the judgment of a competent human being. P rofessionals engaged in the creation, practice, interpretation, and enforcement of the law, such as lawyers, judges, and law enforcement officers, should recognize the specialized scientific and professional expertise required for the ethical and effective application of A/IS to their professional duties. The professional associations to which such legal practitioners belong, such as the American Bar Association, should, through both educational programs and professional codes of ethics, seek to ensure that their members are well informed about the scientific and technical competency requirements for the effective and trustworthy application of A/IS to the law. 89 T he operators of A/IS applied in legal systems whether the operator is a specialist in A/IS or a legal professional should 236 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawunderstand the competencies required for the effective performance of their roles and should either acquire those competencies or identify individuals with those competencies who can support them in the performance of their roles. The operator does not need to be an expert in all the pertinent domains but should have access to individuals with the requisite expertise. R ecommendation 1 under Issue 2, with respect to competence. R ecommendation 2 under Issue 2, with respect to competence. Further Resources C . Garvie, A. M. Bedoya, and J. Frankle, The Perpetual Line-Up: Unregulated Police Face Recognition in America , Georgetown Law, Center on Privacy & Technology, Oct. I nternational Organization for Standardization, ISO/IEC 27050-Information technology Security techniques Electronic discovery Part Code of practice for electronic discovery , Geneva, J . A. Kroll, The fallacy of inscrutability , Philosophical Transactions of the Royal Society A: Mathematical, Physical, and Engineering Sciences, vol. 376, no. 2133, Oct. A . G. Ferguson, Policing Predictive Policing , Washington University Law Review, vol. 94, no. 5 G lobal Governance of AI Roundtable: Summary Report 2018, World Government Summit, Issue Accountability How can the ability to apportion responsibility for the outcome of the application of A/IS foster informed trust in the suitability of A/IS for adoption in legal systems? Background Apportioning responsibility. An essential component of informed trust in a technological system is confidence that it is possible, if the need arises, to apportion responsibility among the human agents engaged along the path of its creation and application: from design through to development, procurement, deployment, 90 operation, and, finally, validation of effectiveness. Unless there are mechanisms to hold the agents engaged in these steps accountable, it will be difficult or impossible to assess responsibility for the outcome of the system under any framework, whether a formal legal framework or a less formal normative framework. A model of A/IS creation and use that does not have such mechanisms will also lack important forms of deterrence against poorly thought-out design, casual adoption, and inappropriate use of A/IS. Simply put, a system that produces outcomes for which no one is responsible cannot be trusted. Those engaged in creating, procuring, deploying, and operating such a system will lack the discipline engendered by the clear 237 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawassignment of responsibility. Meanwhile, those affected by the results of the system s operation will find their questions around a given result inadequately answered, and errors generated by the system will go uncorrected. In the case of A/IS applied in a legal system, where an individual s basic human rights may be at issue, these questions and errors are of fundamental importance. In such circumstances, the only options are either blind trust or blind distrust. Neither of those options is satisfactory, especially in the case of a technological system applied in a domain as fundamental to the social order as the law. Challenges to accountability In the case of A/IS, whether applied in a legal system or another domain, maintaining accountability can be a particularly steep challenge. This challenge to accountability is because of both the perceived black box nature of A/IS and the diffusion of responsibility it brings. The perception of A/IS as a black box stems from the opacity that is an inevitable characteristic of a system that is a complex nexus of algorithms, computer code, and input data. As observed by Joshua New and Daniel Castro of the Information Technology and Innovation Foundation: The most common criticism of algorithmic decision-making is that it is a black box of extraordinarily complex underlying decision models involving millions of and thousands of lines of code. Moreover, the model can change over time, particularly when using machine learning algorithms that adjust the model as the algorithm encounters new data. 91 This opacity of the systems makes it challenging to trace cause to effect, 92 which, in turn, makes it difficult or even impossible, to draw lines of responsibility. The diffuseness challenge stems from the fact that even the most seemingly straightforward A/IS can be complex, with a wide range of agents systems designers, engineers, data analysts, quality control specialists, operators, and others involved in design, development, and deployment. Moreover, some of these agents may not even have been engaged in the development of the A/IS in question; they may have, for example, developed open-source components that were intended for an entirely different purpose but that were subsequently incorporated into the A/IS. This diffuseness of responsibility poses a challenge to the maintenance of accountability. 93 As Matthew Scherer, a frequent writer and speaker on topics at the intersection of law and A/IS, observes: The sheer number of individuals and firms that may participate in the design, modification, and incorporation of an AI system s components will make it difficult to identify the most responsible party or parties. Some components may have been designed years before the AI project had even been conceived, and the components designers may never have envisioned, much less intended, that their designs would be incorporated into any AI system, still less the specific AI system that caused harm. In such circumstances, it may seem unfair to assign 238 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawblame to the designer of a component whose work was far-removed in both time and geographic location from the completion and operation of the AI system. 94 Examples include the following: W hen a judge s ruling includes a long prison sentence, based in part on a flawed A/IS- enabled process that erroneously deemed a particular person to be at high risk of recidivism, who is responsible for the error? Is it the A/IS designer, the person who chose the data or weighed the inputs, the prosecution team who developed and delivered the risk profile to the court, or the judge who did not have the competence to ask the appropriate questions that would have enabled a clearer understanding of the limitations of the system? Or is responsibility somehow distributed among these various agents? 95 W hen a lawyer engaged in civil or criminal discovery believes, erroneously, that all the relevant information was found when using A/IS in a data-intensive matter, who is responsible for the failure to gather important facts? The A/IS designer who typically would have had no ability to foretell the specific circumstances of a given matter, the legal or IT professional who operated the A/IS or erroneously measured its effectiveness, or the lawyer who made a representation to his or her client, to the court, or to investigatory agencies? W hen a law enforcement officer, relying on A/IS, erroneously identifies an individual as being more likely to commit a crime than another, who is responsible for the resulting encroachment on the civil rights of the person erroneously targeted? Is it the A/IS designer, the individual who selected the data on which to train the algorithm, the individual who chose how the effectiveness of the A/IS would be measured, 96 the experts who provided training to the officer, or the officer himself or herself? As a result of the challenges presented by the opacity and diffuseness of responsibility in A/IS, the present-day answer to the question, Who is accountable? is, in far too many instances, It s hard to say. This is a response that, in practice, means no one or, equally unhelpful, everyone . Such failure to maintain accountability will undermine efforts to bring A/IS (and all their potential benefits) into legal systems based on informed trust. Maintaining accountability and trust in A/IS Although maintaining accountability in complex systems can be a challenge, it is one that must be met in order to engender informed trust in the use of A/IS in the legal domain. Blaming the algorithm is not a substitute for taking on the challenge of maintaining transparent lines of responsibility and establishing norms of accountability. 97 This is true even if we allow that, given the complexity of the systems in question, some number of systems accidents is inevitable. 98 Informed trust in a system does not require a belief that zero errors will occur; however, it does require a belief that there are mechanisms in place for addressing errors when 239 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawthey do occur. Accountability is an essential component of those mechanisms. In meeting the challenge, it should be recognized that there are existing norms and controls that have a role to play in ensuring that accountability is maintained. For example, contractual arrangements between the A/IS provider and a party acquiring and applying a system may help to specify who is (and is not) to be held liable in the event the system produces undesirable results. Professional codes of ethics may also go some way toward specifying the extent to which lawyers, for example, are responsible for the results generated by the technologies they use, whether they operate them directly or retain someone else to do so. Judicial systems may have procedures for assessing responsibility when a citizen s rights are improperly infringed. As illustrated by the cases described above, however, existing norms and controls, while helpful, are insufficient in themselves to meet the specific challenge represented by the opacity and diffuseness of A/IS. To meet the challenge further steps must be taken. 99 The first step is ensuring that all those engaged in the creation, procurement, deployment, operation, and testing of A/IS recognize that, if accountability is not maintained, these systems will not be trusted. In the interest of maintaining accountability, these stakeholders should take steps to clarify lines of responsibility throughout this continuum, and make those lines of responsibility, when appropriate, accessible to meaningful inquiry and audit.The goal of clarifying lines of responsibility in the operation of A/IS is to implement a governing model that specifies who is responsible for what, and who has recourse to which corrective actions, i.e., a trustworthy model that ensures that it will admit actionable answers should questions of accountability arise. Arriving at an effective model will require the participation of those engaged in the creation and operation of A/IS, those affected by the results of their use, and those with the expertise to understand how such a model would be used in a given legal system. For example: I ndividuals responsible for the design of A/IS will have to maintain a transparent record of the sources of the various components of their systems, including identification of which components were developed in-house and which were acquired from outside sources, whether open source or acquired from another firm. I ndividuals responsible for the design of A/IS will have to specify the roles, responsibilities, and potential subsequent liabilities of those who will be engaged in the operation of the systems they create. I ndividuals responsible for the operation of a system will have to understand their roles, responsibilities, potential liabilities, and will have to maintain documentation of their adherence to requirements. I ndividuals affected by the results of the operation of A/IS, e.g., a defendant in a criminal proceeding, will have to be given access to information about the roles and responsibilities of those involved in relevant 240 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawaspects of the creation, operation, and validation of the effectiveness of the A/IS affecting them. 100 I ndividuals with legal and political training (e.g., jurists, regulators, as well as legal and political scholars) will have to ensure that any model that is created will provide information that is in fact actionable within the operative legal system. A governing model of accountability that reflects the interests of all these stakeholders will be more effective both at deterring irresponsible design or use of A/IS before it happens and at apportioning responsibility for an undesirable outcome when it does happen. 101 Pulling together the input from the various stakeholders will likely not take place without some amount of institutional initiative. Organizations that employ A/IS for accomplishing legal tasks private firms, regulatory agencies, law enforcement agencies, judicial institutions should therefore develop and implement policies that will advance the goal of clarifying lines of responsibility. Such policies could take the form of, for example, designating an official specifically charged with oversight of the organization s procurement, deployment, and evaluation of A/IS as well as the organization s efforts to educate people both inside and outside the organization on its use of A/IS. Such policies might also include the establishment of a review board to assess the organization s use of A/IS and to ensure that lines of responsibility for the outcomes of its use are maintained. In the case of agencies, such as police departments, whose use of A/IS could impact the general public, such review boards would, in the interest of legitimacy, have to include participation from various citizens groups, such as those representing defendants in the criminal system as well as those representing victims of crime. 102 The goal of opening lines of responsibility to meaningful inquiry is to ensure that an investigation into the use of A/IS will be able to isolate responsibility for errors (or potential errors) generated by the systems and their operation. 103 This means that all those engaged in the design, development, procurement, deployment, operation, and validation of the effectiveness of A/IS, as well as the organizations that employ them, must in good faith be willing to participate in an audit, whether the audit is a formal legal investigation or a less formal inquiry. They must also be willing to create and preserve documentation of key procedures, decisions, certifications, 104 and tests made in the course of developing and deploying the A/IS.105 The combination of a governing model of accountability and an openness to meaningful audit will allow the maintenance of accountability, even in complex deployments of A/IS in the service of a legal system. Additional note The principle of accountability is closely linked with each of the other principles intended to foster informed trust in A/IS: effectiveness, competence, and transparency. With respect to effectiveness, evidence of attaining key metrics and benchmarks to confirm that A/IS are functioning as intended may put questions of where, among creators, 241 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawowners, and operators, responsibility for the outcome of a system lies on a sound empirical footing. With respect to competence, operator credentialing and specified system handoffs enable a clear chain of responsibility in the deployment of A/IS. 106 With respect to transparency, providing a view into the general design and methods of A/IS, or even a specific explanation for a given outcome, can help to advance accountability. Additional note Closely related to accountability is the trust that follows from knowing that a human expert is guiding the A/IS and is capable of overriding them, if necessary. Subjecting humans to automated decisions not only raises legal and ethical concerns, both from a data protection 107 and fundamental rights perspective,108 but also will likely be viewed with distrust if the human component, which can introduce circumstantial flexibility in the interest of realizing an ethically superior outcome, is missing. In addition to ensuring technical safety and reliability of A/IS used in the course of decision-making processes, the legal system should also, where appropriate, provide for the possibility of an appeal for review by a human judge. Careful attention must be paid to the design of corresponding appeal procedures. 109 Illustration Accountability Over the last two decades, criminal justice agencies have increasingly embraced predictive tools to assist in the determination for bail, sentencing, and parole. A mix of companies, government agencies, nonprofits, and universities have built and promoted tools that provide a likelihood that someone may fail to appear or may commit a new crime or a new violent act. While math has played a role in these determinations since at least the 1920s, 110 a new interest in accountability and transparency has brought novel legal challenges to these tools. In 2013, Eric Loomis was arrested for a drive-by shooting in La Crosse, Wisconsin. No one was hit, but Loomis faced prison time. Loomis denied involvement in the shooting, but waived his right to trial and entered a guilty plea to two of the less severe offenses with which he was charged: attempting to flee a traffic officer and operating a motor vehicle without the owner s consent. The judge sentenced him to six years in prison, saying he was high risk . The judge based this conclusion, in part, on the risk assessment score given by Compas, a secret and privately held algorithmic tool used routinely by the Wisconsin Department of Corrections. On appeal, Loomis made three major arguments, two focused on accountability. 111 First, the tool s proprietary nature the underlying code was not made available to the defense made it impossible to test its scientific validity. Second, the tool inappropriately considered gender in making its determination. A unanimous Wisconsin Supreme Court ruled against Loomis on both arguments. The court reasoned that knowing the inputs and output of the tool, and having access to validating studies of the tool s accuracy, were sufficient to prevent infringement of Loomis due process. 112 Regarding the use of gender a protected class in the United States the court said he did not show that there was a reliance on gender in making the output or sentencing decision. 242 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawWithout the ability to interrogate the tool and know how gender is used, the court created a paradox with its opinion. The Loomis decision represents the challenges that judges have balancing accountability of black boxed A/IS and trade secret protections.113 Other decisions have sided against accountability of other risk assessments, 114 probabilistic DNA analysis tools,115 and government remote hacking investigation software.116 Siding with accountability, a federal judge found that the underlying code of a probability software used in DNA comparisons was admissible and relevant to a pretrial hearing where the admissibility of expert testimony is challenged.117 These issues will continue to be litigated as A/IS tools continue to proliferate in judicial systems. To that end, as the Loomis court notes, The justice system must keep up with the research and continuously assess the use of these tools. Recommendations C reators of A/IS to be applied in a legal system should articulate and document well-defined lines of responsibility, among all those who would be engaged in the development and operation of the A/IS, for the outcome of the A/IS. T hose engaged in the adoption and operation of A/IS to be applied in a legal system should understand their specific responsibilities for the outcome of the A/IS as well as their potential liability should the A/IS produce an outcome other than that intended. In the case of A/IS, many questions of legal liability remain unsettled. Adopters and operators of A/IS should nevertheless understand to what extent they could, potentially , be held liable for an undesirable outcome. W hen negotiating contracts for the provision of A/IS products and services for use in the legal system, providers and buyers of A/IS should include contractual terms specifying clear lines of responsibility for the outcomes of the systems being acquired. C reators and operators of A/IS applied in a legal system, and the organizations that employ them, should be amenable to internal oversight mechanisms and inquiries (or audits) that have the objective of allocating responsibility for the outcomes generated by the A/IS. In the case of A/IS adopted and deployed by organizations that have direct public interaction (e.g., a law enforcement agency), oversight and inquiry could also be conducted by external review boards. Being prepared for such inquiries means maintaining clear documentation of all salient procedures followed, decisions made, and tests conducted in the course of developing and applying the A/IS. O rganizations engaged in the development and operation of A/IS for legal tasks should consider mechanisms that will create individual and collective incentives for ensuring both that the outcomes of the A/IS adhere to ethical standards and that accountability for those outcomes is maintained, e.g., mechanisms to ensure that speed and efficiency are not rewarded at the expense of a loss of accountability.243 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law6. T hose conducting inquiries to determine responsibility for the outcomes of A/IS applied in a legal system should take into consideration all human agents involved in the design, development, procurement, deployment, operation, and validation of effectiveness of the A/IS and should assign responsibility accordingly. R ecommendation 1 under Issue 2, with respect to accountability. R ecommendation 2 under Issue 2, with respect to accountability. Further Resources N . Diakopoulos, S. Friedler, M. Arenas, S. Barocas, M. Hay, B. Howe, H. V. Jagadish, K. Unsworth, A. Sahuguet, S. Venkatasubramanian, C. Wilson, C. Yu, and B. Zevenbergen, Principles for Accountable Algorithms and a Social Impact Statement for Algorithms , FAT/ML. F . Doshi-Velez, M. Kortz, R. Budish, C. Bavitz, S. J. Gershman, D. O Brien, S. Shieber, J. Waldo, D. Weinberger, and A. Wood, Accountability of AI Under the Law: The Role of Explanation, Berkman Center Research Publication Forthcoming; Harvard Public Law Working Paper, no. 18-07, Nov. 3, E uropean Commission for the Efficiency of Justice. European Ethical Charter on the Use of Artificial Intelligence in Judicial Systems and their Environment . Strasbourg, J . A. Kroll, J. Huey, S. Barocas, E. W. Felten, J. R. Reidenberg, D. G. Robinson, and H. Yu, Accountable Algorithms , University of Pennsylvania Law Review, vol. 165, pp. 633-Feb. J . New and D. Castro, How Policymakers Can Foster Algorithmic Accountability , Information Technology and Innovation Foundation, May 21, M . U. Scherer, Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies, Harvard Journal of Law & Technology, vol. no. 2, pp. 369-373, J . Tashea, Calculating Crime: Attorneys are Challenging the Use of Algorithms to Help Determine Bail, Sentencing and Parole, ABA Journal, March Issue Transparency How can sharing information that explains how A/IS reached given decisions or outcomes foster informed trust in the suitability of A/IS for adoption in legal systems? Background Access to meaningful information. An essential component of informed trust in a technological system is confidence that the information required for a human to understand why the system behaves a certain way in a specific circumstance (or would behave in 244 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawa hypothetical circumstance) will be accessible. Without transparency, there is no basis for trusting that a given decision or outcome of the system can be explained, replicated, or, if necessary, corrected. 118 Without transparency, there is no basis for informed trust that the system can be operated in a way that achieves its ends reliably and consistently or that the system will not be used in a way that impinges on human rights. In the case of A/IS applied in a legal system, such a lack of trust could undermine the credibility of the legal system itself. Transparency and trust Transparency, by prioritizing access to information about the operation and effectiveness of A/IS, serves the purpose of fostering informed trust in the systems. More specifically, transparency fosters trust that: t he operation of A/IS and the results they produce are explainable; t he operation and results of A/IS are fair;119 t he operation and results of A/IS are unbiased; t he A/IS meet normative standards for operation and results; t he A/IS are effective; t he results of A/IS are replicable;120 and t hose engaged in the design, development, procurement, deployment, operation, and validation of the effectiveness of A/IS can be held accountable, where appropriate, for negative outcomes, and that corrective or punitive action can be taken when warranted.For A/IS used in a legal system to achieve their intended purposes, all those with a stake in the effective functioning of the legal system must have a well-grounded trust that the A/IS can meet these requirements. This trust can be fostered by transparency. The elements of transparency Transparency of A/IS in legal matters requires disclosing information about the design and operation of the A/IS to various stakeholders. In implementing the principle, however, we must, in the interest of both feasibility and effectiveness, be more precise both about the categories of stakeholders to whom the information will be disclosed, and about the categories of information that will be disclosed to those stakeholders. Relevant stakeholders in a legal system include those who: o perate A/IS for the purpose of carrying out tasks in civil justice, criminal justice, and law enforcement, such as a law enforcement officer who uses facial recognition tools to identify potential suspects; re ly on the results of A/IS to make important decisions, such as a judge who draws on the results of an algorithmic assessment of recidivism risk in deciding on a sentence; a re directly affected by the use of A/IS a decision subject , such as a defendant in a criminal proceeding whose bail terms are influenced by an algorithmic assessment of flight risk;245 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law a re indirectly affected by the results of A/IS, such as the members of a community that receives more or less police attention because of the results of predictive policing technology; and h ave an interest in the effective functioning of the legal system, such as judges, lawyers, and the general public. Different types of relevant information can be grouped into high-level categories. As illustrated below, a taxonomy of such high-level categories may, for example, distinguish between: n ontechnical procedural information regarding the employment and development of a given application of A/IS; in formation regarding data involved in the development, training, and operation of the system; in formation concerning a system s effectiveness/performance; in formation about the formal models that the system relies on; and in formation that serves to explain a system s general logic or specific outputs. These more granular distinctions matter because different sorts of inquiries will require different sorts of information, and it is important to match the information provided to the actual needs of the inquiry. For example, an inquiry into a predictive policing system that misdirected police resources may not be much advanced by information about the formal models on which the system relied, but it may well be advanced by an explanation for the specific outcome. On the other hand, an inquiry, undertaken by a designer or operator, into ways to improve system performance may benefit from access to information about the formal models on which the system relies. 121 These distinctions also matter because there may be circumstances in which it would be desirable to limit access to a given type of information to certain stakeholders. For example, there may be circumstances in which one would want to identify an agent to serve as a public interest steward. For auditing purposes, this individual would have access to certain types of sensitive information unavailable to others. Such restrictions on information access are necessary if the transparency principle is not to impinge on other societal values and goals, such as security, privacy, and appropriate protection of intellectual property. 122 The salience of the question, Who is given access to what information? is illustrated by Sentiment Meter, a technology developed by Elucd, a GovTech company that provides cities with near real-time understanding of how citizens feel about their government, in conjunction with the New York Police Department, to assist the NYPD in gauging citizens views regarding police activity in their communities. 123 One of the stated goals of the program is to build public trust in the police department. In the interest of trust, should the public have access to all potentially relevant information, including how the system was designed and developed, what the input data are, who operates the system and what their qualifications are, how the system s effectiveness was tested, and why the public was not brought 246 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawinto the process of construction? If the answer is that the general public should not have access to all this information, then who should? How do we define the public? Is it the whole community represented in its elected officials? Or should certain communities have greater access, for example, those most affected by controversial police practices such as stop, question, and frisk? Such questions must be answered if the program is to achieve its stated goals.Transparency in practice As just noted, although transparency can foster informed trust in A/IS applied in a legal system, its practical implementation requires careful thought. Requiring public access to all information pertaining to the operation and results of A/IS is neither necessary nor feasible. What is required is a careful consideration of who needs access to what information for the specific purpose of building informed trust. The following table is an example of a tool that might be used to match type of information to type of information consumer for the purpose of fostering trust. 124247 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawTypes of information that should be considered in determining transparency demands in relation to a given A/ISStakeholders whose interest in access to different types of information should be considered in determining the transparency demands in relation to a given application of A/IS High-level categorySpecific type of information (examples)Disclosure of...Operators Decision- subjectsPublic interest stewardGeneral public Procedural aspects regarding A/IS employment and developmentthe fact that a given context involves the employment of A/IS N/A ? ? ? how the employment of the system was authorized? ? ? ? who developed the system ? ? ? ? ... Data involved in A/IS development and operationthe origins of training data and data involved in the operation of the system ? ? ? ? the kinds of quality checks that data was subject to and their results? ? ? ? how data labels are defined and to what extent data involves proxy variables? ? ? ? relevant data sets themselves ? ? ? ? ... Effectiveness/ performancethe kinds of effectiveness/performance measurement that have occurred ? ? ? ? measurement results ? ? ? ? any independent auditing or certification ? ? ? ? ... Model specificationthe input variables involved ? ? ? ? the variable(s) that the model optimizes for ? ? ? ? tthe complete model (complete formal representation, source code, etc.)? ? ? ? ... Explanation information concerning the system s general logic or functioning? ? ? ? information concerning the determinants of a particular output 125 ? ? ? ? This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawWhen it comes to deciding whether a specific type of information should be made available and, if so, which types of stakeholders should have access to it, there are various considerations, for example: T he release of certain types of information may conflict with data privacy concerns, commercial or public policy interests such as the promotion of innovation through appropriate intellectual property protections and security interests, e.g., concerns about gaming and adversarial attacks. At the same time, such competing interests should not be permitted to be used, without specific justification, as a blanket cover for not adhering to due process, transparency, or accountability standards. The tension between these interests is particularly acute in the case of A/IS applied in a legal system, where the dignity, security, and liberty of individuals are at stake. 126 T here is tension between the specific goal of explainability, which may argue for limits on system complexity, and system performance, which may be served by greater complexity, to the detriment of explainability. 127 O ne must carefully consider the question that is being asked in an inquiry into A/IS and what information transparency can actually produce to answer that question. Disclosure of A/IS algorithms or training data is, itself, insufficient to enable an auditor to determine whether the system was effective in a specific circumstance. 128 By analogy, transparency into drug manufacturing processes does not, itself, provide information about the actual effectiveness of a drug. Clinical trials provide that insight. In a legal system, an excessive focus on transparency-related information-gathering and assessment may overwhelm courts, legal practitioners, and law enforcement agencies. Meanwhile, other factors, such as measurement of effectiveness or operator competence, coupled with information on training data, may often suffice to ensure that there is a well-informed basis for trusting A/IS in a given circumstance. 129 Given these competing considerations, arriving at a balance that is optimal for the functioning of a legal system and that has legitimacy in the eyes of the public will require an inclusive dialogue, bringing together the perspectives of those with an immediate stake in the proper functioning of a given technology, including those engaged in the design, development, procurement, deployment, operation, and validation of effectiveness of the technology, as well as those directly affected by the results of the technology; the perspectives of communities that may be indirectly impacted by the technology; and the perspectives of those with specialized expertise in ethics, government, and the law, such as jurists, regulators, and scholars. How the competing considerations should be balanced will also vary from one circumstance to another. Rather than aiming for universal transparency standards that would be applicable to all uses of A/IS within a legal system, transparency standards should allow for circumstance-dependent flexibility, in the context of the four constitutive components of trust discussed in this section.249 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawAdditional note The goals of transparency, e.g., answering a question as to why A/IS reached a given decision, may, in some cases, be better served by modes of explanation that do not involve examining an algorithm s terms or opening the black box . A counterfactual explanation taking the form of, for example, You were denied a loan because your annual income was 30,000; if your income had been 45,000, you would have been offered a loan, may provide more insight sooner than the disclosure of an algorithm. 130 Additional note The transparency principle intersects with other principles focused on fostering trust. More specifically, we note the following: T ransparency and effectiveness. Information about the measurement of effectiveness can foster trust only if it is disclosed, i.e., only if there is transparency pertaining to the procedures and results of a measurement exercise. T ransparency and competence. Transparency is essential in ensuring that the competencies required by the human operators of A/IS are known and met. At the same time, questions addressed by transparency extend beyond competence, while the questions addressed by competence extend beyond those answered by transparency. T ransparency and accountability. Transparency is essential in determining accountability, but transparency serves purposes beyond accountability, while accountability seeks to answer questions not addressed directly by transparency. Illustration Transparency In 2004, the city of Memphis, Tennessee, was experiencing an increase in crime rates that exceeded the national average. In response, in 2005, the city piloted a predictive policing program known as Blue CRUSH (Crime Reduction Utilizing Statistical History). 131 Blue CRUSH, developed in conjunction with the University of Memphis, 132 utilizes IBM s SPSS predictive analytics software to identify hot spots : locations and times in which a given type of crime has a greater than average likelihood of occurring. The system generates its results through the analysis of a range of both historical data (type of crime, location, time of day, day of week, characteristics of victim, etc.) and live data provided by units on patrol. Equipped with the predictive crime map generated by the system, the Memphis Police Department can allocate resources dynamically to preempt or interrupt the target criminal activity. The precise response the department takes will vary with circumstance: deployment of a visible patrol car, deployment of an unmarked observer car, increasing vehicle stops in the area, undercover infiltration of the location, and so on. The pilot program of Blue CRUSH focused on gang-related gun violence, which had been on the rise in Memphis prior to the pilot. The program showed an improvement, relative to incumbent methods, in the interdiction of such violence. Based on the success of the pilot, the scope of program was expanded, in 2007, for use throughout the city. By 2013, the policing efforts enabled by Blue CRUSH had helped to reduce overall crime in the city by over 30% and violent crime by 20%. 133 The program The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems250 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Lawalso enabled a dramatic increase in the rate at which crimes were solved: for cases handled by the department s Felony Assault Unit, the percentage of cases solved increased from 16% to nearly 70%. 134 And the program was cost effective: an analysis by Nucleus Research found that the program, when compared to the resources required to achieve the same results by traditional means, realized an annual benefit of approximately $2 million at a cost of just under $400,135 The story of the deployment of Blue CRUSH in the metropolitan Memphis area is not just about the technology; it is equally about the police personnel utilizing the technology and about the communities in which the technology was deployed. As noted by former Memphis Police Department Director Larry Godwin: You can have all the technology in the world but you ve got to have leadership, you ve got to have accountability, you ve got to have boots on the streets for it to succeed. 136 Crucial to the program s success was public support. Blue CRUSH represents a variety of predictive policing technology that limits itself to identifying the where , the when , and the what of criminal activity; it does not attempt to identify the who , and therefore avoids a number of the privacy questions raised by technologies that do attempt to identify individual perpetrators. The technology will still, however, prompt responses by the police that could include more intrusive police activity in identified hot spots. The public must be willing to accept that activity, and that acceptance is won by transparency. To that end, Godwin and Janikowski held more than 200 community and neighborhood watch meetings to inform the public about the technology and how it would be used in policing their communities. 137 Without that level of transparency, it is doubtful that Blue CRUSH would have had the public support needed for its successful deployment. Holding community meetings is an important step in building trust in a predictive policing program. As such programs become more widely implemented, however, and become more widely studied, trust may require more than town-hall meetings. Research into the programs has raised serious concerns about the ways in which they are implemented and their potential for perpetuating or even exacerbating historical bias. 138 Addressing these concerns will require more sophisticated and intrusive oversight than can be realized through community meetings. Included among the questions that must be addressed are the following. I n identifying hot spots, does the program rely primarily on arrest rates, which reflect (potentially biased) police activity, or does it rely on actual crime rates? W hat are the specific criteria for identifying a hot spot and are those criteria free of bias?139 H ow accessible are the input data used to identify hot spots? Are they open to analysis by an independent expert? W hat mechanisms for oversight, review, and remediation of the program have been put in place? Such oversight should have access to the data used to train the system, the models used to identify hot spots, tests of the 251 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Laweffectiveness of the system, and steps taken to remediate errors (such as bias) when they are uncovered. As the public becomes more aware of the potential negative impact 140 of predictive policing programs, law enforcement agencies hoping to build trust in such programs will have to put in place transparency mechanisms that go beyond town-hall meetings and that enable a sophisticated response to such questions. Recommendations G overnments and professional associations should facilitate dialogue among stakeholders those engaged in the design, development, procurement, deployment, operation, and validation of effectiveness of the technology; those who may be immediately affected by the results of the technology; those who may be indirectly affected by the results of the technology, including the general public; and those with specialized expertise in ethics, politics, and the law on the question of achieving a balance between transparency and other priorities, e.g., security, privacy, appropriate property rights, efficient and uniform response by the legal system, and more. In developing frameworks for achieving such balance, policymakers and professional associations should make allowance for circumstantial variation in how competing interests may be reconciled. P olicymakers developing frameworks for realizing transparency in A/IS applied to legal tasks should require that any frameworks they develop are sensitive both to the distinctions among the types of information that might be disclosed and to the distinctions among categories of individuals who may seek information about the design, operation, and results of a given system. P olicymakers developing frameworks for realizing transparency in A/IS to be adopted in a legal system should consider the role of appropriate protection for intellectual property, but should not allow those concerns to be used as a shield to prevent duly limited disclosure of information needed to ascertain whether A/IS meet acceptable standards of effectiveness, fairness, and safety. In developing such frameworks, policymakers should make allowance that the level of disclosure warranted will be, to some extent, dependent on what is at stake in a given circumstance. P olicymakers developing frameworks for realizing transparency in A/IS to be adopted in a legal system should consider the option of creating a role for a specially designated public interest steward , or trusted third party , who would be given access to sensitive information not accessible to others. Such a public interest steward would be charged with assessing the information to answer the public interest questions at hand but would be under obligation not to disclose the specifics of the information accessed in arriving at those answers. D esigners of A/IS should design their systems with a view to meeting transparency requirements, i.e., so as to enable some 252 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawcategories of information about the system and its performance to be disclosed while enabling other categories, such as intellectual property, to be protected. W hen negotiating contracts for the provision of A/IS products and services for use in the legal system, providers and buyers of A/IS should include contractual terms specifying what categories of information will be accessible to what categories of individuals who may seek information about the design, operation, and results of the A/IS. I n developing frameworks for realizing transparency in A/IS to be adopted in a legal system, policymakers should recognize that the information provided by other types of inquiries, e.g., examination of evidence of effectiveness or of operator competence, may in certain circumstances provide a more efficient means to informed trust in the effectiveness, fairness, and safety of the A/IS in question. G overnments should, where appropriate, work together with A/IS developers, as well as other stakeholders in the effective functioning of the legal system, to facilitate the creation of error-sharing mechanisms to enable the more effective identification, isolation, and correction of flaws in broadly deployed A/IS in their legal systems, such as a systematic facial recognition error in policing applications or in risk assessment algorithms. In developing such mechanisms, the question of precisely what information gets shared with precisely which groups may vary from application to application. All government efforts in this regard should be transparent and open to public scrutiny.G overnments should provide whistleblower protections to individuals who volunteer to offer information in situations where A/IS are not designed as claimed or operated as intended, or when their results are not interpreted correctly. For example, if a law enforcement agency is using facial recognition technology for a purpose that is illegal or unethical, or in a manner other than that in which it is intended to be used, an individual reporting that misuse should be given protection against reprisal. All government efforts in this regard should be transparent and open to public scrutiny. R ecommendation 1 under Issue 2, with respect to transparency. R ecommendation 2 under Issue 2, with respect to transparency. Further Resources J . A. Kroll, J. Huey, S. Barocas, E. W. Felten, J. R. Reidenberg, D. G. Robinson, and H. Yu, Accountable Algorithms , University of Pennsylvania Law Review, vol. 165, Feb. J . A. Kroll, The fallacy of inscrutability , Philosophical Transactions of the Royal Society A: Mathematical, Physical, and Engineering Sciences, vol. 376, no. 2133, Oct. W . L. Perry, B. McInnis, C. C. Price, S. C. Smith, and J. S. Hollywood, Predictive Policing: The Role of Crime Forecasting in Law Enforcement Operations , The RAND Corporation, A . D. Selbst and S. Barocas, The Intuitive Appeal of Explainable Machines , Fordham Law Review, vol. 87, no. 3, 253 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law S . Wachter, B. Mittelstadt, and L. Floridi, Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation , International Data Privacy Law, vol. 7, no. 2, pp. 76-99, June S . Wachter, B. Mittelstadt, and C. Russell, Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR , Harvard Journal of Law & Technology, vol. 31, no. 2, R . Wexler, Life, Liberty, and Trade Secrets: Intellectual Property in the Criminal Justice System , Stanford Law Review, vol. 70, no. 5, pp. 1342-1429, 254 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawSection Legal Status of A/IS There has been much discussion about how to legally regulate A/IS-related technologies and the appropriate legal treatment of systems that deploy these technologies. Already, some lawmakers are wrestling with the issue of what status to apply to A/IS. Legal personhood applied to humans and certain types of human organizations is one possible option for framing such legal treatment, but granting that status to A/IS applications raises issues in multiple domains of human interaction. Issue What type of legal status (or other legal analytical framework) is appropriate for A/IS given (i) the legal issues raised by deployment of such technologies, and (ii) the desire to maximize the benefits of A/IS and minimize negative externalities? Background The convergence of A/IS and robotics technologies has led to the development of systems and devices resembling those of human beings in terms of their autonomy, ability to perform intellectual tasks, and, in the case of some robots, their physical appearance. As some types of A/IS begin to display characteristics resembling those of human actors, some governmental entities and private commentators have concluded that it is time to examine how legal regimes should categorize and treat various types of A/IS, often with an eye toward according A/IS a legal status beyond that of mere property. These entities have posited questions such as whether the law should treat such systems as legal persons. 141 While legal personhood is a multifaceted concept, the essential feature of full legal personhood is the ability to participate autonomously within the legal system by having the right to sue and the capacity to be sued in court. 142 This allows legal persons to enter legally binding agreements, take independent action to enforce their own rights, and be held responsible for violations of the rights of others. Conferring such status on A/IS seems initially remarkable until consideration is given to the long-standing legal personhood status granted to corporations, governmental entities, and the like none of which are themselves human. Unlike these familiar legal entities, however, A/IS are not composed of or necessarily controlled by human beings. Recognizing A/IS as independent legal entities could therefore lead to abuses of that status, possibly by A/IS 255 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawand certainly by the humans and legal entities who create or operate them, just as human shareholders and agents have abused the corporate form. 143 A/IS personhood is a significant departure from the legal traditions of both common law and civil law.144 Current legal frameworks provide a number of categories of legal status, other than full legal personhood, that could be used as analogues for the legal treatment of A/IS and how to allocate legal responsibility for harm caused by A/IS. At one extreme, legal systems could treat A/IS as mere products, tools, or other form of personal or intellectual property, and therefore subject to the applicable regimes of property law. Such treatment would have the benefit of simplifying allocation of responsibility for harm. It would, however, not account for the fact that A/IS, unlike other forms of property, may be capable of making legally significant decisions autonomously. In addition, if A/IS are to be treated as a form of property, governments and courts would have to establish rules regarding ownership, possession, and use by third parties. Other legal analogues may include the treatment of pets, livestock, wild animals, children, prisoners, and the legal principles of agency, guardianship, and powers of attorney. 145 Or perhaps A/IS are something entirely without precedent, raising the question of whether one or more types of A/IS might be assigned a hybrid, intermediate, or novel type of legal status? Clarifying the legal status of A/IS in one or more jurisdictions is essential in removing the uncertainty associated with the obligations and expectations for organization and operation of these systems. Clarification along these lines will encourage more certain development and deployment of A/IS and will help clarify lines of legal responsibility and liability when A/IS cause harm. One of the problems of exploiting the existing status of legal personhood is that international treaties may bind multiple countries to follow the lead of a single legislature, as in the EU, making it impossible for a single country to experiment with the legal and economic consequences of such a strategy. Recognizing A/IS as independent legal persons would limit or eliminate some human responsibility for subsequent decisions made by such A/IS. For example, under a theory of intervening causation , a hammer manufacturer is not held responsible when a burglar uses a hammer to break the window of a house. However, if similar relief from responsibility was available to the designers, developers, and users of A/IS, it will potentially reduce their incentives to ensure the safety of A/IS they design and use. In this example, legal issues that are applied in similar chain of causation settings such as foreseeability , complicity , reasonable care , strict liability for unreasonably dangerous goods, and other precedential notions will factor into the design process. Different jurisdictions may reach different conclusions about the nature of such causation chains, inviting future creative legal planners to consider how and where to pursue design, development, and deployment of future A/IS in order to receive the most beneficial legal treatment. The legal status of A/IS thus intertwines with broader legal questions regarding how to ensure 256 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawaccountability and assign and allocate liability when A/IS cause harm. The question of legal personhood for A/IS, in particular, also interacts with broader ethical and practical questions on the extent to which A/IS should be treated as moral agents independent from their human designers and operators, whether recognition of A/IS personhood would enhance or detract from the purposes for which humans created the A/IS in the first place, and whether A/IS personhood facilitates of debilitates the widespread benefits of A/IS. Some assert that because A/IS are at a very early stage of development, it is premature to choose a particular legal status or presumption in the many forms and settings in which those systems are and will be deployed. However, thoughtfully establishing a legal status early in the development could also provide crucial guidance to researchers, programmers, and developers. This uncertainty about legal status, coupled with the fact that multiple legal jurisdictions are already deploying A/IS and each of them, as a sovereign entity, can regulate A/IS as it sees fit suggests that there are multiple general frameworks that can and should be considered when assessing the legal status of A/IS. Recommendations W hile conferring full legal personhood on A/IS might bring some economic benefits, the technology has not yet developed to the point where it would be legally or morally appropriate to generally accord A/IS the rights and responsibilities inherent in the legal definition of personhood as it is now defined. Therefore, even absent the consideration of any negative ramifications from personhood status, it would be unwise to accord such status to A/IS at this time. I n determining what legal status, including granting A/IS legal rights short of full legal personhood, to accord to A/IS, government and industry stakeholders alike should: ( identify the types of decisions and operations that should never be delegated to A/IS; and ( determine what rules and standards will most effectively ensure human control over those decisions. G overnments and courts should review various potential legal models including agency, animal law, and the other analogues discussed in this section and assess whether they could serve as a proper basis for assigning and apportioning legal rights and responsibilities with respect to the deployment and use of A/IS. I n addition, governments should scrutinize existing laws especially those governing business organizations for mechanisms that could allow A/IS to have legal autonomy. If ambiguities or loopholes create a legal method for recognizing A/IS personhood, the government should review and, if appropriate, amend the pertinent laws. M anufacturers and operators should learn how each jurisdiction would categorize a given autonomous and/or intelligent system and how each jurisdiction would treat harm caused by the system. Manufacturers and operators should be required to comply with the applicable laws of all jurisdictions in 257 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawwhich that system could operate. In addition, manufacturers and operators should be aware of standards of performance and measurement promulgated by standards development organizations and agencies. S takeholders should be attentive to future developments that could warrant reconsideration of the legal status of A/IS. For example, if A/IS were developed that displayed self-awareness and consciousness, it may be appropriate to revisit the issue of whether they deserve a legal status on par with humans. Likewise, if legal systems underwent radical changes such that human rights and dignity no longer represented the primary guiding principle, the concept of full personhood for artificial entities may not represent the radical departure it might today. If the development of A/IS were to go in the opposite direction, and mechanisms were introduced allowing humans to control and predict the actions of A/IS easily and reliably, then the dangers of A/IS personhood would not be any greater than for well-established legal entities, such as corporations. I n considering whether to accord or expand legal protections, rights, and responsibilities to A/IS, governments should exercise utmost caution. Before according full legal personhood or a comparable legal status on A/IS, governments and courts should carefully consider whether doing so might limit how widely spread the benefits of A/IS are or could be, as well as whether doing so would harm human dignity and uniqueness of human identity. Governments and decision-makers at every level must work closely with regulators, representatives of civil society, industry actors, and other stakeholders to ensure that the interest of humanity and not the interests of the autonomous systems themselves remains the guiding principle. Further Resources S . Bayern. The Implications of Modern Business-Entity Law for the Regulation of Autonomous Systems . Stanford Technology Law Review 19, no. 1, pp. 93-1 12, S . Bayern, et al., Company Law and Autonomous Systems: A Blueprint for Lawyers, Entrepreneurs, and Regulators . Hastings Science and Technology Law Journal, vol. 9, no. 2, pp. 135-162, D . Bhattacharyya. Being, River: The Law, the Person and the Unthinkable . Humanities and Social Sciences Online , April 26, B . A. Garner , Black s Law Dictionary, 10th Edition, Thomas West, J . Bryson, et al., Of, for, and by the people: the legal lacuna of synthetic persons, Artificial Intelligence Law 25, pp. 273-91, D . J. Calverley, Android Science and Animal Rights, Does an Analogy Exist? Connection Science 18, no. 4, pp. 403-417, D . J. Calverley, Imagining a Non-Biological Machine as a Legal Person . AI & Society 22, pp. 403-417, R . Chatila, Inclusion of Humanoid Robots in Human Society: Ethical Issues, in Springer Humanoid Robotics: A Reference, A. Goswami and P. Vadakkepat, Eds., Springer 258 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law E uropean Parliament Resolution of 16 February 2017 (2015/2103(INL) with recommendations to the Commission on Civil Law Rules on Robotics, L . M. LoPucki, Algorithmic Entities , 95 Washington University Law Review 887, J . S. Nelson, Paper Dragon Thieves. Georgetown Law Journal 105, pp. 871-941, M . U. Scherer, Of Wild Beasts and Digital Analogues: The Legal Status of Autonomous Systems . Nevada Law Journal 19, forthcoming M . U. Scherer, Is Legal Personhood for AI Already Possible Under Current United States Laws? Law and AI , May 14, L . B. Solum. Legal Personhood for Artificial Intelligences . North Carolina Law Review 70, no. 4, pp. 1231 1287, J . F. Weaver. Robots Are People Too: How Siri, Google Car, and Artificial Intelligence Will Force Us to Change Our Laws . Santa Barbara, CA: Praeger, L . Zyga. Incident of drunk man kicking humanoid robot raises legal questions , Techxplore , October 2, 259 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawThanks to the Contributors We wish to acknowledge all of the people who contributed to this chapter. The Law Committee John Casey (Co-Chair) Attorney-at-Law, Corporate, Wilson Sonsini Goodrich & Rosati, P.C. Nicolas Economou (Co-Chair) Chief Executive Officer, H5; Chair, Science, Law and Society Initiativ e at The Future Society; Chair, Law Committee, Global Governance of AI Roundtable; Member , Council on Extended Intelligence Aden Allen Senior Associate, Patent Litigation, Wilson Sonsini Goodrich & Rosati, P.C. Miles Brundage Research Scientist (Policy), OpenAI; Research Associate, Future of Humanity Institute, University of Oxford; PhD candidate, Human and Social Dimensions of Science and Technology, Arizona State University Thomas Burri Assistant Professor of International Law and European Law, University of St. Gallen (HSG), Switzerland Ryan Calo Assistant Professor of Law, the School of Law at the University of Washington Clemens Canel Referendar (Trainee Lawyer) at Hansea tisches Oberlandesgericht, graduate of the U niversity of Texas School of Law and Bucerius Law School Ch andramauli Chaudhuri Senior Data Scientist; Fractal Analytics Da nielle Keats Citron Lois K. Macht Research Professor & Professor of Law, University of Maryland Carey School of Law Fer nando Delgado PhD Student, Information Science, Cornell University. Dev en Desai Associate Professor of Law and Ethics, Georgia Institute of Technology,Scheller College of Business Jul ien Durand International Technology Lawyer; Executive Director Compliance& Ethics, Amgen Biotechnology Tod d Elmer , JD Member of the Board of Directors, National Science and TechnologyMedals Foundation Ka y Firth-Butterfield Project Head, AI and Machine Learning at the World EconomicForum. Founding Advocate of AI-Global;Senior Fellow and Distinguished Scholar,Robert S. Strauss Center for InternationalSecurity and Law, University of Texas, Austin;Co-Founder, Consortium for Law and Ethicsof Artificial Intelligence and Robotics, Universityof Texas, Austin; Partner, Cognitive FinanceGroup, London, U.K. To m D. Grant Fellow, Wolfson College; Senior Associate of the LauterpachtCentre for International Law, Universityof Cambridge, U.K.260 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law C ordel Green Attorney-at-Law; Executive Director, Broadcasting Commission Jamaica M aura R. Grossman Research Professor, David R. Cheriton School of Computer Science, University of Waterloo; Adjunct Professor, Osgoode Hall Law School, York University B ruce Hedin Principal Scientist, H5 Da niel Hinkle Senior State Affairs Counsel for the American Association for Justice De rek Jinks Marrs McLean Professor in Law, University of Texas Law School; Director, Consortium on Law and Ethics of Artificial Intelligence and Robotics (CLEAR), Robert S. Strauss Center for International Security and Law, University of Texas. N icolas Jupillat Adjunct Professor, University of Detroit Mercy School of Law M arwan Kawadri Analyst, Founders Intelligence; Research Associate, The Future Society. M auricio K. Kimura Lawyer; PhD student, Faculty of Law, University of Waikato, New Zealand; LLM from George Washington University, Washington DC, USA; Bachelor of Laws from Sao Bernardo do Campo School of Law, Brazil I rene Kitsara Lawyer; IP Information Officer, Access to Information and Knowledge Division, World Intellectual Property Organization, Switzerland T imothy Lau, J.D., Sc.D. Research Associate, Federal Judicial Center M ark Lyon Attorney-at-Law, Chair, Artificial Intelligence and Autonomous Systems Practice Group at Gibson, Dunn & Crutcher LLP G ary Marchant Regents Professor of Law, Lincoln Professor of Emerging Technologies, Law and Ethics, Arizona State University N icolas Miailhe Co-Founder & President, The Future Society; Member, AI Expert Group at the OECD; Member, Global Council on Extended Intelligence; Senior Visiting Research Fellow, Program on Science Technology and Society at Harvard Kennedy School. Lecturer, Paris School of International Affairs (Sciences Po); Visiting Professor, IE School of Global and Public Affairs P aul Moseley Master s student, Electrical Engineering, Southern Methodist University; graduate of the University of Texas School of Law F lorian Ostmann Policy Fellow, The Alan Turing Institute P edro Pav n Assistant General Counsel, Global Data Protection, Honeywell J osephine Png AI Policy Researcher and Deputy Project Manager, The Future Society; budding barrister; and BA Chinese and Law, School of Oriental and African Studies M atthew Scherer Attorney at Littler Mendelson, P.C., and legal scholar based in Portland, Oregon, USA; Editor, LawAndAI.com B ardo Schettini Gherardini Independent Legal Advisor on standardization, AI and robotics261 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law J ason Tashea Founder, Justice Codes and adjunct law professor at Georgetown Law Center Y an Tougas Global Ethics & Compliance Officer, United Technologies Corporation; Adjunct Professor, Law & Ethics, University of Connecticut School of Business; Fellow, Ethics & Compliance Initiative; Kallman Executive Fellow, Bentley University Hoffman Center for Business Ethics S andra Wachter Lawyer and Research Fellow in Data Ethics, AI and Robotics, Oxford Internet Institute, University of Oxford A xel Walz Lawyer; Senior Research Fellow at the Max Planck Institute for Innovation and Competition, Germany. (Member until October 31, J ohn Frank Weaver Lawyer, McLane Middleton, P.A; Columnist for and Member of Board of Editors of Journal of Robotics, Artificial Intelligence & Law ; Contributing Writer for Slate; Author, Robots Are People Too J ulius Weitzd rfer Affiliated Lecturer, Faculty of Law, University of Cambridge; Research Associate, Centre for the Study of Existential Risk, University of Cambridge Y ueh-Hsuan Weng Assistant Professor, Frontier Research Institute for Interdisciplinary Sciences (FRIS), Tohoku University; Fellow, Transatlantic Technology Law Forum (TTLF), Stanford Law School A ndrew Woods Associate Professor of Law, University of Arizona For a full listing of all IEEE Global Initiative Members, visit standards.ieee.org/content/dam/ ieee-standards/standards/web/documents/other/ec_bios.pdf . F or information on disclaimers associated with EAD1e, see How the Document Was Prepared.262 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawThe Law Committee of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems would like to thank the following individuals for taking the time to offer valuable feedback and suggestions on Section 1 of the Law Chapter, Norms for the Trustworthy Adoption of A/IS in Legal Systems . Each of these contributors offered comments in an individual capacity, not in the name of the organization for which they work. The final version of the Section does not necessarily incorporate all comments or reflect the views of each contributor. R ediet Abebe , PhD Candidate, Department of Computer Science, Cornell University; cofounder, Mechanism Design for Social Good; cofounder, Black in AI. I feoma Ajunwa , Assistant Professor, Labor & Employment Law, Cornell Industrial and Labor Relations School; faculty Associate at Harvard Law, Berkman Klein Center. J ason R. Baron , of counsel, Drinker Biddle; co-chair, Information Governance Initiative; former Director of Litigation, United States National Archives and Records Administration. I rakli Beridze , Head, Centre for Artificial Intelligence and Robotics, United Nations (UNICRI). J uan Carlos Botero , Law Professor, Pontificia Universidad Javeriana, Bogota; former Executive Director, World Justice Project. A nne Carblanc , Principal Administrator, Information, Communications and Consumer Policy (ICCP) Division, Directorate for Science, Technology and Industry, OECD; former criminal investigations judge (juge d instruction), Tribunal of Paris. G allia Daor , Policy Analyst, OECD. L ydia de la Torre , Privacy Law Fellow, Santa Clara University. I sabela Ferrari , Federal Judge, Federal Court, Rio de Janeiro, Brazil. A lbert Fox Cahn , Founder and Executive Director, Surveillance Technology Oversight Project; former Legal Director, CAIR-NY. P aul W. Grimm , United States District Judge, United States District Court for the District of Maryland. G illian Hadfield , Professor of Law and Professor of Strategic Management, University of Toronto; Member, World Economic Forum Future Council for Agile Governance. Sh eila Jasanoff , Pforzheimer Professor of Science and Technology Studies, Harvard Kennedy School of Government. B aroness Beeban Kidron , OBE, Member, United Kingdom House of Lords. Eva K aili, Member, European Parliament; Chair, European Parliament Science and Technology Options Assessment body (STOA). M antalena Kaili , cofounder, European Law Observatory on New Technologies. J on Kleinberg , Tisch University Professor, Departments of Computer Science and Information Science, Cornell University; member of the National Academy of Sciences, the National Academy of Engineering, and the American Academy of Arts and Sciences. Sh uang Lu Frost , Teaching Fellow, PhD candidate, Department of Anthropology, Harvard University.263 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law A rthur R. Miller CBE , University Professor, New York University; former Bruce Bromley Professor of Law, Harvard Law School. M anuel Mu iz , Dean and Rafael del Pino Professor of Practice of Global Leadership, IE School of Global and Public Affairs, Madrid; Senior Associate, Belfer Center, Harvard University. E rik Navarro Wolkart , Federal Judge, Federal Court, Rio de Janeiro, Brazil. A ileen Nielsen , chair, Science and Law Committee, New York City Bar Association. M ichael Philips , Assistant General Counsel, Microsoft. D inah PoKempner , General Counsel, Human Rights Watch. I rina Raicu , Director, Internet Ethics Program, Markkula Center for Applied Ethics, Santa Clara University. D avid Robinson , Visiting Scientist, AI Policy and Practice Initiative, Cornell University; Adjunct Professor of Law, Georgetown University Law Center; Managing Director (on leave), Upturn. A lanna Rutherford , Vice President, Global Litigation & Competition, Visa. G eorge Socha, Esq. , Consulting Managing Director, BDO USA; co-founder, Electronic Discovery Reference Model (EDRM) and Information Governance Reference Model (IGRM). L ee Tiedrich , Partner, IP/Technology Transactions, and Co-Chair, Artificial Intelligence Initiative, Covington & Burling LLP. D arrell M. West , VP, Governance Studies, Director, Center for Technology Innovation, Douglas Dillon Chair in Governance Studies, Brookings Institution. B endert Zevenbergen , Research Fellow, Center for Information Technology Policy, Princeton University; Researcher, Oxford Internet Institute. J iyu Zhang , Associate Professor and Executive Director of the Law and Technology Institute, Renmin University of China School of Law. P eter Zimroth , Director, New York University Center on Civil Justice; retired partner, Arnold & Porter; former Assistant US Attorney, Southern District of New York.264 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawEndnotes 1 S ee S. Jasanoff, Governing Innovation: The Social Contract and the Democratic Imagination, Seminar, vol. 597, pp. 16-25, May 2 A s articulated in EAD General Principles 1 (Human Rights), 2 (Well-Being), and 3 (Data Agency). See also EAD Chapter, Classical Ethics in A/IS, In applying A/IS in pursuit of these goals, tradeoffs are inevitable. Some applications of predictive policing, for example, may reduce crime, and so enhance well-being, but may do so at the cost of impinging on a right to privacy or weakening protections against unwarranted search and seizure. How these tradeoffs are negotiated may vary with cultural and legal traditions. 3 R isks and benefits, and their perception, are neither always well-defined at the outset nor static over time. Social expectations and even ideas of lawfulness constantly evolve. For example, if younger generations, accustomed to the use of social networking technologies, have lower expectations of privacy than older generations, should this be deemed to be a benefit to society, a risk, or neither? 4 R egarding the nature of the guidance provided in this section: Artificial intelligence, like many other domains relied on by the legal realm (e.g., medical and accounting forensics, ballistics, or economic analysis), is a scientific discipline distinct from the law. Its effective and safe design and operation have underpinnings in academic and professional competencies in computer science, linguistics, data science, statistics, and related technical fields. Lawyers, judges, and law enforcement officers increasingly draw on these fields, directly or indirectly, as A/IS are progressively adopted in the legal system. This document does not seek to offer legal advice to lawyers, courts, or law enforcement agencies on how to practice their professions or enforce the law in their jurisdictions around the globe. Instead, it seeks to help ensure that A/IS and their operators in a given legal system can be trusted by lawyers, courts, and law enforcement agencies, and civil society at large, to perform effectively and safely. Such effective and safe operation of A/IS holds the potential of producing substantial benefits for the legal system, while protecting all of its participants from the ethical, professional, and business risks, or personal jeopardy, that may result from the intentional, unintentional, uninformed, or incompetent procurement and operation of artificial intelligence. 5 S ee Rensselaer Polytechnic Institute, A Conversation with Chief Justice John G. Roberts, Jr., April 1 1, YouTube video, April 12, [Online]. Available: . 6 Uninformed avoidance of adoption can be one of two types: (a) avoidance of adoption when the information needed to enable sound decisions is available but is not taken into 265 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawconsideration, and (b) avoidance of adoption when the information needed to enable sound decisions is simply not available. Unlike the former type of avoidance, the latter type is a prudent and well-reasoned avoidance of adoption and, pending better information, is the course recommended by a number experts and nonexperts. 7 F or purposes of this chapter, we have made the deliberate choice to focus on these four principles without taking a prior position on where the deployment of A/IS may or may not be acceptable in legal systems. Where these principles cannot be adequately operationalized, it would follow that the deployment of A/IS in a legal system cannot be trusted. Where A/IS can be evidenced to meet desired thresholds for each duly operationalized principle, it would follow that their deployment can be trusted. Such information is intended to facilitate, not preempt, the indispensable public policy dialogue on the extent to which A/IS should be relied upon to meet the specific needs of the legal systems of societies around the world. 8 I t is beyond the scope of this chapter to discuss the process through which such adherence may become institutionalized in the complex legal, technological, political, and cultural dynamics in which sociotechnical innovation occurs. It is worth noting, however, that this process typically involves four steps. First, a wide range of market and culture-driven practices emerge. Second, a set of best practices arises, reflecting a group s willingness to adopt certain rules. Third, some of these best practices are formulated into standards, which enable enforcement (through private contracts, professional codes of practice, or legislation). Finally, those enforceable standards render the performance of some activities sufficiently reliable and predictable to enable trustworthy operation at the scale of society. Where these elements (rulemaking, enforcement, scalable operation) are present, new institutions are born. 9 F or a discussion of the definition of A/IS, see the Terminology Update in the Executive Summary of EAD. The principles outlined in this section as constitutive of informed trust do not depend on a precise, consensus definition of A/IS and are, in fact, designed to be enable successful operationalization under a broad range of definitions. 10 S uch as Gross Domestic Product (GDP), Gross National Income (GNI) per capita, the WEF Global Competitiveness Index, and others. 11 S uch as life expectancy, infant mortality rate, and literacy rate, as well as composite indices such as the Human Development Index, the Inequality-Adjusted Human Development Index, the OECD Framework for Measuring Well-being and Progress, and others. For more on measures of well-being, see the EAD chapter on Well-being . 12 S ee United Nation General Assembly, Universal Declaration of Human Rights, Dec. 10, 1948, available: ; see also United Nations Office of the High Commissioner: Human Rights, The Vienna Declaration and Programme of Action, June 25, 1993, available: .266 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law13 S ee UNICEF, Convention on the Rights of the Child, Nov. 4, 2014, available: unicef.org/crc/index_30160.html . 14 S ee United Nations Security Council, The Rule of Law and Transitional Justice in Conflict and Post-conflict Societies: Report of the Secretary General, Report S/2004/616 ( . 15 S ee The World Economic Forum, The Global Competitiveness Report: 2018 , ed. K. Schwab (, pp. 12ff. 16 S ee A. Brunetti, G. Kisunko, and B. Weder, Credibility of Rules and Economic Growth: Evidence from a Worldwide Survey of the Private Sector, The World Bank Economic Review, vol. 12, no. 3, pp. 353 384, Available: see also World Bank, World Development Report Governance and the Law, Jan. Available: doi.org/1596/978-1-4648-0950-17 T he question of intellectual property law in an era of rapidly advancing technology (both A/IS and other technologies) is a complex and often contentious one involving legal, economic, and ethical considerations. We have not yet studied the question in sufficient depth to reach a consensus on the issues raised. We may examine the issues in depth in a future version of EAD. For a forum in which such issues are discussed, see the Berkeley-Stanford Advanced Patent Law Institute. See also The World Economic Forum, Artificial Intelligence Collides with Patent Law. April Available: .18 A c omponent of human dignity is privacy, and a component of privacy is protection and control of one s data; in this regard, frameworks such as the EU s General Data Protection Regulation (GDPR) and the Council of Europe s Guidelines on the protection of individuals with regard to the processing of personal data in a world of Big Data have a role to play in setting standards for how legal systems can protect data privacy. See also EAD General Principle 3 (Data Agency). 19 F rameworks such as the Universal Declaration of Human Rights and the Vienna Declaration and Programme of Action (VDPA) have a role to play in articulating human-rights standards to which legal systems should adhere. See also EAD General Principle 1 (Human Rights). 20 F or more on the importance of measures of well-being beyond GDP, see EAD General Principle 2 (Well-being). 21 F or a conceptual framework enabling the country-by-country assessment of the Rule of Law, see World Justice Project, Rule of Law Index . url: 22 Se e D. Kennedy, The Rule of Law, Political Choices and Development Common Sense, in The New Law and Economic Development: A Critical Appraisal, D. M. Trubek and A. Santos, Ed. Cambridge: Cambridge University Press, 2006, pp. 156-157; see also A. Sen, Development as Freedom . New York: Alfred A. Knopf, 267 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law23 Se e Kennedy (: pp. 168- The idea that building the rule of law might itself be a development strategy encourages the hope that choosing law in general could substitute for all the perplexing political and economic choices that have been at the center of development policy making for half a century. The politics of allocation is submerged. Although a legal regime offers an arena to contest those choices, it cannot substitute for them. 24 F airness (as well as bias ) can be defined in more than one way. For purposes of this chapter, a commitment is not made to any one definition and indeed, it may not be either desirable or feasible to arrive at a single definition that would be applied in all circumstances. The trust principles proposed in the chapter (Effectiveness, Competence, Accountability, and Transparency) are defined such that they will provide information that will allow the testing of an application of A/IS against any fairness criteria. 25 T he confidentiality of jury deliberations, certain sensitive cases, and personal data are some of the considerations that influence the extent of appropriate public examination and oversight mechanisms. 26 T he avoidance of negative consequences is important to note in relation to effectiveness. The law can be used for malevolent or intensely disputed purposes (for example, the quashing of dissent or mass incarceration). The instruments of the law, including A/IS, can render the advancement of such purposes more effective to the detriment of democratic values, human rights, and human well-being.27 S tudies conducted by the US National Institute of Standards and Technology (NIST) between 2006 and 201 1, known as the US NIST Text REtrieval Conference (TREC) Legal Track, suggest that some A/IS-enabled processes, if operated by trained experts in the relevant scientific fields, can be more effective (or accurate) than human attorneys in correctly identifying case-relevant information in large data sets. NIST has a long-standing reputation for cultivating trust in technology by participating in the development of standards and metrics that strengthen measurement science and make technology more secure, usable, interoperable, and reliable. This work is critical in the A/IS space to ensure public trust of rapidly evolving technologies so that we can benefit from all that this field has to promise. 28 I n describing the potential A/IS have for aiding in the auditing of decisions made in the civil and criminal justice systems, we are envisioning them acting as aids to a competent human auditor (see Issue in the context of internal or judicial review. 29 O f course, the use of A/IS in improving the effectiveness of law enforcement may raise concerns about other aspects of well-being, such as privacy and the rise of the surveillance state, cf. Minority Report (. If A/IS are to be used for law enforcement, steps must be taken to ensure that they are used, and that citizens trust that they will be used, in ways that are conducive to ethical law enforcement and individual well-being (see Issue .268 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law30 A /IS may also provide assistance in carrying out legal tasks associated with larger transactions, such as evaluating contracts for risk in connection with a M&A transaction or reporting exposure to regulators. 31 T he recommendations provided in this chapter (both under this issue and under the other issues discussed in the chapter) are intended to give general guidance as to how those with a stake in the just and effective operation of a legal system can develop norms for the trustworthy adoption of A/IS in the legal system. The specific ways in which the recommendations are operationalized will vary from society to society and from jurisdiction to jurisdiction. 32 S ee Global Governance of AI Roundtable: Summary Report 2018, World Government Summit, p. Available: . (The February 2018 Dubai Global Governance of AI Roundtable brought together ninety leading thinkers on AI governance.) 33 Se e State v Loomis , 881 N.W.2d 749 (Wis. , cert. denied (; see also Criminal Law Sentencing Guidelines Wisconsin Supreme Court Requires Warning Before Use of Algorithmic Risk Assessments in Sentencing State v. Loomis, 881 N.W.2d 749 (Wis. , Harvard Law Review, vol. 130, no. 5, pp. 1535-1536, Available: see also K. Freeman, Algorithmic Injustice: How the Wisconsin Supreme Court Failed to Protect Due Process Rights in State v. Loomis, North Carolina Journal of Law and Technology, vol. 18, no. 5, pp. 75-76, Available: 34 A n example of an initiative that seeks to bridge the gap between technical and legal expertise is the Artificial Intelligence Legal Challenge, held at Ryerson University and sponsored by Canada s Ministry of the Attorney General: . 35 A nd, in addressing the challenges, consideration must be given to existing modes of proposing and approving innovation in the legal system. Trust in A/IS will be undermined if they are viewed as not having been vetted via established processes. 36 F or an overview of risk and risk management, see Working Party on Security and Privacy in the Digital Economy, Background Report for Ministerial Panel 2, Directorate for Science, Technology and Innovation, Committee on Digital Economy Policy, Managing Digital Security and Privacy Risk, OECD, June 1, 2016; see p. 37 I t is worth emphasizing the informed qualifier we attach to trust here. Far from advocating for a blind trust in A/IS, we argue that A/IS should be adopted only when we have sound evidence of their effectiveness, when we can be confident of the competence of their operators, when we have assurances that these systems allow for the attribution of responsibility for outcomes (both positive and negative), and when we have clear views into their operation. Without those conditions, we would argue that A/IS should not be adopted in the legal system.269 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law38 T he importance of testing the effectiveness of advanced technologies, including A/IS, in the legal system (and beyond) is not new: it was highlighted by Judge Paul W. Grimm in an important early ruling on legal fact-finding, Victor Stanley v. Creative Pipe, Inc ., 250 F.R.D. 251, 257 (D. Md. , followed, among others, by the influential research and educational institute The Sedona Conference as well as the International Organization for Standardization (ISO). See An Open Letter to Law Firms and Companies in the Legal Tech Sector , The Sedona Conference (, and Commentary on Achieving Quality in the E-Discovery Process (: 7; ISO standard on electronic discovery (ISO/IEC 27050-: Most recently, in the summary report of the Global Governance of AI Roundtable at the 2018 World Government Summit, Omar bin Sultan Al Olama, Minister of State for Artificial Intelligence of the UAE, highlighted the importance of empirical information in assessing the suitability of A/IS. 39 I n the terminology of software development, verification is a demonstration that a given application meets a narrowly defined requirement; validation is a demonstration that the application answers its real-world use case. When we speak of gathering evidence of the effectiveness of A/IS, we are speaking of validation. 40 S tandards may include compliance with defined professional competence or other ethical requirements, but also other types of standards, such as data standards. Data standards may serve as a digital lingua franca with the potential of both supporting broad-based technological innovation (including A/IS innovation) in a legal system and facilitating access to justice. As part of interactive technology solutions, appropriate data standards may help connect the ordinary citizen to the appropriate resources and information for his or her legal needs. For a discussion of open data standards in the context of the US court system, see D. Colarusso and E. J. Rickard, Speaking the Same Language: Data Standards and Disruptive Technologies in the Administration of Justice, Suffolk University Law Review, vol. L387, 41 F or measurement of bias in facial recognition software, see C. Garvie, A. M. Bedoya, and J. Frankle, The Perpetual Line-Up: Unregulated Police Face Recognition in America, Georgetown Law, Center on Privacy & Technology, Oct. Available: 42 T he inclusion of such collateral effects in assessing effectiveness is an important element in overcoming the apparent black box or inscrutable nature of A/IS. See, for example, J. A. Kroll, The fallacy of inscrutability, Philosophical Transactions of the Royal Society A: Mathematical, Physical, and Engineering Sciences, vol. 376, no. 2133, Oct. Available: doi.org/1098/rsta.The study addresses, among other questions, how measurement of a system beyond understanding of its internals and its design can help to defeat inscrutability. 43 T he question of the salience of collateral impact will vary with the specific application of A/IS. For example, false positives in document review related to fact-finding will generally not raise acute ethical issues, but false positives 270 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawin predictive policing or sentencing will. In these latter domains, complex and sometimes unsettled issues of fairness arise, particularly when social norms of fairness change regionally and over time (sometimes rapidly). Any A/IS that was designed to replicate some notion of fairness would need to demonstrate its effectiveness, first, at replicating prevailing notions of fairness that have legitimacy in society, and second, at responding to evolutions in such notions of fairness. In the current state of A/IS, in which no system has been able to demonstrate consistent effectiveness in either of the above regards, it is essential that great discretion be exercised in considering any reliance on A/IS in domains such as sentencing and predictive policing. 44 T hese exercises go by various names in the literature: effectiveness evaluations , benchmarking exercises , validation studies , and so on. See, for example, the definition of validation study in AINOW s 2018 Algorithmic Accountability Toolkit ( aap-toolkit.pdf ), p. For our purposes, what matters is that the exercise be one that collects, in a scientifically sound manner, evidence of how fit for purpose any given A/IS are. 45 T his feature of evaluation design is important, as only tasks that accurately reflect real-world conditions and objectives (which may include the avoidance of unintended consequences, such as racial bias) will provide compelling guidance as to the suitability of an application for adoption in the real world. 46 F or TREC generally, see: For the TREC Legal Track specifically, see: .47 W hen a complex system can be broken down into separate component systems, it may be appropriate to assess either the effectiveness of each component, or that of the end-to-end application as a whole (including human operators), depending on the specific question to be answered. 48 Q ualitative considerations may also help counter attempts to game the system (i.e., attempts to use bad-faith methods to meet a specific numerical target); see B. Hedin, D. Brassil, and A. Jones, On the Place of Measurement in E-Discovery, in Perspectives on Predictive Coding and Other Advanced Search Methods for the Legal Practitioner, ed. J. R. Baron, R. C. Losey, and M. D. Berman. Chicago: American Bar Association, 2016, p. 415f. 49 E ven in fact-finding, accurate extraction of facts does not eliminate the need for reasoned judgment as to the significance of the facts in the context of specific circumstances and cultural considerations. Used properly, A/IS will advance the spirit of the law, not just the letter of the law. 50 E lectronic discovery is the task of searching through large collections of electronically stored information (ESI) for material relevant to civil and criminal litigation and investigations. Among applications of A/IS to legal tasks and questions, the application to legal discovery is probably the most mature, as measured against the criteria of having been tested, assessed and approved by courts, and adopted fairly widely across various jurisdictions.271 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law51 W hile there is general consensus about the importance of these metrics in gauging effectiveness in legal discovery, there is not a consensus around the precise values for those metrics that must be met for a discovery effort to be acceptable. That is a good thing, as the precise value that should be attained, and demonstrated to have been attained, in any given matter will be dependent on, and proportional to, the specific facts and circumstances of that matter. 52 D ifferent domains of application of A/IS to legal matters will vary not only with regard to the availability of consensus metrics of effectiveness, but also with regard to conditions that affect the challenge of measuring effectiveness: availability of data, impact of social bias, and sensitivity to privacy concerns all affect how difficult it may be to arrive at consensus protocols for gauging effectiveness. In the case of defining an effectiveness metric for A/IS used in support of sentencing decisions, one challenge is that, while it is easy to find when an individual who has been released commits a crime (or is convicted of committing a crime), it is difficult to assess when an individual who was not released would have committed a crime. For a discussion of the challenges in measuring the effectiveness of tools designed to assess flight risk, see M. T. Stevenson, Assessing Risk Assessment in Action. Minnesota Law Review, vol. 103, Available: doi.org/2139/ssrn.53 S ound measurement may also serve as an effective antidote to the unsubstantiated claims sometimes made regarding the effectiveness of certain applications of A/IS to legal matters (e.g., flight risk assessment technologies); see Stevenson, Assessing Risk Assessment . Unsubstantiated claims are an appropriate source of an informed dis trust in A/IS. Such well-founded distrust can be addressed only with truly meaningful and sound measures that provide accurate information regarding the capabilities and limitations of a given system. 54 S ee the discussion under Illustration Effectiveness in this chapter. 55 F or more on principles for data protection, see the EAD chapter Personal Data and Individual Agency . 56 T he importance of validation by practitioners is reflected in The European Commission s High-Level Expert Group on Artificial Intelligence Draft Ethics Guidelines for Trustworthy AI: Testing and validation of the system should thus occur as early as possible and be iterative, ensuring the system behaves as intended throughout its entire life cycle and especially after deployment. (Emphasis added.) See High-Level Expert Group on Artificial Intelligence, DRAFT Ethics Guidelines for Trustworthy AI: Working Document for Stakeholders Consultation, The European Commission. Brussels, Belgium: Dec. 18, 57 T hat scrutiny need not extend to IP or other protected information (e.g., attorney work product). Validation methods and results are a matter of numbers and procedures for obtaining the numbers, and their disclosure would not impinge on safeguards against the disclosure of legitimately protected information.272 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law58 A re cent matter from the US legal system illustrates how a failure to disclose the results of a validation exercise can limit the exercise s ability to achieve its intended purpose. In Winfield v. City of New York (Opinion & Order. 15-CV-05236 [LTS] [KHP]. SDNY , a party had utilized the A/IS-enabled system to conduct a review of documents for relevance to the matter being litigated. When the accuracy and completeness of the results of that review were challenged by the requesting party, the producing party disclosed that it had, in fact, conducted validation of its results. Rather than requiring that the producing party simply disclose the results of the validation to the requesting party, the judge overseeing the dispute chose to review the results herself in camera , without providing access to the requesting party. Although the judge then said that the evidence she was provided supported the accuracy and completeness of the review, the requesting party could not itself examine either the evidence or the methods whereby it was obtained, and so could not gain confidence in the results. That confidence comes only from examining the metrics and the procedures followed in obtaining them. Moreover, the results of a validation exercise, which are usually simple numbers that reflect sampling procedures, can be disclosed without revealing the content of any documents, any proprietary tools or methods, or any attorney work product. If the purpose of conducting a validation exercise is to gather evidence of the effectiveness of a process, in the event that the process is challenged, keeping that evidence hidden from those who would challenge the process limits the ability of the validation exercise to achieve its intended purpose.59 60 T REC Legal Track (2006-201 : legal.umiacs.umd.edu/ . 61 T he statistical evidence in question here is statistical evidence of the effectiveness of A/IS applied to the task of discovery; it is not statistical evidence of facts actually at issue in litigation. Courts may have different rules for the admissibility of the two kinds of statistical evidence (and there will be jurisdictional differences on these questions). 62 I t is important to underscore that, whereas developers and operators of A/IS should be able to derive sound measurements of effectiveness, the courts should determine what level of effectiveness what score should be demonstrated to have been achieved, based on the facts and circumstances of a given matter. In some instances, the cost (in terms of sample sizes, resources required to review the samples, and so on) of demonstrating the achievement of a high score will be disproportionate to the stakes of a given matter. In others, for example, a major securities fraud claim that potentially affects thousands of citizens, a court might justifiably demand a demonstration of the achievement of a very high score, irrespective of cost. Demonstrations of the effectiveness of A/IS (and of their operators) are instruments in support of, not in substitution of, judicial decision-making. 63 S ee, for example, B. Hedin, S. Tomlinson, J. R. Baron, and D. W. Oard, Overview of the TREC 2009 Legal Track, in NIST Special Publication: SP 500-278, The Eighteenth Text REtrieval Conference (TREC Proceedings (.273 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law64 S ee M. R. Grossman and G. V. Cormack, Technology-Assisted Review in E-Discovery Can Be More Effective and More Efficient Than Exhaustive Manual Review, Richmond Journal of Law and Technology, vol. 17, no. 3, 201 Available: pdf. Note that the two systems that conclusively demonstrated better than human performance took methodologically distinct approaches, but they shared the characteristic of having been designed, operated, and measured for accuracy by scientifically trained experts. 65 D a Silva Moore v. Publicis Groupe , 2012 WL 607412 (S.D.N.Y. Feb. 24, . See also A. Peck, Search, Forward, Legaltech News. Oct. 1, 201 Available: legaltechnews/almID/1202516530534Search-Forward/ . 66 T he fact that NIST has as important role to play in developing standards for the measurement of the safety and security of A/IS was recognized in a recent (September, report from the U.S. House of Representatives: At minimum, a widely agreed upon standard for measuring the safety and security of AI products and applications should precede any new regulations. ... The National Institute of Standards and Technology (NIST) is situated to be a key player in developing standards. (Will Hurd and Robin Kelly, Rise of the Machines: Artificial Intelligence and its Growing Impact on U.S. Policy, U.S. House of Representatives Committee on Oversight and Government Reform Subcommittee on Information Technology, September, .67 T he competence principle is intended to apply to the post design operation of A/IS. Of course, that does not mean that designers and developers of A/IS are free of responsibility for their systems outcomes. As discussed in the background to this issue, it is incumbent on designers and developers to assess the risks associated with the operation of their systems and to specify the operator competencies needed to mitigate those risks. For more on the question of designer incompetence or negligence, see the discussion of software malpractice in Kroll (. 68 T he ISO standard on e-discovery, ISO/IEC 27050-3, does recognize the importance of expertise in applying advanced technologies in a search for documents responsive to a legal inquiry; see ISO/IEC 27050-Information technology Security techniques Electronic discovery Part Code of practice for electronic discovery , Geneva (, pp. 19-69 S ee, for example, ABA Model Rule 1, comment To maintain the requisite knowledge and skill, a lawyer should keep abreast of changes in the law and its practice, including the benefits and risks associated with relevant technology, engage in continuing study and education and comply with all continuing legal education requirements to which the lawyer is subject. Available: . See also, The State Bar of California Standing Committee on Professional Responsibility and Conduct, Formal Opinion No. 2015-Available: 274 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law ethics/Opinions/CAL%202015-193%20%5B1 1-0004%5D%20(06-30-%20-%20FINAL.pdf . 70 I n the deliberations of the Law Committee of the 2018 Global Governance of AI Roundtable, the question of the competencies needed in order to effectively operate and measure the efficacy of AI systems in legal functions that affect the rights and liberty of citizens was cited as one of the considerations that appear to be most overlooked in the current public dialogue. See Global Governance of AI Roundtable: Summary Report 2018, World Government Summit, p. Available: 71 S ee A. G. Ferguson, Policing Predictive Policing, Washington University Law Review, vol. 94, no. 5, 1 109, 1 Available: 72 I n addition, a lack of competence in interpreting the results of a statistical exercise can (and often does) result in an incorrect conclusion (on the part of a party to a dispute or of a judge seeking to resolve a dispute). For example, in In re: Biomet , a judge addressing a discovery dispute interpreted the statistical data provided by the producing party as indicating that the producing party s retrieval process had left behind a comparatively modest number of responsive documents, when the statistical evidence showed, in fact, that a substantial number of responsive documents had been left behind. See In re: Biomet M2a Magnum Hip Implant Prods. Liab. Litig. No. 12-MD-2391 (N.D. Ind. April 18, . 73 F or example, a prior violent conviction may be weighted equally, whether the violent act was a shove or a knife attack. See Human Rights Watch. Q & A: Profile Based Risk Assessment for US Pretrial Incarceration, Release Decisions, June 1, Available: . 74 B ias can be introduced in a number of ways: via the features taken into consideration by the algorithm, via the nature and composition of the training data, via the design of the validation protocol, and so on. A competent operator will be alert to and assess such potential sources of bias. 75 A mong the conditions may be, for example, that the results of the system are to be used only to provide guidance to the human decision maker (e.g., judge) and should not be taken as, in themselves, dispositive. 76 G iven that the effective functioning of a legal system is a matter of interest to the whole of society, it is important that all members of a society be provided with access to the resources needed to understand when and how A/IS are applied in support of the functioning of a legal system. 77 A mong the topics covered by such training should be the potential for automation bias and ways to mitigate it. See L. J. Skitka, K. Mosier, and M. D. Burdick, Does automation 275 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawbias decision-making? International Journal of Human-Computer Studies, vol. 51, no. 5, pp. 991-1006, Available: L. J. Skitka, K. Mosier, and M. D. Burdick, Accountability and automation bias, International Journal of Human-Computer Studies, vol. 52, no. 4, pp. 701-717, Available: S ome government agencies are working toward creating a more effective partnership between the skills found in technology start-ups and the skills required of legal practitioners. See Legal Innovation Zone. Ryerson s Legal Innovation Zone Announces Winners of AI Legal Challenge, March 26, Available: . 79 S ee Amazon. Amazon Rekognition. https:// aws.amazon.com/rekognition/ (. 80 S ee E. Dwoskin, Amazon is selling facial recognition to law enforcement for a fistful of dollars. Washington Post , May 22, Available: news/the-switch/wp/2018/05/22/amazon-is-selling-facial-recognition-to-law-enforcement-for-a-fistful-of-dollars/?noredirect=on&utm_term = .07d9ca13ab77 . 81 S ee, for example, J. Stanley, FBI and Industry Failing to Provide Needed Protections for Face Recognition. ACLU Free Future , June 15, Available: technology/surveillance-technologies/fbi-and-industry-failing-provide-needed .82 I t is also the case that, among the false positives, nonwhite members of Congress were overrepresented relative to their proportion in Congress as a whole, perhaps indicating that the accuracy of the technology is, to some degree, race-dependent. Without knowing more about the composition of the mugshot database, however, we cannot assess the significance of this result. 83 S ee J. Snow, Amazon s Face Recognition Falsely Matched 28 Members of Congress with Mugshots. ACLU Free Future , July 26, Available: technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28 . See also R. Brandom, Amazon s facial recognition matched 28 members of Congress to criminal mugshots. The Verge , July 26, Available: theverge.com/2018/7/26/17615634/amazon-rekognition-aclu-mug-shot-congress-facial-recognition . 84 S ee Amazon Rekognition Developer Guide. Amazon, p. 131, Available: Also see K. Tenbarge, Amazon Responds to ACLU s Highly Critical Report of Rekognition Tech, Inverse, July 26, Available: 85 T he story also highlights the question of accountability, illustrating how the principles discussed in this report intersect with and complement each other.276 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law86 O f course, competent use does not preclude use for bad ends (e.g., government surveillance that impinges on human rights). The principle of competence is one principle in a set that, collectively, is designed to ensure the ethical application of A/IS. See the EAD chapter General Principles . 87 D eveloping well grounded guidelines will typically require that the creators of A/IS gather input from both those operating the technology and those affected by the technology s operation. 88 T he use of facial recognition technologies by security and law enforcement agencies raises issues that extend beyond the question of operator competence. For further discussion of such issues, see C. Garvie, A. M. Bedoya, and J. Frankle, The Perpetual Line-Up: Unregulated Police Face Recognition in America, Georgetown Law, Center on Privacy & Technology , October 18, 2016, Available: 89 A s noted above, some professional organizations, such as the ABA, have begun to recognize in their codes of ethics the importance of technological competence, although the guidance does not yet address A/IS specifically. 90 I ncluding those engaged in the procurement and deployment of a system means that those acquiring and authorizing the use of a system can share in the responsibility for its results. For example, in the case of A/IS deployed in the service of the courts, this could be the judiciary; in the case of A/IS deployed in the service of law enforcement, this could be the agency responsible for the enforcement of the law and the administration of justice; in the case of A/IS used by a party to legal proceedings, this could be the party s counsel. 91 J . New and D. Castro, How Policymakers Can Foster Algorithmic Accountability. Information Technology & Innovation Foundation, p. 5, Available: 92 I ncluded among possible causes for an effect are not only the decision-making pathways of algorithms but also, importantly, the decisions made by humans involved in the design, development, procurement, deployment, operation, and validation of effectiveness of A/IS. 93 T he challenge, moreover, is one not only of assigning responsibility, but of assigning levels of responsibility (a task that could benefit from a neutral model that could consider how much interaction and influence each stakeholder has in every decision). 94 S cherer (: In addition to diffuseness, Scherer identifies discreetness, discreteness, and opacity as features of the design and development of A/IS that make apportioning responsibility for their outcomes a challenge for regulators and courts. 95 I n answering these questions, it will be important to keep in mind the distinction between responsibility (a factual question) and ultimate accountability (a normative question). In the case of the example under discussion, there may be multiple individuals who have 277 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawsome practical responsibility for the sentence given, but the normative framework may place ultimate accountability on the judge. Before normative accountability can be assigned, however, pragmatic responsibilities must be clarified and understood. Hence the focus, in this section, on clarifying lines of responsibility so that ultimate accountability can be determined. 96 I f effectiveness is measured against statistics that themselves may represent human bias (e.g., arrest rates), then the effectiveness measures may just reflect and reinforce that bias. 97 The algorithm did it is not an acceptable excuse if algorithmic systems make mistakes or have undesired consequences, including from machine-learning processes. See Principles for Accountable Algorithms and a Social Impact Statement for Algorithms. FAT/ML Resources. 98 S ee Langewiesche, W. The Lessons of ValuJet 592 . Atlantic Monthly . 81-97; S. D. Sagan. Limits of Safety: Organizations, Accidents, and Nuclear Weapons . Princeton University Press, 99 F or a discussion of the role of explanation in maintaining accountability for the results of A/IS and of the question of whether the standards for explanation should be different for A/IS than they are for humans, see F. Doshi-Velez, M. Kortz, R. Budish, C. Bavitz, S. J. Gershman, D. O Brien, S. Shieber, J. Waldo, D. Weinberger, and A. Wood, Accountability of AI Under the Law: The Role of Explanation (November 3, . Berkman Center Research Publication Forthcoming; Harvard Public Law Working Paper No. 18-Available: or A lso, gaining access to that information should not be unduly burdensome. 101 T hose developing a model for accountability for A/IS may find helpful guidance in considering models of accountability used in other domains (e.g., data protection). 102 F or a discussion of how such policies might be implemented in accordance with protocols for information governance, see J. R. Baron and K. E. Armstrong, The Algorithm in the C-Suite: Applying Lessons Learned and Information Governance Best Practices to Achieve Greater Post-GDPR Algorithmic Accountability, in The GDPR Challenge: Privacy, Technology, and Compliance In An Age of Accelerating Change , A. Taal, Ed. Boca Raton, FL: CRC Press, forthcoming. 103 T hese inquiries can be supported by technological tools that may provide information essential to answering questions of accountability but that do not require full transparency into underlying computer code and may avoid the necessity of an intrusive audit; see Kroll et al. (. Among the tools identified by Kroll and his colleagues are: software verification, cryptographic commitments, zero-knowledge proofs, and fair random choices. While the use of such tools may avoid the limitations of solutions such as transparency and audit, they do require that creators of A/IS design their systems so that they will be compatible with the application of such tests.278 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law104 C ertifications may include, for example, professional certifications of competence, but also certifications of compliance of processes with standards. An example of a certification program specifically addressing A/IS is The Ethics Certification Program for Autonomous and Intelligent Systems (ECPAIS) , ieee.org/industry-connections/ecpais.html . 105 T his means that A/IS used in legal systems will have to be defensible in courts. The margin of error will have to be low or the use of A/IS will not be permitted. 106 I t is also the case that evidence produced by A/IS will be subject to chain-of-custody rules, as are other types of forensic evidence, to ensure integrity, confidentiality, and authenticity. 107 S ee for instance Art. 22( Regulation (EU) 2016/108 Hu man dignity, as a core value protected by the United Nations Universal Declaration of Human Rights, requires us to fully respect the personality of each human being and prohibits their objectification. 109 T his concern is reflected in Principle 5 of the European Ethical Charter on the Use of Artificial Intelligence in Judicial Systems and their Environment, recently published by the Council of Europe s European Commission for the Efficiency of Justice (CEPEJ). Principle 5 ( Principle Under User Control : preclude a prescriptive approach and ensure that users are informed actors and in control of the choices made ) states, with regard to professionals in the justice system that they should at any moment, be able to review judicial decisions and the data used to produce a result and continue not to be necessarily bound by it in the light of the specific features of that particular case, and, with regard to decision subjects, that he or she must be clearly informed of any prior processing of a case by artificial intelligence before or during a judicial process and have the right to object, so that his/her case can be heard directly by a court. See CEPEJ, European Ethical Charter on the Use of Artificial Intelligence in Judicial Systems and their Environment (Strasbourg, , p. 110 J . Tashea, Calculating Crime: Attorneys are Challenging the Use of Algorithms to Help Determine Bail, Sentencing and Parole , ABA Journal (March . 111 Lo omis v. Wisconsin , 68 WI. (. 112 I d. at pp. 46-113 R . Wexler, Life, Liberty, and Trade Secrets: Intellectual Property in the Criminal Justice System , Stanford Law Review, 114 M alenchik v. State , 928 N.E.2d 564, 574 (Ind. . 115 Pe ople v. Chubbs CA2/4, B258569 (Cal. Ct. App. . 116 U .S. v. Ocasio, No. 1 1-cr-02728-KC, slip op. at 1-2, 1 1-12 (W.D. Tex. May 28, . 117 U .S. v. Johnson, No. 15-cr-00565-VEC, order (S.D.N.Y., June 7, . 118 I ndeed, without transparency, there may, in some circumstances, be no means for even knowing whether an error that needs to be corrected was committed. In the case of A/IS 279 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Lawapplied in a legal system, an error can mean real harm to the dignity, liberty, and life of an individual. 119 F airness (as well as bias ) can be defined in more than one way. For purposes of this discussion, a commitment is not made to any one definition and indeed, it may not be either desirable or feasible to arrive at a single definition that would be applied in all circumstances. For purposes of this discussion, the key point is that transparency will be essential in building informed trust in the fairness of a system, regardless of the specific definition of fairness that is operative. 120 T o the extent permitted by the normal operation of the A/IS: allowing for, for example, variation in the human inputs to a system that may not be eliminated in any attempt at replication. 121 W ith regard to information explaining how a system arrived at a given output, GDPR makes provision for a decision subject s right to an explanation of algorithmic decisions affecting him or her: automated processing of personal data should be subject to suitable safeguards, which should include specific information to the data subject and the right to obtain human intervention, to express his or her point of view, to obtain an explanation of the decision reached after such assessment and to challenge the decision. GDPR, Recital 122 E ven among sensitive data, some data may be more sensitive than others. See I. Ajunwa, Genetic Testing Meets Big Data: Tort and Contract Law Issues, 75 Ohio St. L. J. 1225 (. Available: S ee A. Baker, Updated N.Y.P.D. Anti-Crime System to Ask: How We Doing? New York Times , May 8, 2017, com/2017/05/08/nyregion/nypd-compstat-crime-mapping.html ; S. Weichselbaum, How a Sentiment Meter Helps Cops Understand Their Precincts, Wired , July 16, Available: . 124 T his table is a preliminary draft and is meant only to illustrate a useful tool for facilitating reasoning about who should have access to what information. Other categories of stakeholder and other categories of information (e.g., the identity and nature of the designer/manufacturer of the A/IS, the identity and nature of the investors backing a particular system or company) could be added as needed. 125 F or discussions of these two dimensions of explanation, see S. Wachter, et al. (. Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation ; A. Selbst, and S. Barocas, The Intuitive Appeal of Explainable Machines. 126 W exler, Rebecca. Life, Liberty, and Trade Secrets: Intellectual Property in the Criminal Justice System . Stanford Law Review . 70 (: 1342-1429; Tashea, Jason. Federal judge releases DNA software source code that was used by New York City s crime lab. ABA Journal (. 127 O r, if two approaches are found to be, for practical purposes, equally effective, the simpler, more easily explained approach may be preferred.280 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law128 F or a discussion of the limits of transparency and of alternative modes of gaining actionable answers to questions of verification and accountability, see J.A. Kroll, J. Huey, S. Barocas, E.W. Felten, J.R. Reidenberg, D.G. Robinson, H. Yu, Accountable Algorithms (March 2, . University of Pennsylvania Law Review , Vol. 165, 2017 Forthcoming; Fordham Law Legal Studies Research Paper No. Available at SSRN: . See also J.A. Kroll, The fallacy of inscrutability, Phil. Trans. R. Soc . A http:// dx.doi.org/1098/rsta.0084 (Note p. While transparency is often taken to mean the disclosure of source code or data, possibly to a trusted entity such as a regulator, this is neither necessary nor sufficient for improving understanding of a system, and it does not capture the full meaning of transparency. ) 129 I n particular with respect to due process, the current dialogue on the use of A/IS centers on the tension between the need for transparency and the need for the protection of intellectual property rights. Adhering to the principle of Effectiveness as articulated in this work can substantially help in defusing this tension. Reliable empirical evidence of the effectiveness of A/IS in meeting specific real-world objectives may foster informed trust in such A/IS, without disclosure of proprietary or trade secret information. 130 S . Wachter, B. Mittelstadt, and C. Russell, Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR, SSRN Electronic Journal, p. 5, 2017 for the example cited.131 W . L. Perry, B. McInnis, C. C. Price, S. C. Smith, and J. S. Hollywood, Predictive Policing: The Role of Crime Forecasting in Law Enforcement Operations, The RAND Corporation, pp. 67-69, 132 S upport from the University of Memphis was led by Richard Janikowski, founding Director of the Center for Community Criminology and Research (School of Urban Affairs and Public Policy, the University of Memphis) and the Shared Urban Data System (The University of Memphis). 133 E . Figg, The Legacy of Blue CRUSH, High Ground, March 19, 134 F igg, Legacy. 135 Nu cleus Research, ROI Case Study: IBM SPSS Memphis Police Department , Boston, Mass., Document K31, June Perry et al., Predictive Policing , 136 F igg, Legacy. 137 F igg, Legacy. 138 S ee: AI Now, Algorithmic Accountability Policy Toolkit , p. 12, Oct. Available: ; D. Robinson and L. Koepke, Stuck in a Pattern: Early evidence on predictive policing and civil rights , Upturn, Aug. Available: https:// S. Brayne, Big Data Surveillance: The Case of Policing, American Sociological Review, Available: 177/0003122417725865 ; A. G. Ferguson, Policing Predictive Policing, 281 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems LawWashington University Law Review, vol. 94, no. 5, Available: ; K. Lum and W. Isaac, To predict and serve? Significance Available: onlinelibrary.wiley.com/doi/epdf/1 1 1 1/j.1740-x ; B. J. Jefferson, Predictable Policing: Predictive Crime Mapping and Geographies of Policing and Race, Annals of the American Association of Geographers, vol. 108, no. 1, pp. 1-16, Available: . 139 F or a discussion of the criteria that may define a high-crime area, and so potentially license more intrusive policing, see A. G. Ferguson and D. Bernache, The High-Crime Area Question: Requiring Verifiable and Quantifiable Evidence for Fourth Amendment Reasonable Suspicion Analysis, American University Law Review, vol. 57, pp. 1587-140 W hile A/IS, if misapplied, may perpetuate bias, it holds at least the potential, if applied with appropriate controls, to reduce bias. For a study of how an impersonal technology such as a red light camera may reduce bias, see R. J. Eger, C. K. Fortner, and C. P. Slade, The Policy of Enforcement: Red Light Cameras and Racial Profiling, Police Quarterly , pp. 1-17, Available: . 141 S ee, for example: J. Tashea, Estonia considering new legal status for artificial intelligence, ABA Journal, Oct. 20, 2017, and European Parliament Resolution of Feb. 16, 2017 . 142 S ee Legal Entity, Person, in B. Bryan A. Garner, Black s Law Dictionary, 10th Edition. Thomas West, 143 J . S. Nelson, Paper Dragon Thieves. Georgetown Law Journal 105 (: 871-144 M . U. Scherer, Of Wild Beasts and Digital Analogues: The Legal Status of Autonomous Systems . Nevada Law Journal 19, forthcoming 145 S ee M. U. Scherer, Of Wild Beasts and Digital Analogues: The Legal Status of Autonomous Systems . Nevada Law Journal 19, forthcoming 2018; J. F. Weaver. Robots Are People Too: How Siri, Google Car, and Artificial Intelligence Will Force Us to Change Our Laws . Santa Barbara, CA: Praeger, 2013; L. B. Solum. Legal Personhood for Artificial Intelligences . North Carolina Law Review 70, no. 4 (: 1231 282 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems About Ethically Aligned DesignThe Mission and Results of The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems To ensure every stakeholder involved in the design and development of autonomous and intelligent systems is educated, trained, and empowered to prioritize ethical considerations so that these technologies are advanced for the benefit of humanity. To advance toward this goal, The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems brought together more than a thousand participants from six continents who are thought leaders from academia, industry, civil society, policy, and government in the related technical and humanistic disciplines to identify and find consensus on timely issues surrounding autonomous and intelligent systems. By stakeholder we mean anyone involved in the research, design, manufacture, or messaging around intelligent and autonomous systems including universities, organizations, governments, and corporations all of which are making these technologies a reality for society. 283 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems About Ethically Aligned DesignFrom Principles to Practice Results from our Work to Date In addition to the creation of Ethically Aligned Design , The IEEE Global Initiative, independently or through the IEEE Standards Association, has directly inspired the following works: T he launch of the IEEE P7000 series of approved standardization projects This is the first series of standards in the history of the IEEE Standards Association that explicitly focuses on societal and ethical issues associated with a certain field of technology More information can be found at: ethicsinaction.ieee.org A rtificial Intelligence and Ethics in Design These ten courses are designed for global professionals, as well as their managers, working in engineering, IT, computer science, big data, artificial intelligence, and related fields across all industries who require up-to-date information on the latest technologies. The courses explicitly mirror content from Ethically Aligned Design , and feature numerous experts as instructors who helped create Ethically Aligned Design . More information can be found at: innovationatwork.ieee.org/courses/artificial-intelligence-and-ethics-in-design T he creation of an A/IS Ethics Glossary The Glossary features more than two hundred pages of terms that help to define the context of A/IS ethics for multiple stakeholder groups, specifically: engineers, policy makers, philosophers, standards developers, and computational disciplines experts. It is currently in its second iteration and has also been informed by the IEEE P7000 standards working groups. Download the Glossary at: standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ead1e_glossary.pdf T he launch of OCEANIS The IEEE Standards Association, inspired by the work of The IEEE Global Initiative, has contributed significantly to the establishment of The Open Community for Ethics in Autonomous and Intelligent Systems (OCEANIS). It is a global forum for discussion, debate, and collaboration for organizations interested in the development and use of standards to further the creation of autonomous and intelligent systems. OCEANIS members are working together to enhance the understanding of the role of standards in facilitating innovation, while addressing problems that expand beyond technical solutions to addressing ethics and values. More information can be found at: ethicsstandards.org 284 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems About Ethically Aligned Design T he launch of ECPAIS The Ethics Certification Program for Autonomous and Intelligent Systems (ECPAIS) has the goal to create specifications for certification and marking processes that advance transparency, accountability, and reduction in algorithmic bias in autonomous and intelligent systems. ECPAIS intends to offer a process and define a series of marks by which organizations can seek certifications for their processes around the A/IS products, systems, and services they provide. More information can be found at: standards.ieee.org/industry-connections/ecpais.html T he launch of CXI The Council on Extended Intelligence (CXI) was directly inspired by the work of The IEEE Global Initiative and the work of The MIT Media Lab around Extended Intelligence . CXI was launched jointly by the IEEE Standards Association and The MIT Media Lab. CXI s mission is to proliferate the ideals of responsible participant design, data agency, and metrics of economic prosperity, prioritizing people and the planet over profit and productivity. Membership includes thought leaders from the EU Parliament and Commission, the UK House of Lords, the OECD, the United Nations, local and national administrations, and renowned experts in economics, data science, and multiple other disciplines from around the world. More information can be found at: globalcxi.org T he launch of EADUC The Ethically Aligned Design University Consortium (EADUC) is being established with the aim to reach every engineer at the beginning of their studies to help them prioritize values-driven, applied ethical principles at the core of their work. Working in conjunction with philosophers, designers, social scientists, academics, data scientists, and the corporate and policy communities, EADUC also has the goal that Ethically Aligned Design will be used in teaching at all levels of education globally as the new vision for design in the algorithmic age. T he launch of AI Commons The work of The IEEE Global Initiative has delivered key ideas and inspiration that are rapidly evolving toward establishing a global collaborative platform around A/IS. The mission of AI Commons is to gather a true ecosystem to democratize access to AI capabilities and thus to allow anyone, anywhere to benefit from the possibilities that AI can provide. In addition, the group will be working to connect problem owners with the community of solvers, to collectively create solutions with AI. The ultimate goal is to implement a framework for participation and cooperation to make using and benefiting from AI available to all. More information can be found at: This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems About Ethically Aligned DesignIEEE P7000 Approved Standardization Projects The IEEE P7000 series of standards projects under development represents a unique addition to the collection of over 1,900 global IEEE standards and projects. Whereas more traditional standards have a focus on technology interoperability, functionality, safety, and trade facilitation, the IEEE P7000 series addresses specific issues at the intersection of technological and ethical considerations. Like its technical standards counterparts, the IEEE P7000 series empowers innovation across borders and enables societal benefit. For more information or to join any working group, please see the links below. Committees that authored Ethically Aligned Design, as well as other committees within IEEE, that created specific working groups are listed below each project. IEEE P7000 - IEEE Standards Project Model Process for Addressing Ethical Concerns During System Design Inspired by Methodologies to Guide Ethical Research and Design Committee, and supported by IEEE Computer Society standards.ieee.org/project/html IEEE P7001 - IEEE Standards Project for Transparency of Autonomous Systems Inspired by the General Principles Committee, and supported by IEEE Vehicular Technology Society standards.ieee.org/project/html IEEE P7002 - IEEE Standards Project for Data Privacy Process Inspired by The Personal Data and Individual Agency Control Committee, and supported by IEEE Computer Society standards.ieee.org/project/html IEEE P7003 - IEEE Standards Project for Algorithmic Bias Considerations Supported by IEEE Computer Society standards.ieee.org/project/html IEEE P7004 - IEEE Standards Project for Child and Student Data Governance Inspired by The Personal Data and Individual Agency Control Committee, and supported by IEEE Computer Society standards.ieee.org/project/html 286 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems About Ethically Aligned Design IEEE P7005 - IEEE Standards Project for Employer Data Governance Inspired by The Personal Data and Individual Agency Control Committee, and supported by IEEE Computer Society standards.ieee.org/project/html IEEE P7006 - IEEE Standards Project for Personal Data AI Agent Working Group Inspired by The Personal Data and Individual Agency Control Committee, and supported by IEEE Computer Society standards.ieee.org/project/html IEEE P7007 - IEEE Standards Project for Ontological Standard for Ethically Driven Robotics and Automation Systems Supported by IEEE Robotics and Automation Society standards.ieee.org/project/html IEEE P7008 - IEEE Standards Project for Ethically Driven Nudging for Robotic, Intelligent and Autonomous Systems Inspired by the Affective Computing Committee, and supported by IEEE Robotics and Automation Society standards.ieee.org/project/html IEEE P7009 - IEEE Standards Project for Fail-Safe Design of Autonomous and Semi-Autonomous Systems Supported by IEEE Reliability Society standards.ieee.org/project/html IEEE P7010 - IEEE Standards Project for Well-being Metric for Autonomous and Intelligent Systems Inspired by the Well-being Committee, and supported by IEEE Systems, Man and Cybernetics Society standards.ieee.org/project/html IEEE P7011 - IEEE Standards Project for the Process of Identifying and Rating the Trustworthiness of News Sources Supported by IEEE Society on Social Implications of Technology standards.ieee.org/project/html IEEE P7012 - IEEE Standards Project for Machine Readable Personal Privacy Terms Supported by IEEE Society on Social Implications of Technology standards.ieee.org/project/html IEEE P7013 - IEEE Standards Project for Inclusion and Application Standards for Automated Facial Analysis Technology Supported by IEEE Society on Social Implications of Technology standards.ieee.org/project/html 287 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems About Ethically Aligned DesignWho We Are About IEEE IEEE is the largest technical professional organization dedicated to advancing technology for the benefit of humanity, with over 420,000 members in more than 160 countries. Through its highly cited publications, conferences, technology standards, and professional and educational activities, IEEE is the trusted voice in a wide variety of areas ranging from aerospace systems, computers, and telecommunications to biomedical engineering, electric power, and consumer electronics. To learn more, visit the IEEE website: About the IEEE Standards Association The IEEE Standards Association (IEEE-SA), a globally recognized standards-setting body within IEEE, develops consensus standards through an open process that engages industry and brings together a broad stakeholder community. IEEE standards set specifications and best practices based on current scientific and technological knowledge. The IEEE-SA has a portfolio of over 1,900 active standards and over 650 standards under development. For more information, visit the IEEE-SA website: standards.ieee.org About The IEEE Global Initiative The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems (The IEEE Global Initiative) is a program of the IEEE Standards Association with the status of an Operating Unit of The Institute of Electrical and Electronics Engineers, Incorporated (IEEE), the world s largest technical professional organization dedicated to advancing technology for the benefit of humanity with over 420,000 members in more than 160 countries. To learn more, visit The IEEE Global Initiative website: standards.ieee.org/industry-connections/ec/autonomous-systems.html The IEEE Global Initiative provides the opportunity to bring together multiple voices in the related technological and scientific communities to identify and find consensus on timely issues. Names of experts involved in the various committees of The IEEE Global Initiative can be found at: standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ec_bios.pdf IEEE makes all versions of Ethically Aligned Design available under the Creative Commons Attribution-Non-Commercial 0 United States License. Subject to the terms of that license, organizations or individuals can adopt aspects of this work at their discretion at any time. It is also expected that Ethically Aligned Design content and subject matter will be selected for submission into formal IEEE processes, including standards development and education purposes. The IEEE Global Initiative and Ethically Aligned Design contribute, together with other efforts within IEEE, such as IEEE TechEthics , (techethics.ieee.org ), to a broader effort at IEEE to foster open, broad, and inclusive conversation about ethics in technology.288 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems About Ethically Aligned DesignOur Process To ensure the greatest cultural relevance and intellectual rigor possible in our work, The IEEE Global Initiative has sought for and received global feedback for versions 1 and 2 (after hundreds of experts created first drafts) to inform this Ethically Aligned Design , First Edition (EAD1e). We released Ethically Aligned Design, Version 1 (EADv1) as a Request for Input in December of 2016 and received over two hundred pages of in-depth feedback about the draft. We subsequently released Ethically Aligned Design, Version 2 (EADv2) in December 2017 and received over three hundred pages of in-depth feedback about the draft. This feedback included further insights about the eight original sections from EADv1, along with unique/new input for the five new sections included in EADv2. Both versions included candidate recommendations instead of direct recommendations , because our communities had been engaged in debate and weighing various options. This process was taken to the next level with Ethically Aligned Design, First Edition (EAD1e), using EADv1 and EADv2 as its initial foundation. Although we expect future editions of Ethically Aligned Design , a vetting process has taken place within the global community that gave rise to this seminal work. Therefore, we can now speak of recommendations without any further restriction, and EAD1e also includes a set of policy recommendations. This process included matters of internal consistency across the various chapters of EAD1e and also more specific or broader criteria, such as maturity of the specific chapters and consistency with respect to policy statements of IEEE. The review also considered the need for IEEE to maintain a neutral and thus credible position in areas and processes where it is likely that IEEE may become active in the future. Beyond these formal procedures, the Board of Governors of IEEE Standards Association has endorsed the work of the IEEE Global Initiative and offers it for consideration by governments, businesses, and the public at large with the following resolution: Whereas the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems is an authorized activity within the IEEE Standards Association Industry Connections program created with the stated mission: To ensure every stakeholder involved in the design and development of autonomous and intelligent systems is educated, trained, and empowered to prioritize ethical considerations so that these technologies are advanced for the benefit of humanity; Whereas versions 1 and 2 of Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems (A/IS) were developed as calls for comment and candidate recommendations by several hundred professionals including engineers, scientists, 289 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems About Ethically Aligned Designethicists, sociologists, economists, and many others from six continents; Whereas the recommendations contained in Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems (A/IS), First Edition are the result of the consideration of hundreds of comments submitted by professionals and the public at large on versions 1 and 2; Whereas through an extensive, global, and open collaborative process, more than a thousand experts of The IEEE Global Initiative have developed and are in the process of final editing and publishing Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems (A/IS), First Edition ; now, therefore, be it Resolved, that the IEEE Standards Association Board of Governors: e xpresses its appreciation to the leadership and members of the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems for the creation of Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems (A/IS) , First Edition; and s upports and commends the collaborative process used by The IEEE Global Initiative to achieve extraordinary consensus in such complex and vast matters in less than three years; and3. e ndorses and offers Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems (A/IS) , First Edition to businesses, governments and the public at large for consideration and guidance in the ethical development of autonomous and intelligent systems. Terminology Update For Ethically Aligned Design , we prefer not to use as far as possible the vague term AI and use instead the term, autonomous and intelligent systems (or A/IS). Even so, it is inherently difficult to define intelligence and autonomy . One could, however, limit the scope for practical purposes to computational systems using algorithms and data to address complex problems and situations, including the capability of improving their performance based on evaluating previous decisions, and say that such systems could be considered as intelligent . Such systems could be regarded also as autonomous in a given domain as long as they are capable of accomplishing their tasks despite environment changes within the given domain. This terminology is applied throughout Ethically Aligned Design , First Edition to ensure the broadest possible application of ethical considerations in the design of the addressed technologies and systems. 290 This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems About Ethically Aligned DesignHow the Document Was Prepared This document was developed by The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems, which is an authorized Industry Connections activity within the IEEE Standards Association, a Major Organizational Unit of IEEE. It was prepared using an open, collaborative, and consensus building approach, following the processes of the Industry Connections framework program of the IEEE Standards Association ( standards.ieee. org/industry-connections ). This process does not necessarily incorporate all comments or reflect the views of every contributor listed in the Acknowledgements above or after each chapter of this work. The views and opinions expressed in this collaborative work are those of the authors and do not necessarily reflect the official policy or position of their respective institutions or of the Institute of Electrical and Electronics Engineers (IEEE). This work is published under the auspices of The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems for the purposes of furthering public understanding of the importance of addressing ethical considerations in the design of autonomous and intelligent systems. In no event shall IEEE or IEEE-SA Industry Connections Activity Members be liable for any errors, omissions or damage, direct or otherwise, however caused, arising in any way out of the use of or application of any recommendation contained in this publication. The Board of Governors of the IEEE Standards Association, its highest governing body, commends the consensus-building process used in developing Ethically Aligned Design , First Edition, and offers the work for consideration and guidance in the ethical development of autonomous and intelligent systems. How to Cite Ethically Aligned Design Please cite Ethically Aligned Design, First Edition in the following manner: The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems , First Edition. IEEE, autonomous-systems.htmlThe IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Key ReferencesThis work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Key References Key reference documents listed in Ethically Aligned Design , First Edition: Appendix 1 - The State of Well-being Metrics (An Introduction) bit.ly/ead1e-appendix1 (Referenced in Well-being Section) Appendix 2 - The Happiness Screening Tool for Business Product Decisions bit.ly/ead1e-appendix2 (Referenced in Well-being Section) Appendix 3 - Additional Resources: Standards Development Models and Frameworks bit.ly/ead1e-appendix3 (Referenced in Well-being Section) Glossary bit.ly/ead1e-glossary 291 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems ( The IEEE Global Initiative ) is a program of The Institute of Electrical and Electronics Engineers, Incorporated ( IEEE ), the world s largest technical professional organization dedicated to advancing technology for the benefit of humanity with over 420,000 members in more than 160 countries. The IEEE Global Initiative and Ethically Aligned Design contribute to broader efforts at IEEE about ethics in technology. 201 9 IEEE 082-03-19",en,"Autonomous and intelligent systems should prioritize and have as their goal the explicit honoring of our inalienable fundamental rights and dignity as well as the increase of human flourishing and environmental sustainability. We understand ethical to go beyond moral constructs and include social fairness, environmental sustainability, and our desire for self-determination. They can significantly alter institutions and institutional relationships toward more human-centric structures, and they can address humanitarian and sustainable development issues resulting in increased individual societal and environmental well-being. Psychological, social, economic fairness, and environmental factors matter. They can be legal, profitable, and safe in their usage, yet not positively contribute to human and environmental well-being. O ECD Guidelines on Measuring Subjective Well-being , O ECD Better Life Index , W orld Happiness Reports , 2012 Uni ted Nations Sustainable Development Goal (SDG) Indicators , B eyond GDP , European Commission, From the site: The Beyond GDP initiative is about developing indicators that are as clear and appealing as GDP, but more inclusive of environmental and social aspects of progress. Greenberg, Hackers Fool Tesla S s _Autopilot to Hide and Spoof Obstacles , Wired, August C . R. Brooks Professor, Holcombe Department of Electrical and Computer Engineering, Clemson University N icolas Economou Chief Executive Officer, H5; Chair, Science, Law and Society Initiative at The Future Society Chair, Law Committee, Global Governance of AI Roundtable; Member, Council on Extended Intelligence (CXI) H ugo Giordano Engineering Student at Texas A&M University A lexei Grinbaum Researcher at CEA (French Alternative Energies and Atomic Energy Commission) and Member of the French Commission on the Ethics of Digital Sciences and Technologies CERNA J ia He Independent Researcher, Graduate Delft University of Technology in Engineering and Public Policy, project member within United Nations, ICANN, and ITU Executive Director of Toutiao Research (Think Tank), Bytedance Inc. B ruce Hedin Principal Scientist, H5 C yrus Hodes Advisor AI Office, UAE Prime Minister s Office, Co-founder and Senior Advisor, AI Initiatives@The Future Society; Member, AI Expert Group at the OECD, Member, Global Council on Extended Intelligence; Co-founder and Senior Advisor, The AI Initiative @ The Future Society N athan F. Hutchins Applied Assistant Professor, Department of Electrical and Computer Engineering, The University of Tulsa N arayana GPL. L ondon: Longman, Roberts & Green, P . Classical Ethics in A/ISQuoting Rene Von Schomberg, science and technologies studies specialist and philosopher, Responsible Research and Innovation is a transparent, interactive process by which societal actors and innovators become mutually responsive to each other with a view to the (ethical) acceptability, sustainability and societal desirability of the innovation process and its marketable products (in order to allow a proper embedding of scientific and technological advances in our society). C. Stahl, M. Obach, E. Yaghmaei, V. Ikonen, K. Chatfield, and A. Brem, The Responsible Research and Innovation (RRI) Maturity Model: Linking Theory and Practice , Sustainability, vol. Finding the answer to this question is further complicated when A/IS are within a holistic and interconnected framework of well-being in which individual well-being is inseparable from societal, economic, and environmental systems. A/IS technologies affect human agency, identity, emotion, and ecological systems in new and profound ways. While important, these metrics fail to encompass the full spectrum of well-being impacts on individuals and society, such as psychological, social, and environmental factors. For instance, while it is commonly recognized that autonomous vehicles will save lives when safely deployed, a topic of less frequent discussion is how self-69 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.driving cars also have the potential to help the environment by reducing greenhouse gas emissions and increasing green space . We believe that A/IS creators can profoundly increase human and environmental flourishing by prioritizing well-being metrics as an outcome in all A/IS system designs now and for the future. This section defines well-being, discusses the value of well-being metrics to A/IS creators, and notes how similar frameworks like sustainability and human rights can be complemented by incorporating well-being metrics. The conception of well-being encompasses the full spectrum of personal, social, and environmental factors that enhance human life and on which human life depend. There is now a thriving area of scientific research into the psychological, social, behavioral, economic, and environmental determinants of human well-being. For example: economists identifying economic welfare with levels of consumption and economic vitality, psychologists highlighting subjective experience, and sociologists emphasizing living, labor, political, social, and environmental conditions. While recognizing a scope for differences across well-being indicators, we note that the richest conception of well-being encompasses the full spectrum of personal, social, and environmental goods that enhance human life. Among the most important and recognized aspects of well-being are (in alphabetical order): C ommunity: Belonging, Crime & Safety, Discrimination & Inclusion, Participation, Social Support C ulture: Identity, Values E conomy: Economic Policy, Equality & Environment, Innovation, Jobs, Sustainable Natural Resources & Consumption & Production, Standard of Living E ducation: Formal Education, Lifelong Learning, Teacher Training E nvironment: Air, Biodiversity, Climate Change, Soil, Water G overnment: Confidence, Engagement, Human Rights, Institutions Hu man Settlements: Energy, Food, Housing, Information & Communication Technology, Transportation P hysical Health: Health Status, Risk Factors, Service Coverage P sychological Health: Affect (feelings), Flourishing, Mental Illness & Health, Satisfaction with Life W ork: Governance, Time Balance, Workplace Environment72 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.In an effort to provide a basic orientation to well-being metrics, information about well-being indicators can be segmented into four categories: S ubjective or survey-based indicators Survey-based well-being indicators, subjective well-being (SWB) indicators, and multidimensional measurements of aspects of well-being, are being used by national institutions, international institutions, and governments to better understand levels of psychological well-being within countries and aspects of a country s population. These indicators have been used to understand conditions that support the well-being of countries and populations, and to measure the societal and environmental impact of companies. As long as organizations exist in a larger societal system which prioritizes financial success, these companies will remain under pressure to deliver financial results that do not fully incorporate societal and environmental impacts, measurements, or priorities. PricewaterhouseCoopers defines total impact as a holistic view of social, environmental, fiscal and economic dimensions the big picture . The B-Corporation movement has created a new legal status for a new type of company that uses the power of business to solve social and environmental problems . Specifically: A /IS creators should work directly with experts, researchers, and practitioners in well- being concepts and metrics to identify existing metrics and combinations of indicators that would bring support a triple bottom line , i.e., accounting for economic, social, and environmental impacts, approach to well-being. A range of studies, from the historic to more recent , prove that ecological collapse endangers human existence. Moreover, biodiversity and ecological integrity have intrinsic merit beyond simply their instrumental value to humans. Technology has a long history of contributing to ecological degradation through its role in expanding the scale of resource extraction and environmental pollution, for example, the immense power needs of network computing, which leads to climate change , water scarcity , soil degradation , species extinction , deforestation , 76 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.biodiversity loss , and destruction of ecosystems which in turn threatens humankind in the long run. At the same time, there are many examples, such as photovoltaics and smart grid technology that present potential ways to restore earth s ecosystems if undertaken within a systems approach aimed at sustainable economic and environmental development. Environmental justice research demonstrates that the negative environmental impacts of technology are commonly concentrated on the middle class and working poor, as well as those suffering from abject poverty, fleeing disaster zones, or otherwise lacking the resources to meet their needs. Ecological impact can thus exacerbate the economic and sociological effects of wealth disparities on human well-being by concentrating environmental injustice onto those who are less well off. In these respects, A/IS are no exception; they can be used in ways that either help or harm the ecological integrity of the planet. It may be fair to say that ecological health and human well-being will, increasingly, depend upon A/IS creators. It is imperative that A/IS creators and stakeholders find ways to use A/IS to do no harm and to reduce the environmental degradation associated with economic growth while simultaneously identifying applications to restore the ecological health of the planet and thereby safeguarding the well-being of humans. For A/IS to reduce environmental degradation and promote well-being, it is required that not only A/IS creators act along such lines, but also that a systems approach is taken by all A/IS stakeholders to find solutions that safeguard human well-being with the understanding that human well-being is inextricable from healthy social, economic, and environmental systems. Recommendations A/IS creators need to recognize and prioritize the stewardship of the Earth s natural systems to promote human and ecological well-being. Specifically: Hu man well-being should be defined to encompass ecological health, access to nature, safe climate and natural environments, biosystem diversity, and other aspects of a healthy, sustainable natural environment. A /IS systems should be designed to use, support, and strengthen existing ecological sustainability standards with a certification or similar system, e.g., LEED , Energy Star , or Forest Stewardship Council . This directs automation and machine intelligence to follow the principle of doing no harm and to safeguard environmental, social, and economic systems. A c ommittee should be convened to issue findings on ways in which A/IS can be used by business, NGOs, and governmental agencies to promote stewardship and restoration of natural systems while reducing the harmful impact of economic development on ecological sustainability and environmental justice.77 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Well-being This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License.Further Resources D . Cutting Through Environmental Issues: Technology as a double-edged sword . Call for New Technologies to Avoid Ecological Destruction . To date, a number of successful concepts and models exist in the fields of sustainability, economics, industrial design and manufacturing, architecture and urban development, and governmental policy. (See page 191, Additional Resources: Additional Resources: Standards Development Models and Frameworks ) T he development of a well-being metrics standard for A/IS that encompasses an understanding of well-being as holistic and interlinked to social, economic, and ecological systems. Greene, et al. A potential entry point for exploring these unintended consequences is computational sustainability. Computational-Sustainability.org defines the term as an interdisciplinary field that aims to apply techniques from computer science, information science, operations research, applied mathematics, and statistics for balancing environmental, economic, and societal needs for sustainable development . The Institute of Computational Sustainability states that the intent of computational sustainability is provide computational models for a sustainable environment, economy, and society . Examples of applied computational sustainability can be seen in the Stanford University Engineering Department s course in computational sustainability presentation . Computational sustainability technologies designed to increase social good could also be tied to existing well-being metrics. A /IS creators working toward computational sustainability should integrate well-being concepts, scientific findings, and indicators into current computational sustainability models. C ross-pollination should be developed between computational sustainability and well-being professionals to ensure integration of well-being into computational sustainability frameworks, and vice versa. Further Resources for Computational Sustainability S tanford Engineering Department, Topics in Computational Sustainability Course Presentation , C omputational Sustainability, Computational Sustainability: Computational Methods for a Sustainable Environment, Economy, and Society Project Summary. P. Gomes, Computational Sustainability: Computational Methods for a Sustainable Environment, Economy, and Society in The Bridge: Linking Engineering and Society . A/IS for Sustainable Development Section 1 A/IS in Service to Sustainable Development for All A/IS have the potential to contribute to the resolution of some of the world s most pressing problems, including: violation of fundamental rights, poverty, exploitation, climate change, lack of high-quality services to excluded populations, increased violence, and the achievement of the SDGs. A/IS for Sustainable Development new sources of sustainable energy harnessed to power A/IS in the future, there is a risk that it will increase fossil fuel use and have a negative impact on the environment and the climate. Specific objectives to consider include: I dentifying and experimenting with A/IS technologies relevant to the SDGs, such as: big data for development relevant to, for example, agriculture and medical tele-diagnosis; geographic information systems needed in public service planning, disaster prevention, emergency planning, and disease monitoring; control systems used in, for example, naturalizing intelligent cities through energy and traffic control and management of urban agriculture; applications that promote human empathy focused on diminishing violence and exclusion and increasing well-being. R esearching sustainable energy to power A/IS computational capacity. 19 An offsetting factor is the reality that many LMIC lack the communication, energy, and IT infrastructure required to support highly automated industries. Background Multiple international institutions, in particular educational engineering organizations,27 have called on universities to play an active role, both locally and globally, in the resolution of the enormous problems that the world faces in securing peace, prosperity, planet protection, and universal human dignity: armed conflict, social injustice, rapid climate change, abuse of human rights, etc. A/IS for Sustainable Development A/IS, and adapting university curricula to provide a broad, integrated perspective which allows students to understand the impact of A/IS in the global, economic, environmental, and sociocultural domains and trains them as future policy makers in A/IS fields. 3 I bid, paragraph 4 A /IS has the potential to advance positive change toward all seventeen 2030 Sustainable Development Goals, which are: Goal End poverty in all its forms everywhereGoal End hunger, achieve food security and improved nutrition and promote sustainable agriculture Goal Ensure healthy lives and promote well-being for all at all ages Goal Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all Goal Achieve gender equality and empower all women and girls Goal Ensure availability and sustainable management of water and sanitation for all Goal Ensure access to affordable, reliable, sustainable and modern energy for all Goal Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all Goal Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation Goal Reduce inequality within and among countries Goal Make cities and human settlements inclusive, safe, resilient and sustainable Goal Ensure sustainable consumption and production patterns Goal Take urgent action to combat climate change and its impactsEndnotes166 The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems This work is licensed under a Creative Commons Attribution-NonCommercial 0 United States License. Allowing a system to learn new norms without public or expert review has detrimental consequences (Green and Hu 2018 . Green and L. Hu. Embedding Values into Autonomous and Intelligent Systems E bru Dogan Research Engineer, VEDECOM T akashi Egawa Cloud Infrastructure Laboratory, NEC Corporation, Tokyo V anessa Evers Professor, Human-Machine Interaction, and Science Director, DesignLab, University of Twente M ichael Fisher Professor of Computer Science, University of Liverpool, and Director of the UK Network on the Verification and Validation of Autonomous Systems, vavas.org K en Fleischmann Associate Professor in the School of Information at The University of Texas at Austin Ed ith Pulido Herrera Bioengineering group, Antonio Nari o University, Bogot , Colombia R yan Integlia assistant professor, Electrical and Computer Engineering, Florida Polytechnic University; Co-Founder of the em[POWER] Energy Group C atholijn Jonker Full professor of Interactive Intelligence at the Faculty of Electrical Engineering, Mathematics and Computer Science of the Delft University of Technology. Green and L. Hu. Many industries in particular, system industries (automotive, air and space, defense, energy, medical systems, manufacturing) will be changed by the growing use of A/IS. 4 T his includes consideration regarding application of the precautionary principle, as used in environmental and health policy-making, where the possibility of widespread harm is high and extensive scientific knowledge or understanding on the matter is lacking.5 Hu man rights based approaches have been applied to development, education, and reproductive health. 16 Wealth, in turn, can enable policies that support improved education, health, environmental protection, equal opportunity, and, in democratic societies, greater individual freedom. The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Law C ordel Green Attorney-at-Law; Executive Director, Broadcasting Commission Jamaica M aura R. Grossman Research Professor, David R. Cheriton School of Computer Science, University of Waterloo; Adjunct Professor, Osgoode Hall Law School, York University B ruce Hedin Principal Scientist, H5 Da niel Hinkle Senior State Affairs Counsel for the American Association for Justice De rek Jinks Marrs McLean Professor in Law, University of Texas Law School; Director, Consortium on Law and Ethics of Artificial Intelligence and Robotics (CLEAR), Robert S. Strauss Center for International Security and Law, University of Texas.",risk
F-Secure Corporation (Finland),F551032,10 September 2020,Company/business,Large (250 or more),Finland,"F-Secure is very happy to see the discussion on ethical and legal requirements of AI progressing, as this is an important topic and a core part of the EU position towards AI. One fundamental aspect we fully agree with is having EU level approach instead of country-specific ones. We are happy to participate in the discussions and provide our expertise in preparing next steps and possible policy and legislative initiatives. Even though AI has gotten a lot of public attention lately, the technologies themselves are not that new AI and machine learning have been around for decades, and also the commercial use of AI has been active for years. It is also important to remember that current AI is not general AI but still even if very impressive very narrow. Current AI can be used for multiple purposes but still fundamentally does what it is set to do as opposed to having intent of its own. We should consider AI together with other software, automation and data processing technologies as it is a very similar technological enabler. As such, a very important consideration is whether AI specific regulation is an approach that should be taken and if so, to what extent. As stated, there are several existing frameworks that (partially) apply, and we support augmenting those, also because finding a holistic definition of AI that is neither too narrow or too broad is very difficult. We would however challenge the logic that regulation consistently increases trust and with increased obligations benefits also increase. Regulation increases cost, but the link to increased benefits is not as obvious and may not realize to even near the expected extent in fact regulation may at worst be mostly a cost for development, and if applicable to EU players primarily, it may severely hamper European competitiveness a risk that must not be taken lightly. AI technologies already provide significant benefits in multiple industries without excessive regulation. There are cases where regulation can be beneficial, e.g. in medical applications, but it should not be generalized too widely also a reason for avoiding too broad AI specific regulation. A note on the energy impacts; even though the most prominent methods today are based on complex deep learning networks and are computationally very expensive, that is not the case for all AI. Actually, more efficient AI is one area where we see research focus should be directed to as that could make a huge impact also for the future development of AI. Some high-risk applications may need strict regulation, but there is a significant challenge in defining those. The application depends on the domain, but even more on the use case and the data - making the right level of granularity challenging. E.g. there are HR related AI applications which can be high risk for discrimination but a chatbot assistant that answers questions on open positions would hardly be one. If employed, great care must be taken and a very accurate and detailed description of high-risk applications is needed. That said, a very detailed description leaves the opportunity to circumvent the regulation and limit its effectiveness a difficult problem to solve. To encourage wide development and adaption of AI, as well as the growth in SMEs, as high regulatory requirements will favor large organizations with capability to address them, we recommend to have as little AI specific regulation as possible and even if such is needed, voluntary labeling and soft law would be preferred. The use of data and AI is the future and will bring immense value to those who embrace it. We do need to protect individuals and human rights, but as we live in a global economy we should take extra care to allow European development to be in the forefront and avoid hindering advances and new solutions unnecessarily. It is a balance that is not easy to strike, but getting it right is absolutely vital for our future.",en,"A note on the energy impacts; even though the most prominent methods today are based on complex deep learning networks and are computationally very expensive, that is not the case for all AI.",risk
Eurocities (Belgium),F551016,10 September 2020,Public authority,Medium (50 to 249 employees),Belgium,"April 2020 | | EUROCITIES statement on AI 1 Artificial intelligence and cities The Covid -19 pandemic has disrupted our societies beyond precedent. The global health crisis has enormous economic and social repercussions. It has also highlighted the urgency of securing a people -centred digital transformation in Europe, which is essential to ensure social and economic resilience in our cities. European digital strategies must fully support the mainstreaming of digitalisation as part of rights -based public service delivery. We recognise the crucial role of AI technologies to respond to global public health emergencies. At the same time, human rights and European fundamental values must be fully protected. The next EU regulatory framework on trustworthy AI must specifically addr ess the use of AI in public health emergency prediction and management making sure AI is used without undermining ethics and fundamental rights. More generally, a rtificial intelligence is an enabler of change for local governments. AI is already transfor ming the governance of the city and society . Public administration s can increase Response to EU s white paper on AI AI needs data. A huge amount of data produced and collected in cities, crucial for local governments to improve public services and policies, is currently gathered and owned by the private sector. A single market for data needs to benefit all ecosystem pla yers, including local governments. We call for legislative action to ensure access and use of business to government (B2G) data sharing in the proposed Data Act planned for Liability and accountability in AI are key to guaranteeing people s safety. A I safety and reliability tests are burdensome; local public authorities do not have the expertise or the budget to carry them out. A central EU body or agency should develop the necessary verification and validation procedures, and guarantee security and p ublic safety. Some AI uses can be considered high -risk. More detailed and clear descriptions of the possible high -risk uses are necessary to clearly understand when a local public administration is affected. We propose establishing a task force composed by AI experts and local public administration experts to better define the different possible high -risk uses. AI could have a disruptive effect on the future of the job market and skills development in cities. ESF+ and the Youth Guarantee funding should focus more on digital literacy training, skills development and gender equality. Local governments are crucial t o fostering an ecosystem of excellence and trust in AI in Europe. The EU must work with local governments in the development of the future regulatory framework for a trustworthy AI, taking into account the principles defined by the Cities for Digital Right s coalition. People -centred Artificial Intelligence ( AI) in cities Response to EU s white paper on AI March 20 20 April 2020 | | EUROCITIES statement on AI 2 efficiency and productivity while reducing costs through automation of processes and tasks . City governments can use algorithms and machine learning applications to predict ser vice demand and anticipate urban problems , improv e decision making and the delivery of more and innovative public services . As AI is becoming more advanced and more accessible, city authorities are increasingly experimenting and piloting A I, in many cases in combination with other tools such as IoT , 5G or Big Data technology , leveraging their potential while understanding new patterns and trends . As with the first smart city projects1, cities use the results and lessons from AI experimentati on to develop action plans and strategies, often in collaboration with national governments and with the support of local and regional stakeholders. However, AI adoption in cities is a long and costly process . Good data collection, processing, management and opening is expensive and requires specific, high -level competences as well as new governance approaches. A broader uptake of AI technology locally requires intensive financial investment from the national and EU level to ensure adequate technological and skills development in city administrations . Opportunities and c hallenges for cities AI is a powerful means to fully transform Europe s cities into sustainable, inclusive and smart places for people to liv e2. AI development also helps cities to reach the sustainability goals of the Green Deal3 especially in sectors such as air quality and mobility. Local authorities use AI to leverage IoT applications and to predict the level of pollution in cities. City governments can get valuable information by analysing social media data on tourists behaviour and to adapt cultural policies and investments to needs. Cities use machine learning to predict parking space availability to then efficiently redirect drivers into a free area. Through AI chatbots, governments can communicate faster with citizens and stakeholders increasing their sense of participation . While AI adoption is increasing in cities, challenges connected to it are too. Disruption in the labour market and skills gap, safety and liability , and digital right s protection are the most important one s for cities. Disruption in the labour market and skills gap The rapid and exponential technological progress in AI will have a disruptive effect on the job market. While it is still difficult to predict to which extend AI and robotics will affect unemployment rate, polarisation of jobs, income inequalities or discr imination in the labour market, the risks seem high. AI, powered by machine learning, automates tasks that might cause displacement or even replacement of workers, polarisation of job demand between high -skilled and low -skilled jobs , and worsen the status of already fragile groups of people, such as the digitally excluded, long -term unemployed and low -skilled people. There is a strong need in cities for skills development including long -life learning programmes, training for low -skilled people and early edu cation on digital skills - also oriented to engage more young women into technology4. Safety and liability AI applications can hide bias or amplify existing bias. Machine learning, powered by data, ha s no consciousness, reflect s the human prejudices and opinions of the developers , and repeat s and perpetuate s data which, if flawed or incorrect , can lead to misinterpretations and errors. When AI is used , for example, for urban mobility (e.g. autonomous cars) or environm ental (e.g. air p ollution or water security ) purposes , data bias might lead to high er risks for people s 1 Becoming cities of the future, lessons learned from experimenting smart cities, October 2016 , 2 EUROCITIES statement Smart cities in the age of the digital revolution , March 2019, 3 EUROCITIES statement on the Green Deal (to be finalised), March 2020, 4 EUROCITIES report on equal opportunities and access to the labour market, December 2018, April 2020 | | EUROCITIES statement on AI 3 safety5. AI systems and algorithms must undergo rigorous testing to check their reliability and safety , meaning time, the right expertise and costs. Liability issues a re also a major concern for cities. If a driverless bus runs over a pedestrian, who is responsible? There are many parties involved in an AI system (data provider, designer, manufacturer, programmer, developer, user and AI system itself), liability is difficult to establish when something goes wrong and where there are many factors to be taken into consid eration. Digital rights Fundamental human rights might be put at risk in an AI age. AI systems can impair freedom of expression, privacy and data protection, equality and fairness. Local governments are actively committed to protect ing, promot ing and mon itoring citizens human rights in the digital sphere. Through the Cities for Digital Rights6 coalition , supported by EUROCITIES , over 50 cities all over the world are working to provide trustworthy and secure digital services and infrastructure s for the common good . From apps that gamify participation in local consultation s to vide o-based solution s that promote community interaction , cities are develop ing services to decrease inequalities, discrimination and help reach traditionall y excluded communities. Local governments are implementing mobility plans and actions in cooperation with local stakeholders to secure people s privacy while obtaining crucial real -time data visualisation through traffic cameras. Guiding AI principles for Europe Despite the differences in the level of AI experiments and deployment across Europe, local governments share the same values and principles when it comes to using AI. People -focused AI : People are at the centre of AI deployment in cities. AI shoul d be used to facilitate access and deliver better services to citizens - not to track, control or direct people s behaviour. AI systems should serve people , and solutions should be based on EU societal and ethical values. Collaborative intelligence for successful AI deployment in cities : AI must complement and augment human capabilities , not replace them. Data is the engine of AI : The quantity, quality and t ransparency of used data is a key success factor for AI adoption. High quality annotated open data should be more available for use by all actors . Those using the data have the responsibility to ensure its integrity, authenticity, consistency and accuracy. A description of the data on which an algorithm is trained should be published. Safety and secur ity: AI must serve and protect people; systems should be accurate and perform reliably. Security and privacy should be integrated into systems from the design phase. Accountability and transparency : As the impact of AI on people s safety can be high, strong accountability and transparency measures and mechanisms should be ensured. Oversight measures covering responsibility and liability need to be put in place. Response to EU s white paper on AI Facilitate access and use of private data AI thrives on data. Full access, share and use of data by all ecosystem actors is a prerequisite for AI development and implementation. A growing amount of data is generated every day in cities 5 EUROCITIES statement on Integrating transport automation in the urban system, February 2019, 6 Cities for Digital Rights April 2020 | | EUROCITIES statement on AI 4 by people and machines, but its use and re -use is not fully exploited7. We welcome the Commission proposal to improve access to data establi shing common European data spaces that will facilitate data be ing used and shared between different players, also cross -border, as well as data quality, interoperability and standards within and across sectors. We strongly believe in strenghtening this eff ort through initiatives that focus on semantic interoperability by shared definitions, both technical and multilingual. Ensuring GDPR compliance, we will monitor the governance process and decisions on which data will be used and in which situations. We will also monitor the standardisation of data formats, protocols and registry that could help promote exchange of data and knowledge. However, a huge amount of data is currently gathered and owned by the private sector . The use and management of th at data is crucial for public authorities to improve public services and policies including urban planning, mobility and housing w hile ensuring democratic control over data and mitigating the negative societal impact of AI. Access and use of business to government (B2G) data need s specific regulation. We therefore call for action to be taken on B2G data sharing in the proposed Data A ct planned for Accountability and transparency measures AI systems and algorithms can hide bias which might lead to high risk s for people s safety . Safety and reliability test s of AI systems are burdensome; local public authorities do not have the expertise or the budget to carry them out. A central EU body or agency should develop the necessary verification and validation procedures , and guarantee security and public safety . Liability and accountability in AI are key to guarantee ing people s safety . Tangible measures must be applied to close the accountability gap such as: - full access to the algorithm code by the competent authorities whenever needed for inspection or verification purposes - obligations to report which algorithm s are used - a framework for algorithmic auditing that supports AI system development end -to-end - fostering an open source code philosophy Scope of a future EU regulatory framework We agree with the Commission s opinion on applying the high -risk approach and to use the cumulative criteria of the sector where AI applications are employed as well as the intended use of AI within each sector. More detailed and clear description of the possible uses that are considered high-risk is essential to clearly understand when a local public administration is affected by a specific use . Considering the multitude of possible uses of AI in several sectors o f responsibility for public authorities and the complexity of the classification exercise, we recommend a task force composed of experts on AI and public administration experts , also at local level, to better define the different possible high -risk uses . Funding for skills development Local governments need support in managing the digital transition and to tackle the possible disruptive effects of emerging technologies in the labour market. City governments are key players in supporting the creation of new jobs as well as the development of skills and knowledge; they are the right level to facilitate the dialogue between all actors involved, including national and regional governments, educational institutions and the private sector. Digital skills developm ent must be enhanced in city administrations , but local governments fac e strong competition from global technological companies in terms of attracting employees with the right digital skills . We welcome the Commission s strong focus on skills development a s part of its digital strategy including the update of the EU Skills Agenda, the dedicated funding in the 7 Data, People, Cities EUROCITIES citizen data principles in action, November 2019, April 2020 | | EUROCITIES statement on AI 5 proposed Digital Europe Programme to advanced digital skills and the update of the Digital Education Action Plan to increase education quality through AI. However, cities should also be able to access and use ESF+ and the Youth Guarantee funding t hat focus more on digital literacy training and skills development to tackle disruption in the labour market and skills gap or mismatch . This should specifically include women empowerment measures to combat gender imbalance in STEM . Building an ecosystem of excellence and trust together Local governments are crucial to foster ing an ecosystem of excellence and trust in AI in Europe. With increasing urban populations and access to talent and skills, universities, companies and infrastructure s, city authorities can facilitate research and innovation activities and processes , and create the right conditions to boost AI deployment, including by SMEs. Acting as open participation and collaboration platforms, using and making data and information available, city authorities enable crowd -creation, foster experimentation, share te chnological expertise and co-develop ideas and solutions. Local governments are key to build ing confidence and trust in AI. Through experimentation and early adoption of AI, city governments identify possible safety and fundamental rights risks , and propo se trustworthy solutions. As the level closest to citizens, local authorities engage with citizens, understand their fears and concerns and develop together possible solutions. The EU must work with local governments in the development of a future regulato ry framework for a trustworthy AI that takes into account the principles defined by the Cities for Digital Rights coalition8 and that supports cities to uptake and upscale AI in Europe . 8 Cities for Digital Rights hts.org/",en,"AI development also helps cities to reach the sustainability goals of the Green Deal3 especially in sectors such as air quality and mobility. Local authorities use AI to leverage IoT applications and to predict the level of pollution in cities. air p ollution or water security ) purposes , data bias might lead to high er risks for people s 1 Becoming cities of the future, lessons learned from experimenting smart cities, October 2016 , 2 EUROCITIES statement Smart cities in the age of the digital revolution , March 2019, 3 EUROCITIES statement on the Green Deal (to be finalised), March 2020, 4 EUROCITIES report on equal opportunities and access to the labour market, December 2018, April 2020 | | EUROCITIES statement on AI 3 safety5.",risk
European Banking Federation (Belgium),F551015,10 September 2020,Business association,Small (10 to 49 employees),Belgium,"European Banking Federation aisbl Brussels / Avenue des Arts 56, 1000 Brussels, Belgium / +32 2 508 3711 / info@ebf.eu Frankfurt / Wei frauenstra e 12 -16, 60311 Frankfurt, Germany EU Transparency Register / ID number: -23 10 September 2020 EBF_042415 EBF response to the European Commission Inception Impact Assessment on a Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence The European Banking Federation (EBF) welcomes the opportunity to respond to the European Commission s Consultation on its Inception Impact Assessment (IIA) on a Proposal for a legal act of the European Parliament and the Council laying down requirements f or Artificial Intelligence (AI). The responses to the different elements in the IIA complement the EBF s response to the European Commission s Consultation on the White Paper on Artificial Intelligence . Context and Problem Definition EBF members agree that AI can contribute to a wide array of economic and societal benefits across the entire spectrum of industries. For the European banking sector, AI provides significant opportunities ranging from enhancing customer experienc e, improving financial inclusion, cybersecurity, and consumer protection, to strengthening risk management and process efficiency . To realize these benefits, the EU regulatory environment needs to be fit for the use of AI and enable innov ation , while providing legal certainty and maintain ing a strong level of consumer protection. This is essential for AI development and uptake across industries, including in the financial sector. It is also crucial for the competitiveness of the EU globally, so mething which the context and problem definition section in the IIA does not mentio n. Objectives and Policy Options Before considering the different Policy Options proposed in the IIA, w e would like to emphasize the need to avoid any duplication of req uirements or conflicting requirements for areas that are already regulated in order to prevent additional burdens and legal uncertainties. For example, the General Data Protection Regulation (GDPR ) and Law Enforcement Directive already provide strong privacy and data protection regulation, which should not be duplicate d. On the policy options specifically, we are of the opinion that new, AI specific legislation is not required and believe it is key to take into account the consequences that any rules could have for the competitiveness of European companies globally . This is particularly 2 the case for prescriptive, wider regulation, as proposed in Option 3 (sub -option three) , which could hamper the ad option of AI in Europe by companies of all sizes. As a general principle, we believe that regulation should remain technology neutral . Legal requirements or ethical principles should not apply to the underlying technology but to its application. We would instead recommend a policy option centred on guidance developed by competent authorities on how to apply existing requirements to AI use cases , which could help firms to effectively apply their obligations under different regulatory r egimes . This should be a collaborative process, with competent authorities working with each other, with input from industry and civil society . In this view, Option 1 would be preferable. Any guidelines should take a technology neutral approach and not be overly prescriptive, as this would be in friction with the rapidly evolving nature of AI - related technologies . However, EBF members also have some concerns on Option 1 , particularly the lack of detail on what the proposed self-reporting /assessment on voluntary compliance would entail, and we would welcome clarification on them. On Option 2, an EU legislative instrument setting up a voluntary labelling scheme , EBF members have s ignificant concerns . A trustworthy label for non -high risk applications implies that any application that does not carry the label is deemed not trustworthy . By asserting this kind of social value, the label can no longer be considered truly voluntary. This runs counter to a risk -based framework as it would implicitly increase the requirements placed upon applications that are not high -risk, due to market discipline pressures. We would also disagree with the assertion in the IIA that volun tary labelling would likely entail limited costs as there are many elements to consider . Labelling schemes are difficult to set up, and are likely to complicate and prolong the development and implementation of AI systems (which are often scalable or self-learning building blocks, or encapsulated into larger systems in the form of internal or external components that are difficult to isolate) , which could therefore imply additional costs. The supply chain perspective on a labelling scheme is also missing from the IIA (and in the different policy options overall, see below) . Would organisations service consumers that are placed later on the chain have to inherit all the high risks resulted from service providers? Finally, the IIA states that the label would function as an indication to the market that the labelled AI application is trustworthy . There are three concerns with this: a) As the label would be voluntary, what would be the ability of the consumer to recognize whether, if a particular product or service does not have a label, the company did not apply for it or whether its absence means that the product/service does not comply with the requirements of the label scheme. b) Subscribing to a scheme that would o nly sh ow trustworthy AI, regardless of the other technologies that may be in use, could be misleading. The consumer needs to have access to trustworthy products and services as a whole, regardless of the technologies behind it. c) Subscribing to a labelling scheme of a trustworthy AI application is something 3 different that the trustworthy output of an AI application. A label on a trustworthy AI application itself does not need to cohere with the trustworthiness of the generated output. For the user of a service, this distinction might not be understandable. If the Commission proceeds with horizon tal AI regulation and takes option 3, as described in the AI White Paper, we would generally support the proposed approach to focus only on high risk AI applications (Sub -option b). In this case, the Commission should develop clear and objective criteria for high -risk applications ensuring that only those applications that could cause serious harm to citizens (e.g. by putting their lives at risk) are captured . These potential new requirements should be consistent ly applied only to use cases that pose the same level of high risk regardless of the type of provider, and not be extended to other cases with significantly lower risks. We believe that in the financial sector , current regulation (both sector specific regulation as well as cross -sectorial regulation such as the GDPR) and the inter nal processes it requires is already sufficient to guarantee consumer protection, risk management, financial stability and data protection, in all services prov ided to customer, including those applications that could include the use of AI. Best practices from the financial services sector could serve as inspiration for creating or adapting the legal framework to address AI -specific risks. Finally, several EBF m embers would like to point out that the Objectives and Policy Options section does not include the aspect of the responsibility of third party provider s (TPPs). When AI is provided by a TPP and is then integrated into a service/product provided by the final service provider, only the TPP will have the full knowledge of all the specific features of the AI at any particular time, not the final service provider . This makes the supply chain perspective an important one and we recommend that it is in cluded in the Commission s assessment of policy options . In addition, some providers and solutions have a predominant influence on the market ; a failure in these cases could cause wider dis ruption and needs to be considered. Definition of AI The IIA states that Another core question relates to the scope of the initiative, notably how AI should be defined (narrowly or broadly) (e.g. machine learning, deep neural networks, symbolic reasoning, expert systems, automated decision -making). EBF members agree with the European Commission that the question of how to define AI within any future initiative is a crucial one. We would like to stress that any definition of AI must be future -proof and avoid being overly broad in a way that could inadvertently include technologies that are not AI and do not pose the same risks . It should be clear, narrow, and specific , as well as future -proofed , for example by focusing on the technology s adaptive qualities. Using a definition such as t he one proposed by the AI High -Level Expert Group (HLEG), risks that any system, including general automation processes, is subject to AI specific rules. In terms of the risks, it is also important to keep in mind that it would not be fair to have a bad human -made -rule based system not treated as risky, but a well -validated data -driven ML -system as high - risk. 4 In addition, it seems essential, for a good understanding of the issues related to AI, to exclude from the scope of any possible , future Regulation any reference, even implicit, to the emergence of a ""strong artificial intelligence"" or ""artificial consciousness"". It follows that the AI to which we are referring can only be considered as a technical object , therefore not having in any way the character of an autonomous subject . Regarding the different policy options in the IIA, if Policy Option 3, sub -option b is pursued by the Commission, the definition should not bring into scope non -high risk applications. We welcome the opportunity to discuss in more detail, ahead of the issuance of potential legislation, a potential definition of AI that focuses on its key characteristics and is fit fo r regulatory purposes, i.e. easily understood, unambiguous and succinct. Governance On the proposal to create a framework for the cooperation of national competent authorities, we are concerned that this may not fully address the risk of fragmentation of supervisory and regulatory practices. If such a structure is created, we recommend that encouraging information sharing and cooperation on AI issues by different sectoral authorities is included as part of the mandate . This would hel p to avoid situations were standards for similar activities are regulated more rigorously in some sectors than in others. Especially important is the collaboration between Data Protection Authorities (DPAs) and se ctoral authorities in order to avoid overlap ping of contradictory practices. It would also be important to help authorities in this network increase their knowledge of AI, to better understand the technolog y and gain insight on how companies are using AI on specific applic ations. Preliminary assessment of expected outcomes In regard to likely environment impacts, we would encourage the Commission to continue exploring the environmental considerations with regards to AI. ENDS For more information: Liga Semane Policy Adviser Data & Innovation l.semane@ebf.eu About the EBF The European Banking Federation is the voice of the European banking sector, bringing together 32 national banking associations in Europe that together represent a significant majority of all banking assets in Europe, with 3,500 banks - large and small, wholes ale and retail, local and international while employing approximately two million people. EBF members represent banks that make available loans to the European economy in excess of 20 trillion and that reliably handle more than 400 million payment transactions per day. Launched in 1960, the EBF is committed to a single market for financial services in the European Union and to supporting policies that foster economic growth. @EBFeu",en,"Preliminary assessment of expected outcomes In regard to likely environment impacts, we would encourage the Commission to continue exploring the environmental considerations with regards to AI.",risk
Johann CAS (Austria),F551005,10 September 2020,EU citizen,,Austria,"Comments by Johann as on the European Commissions consultation on the inception impact assessment as part of the initiative Artificial intelligence ethical and legal requirements . Johann as is a senior researcher at the Institute of Technology Assessment of the Austrian Academy of Sciences. Among other things he is co -author of the study When algorithms decide in our place: the challenges of artificial intelligence in Switzerland ( - social -effects -of-artificial -intelligence/ ) (the main text is only available in German) and a team member of the H2020 PANELFIT project consortium ( ). All opinions and views expressed are personal. Welcom ing the opportunity of providing feedback on this legislative initiative I would like to provide the following comments and suggestions: A. Context, Problem definition and Subsidiarity Check The introduction appears to be based on overoptimistic perspective. Whereas the positive potentials certainly exist it should be also mentioned that they are also accompanied by corresponding risks, that their materialization depends on the creation of suitable framework conditions, which must also include a fair distribution of the possible benefits. A more realistic and sober assessment of opportunities and risks would be mo re helpful as a basis for deciding on regulatory options. To explain this recommendation in more detail, here an excerpt from a commentary I wrote during the consultations on the Ethics guidelines for trustworthy AI of the High -Level Expert Group on Artifi cial Intelligence (HLEG): Last but not least, the working document [draft Ethics guidelines for trustworthy AI ] appears to overestimate the actual and potentially positive contributions of AI to solve the grand challenges our world is facing, missing to p rovide evidence for this positive overall evaluation. Whereas large and important positive potentials can be envisaged, past and current use of AI does not appear to support this judgement. Taking increasing economic inequality as an example, AI has rather contributed to it e.g. in form of a key enabling technology of high -frequency trading on financial markets - but I m not aware of making serious attempts to use AI to resolve imbalances on labour markets. On the political level, AI is rather threatening civil liberties and democratic systems than empowering citizens. On a global level, AI is rather supporting the establishment of worldwide monopolies than empowering consumers. Data based businesses possess unprecedented economic capacities, unparalleled political influence, powers to shape the results of democratic votes, unique possibilities to influence or to manipulate individuals in the information they receive or decisions they take. By disproportionately stressing the potential positive impacts and neglecting the already materialised threats the working document in the current form might contribute to an inappropriate reliance on AI when tackling urgent problems of the EU and the world. By missing to mention these dangers and threats it also misses t o analyse them and consequently to develop measures and policies to counter them. This leads back to the first [next] critical comment: we are primarily not in need of a trustworthy technology but of making good use of opportunities offered by technology i n the human interest and keeping human agency. The term "" trustworthy AI"" should be avoided, although it is used in many key documents. This term suggests that, provided certain conditions are met, this technology can be used without further concern or hu man supervision. Thus, the use of this pair of terms contradicts one of the key requirements of "" trustworthy AI"", namely human agency and oversight. To explain this recommendation in more detail, here another excerpt from the commentary that I wrote durin g the consultations on the Ethics guidelines for trustworthy AI of the High -Level Expert Group on Artificial Intelligence (HLEG): The first critical comment is related to the selected title as such: there are only a few technologies imaginable that can be regarded as trustworthy on their own. In the case of AI, the naming of ethics for trustworthy AI is in appropriate and misleading for several reasons. Depending on the concrete AI technology in mind, the results produced by these technologies are at lea st prone to statistical errors, some also show completely unexplainable (and unpredictable) behaviour. The labelling as guidelines for trustworthy AI contains at least implicitly the message that trust in these technologies is in principle justified as lon g as the developed guidelines are respected, neglecting the fact that it is the use of technology only, which could deserve this marking. In the case of AI this comment, which might be regarded as a linguistic sophistry , is doubly important . AI is threatening human autonomy and agency, as acknowledged in the working document; neglecting this fact in the very title is additionally endangering human agency. At least a renaming in the form of ethics guidelines for trustworthy use of AI or something s imilar should be considered. Otherwise the guidelines could become self -contradicting to one of the core principles mentioned in the document. The statement that Just like for actions and decisions taken by humans, the use of AI to either directly take d ecisions or to support decision -making may lead to violations of fundamental rights, as guaranteed by and implemented in EU law. negates fundamental existing ethical concerns and legal restrictions on automated individual decision making in the European U nion. By including direct decisions by AI as a possibility, it pre -empts debates on this issue and the extent to which we should grant such powers to A I. B. Objectives and Policy options AI is a very potent technology, affecting almost all areas of life and the economy, which brings with it great opportunities but correspondingly high risks. Accordingly, option 4 should be chosen and designed, as none of the previous options adequately addresses the positive and negative potentials. Option 4 does not exclu de corresponding initiatives by industry or voluntary forms of labelling. However, such activities can by no means be sufficient, but must be embedded in an appropriate regulatory framework. The regulatory framework should be designed gradually according to the magn itude of risks, in analogy to the risk pyramid of the German Data Ethics Commission. AI applications that do not or only indirectly affect humans do not require specific AI regulation s. This includes, for example, AI applications for the analysis of large amounts of data in astronomy or physics, or applications in the field of industrial automation. However, the second example does require political measures, more on this below. For all other AI applications, appropriate precautions must be taken according to the risks involved. For example, recommendation systems or filter algorithms in social networks also need to meet transparency requirements. The restriction to high risk applications proposed in Option 3B is totally insufficient. First, it is not clear why two risk conditions have to be fulfilled simultaneously for AI applications to fall into this category of need of regulation . Second, such classification criteria hinder dynamic adaptations to new applications and associated risks. Moreover, high -risk applications should always be questioned in principle, taking into account not only the effects on individuals but also societal consequences (see below). In principle, a broad, technology -neutral definition of AI is preferable. C. Preliminary Assessment of Expected Impacts In the section on possible economic impacts, a comparison of compliance costs caused by regulation with the benefits of AI is addressed as one basis for decision making on legal obligations. The potential costs of non -compliance are, h owever, almost completely neglected. It is i n principle questionable whether calculations which compare the protection of fundamental rights with economic costs should be permissible in democratic societies. If such comparisons are made, however, at least the costs of non -compliance should be considered accordingly. This includes not only the economic costs, for example in the form of external effects or costs caused for the industry by acceptance problems, bu t also the costs for society, social coexistence and democracy. If the broad and deep impacts of AI applications are realistic, which are always emphasised in the case of positive effects, the y should also be taken into account in the case of possible nega tive effects. These include in particular effects on the social and political climate that have already occurred or are becoming apparent. The principle of precaution requires that preventive effects be assessed and controlled in order to avoid future pois oning of social coexistence or overheating of the global political climate in analogy to the environmental pollution or global warming The section on possible social impacts contains a statement in which two auxiliary verbs have been mixed up: while AI -enabled automation may replace some jobs, the use of AI will simultaneously lead to the creation of new jobs , correctly it should read like this: while AI -enabled automation will replace some jobs, the use of AI may simultaneously lead to the creation of new jobs , Automation will in any case replace jobs as a direct effect ; no company will invest in new technologies unless the investment in technology is expected to yield more than compensating savings in labour c osts. The extent to which these lost jobs can be partly or fully compensated by newly created jobs depends on a number of external factors that cannot be predicted. A trustworthy policy must ensure and guarantee that, whatever the actual development will b e, the benefits of automation are fairly distributed and that this does not translate into high and persistent unemployment. This demand goes far beyond the direct regulation of AI applications, but it is indispensable for a broad acceptance and therefore also the fullest possible use of the rationalisation potential of AI if the social coherence and political stability of the European Union is not to be jeopardised even more. D. Evidence Base, Data collection and Better Regulation Instruments As the resp onse to this initiative also shows, online consultation is not sufficient for a broad participation of citizens. In view of the great importance and the many changes that AI brings to citizens of the European Union, targeted citizen participation activitie s involving all Member States would be appropriate and should be considered .",en,"These include in particular effects on the social and political climate that have already occurred or are becoming apparent. The principle of precaution requires that preventive effects be assessed and controlled in order to avoid future pois oning of social coexistence or overheating of the global political climate in analogy to the environmental pollution or global warming The section on possible social impacts contains a statement in which two auxiliary verbs have been mixed up: while AI -enabled automation may replace some jobs, the use of AI will simultaneously lead to the creation of new jobs , correctly it should read like this: while AI -enabled automation will replace some jobs, the use of AI may simultaneously lead to the creation of new jobs , Automation will in any case replace jobs as a direct effect ; no company will invest in new technologies unless the investment in technology is expected to yield more than compensating savings in labour c osts.",risk
SIENNA Project (United Kingdom),F550986,10 September 2020,Other,Small (10 to 49 employees),United Kingdom,"The SIENNA project - Stakeholder-informed ethics for new technologies with high socio-economic and human rights impact - has received funding under the European Union s H2020 research and innovation programme under grant agreement No 741716 SIENNA submission to the European Commission: Feedback on the Inception Impact Assessment on the proposal for a legal act for artificial intelligence 10 September 2020 Introduction This document provides feedback on the European Commission s Inception Impact Assessment on the proposal for a legal act laying down a requirement for artificial intelligence, based on the findings and results of SIENNA, a European Horizon 2020-funded project (2017-.1 This submission reflects only the views of contributors2 that have prepared this input based on their research in the SIENNA project. SIENNA (Stakeholder-Informed Ethics for New technologies with high socio-ecoNomic and human rights impAct) is looking into ethical, legal and human rights issues and is developing ethical guidelines for human genomics, human enhancement and AI & robotics. It has received funding under the European Union s H2020 research and innovation programme under grant agreement No Recommendations on A. Context, Problem definition and Subsidiarity Check We welcome the language that puts fundamental rights and societal values first. In particular, we strongly support the statement that the ultimate objective is to foster the development and uptake of safe and lawful AI that respects fundamental rights across the Single Market by both private and public actors while ensuring inclusive societal outcomes. Unlike the European Commission White Paper on AI where the focus was on building consumers and businesses trust in AI to increase uptake of the technology, the Inception Impact Assessment rightly prioritises the protection and safety of individuals and society at large. While we recognize there are many beneficial applications of AI, any deployment must be accompanied by a necessity and proportionality assessment, as well as ensure adequate and sufficient measures are taken to protect the fundamental rights and societal values from potential harms. In developing this objective further, impacted human rights that must be considered include social, economic, and political rights (e.g., right of self-determination; to work; to the enjoyment of just and favourable conditions of work; to social security; to education; to the enjoyment of the highest attainable standard of physical and mental health; to vote; to equality before the law; to an effective remedy and to a fair trial), as well as related impacts on democratic processes. Additionally, attention 1 SIENNA project: 2 Konrad Siemaszko (Helsinki Foundation for Human Rights); Rowena Rodrigues, Anais Resseguier, Nicole Santiago (Trilateral Research); Javier Valls Prieto (University of Granada); Robert Gianni (Maastricht University). 2 must be paid to enhancing protection of vulnerable groups and individuals, who might be especially affected by the adverse impacts of AI.3 Recommendations on B. Objectives and Policy options We recommend a combination of Option 2 (EU legislative instrument setting up a voluntary labelling scheme) and Option 3 (EU legislative instrument establishing mandatory requirements). A legislative instrument should go beyond high-level principles and obligations. However, the framework cannot be too specific or detailed, as that risks unintentionally narrowing the scope and impact of the framework. Furthermore, the legislative framework must be agile enough to respond to the rapid development of AI and related technologies. As such, an EU legislative instrument should involve both ex-ante and ex-post enforcement mechanisms. In regard to the sub-options of Option 3, we believe that Option 3(b) (limiting the scope to high-risk applications) may be sufficient if the determination of high-risk is well-constructed. However, given this determination will be a crucial element of the whole framework and will have a significant impact on whether the framework will effectively serve its purpose, extreme caution should be taken to set the criteria in a manner that is robust. We do not believe the approach proposed in the European Commission White Paper and reiterated in this document is sufficient. To avoid the danger of leaving some high-risk applications under-regulated, this approach could be supplemented with an open-ended clause, for instance, with a requirement to conduct a human rights impact assessment (HRIA) or other relevant impact assessment. Additionally, those applications deemed high-risk should be subject to rigorous prior conformity assessment. Regardless of whether the framework is limited to high-risk applications, specific rules will be necessary for specific categories of AI applications, such as biometric identification. In some cases, this should include a ban on some applications, including AI-enabled large-scale scoring of individuals,4 AI-based racial profiling systems and biometric recognition facilitating mass surveillance (understood as a surveillance that is indiscriminate, not targeted against a specific individual5). Option 2 (EU legislative instrument setting up a voluntary labelling scheme) can be one complementary way to enhance trust and verify compliance with certain rules. However, it is vital that this must not be understood as a replacement for legal responsibility. Additionally, voluntary certification labels must not become self-serving or a business-manipulated front for hiding risks and harms. 3 Jansen, Philip., et al, SIENNA D4.State-of-the-art Review: AI and robotics , April 2018, 4 High-Level Expert Group on Artificial Intelligence, Policy and Investment Recommendations for Trustworthy Artificial Intelligence, Brussels, 2019, p. 20, EDRi, Ban Biometric Mass Surveillance A set of fundamental rights demands for the European Commission and EU Member States, Brussels, 2020, 3 We strongly recommend against Option 0 (baseline) or Option 1 (no EU legislative instrument). We believe that the EU should lead in setting baseline standards of protection of fundamental rights and societal values. There are already many soft initiatives in existence, to varying degrees of efficacy, and they have a role to play in protecting and promoting ethical values and fundamental rights. However, the role of the EU should be in establishing a strong legal framework to guarantee the protection and promotion of these rights, including, for example, addressing legal issues of liability and other ensuing harms. Without EU leadership, Member States will continue to act individually, which creates fragmentation and confusion for all stakeholders. Recommendations on C. Preliminary Assessment of Expected Impacts We do not agree that there are no direct significant negative social impacts or environmental impacts expected from the proposed measures. Further assessment of expected impacts associated with the proposed regulatory framework for AI is needed, particularly in regard to social and environmental impacts. For example, more understanding is needed on how the various proposed options influence the way individuals and society understand trust and whether certain options might promote a false sense of trustworthiness in AI and algorithmic decision-making. Additionally, as AI is a technology with a significant environmental impact (e.g., energy consumption, resource extraction, disposal), certain proposals that encourage the development and use of AI will lead to more environmental impacts. Furthermore, the impact assessment should evaluate the risks of not acting.",en,"Recommendations on C. Preliminary Assessment of Expected Impacts We do not agree that there are no direct significant negative social impacts or environmental impacts expected from the proposed measures. Further assessment of expected impacts associated with the proposed regulatory framework for AI is needed, particularly in regard to social and environmental impacts. Additionally, as AI is a technology with a significant environmental impact (e.g., energy consumption, resource extraction, disposal), certain proposals that encourage the development and use of AI will lead to more environmental impacts.",risk
Pharmaceutical Group of the European Union (PGEU) (Belgium),F550977,10 September 2020,Non-governmental organisation (NGO),Micro (1 to 9 employees),Belgium,"Ref 20E 00 1 The Pharmaceutical Group of the European Union (PGEU) is the association representing community pharmacists in 3 1 European countries. In Europe over 000 community pharmacists provide services throughout a network of more than 000 pharmacies, to an estimated 46 million European citizens daily. PGEU s objective is to promote the role of pharmacists as key players in healthcare systems throughout Europe and to ensure that the views of the pharmacy profession are taken into account in the EU decision -making process. Position Paper on Big Data & Artificial Intelligence in Healthcare Page 2 of 8 Introduction Dgdgd d Ddffdf dfdf The shift towards the digital economy has accelerated the pace at which new technologies are transforming the healthcare sector. Health systems in Europe are awash with data, whose range and volume are growing exponentially. Increasingly generated data is opening up possibilities for the use of technologies such as Artificial Intelligence (AI) and blockchains which are poised to disrupt healthcare on a global scale. Community pharmacy acknowledges the value Big Data and AI can have for European health syst ems and is ready to ensure that the use of new, innovative and automated technologies is always accompanied by expert and professional advice , such that their potential can be fully utilized to deliver more efficient, sustainable and high -quality healthcar e services to the patients, supporting and complementing the face - to-face interaction between healthcare providers and patients. In order to accomplish this, European community pharmacists are committed to: Build on their fully computerized systems and on innovative technologies to improve workflow efficiency while promoting patient safety, therapy effectiveness and offering the highest standard of pharmacy services to its patients. Remain a trusted source of reliable and independent health information for patients in the era of digitalization and of multiplication of information sources, by making the innovative digital solutions integral to community pharmacy practice. Play a pivotal role in the design, development, testing, implementation and uptake of ICT innovations to ensure they are fit for practice. Continue to provide advanced pharmacy services and promote remote monitoring and care , read -write access to shared electronic health records , use of electronic prescription and secure analy sis of big data repositories, registries and other pharmacy -held databases. Use their unique position at the heart of European communities and leverage the potential of Big Data and AI to provide more personalized advice to patients and robust, evidence -based information on issues related to their therapies while promot ing safe and rational medicines use . This paper is aimed to show how community pharmacists are equipped to address the challenges and opportunities arising from digitalization in healthcare. It also provides key policy recommendations to take full benefit of the potential of Big Data and AI in healthcare and promote sustainable and resilient health systems in Europe . Page 3 of 8 Big Data and AI in Healthcare explained Dgdgd d Ddffdf dfdf Big Data is generated today through a plurality of sources and is defined in various ways. In healthcare, Big Data refers to large routinely or automatically collected data, which is electronically stored. This data can be reused and comprise links among existing databases to improve health system performance.1 The possibility to merge and connect existing databases is envisaged by the European Commission strategy to boost healthcare data sharing in the EU2. PGEU welcomes th e Commission s strategy : community pharm acists want to secure patients access to health data and subject to each patient s consent promote the sharing of their data across borders to enable more personalised diagnoses and medical treatments. Within the Digital Single Market strategy, the Eur opean Commission has put forward a proposal for a European approach to boost investment and set ethical guidelines in Artificial Intelligence (AI)3, encouraging uptake of these technologies by public and private sectors. The European Commission intends to use the potential of new technologies to improve healthcare and medical research, by setting out a plan to secure citizens healthcare while fostering European cooperation and health data sharing. The implementation of eHealth, mHealth, telemedicine is li nked to the collection, analysis and the speed in the application of Big Data in health. 4 The remarkable amount of data in health contributed so far to the widespread adoption of electronic health records and e -prescribing systems, with community pharmaci sts being at the forefront of these developments in several European countries. As it turns out from daily practice, more and more patients ask pharmacists to provide advice on how to interpret health (medicine) information they acquire from other sources, such as the media, the interne t or mobile apps. This involves the interpretation by pharmacists of health data generated through wearable devices and digital point -of-care tests in community pharmacies, which have an enormous capability in early detection of undiagnosed chronic disease and potential adverse events and in monitoring of adherence and effectiveness of therapies. Along with increasing availability of Big Data, there have been advancements in AI techniques that enable machines and computers to sense and act, either on their own or to augment human activities. In healthcare, as in other sectors, AI has the potential to introduce new sources of growth, changing how 1 Source: 2 Source : -release_IP -18-3364_en.htm 3 Source : -release_IP -18-3362_en.htm 4 Big Data is often referred to as being characterized by four dimensions: Volume, Velocity, Variety and Veracity the latter being a mix of variability and complexity - the so -called four s V of Big Data. Page 4 of 8 work is done and reinforcing the role of people to drive growth in the sector.5 AI and machine learning can potentially free health professionals from routine tasks and save lives through efficient early detection. In the pharmacy sector, the widest use of AI is automated dispensing technology: in Europe between 30 - 40% of community pharmacies use this technology.6 This is applied in the pharmacy as automated pack dispensing robots, central filling systems and automated daily dosing systems. After deployment of this technology, pharmacists would see significant benefits in terms of sa fe dispensing and saved working time on dispensing which pharmacists can spend on providing patients with professional advice and services. The take -up of automation in community pharmacy is set to grow, with robots becoming smaller and more affordable. The evolution of automated dispensing technology goes hand in hand with the growing implementation of clinical decision support systems in community pharmacy7. Clinical decision support systems interact with electronic health record systems by receiving the patient data and medicine characteristics as input and by providing alerts for potentially expected adverse reactions (e.g. medication interactions, allergies) and medication errors (e.g. overdosing). These are increasingly linked with algorithms in the f orm of clinical rules8 and take into account more and more relevant patient data and medicine characteristics (where these are available to the pharmacist) such as lab and pharmacogenetic test results9. These allow for rapid and comprehensive assessments o f the patients medication safety at the point of dispensing in the pharmacy. Another type of AI technology with great potential in healthcare is blockchain.10 At its core, blockchain is a technology to create immutable and distributable data and transacti on records which can be shared peer to peer between networked database systems. Data stored in blockchain cannot be changed or recognized until it reaches the recipients that is what makes blockchain a theoretically secure technology concerning data inte grity. Managing and securing data within healthcare and supply chain management are two clear examples of principal concepts influencing and being impacted by possible blockchain adoption. Blockchain s records can be used to provide health records for ind ividuals, while giving all patients more control over their own information through verifiable consent . 5 Source: -en/insight -artificial -intelligence -future -growth 6 Source : -robots -are-coming PGEU Statement on eHealth Annex: eHealth Solutions in European Community Pharmacies 8 Helmons PJ, et al. J; Drug -drug interaction checking assisted by clinical decision support: a return on investment analysis; Am Med Inform Assoc 2015;764 doi:1093 9 -pharmacogenomics.pdf 10 Source: -sector/articles/blockchain -opportunities -for-health -care.html Page 5 of 8 Big Data and AI to make European health systems more sustainable Dgdgd d Ddffdf dfdf European health systems are facing major challenges related to the sustainability and quality of healthcare provision, as a consequence of demographic change, population ageing and rising prevalence of chronic conditions. Public expenditure on health and long -term care has been increasing over th e last decades, accounting for 5% of GDP in the EU and is expected to rise by an additional 2% to 4% of GDP by 11 EU Member States capacity to provide high quality care to all will depend on whether health systems will manage to become more resilien t and sustainable. To this end, the State of Health in the EU 12 recommended Member States to pursue the following policies: strengthening health promotion and prevention; investing in primary care systems and shifting healthcare out of the hospital sect or towards more cost -effective primary and ambulatory care; as well as promoting integrated care. In this context, innovative solutions that make use of digital technologies, including eHealth, Big Data, AI are seen by the European Commission as opportuni ties to transform healthcare systems .13 Big data and improved data analytics capabilities, as well as the use of clinical decision support systems by health professionals and use of mobile health tools for individuals to manage their own health and chronic conditions are just some of the possibilities offered by digitalization to achieve more sustainable healthcare14. To promote the use of eHealth, Big Data and AI in health systems, PGEU believes that recommending, monitoring and advising patients on their conditions with mHealth and eHealth solutions should be a reimbursed service for community pharmacists . Given the excellent accessibility of the community pharmacist , a healthcare professional at the heart of each community, and given the pharmacists existing services (in pharmacovigilance, managing side effects, adverse reactions, interactions, dose adjustments, therapeutic recommendations, providing information to patients and public health promotion), the PGEU believes community pharmacists are well pla ced to deliver such a funded service . Community pharmacists use eHealth tools and mHealth applications daily, when dispensing electronic prescriptions, checking for medication interactions, accessing electronic medications records or providing support for adherence via a mobile app. As such, they should be seen by European and national 11 Source: 12 Source: OECD/EU (, Health at a Glance: Europe 2016 State of Health in the EU Cycle, OECD Publishi ng, Paris. -en 13 Source: Health and care has been identified by most of the digital Public -Private Partnerships in Horizon 2020 as a core business area whe re digital technologies can play a major role. The Digitising European Industry (DEI) high level group recently established a working group on health. 14 Source : -lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:C:EN:PDF Page 6 of 8 What is next for Community Pharmacy? Dgdgd d Ddffdf dfdf policymakers as key reference in the formulation of eHealth policies and in developing policies on Big Data & AI. The advent of digitalization, Big Data and AI to healthcare presents the pharmacy professions with challenges and opportunities which can be summarized as follows: Challenges The extent to which Big Data and AI will have a positive impact on improving accessibility of care, integration of primary care systems, health outcomes (e.g. in terms of medicine safety and therapy effectiveness) for the patients as well as on increasing cost effectiveness of health interventions, will depend on a number of factors. These factors include, for instance, the usability, quality and interoperability of data collected as well as the quantity of those data . There is indeed one crucial prerequisi te for the use and the development of AI-driven health technologies: the availability of a large amount of informative health data. The more the data, the higher is the level of intelligence which machine learning tools can produce. The more relevant and i nformative the data , the more accurate AI predictions will be. In addition, it can be expected that the adoption of big data and related analytics technologies in healthcare will also raise some barriers and challenges concerning the use of sensitive infor mation belonging to patients. To keep patients trust in health systems unchanged, it will be essential that the collection of patient data and information will be done in compliance to GDPR (General Data Protection Regulation). As the closest and most acc essible point of access to care in Europe, community pharmacists are key to bridge patients and health systems and ensure patients are well informed on how their healthcare data is used to improve the safety and quality of their treatment . Opportunities As the quality of pharmacy healthcare services continues to grow, there are three main ways by which pharmacy can leverage the Big Data and AI disruption in healthcare to shape healthcare outcomes: Page 7 of 8 Conclusion Dgdgd d Ddffdf dfdf Being the most accessible and affordable poi nt of access in health systems , community pharmacists can use AI and new digital technologies to dedicate more working time to the provision of healthcare services and to direct patient care. Big Data and AI in pharmacy, if adopted within interoperable inf ormation systems, can use patient data and clinical history to support the pharmacists in providing more personalized healthcare services and expert advice. The potential of eHealth and mHealth tools can be used to provide real -time capture of data which can enable community pharmac ists to follow up with at -risk patients on their conditions and to monitor their progress during therapy. In addition, pharmacists already have an early form of AI in place : it is the pharmacy software which provides housing for data concerning medication history of the patient, patient use of medication , clinical rules (clinical decision support) and adherence data , gathered in compliance with GDPR. The next generation of pharmacy software using AI to implement a technology -based information expert system to identify timely adverse drug -reaction or medicines interaction problems based on patient data captured from the pharmacy system and other external data systems. In this way, pharmacists would need to spend less working time on identifying serious drug -related problems. These time savings coupled with potential automation of dispensing process could free a significant amount of time for the pharmacists to provide a broader range of patient -centered h ealthcare services. Pharmacists can contribute to the gathering of large amount of data in healthcare. They have been using electronic health records for almost 20 years to provide better patient care and monitor the patients conditions. Informatic tools in pharmacies have b een crucial as an information source for medicines and medical devices, use of e -prescription systems, repeat prescriptions systems, invoicing, follow -up services for patients, traceability and pharmacy services. This variety of sources can generate Big D ata which can be further analyzed, provided that they are compliant with the provisions of the GDPR. Big data analytics and AI may be useful tools to provide patients with better guidance on how to use their medication; to optimize value of data from m -health Page 8 of 8 technologies; to promote prevention and better everyday lifestyle guidance; to support patient monitoring and adherence as well as to obtain better health outcomes. Community pharmacists have the infrastructure, culture and expertise to make use of th e potential of Big Data and AI in healthcare. These technologies can increase the efficiency of processes within the pharmacy which in turn can facilitate added value service implementation. Therefore, c ommunity pharmacists acknowledge the benefits of appr opriately integrated digital solutions to complement practice. To fully take advantage of the potential of Big Data & AI in healthcare and promote sustainable and resilient health systems in Europe, this paper includes the following five recommendations: Involve community pharmacists as experienced users of digital health tools in the formulation of digital policies at local, national and European level as well as in the development of guidelines and methods on the sharing of Big data and deployment of AI in healthcare. Reward with reimbursement community pharmacy services involving recommending, monitoring and advising patients via mHealth and eHealth tools. Facilitate the production of Big Data in healthcare, via linking electronic hea lth records with e - Prescribing systems, allowing health professionals involved in patient care to access the necessary patient s information , subject to the patient s consent. Promote interoperability of information systems in Europe to foster exchange of data across community pharmacies. Enable community pharmacists to update electronic health records , if needed, to identify and address potential medication and patient safety -related issues. Harness the potential of AI in healthcare and use it to promote more collaboration across many different health professionals serving the same patients as well as to promote integration of primary care systems. END.",en,"J; Drug -drug interaction checking assisted by clinical decision support: a return on investment analysis; Am Med Inform Assoc 2015;764 doi:1093 9 -pharmacogenomics.pdf 10 Source: -sector/articles/blockchain -opportunities -for-health -care.html Page 5 of 8 Big Data and AI to make European health systems more sustainable Dgdgd d Ddffdf dfdf European health systems are facing major challenges related to the sustainability and quality of healthcare provision, as a consequence of demographic change, population ageing and rising prevalence of chronic conditions.",risk
Technology Industries of Finland (Finland),F550962,10 September 2020,Business association,Medium (50 to 249 employees),Finland,"1 ( 9/10/2020 Technology Industries of Finland Etel ranta 10, P.O.Box 10, FI -00131 Helsinki Telephone +358 9 192 31 Business ID: 0215289 -2 Technology Industries of Finland input to the Commission Impact Assessment on AI Technology Industries of Finland (TIF) represents more than 1,600 companies, active in Finland in various sectors of technology industr ies. Our member companies cover both developers and deployers of AI. Our main messages are: There is no one, overarching AI. AI is a set of tech nologies for processing and extracting value out of data, automating decisions and extracting insights . According to latest studies, solutions and tools of data economy, such as AI will play a major role in the green transformation of the industries. Existing Produc t Safety legislation is technology -neutral and has stood the test of time. We urge Commission to carefully analyse whether use of AI would call for a piece of technology -specific piece of regulation? Instead of technology, the attention should be on data a nd the purpose of its processing. These define the riskiness of the use case. On assessment of regulatory options, it is wise to apply legislation only when it is needed. However, singling out high -risk cases may prove problematic in terms of predictabili ty and legal certainty. Green transformation and ramping up productivity of European industries are our major challenges. Technology plays a major role when solutions are developed. To achieve this, we need steady and predictable environment for investmen ts. Europe needs to remain a predictable member in global value networks and not use regulation for short -sighted leaps. Excellence is best built by building on strengths and agile solutions for European data economy. Support for the Single Market TIF agrees with the Commission on the first point of the assessment: the EU -level is the right level of regulation in order to avoid legal fragmentation. We need streamlined digital single market, where regulation is in place when and were needed. On the IIA document, t he need for regulation is based on lack of trust and potential threats to Human Rights. This leads us to ask: is the link between trust and regulation as imminent as the Commission it depicts? Objectives and Policy Options High level of protection of privacy is one of the core values of EU and it is well established that the level of protection is indeed high in the EU. However, this does not seem to play in favo ur of European (digital) companies. Basically, r egulation establishes for th e companies a set of technical and organisational requirements. Regulation is justified and good if these requirements link directly to the objective that is desired and are proportionate to reach that end (e.g. requirements in nuclear power plants to adva nce nuclear safety). It is very challenging to come up with proper set of requirements , though, when the objective is as broad as protection of human rights even more challenging it is when the domain is AI. AI is 2 ( a technology that can be used in wide a rray of solutions, it can be used for different purposes and for processing of different sets of data. The regulatory option of applying full -scale regulation only to high -risk cases (Regulatory o ption 3 b) of application of AI seems plausible at first . But when analysed in greater detail, it becomes very challenging to come up with a general criterion on how to single out cases where the risk is high. At the end , it is down to the data and the purpose of processing or usage of AI system that defines the risk on a meaningful way. Therefore, we need a framework that helps developers and deployers to identify, evaluate and mitigate the risks involved. In most cases, the risks stem from processing of personal data or the AI -driven systems/vehicles moving abou t freely among people, like autonomous cleaning robots or vehicles . On biometric identification (option 3 a) it is not the technology but the purpose that is decisive: it is ok to use it properly executed to make things easier, like opening one s phone or laptop but most of us feel unease if it is used for mass surveillance. What are the cross -cutting key prerequisites for trust? People need to know who are in charge of the systems (this is already established in Product Safety Law of the EU) , people need to know whether they are interacting with an AI system or a human being, people need to able to understand what data (and why and how ) is being processed. It may also be wise to tune the framework on a manner that serves for careful mode of developmen t and running of AI systems where key decisions are documented on a way that is justified and proportionate . As written in the assessment, too many requirements are prone to lead to a situation where costs outweigh the benefits. Therefore, the regula tory option 1, and in some cases option 2, of the assessment are a solid starting point. We do recognise that there might be a need for actual legal requirements such as in cases where AI -driven vehicles interact with human s and human -driven vehicles. Thes e need to be carefully analysed, on basis of existing legislation and keeping open the possibility to use existing, preferably technology neutral , regulatory base on a coordinated manner. When setting up the regulatory framework, the Commission should keep in mind that many European AI companies tend to be of SME category, usually concentrating on one specific area of technology. If legal requirements are numerous, in very high detail and accompanied by extensive ex ante conformity assessment mechanisms , the big players will have the advantage. Therefore, as a general rule, we favour ex -post solutions for enforcement structures, when needed. The same call for proportionality applies for the scope of future regulation. What would be the ratio of applyi ng regulation to simple automated decision -making systems? Would this in fact produce an outcome a relevant set of requirements which help us to better protect human rights? Or would it actually create an environment that encourages to use secondary solutions, such as an ever -going chain of if -commands instead of adequate solutions. The key requirements for regulation that encourages development of and investments to AI, are predictability and proportionality. It should also be remembered that Eu ropean firms operate on a global market. The regulatory framework should not establish unfounded requirements that make development of global data solutions and transfers challenging. Impacts TIF does agree with the impact assessment as to the proport ionality presented on economic impacts and likes to point out that in order to meet the decarbonisation and productivity goals, 3 ( we need to have proportionate and predictable legal environment to facilitate the necessary investments to the solutions of data economy. As to the social impacts, it is not only use of AI, but the digitalisation of industries that calls for re-skilling of workforce . As to the environmental impacts, there are various studies published in Finland pointing out that solutions of d ata economy, such as AI, algorithms and data -driven optimisation are the most readily available high -impact methods of bringing down co2 output throughout in the society, coupled to use of fossil free electricity to replace fossil -based processes. Accordin g to the TIF roadmap to fossil free technology industries by 2035, it is stated that the objective can be reached, if we are able to fully deploy the new technologies and the environment is stable enough to facilitate the required investments. The option o f not being able to use the optimal technology should also be weighed. As digital solutions facilitate new ways, we urge the Commission to look new kinds of co - operation between regulators and developers. New solutions, such as regulatory sandboxes facilitate transfer of crucial information and building of trust between regulators, developers and users of solutions. Conclusion In order to be able to create a future -proof and stable regulatory environment we still need to elaborate the objectives of the regulation and have enough fact-base combined to thorough analysis of the actual gaps of the existing regulatory framework. The great challenges or our generation climate change and productivity call for ability to use all available technical solutions on a rational manner in order to guarantee sustainable future for future generations. By having the facts in order and objectives clear, we can evaluate what is the proportionate and balanced regulatory approach . Further information: Head of Digi tal Regulation, Jussi M kinen Jussi.makinen@techind.fi, +358 40 900 3066",en,"According to latest studies, solutions and tools of data economy, such as AI will play a major role in the green transformation of the industries. Green transformation and ramping up productivity of European industries are our major challenges. Impacts TIF does agree with the impact assessment as to the proport ionality presented on economic impacts and likes to point out that in order to meet the decarbonisation and productivity goals, 3 ( we need to have proportionate and predictable legal environment to facilitate the necessary investments to the solutions of data economy. As to the environmental impacts, there are various studies published in Finland pointing out that solutions of d ata economy, such as AI, algorithms and data -driven optimisation are the most readily available high -impact methods of bringing down co2 output throughout in the society, coupled to use of fossil free electricity to replace fossil -based processes. The great challenges or our generation climate change and productivity call for ability to use all available technical solutions on a rational manner in order to guarantee sustainable future for future generations.",risk
ALLAI (Netherlands),F550937,10 September 2020,Non-governmental organisation (NGO),Micro (1 to 9 employees),Netherlands,"FEEDBACK ON THE INCEPTION IMPACT ASSESSMENT Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence September 10, 2020 Virginia Dignum Catelijne Muller Andreas Theodorou 2 Executive Summary First and foremost, we would like express our support for the European Commission's efforts to establish an appropriate regulatory framework for AI. In establishing such a framework, one should both look at existing laws and regulations and determine if their are 'fit for purpose' for a world with AI as well as consider establishing new rules where current legislation is not adequate. In general, we recommend to broaden the description of the problem that the initiative aims to tackle, i.e. addressing a number of ethical and legal issues raised by AI, and include ""societal issues raised by AI"". In the same spirit, we recommend to broaden the description of the ultimate policy objective of the proposal, i.e. to foster the development and uptake of safe and lawful AI that respects fundamental rights across the Single Market by both private and public actors while ensuring inclusive societal outcomes, so as to include ""fair societal outcomes"". The issue of defining the scope of a new legislative initiative for AI is the core element that needs to be addressed. Whereas the Inception Impact Assessment mentions a number of AI-techniques that either should or should not be covered by the instrument, we would like to recommend a different approach toward defining the scope of the instrument: an approach that looks at the level of impact of the technology on people and society at large, rather than (merely) on the technical specifications of a particular AI-system. An impact-level based approach lowers the risk of loopholes that could be exploited. As for existing legislation, we call for a broad legal AI stress test, because we see a large number of additional legal lacunae where it comes to AI, that were not mentioned in the Inception Impact Assessment, such as the GDPR, law enforcement, competition law, transportation, trade of dual use technology, medical devices, energy and the environment, to name a few. The Inception Impact Assessment lays down 5 policy options ranging from keeping the 'Baseline scenario' to a combination of several policy options. In its Whitepaper on Artificial Intelligence, Europe took a clear stance on AI; foster uptake of AI technologies, underpinned by what it calls an ecosystem of excellence , while also ensuring their compliance with to European ethical norms, legal requirements and social values, an ecosystem of trust . The Inception Impact Assessment of a ""Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence"" now presents a number of objectives and policy options. This paper provides feedback to these objectives and policy options. By Virginia Dignum, Catelijne Muller and Andreas Theodorou 3 ALLAI would be most in favor of a combination of the policy options 2, 3a and 3b as described in the Inception Impact Assessment. This combination would entail soft law for low impact AI applications (or uses) including volulntary labelling, and EU instrument with mandatory labelling covering two elements: (i) clear restrictions, conditions, safeguards and/or boundaries for a limited number of exceptionally impactful AI-applications or uses and (ii) mandatory requirements for medium to high impact AI based on common denominators to determine the level of impact. Finally, we call for an ex-durante (which would include ex-ante and ex-post mechanisms) mechanism to ensure a continuous, systematic, socio-technical governance approach, looking at the technology from all perspectives and through various lenses. For this we recommend to set up European AI Authority as part of a global framework of AI Authorities. A. Context, Problem Definition and Subsidiarity Check Defining AI for regulatory purposes While we realize that the Inception Impact Assessment is not the place to provide for a definition of AI, it rightly identifies the need to define the scope of the initiative as a core question that needs to be answered. We would like to make some remarks on the issue of defining AI for regulatory purposes. As we already indicated in our ""Analysis of the EU Whitepaper on Artificial Intelligence""1, we want to emphasize that AI is more than data and algorithms, powered by Computer Processing Power (CPU). While this is the case for the most widely used AI-systems at present, this is only a very limited description of what AI is. AI is a container term for many computer applications, some of which combine data and algorithms, but other, non-data-driven AI approaches, also exist, e.g. expert systems, knowledge reasoning and representation, reactive planning, argumentation and others. This should be kept in mind when defining the scope of the initiative, to avoid over- or underinclusion. In the same analysis we have elaborated on the fact that there is no universally accepted definition of AI and how we would describe AI technically. Nevertheless, it should be noted that many of the wide applied AI systems are indeed examples of data-driven AI, that have a number of typical charachteristics that can make them brittle, unstable and unpredictable (such as opacity, correlation in stead of causality, insufficient or low quality data, unclear 'goals', a lack of common sense, etc.). 1 4 Legal versus technical definition - a need to look at the impact of AI It is important to realize that legal definitions differ from purely technical definitions whereas legal definitions should meet a number of different or additional requirements2 such as inclusiveness, preciseness, comprehensiveness, practicability, permanence, some of which are legally binding, and some are considered good regulatory practice3. In general, we feel obliged to emphasise that the focus on a definition through any attempts at defining AI-techniques in order to determine what is and what is not AI creates loopholes that could be exploited. This brings us to a number of important groundrules that should be guiding AI regulation and how one should set the scope of such regulation or, in other words, define AI for regulatory purposes: We recommend to focus on the effects and impact of the systems, not on a particular AI-technology/technique. AI-systems are more than just the sum of their technical or software components. AI systems also comprise the socio-technical system around it. When considering regulation, the focus should not just be on the technology, but more on the social structures around it: the organisations, people and institutions that create, develop, deploy, use, and control it, and the people that are affected by it, such as citizens in their relation to governments, consumers, workers or even entire society. An ""AI lifecycle approach"" should be followed, that considers not only the development stage of AI, but also the deployment and use stages. It should be kept in mind that most AI-applications currently being used could enshrine, exacerbate and amplify the impact on existing laws and fundamental rights as well as society at scale, affecting larger parts of society and more people at the same time. 2 A Legal Definition of AI Jonas Schuett Goethe University Frankfurt September 4, 2019 (Legal definitions must be: (i) inclusive: the goals of regulation must not over- or under-include. (Julia Black. Rules and Regulators. Oxford University Press, [32] Robert Baldwin, Martin Cave, and Martin Lodge. Understanding Regulation: Theory, Strategy, and Practice. Oxford University Press, 2nd edition, ); (ii) Precise: it should be clear which case falls under the definition and which does not; (iii) Comprehensive: the definition should be understandable by those who are regulated; (iv) Practicable: legal professionals should be able to easily determine whether a case falls under the definition; (v) Permanent: the need for continued legal updating should be avoided. 3 Inclusiveness can be derived from the principle of proportionality in EU law (art. 5( of the Treaty on European Union. The criteria precision and comprehensiveness are based on the principle of legal certainty in EU law. The criteria practicability and permanent are considered good legislative practice. ALLAI Project: Defining AI for Regulatory Purposes ALLAI is currently evaluating the feasibility of setting up a project called ""Defining AI for Regulatory Purposes"". Working with a small group of experts and scientists from different backgrounds the project aims to provide guidance for the open core question relating to the scope of the initiative, notably how AI should be defined. 5 Problem the initiative aims to tackle On: ""Ultimate objective"" ALLAI for the main part supports the ultmate policy objective of the proposal, i.e. to foster the development and uptake of safe and lawful AI that respects fundamental rights across the Single Market by both private and public actors while ensuring inclusive societal outcomes. ALLAI would however advise to broaden the final part of the ultimate policy objective to not only ensure inclusive societal outcomes, but to also ensure ""fair societal outcomes"". As mentioned in the Inception Impact Assessment, the complexity (and many times opacity) of certain systems and granular applicability of outcomes to individuals in combination with the scalability of AI systems, presents a range of difficulties as regards enforcement of existing legislation meant to protect human rights and could generate new safety risks. On top of that, these characteristics could enshrine, exacerbate and amplify these risks and adverse impacts on society at scale, affecting more people at the same time. As such there is a serious risk that unfair societal outcomes become ever more enshrined, exacerbated and amplified, thus potentially leading to wider and deeper societal gaps between groups of people, propagating inequality and, as a consequence, entrenching political polarisation. The addition of fairness to ultimate policy objective would also reflect the European Commissions' adoption of the 7 Requirements of the High Level Expert Group on AI, i.e. requirement no. Inclusiveness, non-discrimination and fairness. On: ""Harm caused by AI-systems and risks not covered by existing legislation"" We agree with the indication that harm caused by the use of AI may be the consequence of multiple causes. We would like to stress however that the causes for harm go beyond just flaws in the technical and digital components (including data) and charachteristics of the system. While these flaws do often play a major and sometimes decisive role in causing harm, one should not forget that even the most technically robust systems can still cause harm. Imagine a facial recognition system that does recognize people of all colors, genders, ages, etc. correctly. Technically such a system could be considered robust and non-discriminatory. Lawfully however, the system could still cause harm. AI-driven (mass) surveillance with facial recognition, involves the capture, storage and processing of personal (biometric) data (our faces), but it also affects our 'general' privacy, identity and autonomy in such a way that it creates a situation where we are (constantly) being watched, followed and identified. As a psychological chilling effect, people might feel inclined to adapt their behaviour to a certain norm, which shifts the balance of power between the state or private organisation using facial recognition and the individual. 6 In legal doctrine and precedent the chilling effect of surveillance can constitute a violation of the private space, which is necessary for personal development and democratic deliberation. Even if our faces are immediately deleted after capturing, the technology still intrudes our psychological integrity. This is just one example where the mere technical elements of a system are less relevant as regards harm than the actual use of the system. That is why we advocate to continuously ask question zero : Do we want to allow this particular AI-system and technique in the first place, or are there reasons not to allow its use at all? And if we were to consider implementing, deploying and using such a system, what are the conditions we should set for its use? In either case, for the purposes of any regulatory framework we should not merely focus on technical solutions at dataset or algorithm level, but devise socio-technical processes that help us: a) Understand the potential legal, ethical and social effects of the AI-system and improve our design and implementation choices based on that understanding; b) Audit our algorithms and their output to make any undesirable outcomes transparent; and c) Continuously monitor the workings of the systems to mitigate the ill effects of AI. On: ""Risks not adequately covered by existing legislation"" ALLAI agrees with the Inception Impact Assessment that there are ample risks that are not adequately covered by existing legislation on cybersecurity, protection of employees and anti-discrimination. The Inception Impact Assessment identifies as main issues: Effective enforcement of existing EU rules to protect fundamental rights Application of EU rules on safety Application of EU the rules on liability AI & the impact on fundamental rights ALLAI strongly commends the focus on the protection of fundamental rights and would like to draw your attention to a report it delivered to the Council of Europe on the ""Impact of AI on Human Rights, Democracy and the Rule of Law"".4 This report identifies those human rights, as set out by the European Convention on Human Rights (""ECHR""), its Protocols and the European Social Charter (""ESC""), that are currently most impacted or likely to be impacted by AI. It aims to provide a number of possible strategies that could be implemented simultaneously, ranging from addressing the impact within the existing framework of human rights, democracy and the rule of law to establishing new human rights should the existing framework fail to adequately protect us. 4 7 While states are obliged to protect individuals and groups against breaches of fundamental rights perpetrated by other actors, appreciation of non-state actors influence on human rights has steadily grown.5 As (large) tech companies have now become operators that are capable of determining and perhaps altering our social and even democratic structures, the impact of their AI(-use) on fundamental rights becomes more prevalent. In this respect, AI might serve as a good opportunity and think of a structure that goes beyond the 'horizontal effect' of the EU Charter of Fundamental Rights of the European Union. Such a structure could entail a legal obligation for private actors to comply with fundamental rights and to grant access to justice if they fail to do so.6 AI & the impact on work The Inception Impact Assessment mentions that there are legal issues as regads the protection of employees, but does not yet specify these issues. We would like to stress that he introduction and use of AI in the workplace can cause effects as to health and safety in the workplace, job security, worker privacy, the balance between worker and employer and so on. For that reason, we have been advocating early and close involvement of workers and service providers of all types, including freelancers, the self-employed and gig workers not just people who design or develop AI, but also those who purchase, implement, work with or are affected by AI systems. Social dialogue must take place before the introduction of AI technologies in the workplace, in line with the relevant applicable national rules and practices. Additionally, we would like to draw special attention to AI used in hiring, firing and worker assessment and evaluation processes. The White Paper on AI mentions AI used in recruitment as an example of a high-risk application that would be subject to regulation irrespective of the sector. We recommend extending this use to include AI used in firing and in worker assessment and evaluation processes, but also to explore the common characteristics of AI applications that would make for a high risk use in the workplace, irrespective of the sector. AI applications that have no scientific basis, such as emotion detection through biometric recognition, should not be allowed in workplace environments. Additional legal lacunae In addition to the issues identified in the Inception Impact Assessment, ALLAI also identifies a number of additional regulatory lacunae related to existing legislation. As AI is evolving quickly and the wider impact of AI on the full acquis has still not been fully identified, these lacunae are not exhaustive but below are the areas for attention to the extent that they can be identified today7: 5 Business and Human Rights, A Handbook for Legal Practitioners, Claire Methven O Brien, Council of Europe 6 This also means going beyond merely referring to the Recommendation CM/Rec(3 on human rights and business of the Committee of Ministers of the Council of Europe (and the UN Guiding Principles on Business and Human Rights) 7 See also: STOA Policy Briefing: Legal and ethical reflections concerning robotics, The Scientific Technopolis Group, Maastricht University, Scientific Foresight Unit (DG EPRS) of the European Parliament. 8 GDPR Law Enforcement Competition law Transportation Trade of dual-use technology Consumer protection Healthcare Energy & Environment Ad GDPR A much cited existing EU regulation in the context of AI is the GDPR. This regulation provides frameworks for data protection, the right to an explanation of AI decisions and safeguards for the use of biometric recognition. Biometric recognition: It should be noted that GDPR restricts the processing of biometric data only to some extent. Biometric data according to the GDPR is personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, which allow or confirm the unique identification of that natural person. The last part of the sentence is crucial, because if biometric recognition is not aimed at identification (but for example at categorization, profiling or affect recognition), it might not fall under the GDPR-definition. In fact, recital 51 of the GDPR says that 'the processing of photographs [is considered] biometric data only when processed through a specific technical means allowing the unique identification or authentication of a natural person.' Many biometric recognition technologies are however not aimed at processing biometric data to uniquely identify a person, but merely to assess a person s behaviour (for example in the classroom) or to categorize individuals (for example for the purpose of determining their insurance premium based on their statistical prevalence to health problems). These uses might not fall under the definition of biometric data (processing) in the GDPR. Right to explanation of ADM, including profiling: Particularly with regard to the right to an explanation of automated decisions, a debate is underway about whether the GDPR gives a right to an explanation of automated decisions or not. Under the GDPR, controllers who use personal data to make automated decisions are obliged to inform individuals in advance and to provide meaningful information about the logic and importance of the decision-making and the consequences for the data subject. The so-called art. 29 Working Group recognizes that ""the growth and complexity of machine learning can make it challenging to understand how automated decision-making or profiling works,"" but that, despite this, ""the company [must] find simple ways to tell the individual about the reason, or the criteria on which the decision is based, without necessarily always attempting a complex explanation of the algorithms used or disclosure of the full algorithm."" 9 Also, the GDPR requires a controller to implement appropriate safeguards when designing automated decisions, such as the right to human intervention and the right to express his or her point of view and contest the decision. The recitals of the GDPR also include the right to an explanation of a fully automated decision. It is however debatable whether there is a full right to an explanation of an automated decision in all cases. The fact that the right to explanation is included only in the recitals seems to be a hurdle that can be overcome, but in particular the fact that the right to explanation only exists as regards to fully automated decision-making, makes it insufficient to ensure adequate transparency in the automated decision-making process. After all, a single human 'check' on an automatic decision (regardless of whether this person has been able to judge the decision on its merits, could lead to the conclusion that an explanation within the meaning of the GDPR would not be necessary. It must also be assessed whether the GDPR offers sufficient protection when the decision is based on non-personal data. AI-driven profiling and the categorization of people or groups of people is regularly being done by finding inferences made about an individual, even without using personal data or resulting in identification of a person. There is no consensus whether these inferences in itself should count as personal data, but there are experts arguing that all data processing that has an impact on people should be protected8 or even that the distinction between personal and non-personal data should be suspended9. Here the issue also ventures from the GDPR into other fundamental rights such as the right non-discrimination, covered in multiple EU directives and the ECHR. Ad Law enforcement In line with the above, EU regulation such as the Police Directive (EU 2016/, that regulates the processing of personal data in particular for the purpose of profiling by law enforcement in the Member States, does not adequately cover the issue of profiling that is based on mere inferences rather than personal data, as described above. Ad Competition Many AI-applications are developed and deployed by a handful of private actors. These actors are also present in multiple market segments that are related to AI or use AI, such as finance, insurance, etc. If too much market power in these different markets is concentrated in a few companies, this could lead to unfair competition and difficulty for new (smaller) players to enter the market. 8 Purtova ( 9 Koops ( 10 Ad Transportation While autonomous driving has not yet fully materialized, the autonomous vehicles industry is well underway to making vehicles behave more autonomous and needing less 'active involvement' from their drivers. These developments call for a critical review of the various EU regulations and directives that deal with transportation, such as Regulation (EC) 561/2006 and Regulation (EEC) 3821/85 regarding driving and resting times and digital tachygraphy (for truck platooning), Directive 2014/45/EU on Roadworthiness, Directive 2010/40/EU on Intelligent Transport Systems in the field of road transport and for interfaces with other modes of transport and Directive 2003/59/EC on training and initial qualifications of professional drivers10. Ad Trade of dual-use technology Following an impact assessment in 2016, a new reform process was started to (a.o.) future-proof the export control regime for rapidly developing emerging technologies. In sum, the main goal of this reform is to control the export of information technologies that can be used for the suppression of human rights, thus increasing the scope and scale of dual-use governance. In 2016 the focus was on the inclusion of so called cyber-surveillance technologies 11, but recently AI applications such as facial recognition have also entered this debate as a possible next step for dual-use regulation. Ad Consumer protection At this point, consumers do not receive adequate protection against unacceptable impact.12 Many of these protections would need to be better covered in different existsing legislative instruments. As an example, the EU legal protection of consumers against unfair AI-driven personalized pricing is mostly principle-based, leading to uncertainty on its interpretation as long as there is no clarification in case law.13 Ad Health In the Medical Devices Regulation software is considered a medical device, making AI-driven applications in healthcare subject to certain requirements and a mandatory labelling scheme. Alignment of this regulation with a new legislative instrument on AI is important. 10 Also: Directive 2009/103/EC on motor vehicle insurance; Directive 2007/46/EC on vehicle approval; Directive 2006/126/EC on requirements for driving licences 11 Immenkamp, 2019 12 BEUC, EU Rights for Consumers 13 de Streel, Alexandre; Jacques, Florian ( : Personalised pricing and EU law, 30th European Conference of the International Telecommunications Society (ITS): ""Towards a Connected and Automated Society"", Helsinki, Finland, 16th-19th June, 2019, International Telecommunications Society (ITS), Calgary 11 Also, now that the Medical Devices Regulation has been postponed to enter into effect in May 2021, existing legislation, and the current practice of assessment and certification of AI-diven healthcare devices, should be critically looked at. Ad Energy & Environment The High Level Expert group on AI (HLEG AI) has argued that sustainability and ecological responsibility of AI systems should be encouraged and has made ""Societal and Environmental Well-being"" one of the 7 requriements for Trustworthy AI of its Ethics Guidelines for Trustworthy AI. According to the HLEG AI, it must be ensured that AI-systems operate in the most environmentally friendly way possible. The system s development, deployment and use process, as well as its entire supply chain, should be assessed in this regard, e.g. via a critical examination of the resource usage and energy consumption during training, opting for less harmful choices. Measures securing the environmental friendliness of AI systems entire supply chain should be encouraged. This could trigger a necessery review and possible adaptation of EU engery regulations such as Directive 2010/30/EU on the indication by labelling and standard product information of the consumption of energy and other resources by energy-related products. AI stress test for EU regulation The foregoing calls for a much broader AI stress test for existing EU regulation. Three questions need to be answered in particular: To what extent are the policy and legal objectives underlying these regulations affected by AI systems and in what ways? What are the existing monitoring, information gathering and enforcement frameworks capable of providing meaningful and effective oversight to ensure that policy and legal objectives are still effectively achieved? To what extent does existing legislation work in a way that they promote and safeguard the ethical principles and requirements (as described in the AI HLEG's Ethics Guidelines for Trustworthy AI)? ALLAI points out that the AI HLEG has already advised the European Commission to perform this 'stress test' at EU level, in order to arrive at an unambiguous regulatory framework across Europe. It should be noted that EU legislation does not affect all national legislative areas. Where the EU has no legislative competence, ALLAI advises the Dutch legislator to initiate the stress test at national level. Think of elements of labor law, social security law, administrative law and criminal law. 12 Basis for EU intervention ALLAI fully agrees with the Inception Impact Assessment's reasoning that the objectives described cannot be reached effectively by Member States alone, but can be better reached at Union level. We agree that it is important that fragmantation would prevent the free circulation of goods and services containing AI and thus negatively affect the Digital Single Market. Fragmentation would also lead to divergence in levels of protection of citizens and society against the abovementioned harms and risks, which could lead to an unacceptable ""race to the bottom"" of AI regulation and protections in an effort to attract more AI investment. B. Objectives and Policy Options Objective ALLAI supports the overall objective of the instrument, i.e. to ensure the development and uptake of lawful and trustworthy AI across the Single Market through the creation of an ecosystem of trust. ALLAI would like to suggest the following additional aims of the initiative: To fill any legal lacunae either regarding the effectivenes, applicabilty or enforceability of existing EU law and where no EU law exists, so as to ensure that overall EU policy and legal objectives as regards trustworthy AI are promoted and safeguarded; In addition to what is expressed in aim (c) we strongly recommend to broaden this aim so as to include risks for people and society. As part of aim (c) and aim (e) it is important to be able to effectively monitor the future developments of AI within and outside the Single Market, so as to make sure that new opportunities for trustworthy AI are identified and promoted, but also that new challenges are adequately and timely addressed. In addition to aim (e), we suggest to set up a structure that includes not only the relevant authorities in the Member States, but also all relevant stakeholders, such as workers' and business' representatives, other civil society organisations, NGO's, academia (various disciplines), policy makers, etc. As an additional aim we reccommend to set up a European AI Authority as part of a global framework of AI Authorities. Such a framework could be set up as folows: o A global AI Authority; o Several regional sub-authorities (e.g. EU, the Americas, Asia, Oceanea, Middle East); o National executive authorities (either existing or new), for the EU to be appointed or set up by the Member States. 13 The roles and responsibilities of these authorities should be carefully considered, but they should have broad expertise of the different elements and impact domains of AI, including but not limited to technical, legal and ethical expertise, as well as knowledge of behavioural effects, labour market effects, economic and societal effects of AI. Policy Options On Option ""Baseline ALLAI agrees that the current ""Baseline"" or Option ""0"" does not suffice to adequately address the risks and potential harms connected to AI and thus considers this option not viable. On Option ""EU soft law"" The option of EU soft law should only apply to AI-systems or uses that have no adverse impact on people, society or the environment nor on our (fundamental) rights, democracy and the rule of law. The two factor approach that was suggested by the European Commission in its Whitepaper on AI does however not suffice to determine these types of AI-systems. As an example consider targeted online advertising. The Commission will likely qualify advertising as a low-risk sector, and AI-driven targeted adds as a low risk AI-application. Targeted advertising however, has shown to have a potential segregating and dividing effect. This is the reason why we recommend looking at the level of societal or personal impact of an AI-system or use to determine the risk level of the system. On Option ""EU legislative instrument setting up a voluntary labelling scheme"" The labelling scheme is not a standalone option, but could be split into two options, the first to be joined with option 1 and second to be joined with option Where EU soft law would suffice (option , a voluntary labelling scheme could be an interesting addition for industry players to gain competitive advantage, or to feel confident to define or explore a niche area of application. Where an EU legislative instrument establishing mandatory requirements for AI is necessary (option , a mandatory labelling scheme (like the CE-marking) could be considered to avoid any untrustworthy AI being deployed on the Digital Single Market. Current practices around certification of AI, such as those already executed in the healthcare sector, should be carefully reviewed, to see if those practices are sufficient, or should be amended or replaced by a new mandatory labelling scheme to avoid overlap with possible new labelling scheme. 14 Labels should not merely refer to the technical characteristics of the system, but more importantly also to the effects and impacts of the system. On Option ""EU legislative instrument establishing mandatory requirements for AI"" ALLAI is in favour of introducing an EU legislative instrument, in the form of a combination of sub-options (a) and (b). Exceptionally impactful AI The following AI-systems or uses that are considered to be too impactful could give rise to the necessity of a ban, moratorium, strong restrictions or conditions for exceptional and controlled use: Indiscriminate use of facial recognition and other forms of biometric recognition either by state actors or by private actors; AI-powered mass surveillance (using facial/biometric recognition but also other forms of AI-tracking and/or identification such as through location services, online behaviour, etc.); Personal, physical or mental tracking, assessment, profiling, scoring and nudging through biometric and (online) behaviour recognition in violation of fundamental rights (AI-enabled Social/Citizen Scoring); Covert AI systems and deep fakes; Implanted human-AI interfaces; Exceptional use of these technologies, such as for national security purposes or medical treatment or diagnosis, could be allowed but should be evidence based, necessary and proportionate and only be executed in controlled environments or cleary identified contexts and (if applicable) for limited periods of time. As regards biometric recognition14 (including facial recognition) we recommend that any use of biometric recognition only be allowed under the followoing cumulative conditions: i) there is a scientifically proven effect; (ii) it is used in controlled environment (e.g. a hospital); (iii) it is used under strict conditions (e.g. limited in time, for a specific purpose, etc.). Widespread and/or public use of AI-driven biometric recognition to surveil, trace, track, assess or categorise humans or human behaviour or emotions should not be allowed. As regards AI-driven mass surveillance we refer to the recommendation of the HLEG AI in its Ethics Guidelines for Trustworthy AI that automatic identification raises strong concerns of both a legal and ethical nature, as it may have an unexpected impact on many psychological and sociocultural levels. 14 Biometric recognition of micro-expressions, gait, (tone of) voice, heart rate, temperature, etc. is being used in various ways, one of which is to assess or even predict our behaviour, mental state, and emotions. As Barret et al. (Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements, have shown however, no sound scientific evidence exists to suggest that a person's inner emotions or mental state can be accurately 'read' from their facial expression, gait, heart rate, tone of voice or temperature, let alone that (future) behaviour could be predicted by it. 15 A proportionate use of control techniques in AI is needed to uphold the autonomy of European citizens. Clearly defining if, when and how AI can be used for mass surveillance both by public or private actors, differentiating between the identification of an individual versus the tracing and tracking of individual, will be crucial for the achievement of Trustworthy AI. As regards AI-enabled Social Scoring we refer to the recommendation of the HLEG AI in its Ethics Guidelines for Trustworthy AI, that 'any form of citizen scoring should only be used if there is a clear justification, and where measures are proportionate and fair. Normative citizen scoring (general assessmentof moral personality or ethical integrity ) in all aspects and on a large scale by public authorities or private actors endangers these values, especially when used not in accordance with fundamental rights, and when used disproportionately and without a delineated and communicated legitimate purpose.' The HLEG AI also argues 'that citizen scoring on a larger or smaller scale is already often used in purely descriptive and domain-specific scorings (e.g. school systems, e-learning, and driver licences). Even in those more narrow applications, a fully transparent procedure should be made available to citizens, including information on the process, purpose and methodology of the scoring.' It also notes that 'mere transparency cannot prevent non-discrimination or ensure fairness, and is not the panacea against the problem of scoring and that ideally the possibility of opting out of the scoring mechanism without detriment should be provided otherwise mechanisms for challenging and rectifying the scores must be given. This is particularly important in situations where an asymmetry of power exists between the parties. Such opt-out options should be ensured in the technology s design in circumstances where this is necessary to ensure compliance with fundamental rights and is necessary in a democratic society.' As regards covert AI systems and deep fakes we again refer to the recommendation of the HLEG AI in its Ethics Guidelines for Trustworthy AI that 'human beings should always know if they are directly interacting with another human being or a machine, and it is the responsibility of AI practitioners that this is reliably achieved. AI practitioners should therefore ensure that humans are made aware of or able to request and validate the fact that they interact with an AI system (for instance, by issuing clear and transparent disclaimers ). Note that borderline cases exist and complicate the matter (e.g. an AI-filtered voice spoken by a human). It should be borne in mind that the confusion between humans and machines could have multiple consequences such as attachment, influence, or reduction of the value of being human.' As regards Implanted human-AI interfaces such as Elon Musk's Neuralink brain implant give rise to a plethora of legal (in particular as regards to (fundamental) rights that protect physical and psychological integrity, autonomy, free will, privacy etc.) and ethical concerns that should be carefully assessed and addressed in order to establish the appropriate regulatory framework. 16 Common denominators for determining the level of impact of AI For all other AI applications, mandatory requirements on issues such as robustness, accuracy and reproducibility, traceability, transparency, human oversight and data governance15 could be set, relative to the degree of impact of the AI-application. We would like to stress again that these mandatory requirements should not be aimed merely at the data used to train and feed the AI-system, but also at the model(s) and algorithm(s) that comprise the system. These requirements should be met prior to the deployment of the system and be maintained throughout the use of the system. We already expressed our concerns on the suggested 'two-factor' approach to identify 'high risk AI' (high risk application + high risk sector) that would then be subject to mandatory requirements. We also expressed our concerns on the list based approach of AI-applications that would be considered high risk 'as is', i.e. irrespective of the sector. For both structures, we fear that multiple AI applications or uses could fall (or actively be kept) outside the scope of the legislative initiative.16 In stead of the two-factor approach, we recommend to determine common denominators for AI applications or uses that are to be considered high risk, or rather medium or high impact, and would fall within the scope of the legal instrument. The following elements could serve as a guidance to set such common denominators: Common denominators should be determined based on the impact of the system on people and/or society, rather than (merely) on the technical characteristics of the system, or the sector in which the system is being used; 'AI impact' is to be considered both at individual and at societal/collective level whereas AI can impact both the individual as well as larger parts of our collective society; Context, severity, scale and likelihood of the impact is important to determine the appropriate and proportionate mandatory requirements, which could result in different requirements for different levels of impact; For high impact AI applications that generate unacceptable risks or pose threats of harm or systemic failure that are substantial, a precautionary and principle-based regulatory approach should be adopted; For medium impact AI applications a risk-based approach could be more appropriate. On Option ""A combination of either or all of the previous options"" As can be concluded from the above, ALLAI would be most in favor of a combination of the policy options described in the Inception Impact Assessment. In short we would envisage the following structure: Soft law for low impact AI applications, including voluntary labelling EU instrument with mandatory labelling covering: 15 We have elaborated on these requirements in our Final Analysis of the EU Whitepaper on AI, ALLAI ( 16 Final Analysis of the EU Whitpaper on Artificial Intelligence, ALLAI ( 17 o Clear restrictions, conditions, safeguards and/or boundaries for a limited number of exceptionally impactful AI-applications or uses; o Mandatory requirements for medium to high impact AI based on common denominators to determine the level of impact. On: ""Ex-ante and/or ex-post enforcement mechanisms"" While ex-ante and ex-post enforcement mechanisms are necessary, in our opinion, trustworthy AI primarily needs an ex-durante mechanism to ensure a continuous, systematic, socio-technical governance approach, looking at the technology from all perspectives and through various lenses. This requires a multidisciplinary approach where policy makers, academics from a variety of fields (AI, data-science, law, ethics, philosophy, social sciences, psychology, economics, cyber security), social partners and NGO s work together on an ongoing basis. It also requires socio-technical governance processes and structures that facilitate: Understanding of the potential legal, ethical and social effects of the AI-system and improve the governance/legislative choices based on that understanding; Monitoring of the workings and developments of AI to address and mitigate any new ill effects. We thus call for a European AI Authority, as part of a global stucture of AI Authorities as mentioned above under B.C. Preliminary Assessment of Expected Impacts We agree that SME's should not be excluded from the application of the regulatory framework, precisely because of the high scalability of AI-systems. As regards societal impact, we would like to draw attention to the fact that is still not clear whether AI will cause major loss of jobs or that this loss outweighs the number of new jobs it could bring. The maintenance or acquisition of AI skills is necessary in order to allow people to adapt to the rapid developments in the field of AI is therefore important. But policy and financial resources will also need to be directed at education and skills development in areas that will not be threatened by AI systems (i.e. tasks in which human interaction is vital, such as public interest services related to health, safety and wellbeing of people and based on trust, where humans and machines cooperate, or tasks we would like human beings to continue doing). As for environmental impacts, we expect that if energy regulation would be adapted to also cover AI, legislation could have a positive environmental impact. 18 Authors Virginia Dignum is professor of Artificial Intelligence at Ume University, program director of the Wallenberg AI, Autonomous Systems and Software Program Humanities and Society (WASP-HS), co-founder of ALLAI, member of the European High Level Expert group on AI and of the World Economic Forum AI Board, and currently working as an expert advisor for UNICEF. Catelijne Muller is co-founder and president of ALLAI, member of the European High Level Expert group on AI, Rapporteur on AI for the European Economic and Social Committee, and currently working as an expert advisor for the Council of Europe on AI & Human Rights, Democracy and the Rule of Law. Andreas Theodorou is postdoctoral researcher on Responsible AI at Ume University, member of the AI4EU consortium, member of the external ethics board of the ROXANNE project, and committee member on the IEEE Standards Association P70xx series of standards on AI. ALLAI refers to Stichting ALLAI Nederland, a foundation under Dutch Law. No entity or person connected to ALLAI, including its Board Members, Advisory Board Members, employees, experts, volunteers and agents, is responsible or liable for any direct or indirect loss or damage suffered by any person or entity relying wholly or partially on this communication. 2020 ALLAI, The Netherlands",en,"As for existing legislation, we call for a broad legal AI stress test, because we see a large number of additional legal lacunae where it comes to AI, that were not mentioned in the Inception Impact Assessment, such as the GDPR, law enforcement, competition law, transportation, trade of dual use technology, medical devices, energy and the environment, to name a few. 8 GDPR Law Enforcement Competition law Transportation Trade of dual-use technology Consumer protection Healthcare Energy & Environment Ad GDPR A much cited existing EU regulation in the context of AI is the GDPR. Ad Energy & Environment The High Level Expert group on AI (HLEG AI) has argued that sustainability and ecological responsibility of AI systems should be encouraged and has made ""Societal and Environmental Well-being"" one of the 7 requriements for Trustworthy AI of its Ethics Guidelines for Trustworthy AI. According to the HLEG AI, it must be ensured that AI-systems operate in the most environmentally friendly way possible. via a critical examination of the resource usage and energy consumption during training, opting for less harmful choices. Measures securing the environmental friendliness of AI systems entire supply chain should be encouraged. This could trigger a necessery review and possible adaptation of EU engery regulations such as Directive 2010/30/EU on the indication by labelling and standard product information of the consumption of energy and other resources by energy-related products. As for environmental impacts, we expect that if energy regulation would be adapted to also cover AI, legislation could have a positive environmental impact.",risk
Anonymous,F550936,10 September 2020,,,,"FEEDBACK ON THE INCEPTION IMPACT ASSESSMENT Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence September 10, 2020 Virginia Dignum Catelijne Muller Andreas Theodorou 2 Executive Summary First and foremost, we would like express our support for the European Commission's efforts to establish an appropriate regulatory framework for AI. In establishing such a framework, one should both look at existing laws and regulations and determine if their are 'fit for purpose' for a world with AI as well as consider establishing new rules where current legislation is not adequate. In general, we recommend to broaden the description of the problem that the initiative aims to tackle, i.e. addressing a number of ethical and legal issues raised by AI, and include ""societal issues raised by AI"". In the same spirit, we recommend to broaden the description of the ultimate policy objective of the proposal, i.e. to foster the development and uptake of safe and lawful AI that respects fundamental rights across the Single Market by both private and public actors while ensuring inclusive societal outcomes, so as to include ""fair societal outcomes"". The issue of defining the scope of a new legislative initiative for AI is the core element that needs to be addressed. Whereas the Inception Impact Assessment mentions a number of AI-techniques that either should or should not be covered by the instrument, we would like to recommend a different approach toward defining the scope of the instrument: an approach that looks at the level of impact of the technology on people and society at large, rather than (merely) on the technical specifications of a particular AI-system. An impact-level based approach lowers the risk of loopholes that could be exploited. As for existing legislation, we call for a broad legal AI stress test, because we see a large number of additional legal lacunae where it comes to AI, that were not mentioned in the Inception Impact Assessment, such as the GDPR, law enforcement, competition law, transportation, trade of dual use technology, medical devices, energy and the environment, to name a few. The Inception Impact Assessment lays down 5 policy options ranging from keeping the 'Baseline scenario' to a combination of several policy options. In its Whitepaper on Artificial Intelligence, Europe took a clear stance on AI; foster uptake of AI technologies, underpinned by what it calls an ecosystem of excellence , while also ensuring their compliance with to European ethical norms, legal requirements and social values, an ecosystem of trust . The Inception Impact Assessment of a ""Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence"" now presents a number of objectives and policy options. This paper provides feedback to these objectives and policy options. By Virginia Dignum, Catelijne Muller and Andreas Theodorou 3 ALLAI would be most in favor of a combination of the policy options 2, 3a and 3b as described in the Inception Impact Assessment. This combination would entail soft law for low impact AI applications (or uses) including volulntary labelling, and EU instrument with mandatory labelling covering two elements: (i) clear restrictions, conditions, safeguards and/or boundaries for a limited number of exceptionally impactful AI-applications or uses and (ii) mandatory requirements for medium to high impact AI based on common denominators to determine the level of impact. Finally, we call for an ex-durante (which would include ex-ante and ex-post mechanisms) mechanism to ensure a continuous, systematic, socio-technical governance approach, looking at the technology from all perspectives and through various lenses. For this we recommend to set up European AI Authority as part of a global framework of AI Authorities. A. Context, Problem Definition and Subsidiarity Check Defining AI for regulatory purposes While we realize that the Inception Impact Assessment is not the place to provide for a definition of AI, it rightly identifies the need to define the scope of the initiative as a core question that needs to be answered. We would like to make some remarks on the issue of defining AI for regulatory purposes. As we already indicated in our ""Analysis of the EU Whitepaper on Artificial Intelligence""1, we want to emphasize that AI is more than data and algorithms, powered by Computer Processing Power (CPU). While this is the case for the most widely used AI-systems at present, this is only a very limited description of what AI is. AI is a container term for many computer applications, some of which combine data and algorithms, but other, non-data-driven AI approaches, also exist, e.g. expert systems, knowledge reasoning and representation, reactive planning, argumentation and others. This should be kept in mind when defining the scope of the initiative, to avoid over- or underinclusion. In the same analysis we have elaborated on the fact that there is no universally accepted definition of AI and how we would describe AI technically. Nevertheless, it should be noted that many of the wide applied AI systems are indeed examples of data-driven AI, that have a number of typical charachteristics that can make them brittle, unstable and unpredictable (such as opacity, correlation in stead of causality, insufficient or low quality data, unclear 'goals', a lack of common sense, etc.). 1 4 Legal versus technical definition - a need to look at the impact of AI It is important to realize that legal definitions differ from purely technical definitions whereas legal definitions should meet a number of different or additional requirements2 such as inclusiveness, preciseness, comprehensiveness, practicability, permanence, some of which are legally binding, and some are considered good regulatory practice3. In general, we feel obliged to emphasise that the focus on a definition through any attempts at defining AI-techniques in order to determine what is and what is not AI creates loopholes that could be exploited. This brings us to a number of important groundrules that should be guiding AI regulation and how one should set the scope of such regulation or, in other words, define AI for regulatory purposes: We recommend to focus on the effects and impact of the systems, not on a particular AI-technology/technique. AI-systems are more than just the sum of their technical or software components. AI systems also comprise the socio-technical system around it. When considering regulation, the focus should not just be on the technology, but more on the social structures around it: the organisations, people and institutions that create, develop, deploy, use, and control it, and the people that are affected by it, such as citizens in their relation to governments, consumers, workers or even entire society. An ""AI lifecycle approach"" should be followed, that considers not only the development stage of AI, but also the deployment and use stages. It should be kept in mind that most AI-applications currently being used could enshrine, exacerbate and amplify the impact on existing laws and fundamental rights as well as society at scale, affecting larger parts of society and more people at the same time. 2 A Legal Definition of AI Jonas Schuett Goethe University Frankfurt September 4, 2019 (Legal definitions must be: (i) inclusive: the goals of regulation must not over- or under-include. (Julia Black. Rules and Regulators. Oxford University Press, [32] Robert Baldwin, Martin Cave, and Martin Lodge. Understanding Regulation: Theory, Strategy, and Practice. Oxford University Press, 2nd edition, ); (ii) Precise: it should be clear which case falls under the definition and which does not; (iii) Comprehensive: the definition should be understandable by those who are regulated; (iv) Practicable: legal professionals should be able to easily determine whether a case falls under the definition; (v) Permanent: the need for continued legal updating should be avoided. 3 Inclusiveness can be derived from the principle of proportionality in EU law (art. 5( of the Treaty on European Union. The criteria precision and comprehensiveness are based on the principle of legal certainty in EU law. The criteria practicability and permanent are considered good legislative practice. ALLAI Project: Defining AI for Regulatory Purposes ALLAI is currently evaluating the feasibility of setting up a project called ""Defining AI for Regulatory Purposes"". Working with a small group of experts and scientists from different backgrounds the project aims to provide guidance for the open core question relating to the scope of the initiative, notably how AI should be defined. 5 Problem the initiative aims to tackle On: ""Ultimate objective"" ALLAI for the main part supports the ultmate policy objective of the proposal, i.e. to foster the development and uptake of safe and lawful AI that respects fundamental rights across the Single Market by both private and public actors while ensuring inclusive societal outcomes. ALLAI would however advise to broaden the final part of the ultimate policy objective to not only ensure inclusive societal outcomes, but to also ensure ""fair societal outcomes"". As mentioned in the Inception Impact Assessment, the complexity (and many times opacity) of certain systems and granular applicability of outcomes to individuals in combination with the scalability of AI systems, presents a range of difficulties as regards enforcement of existing legislation meant to protect human rights and could generate new safety risks. On top of that, these characteristics could enshrine, exacerbate and amplify these risks and adverse impacts on society at scale, affecting more people at the same time. As such there is a serious risk that unfair societal outcomes become ever more enshrined, exacerbated and amplified, thus potentially leading to wider and deeper societal gaps between groups of people, propagating inequality and, as a consequence, entrenching political polarisation. The addition of fairness to ultimate policy objective would also reflect the European Commissions' adoption of the 7 Requirements of the High Level Expert Group on AI, i.e. requirement no. Inclusiveness, non-discrimination and fairness. On: ""Harm caused by AI-systems and risks not covered by existing legislation"" We agree with the indication that harm caused by the use of AI may be the consequence of multiple causes. We would like to stress however that the causes for harm go beyond just flaws in the technical and digital components (including data) and charachteristics of the system. While these flaws do often play a major and sometimes decisive role in causing harm, one should not forget that even the most technically robust systems can still cause harm. Imagine a facial recognition system that does recognize people of all colors, genders, ages, etc. correctly. Technically such a system could be considered robust and non-discriminatory. Lawfully however, the system could still cause harm. AI-driven (mass) surveillance with facial recognition, involves the capture, storage and processing of personal (biometric) data (our faces), but it also affects our 'general' privacy, identity and autonomy in such a way that it creates a situation where we are (constantly) being watched, followed and identified. As a psychological chilling effect, people might feel inclined to adapt their behaviour to a certain norm, which shifts the balance of power between the state or private organisation using facial recognition and the individual. 6 In legal doctrine and precedent the chilling effect of surveillance can constitute a violation of the private space, which is necessary for personal development and democratic deliberation. Even if our faces are immediately deleted after capturing, the technology still intrudes our psychological integrity. This is just one example where the mere technical elements of a system are less relevant as regards harm than the actual use of the system. That is why we advocate to continuously ask question zero : Do we want to allow this particular AI-system and technique in the first place, or are there reasons not to allow its use at all? And if we were to consider implementing, deploying and using such a system, what are the conditions we should set for its use? In either case, for the purposes of any regulatory framework we should not merely focus on technical solutions at dataset or algorithm level, but devise socio-technical processes that help us: a) Understand the potential legal, ethical and social effects of the AI-system and improve our design and implementation choices based on that understanding; b) Audit our algorithms and their output to make any undesirable outcomes transparent; and c) Continuously monitor the workings of the systems to mitigate the ill effects of AI. On: ""Risks not adequately covered by existing legislation"" ALLAI agrees with the Inception Impact Assessment that there are ample risks that are not adequately covered by existing legislation on cybersecurity, protection of employees and anti-discrimination. The Inception Impact Assessment identifies as main issues: Effective enforcement of existing EU rules to protect fundamental rights Application of EU rules on safety Application of EU the rules on liability AI & the impact on fundamental rights ALLAI strongly commends the focus on the protection of fundamental rights and would like to draw your attention to a report it delivered to the Council of Europe on the ""Impact of AI on Human Rights, Democracy and the Rule of Law"".4 This report identifies those human rights, as set out by the European Convention on Human Rights (""ECHR""), its Protocols and the European Social Charter (""ESC""), that are currently most impacted or likely to be impacted by AI. It aims to provide a number of possible strategies that could be implemented simultaneously, ranging from addressing the impact within the existing framework of human rights, democracy and the rule of law to establishing new human rights should the existing framework fail to adequately protect us. 4 7 While states are obliged to protect individuals and groups against breaches of fundamental rights perpetrated by other actors, appreciation of non-state actors influence on human rights has steadily grown.5 As (large) tech companies have now become operators that are capable of determining and perhaps altering our social and even democratic structures, the impact of their AI(-use) on fundamental rights becomes more prevalent. In this respect, AI might serve as a good opportunity and think of a structure that goes beyond the 'horizontal effect' of the EU Charter of Fundamental Rights of the European Union. Such a structure could entail a legal obligation for private actors to comply with fundamental rights and to grant access to justice if they fail to do so.6 AI & the impact on work The Inception Impact Assessment mentions that there are legal issues as regads the protection of employees, but does not yet specify these issues. We would like to stress that he introduction and use of AI in the workplace can cause effects as to health and safety in the workplace, job security, worker privacy, the balance between worker and employer and so on. For that reason, we have been advocating early and close involvement of workers and service providers of all types, including freelancers, the self-employed and gig workers not just people who design or develop AI, but also those who purchase, implement, work with or are affected by AI systems. Social dialogue must take place before the introduction of AI technologies in the workplace, in line with the relevant applicable national rules and practices. Additionally, we would like to draw special attention to AI used in hiring, firing and worker assessment and evaluation processes. The White Paper on AI mentions AI used in recruitment as an example of a high-risk application that would be subject to regulation irrespective of the sector. We recommend extending this use to include AI used in firing and in worker assessment and evaluation processes, but also to explore the common characteristics of AI applications that would make for a high risk use in the workplace, irrespective of the sector. AI applications that have no scientific basis, such as emotion detection through biometric recognition, should not be allowed in workplace environments. Additional legal lacunae In addition to the issues identified in the Inception Impact Assessment, ALLAI also identifies a number of additional regulatory lacunae related to existing legislation. As AI is evolving quickly and the wider impact of AI on the full acquis has still not been fully identified, these lacunae are not exhaustive but below are the areas for attention to the extent that they can be identified today7: 5 Business and Human Rights, A Handbook for Legal Practitioners, Claire Methven O Brien, Council of Europe 6 This also means going beyond merely referring to the Recommendation CM/Rec(3 on human rights and business of the Committee of Ministers of the Council of Europe (and the UN Guiding Principles on Business and Human Rights) 7 See also: STOA Policy Briefing: Legal and ethical reflections concerning robotics, The Scientific Technopolis Group, Maastricht University, Scientific Foresight Unit (DG EPRS) of the European Parliament. 8 GDPR Law Enforcement Competition law Transportation Trade of dual-use technology Consumer protection Healthcare Energy & Environment Ad GDPR A much cited existing EU regulation in the context of AI is the GDPR. This regulation provides frameworks for data protection, the right to an explanation of AI decisions and safeguards for the use of biometric recognition. Biometric recognition: It should be noted that GDPR restricts the processing of biometric data only to some extent. Biometric data according to the GDPR is personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, which allow or confirm the unique identification of that natural person. The last part of the sentence is crucial, because if biometric recognition is not aimed at identification (but for example at categorization, profiling or affect recognition), it might not fall under the GDPR-definition. In fact, recital 51 of the GDPR says that 'the processing of photographs [is considered] biometric data only when processed through a specific technical means allowing the unique identification or authentication of a natural person.' Many biometric recognition technologies are however not aimed at processing biometric data to uniquely identify a person, but merely to assess a person s behaviour (for example in the classroom) or to categorize individuals (for example for the purpose of determining their insurance premium based on their statistical prevalence to health problems). These uses might not fall under the definition of biometric data (processing) in the GDPR. Right to explanation of ADM, including profiling: Particularly with regard to the right to an explanation of automated decisions, a debate is underway about whether the GDPR gives a right to an explanation of automated decisions or not. Under the GDPR, controllers who use personal data to make automated decisions are obliged to inform individuals in advance and to provide meaningful information about the logic and importance of the decision-making and the consequences for the data subject. The so-called art. 29 Working Group recognizes that ""the growth and complexity of machine learning can make it challenging to understand how automated decision-making or profiling works,"" but that, despite this, ""the company [must] find simple ways to tell the individual about the reason, or the criteria on which the decision is based, without necessarily always attempting a complex explanation of the algorithms used or disclosure of the full algorithm."" 9 Also, the GDPR requires a controller to implement appropriate safeguards when designing automated decisions, such as the right to human intervention and the right to express his or her point of view and contest the decision. The recitals of the GDPR also include the right to an explanation of a fully automated decision. It is however debatable whether there is a full right to an explanation of an automated decision in all cases. The fact that the right to explanation is included only in the recitals seems to be a hurdle that can be overcome, but in particular the fact that the right to explanation only exists as regards to fully automated decision-making, makes it insufficient to ensure adequate transparency in the automated decision-making process. After all, a single human 'check' on an automatic decision (regardless of whether this person has been able to judge the decision on its merits, could lead to the conclusion that an explanation within the meaning of the GDPR would not be necessary. It must also be assessed whether the GDPR offers sufficient protection when the decision is based on non-personal data. AI-driven profiling and the categorization of people or groups of people is regularly being done by finding inferences made about an individual, even without using personal data or resulting in identification of a person. There is no consensus whether these inferences in itself should count as personal data, but there are experts arguing that all data processing that has an impact on people should be protected8 or even that the distinction between personal and non-personal data should be suspended9. Here the issue also ventures from the GDPR into other fundamental rights such as the right non-discrimination, covered in multiple EU directives and the ECHR. Ad Law enforcement In line with the above, EU regulation such as the Police Directive (EU 2016/, that regulates the processing of personal data in particular for the purpose of profiling by law enforcement in the Member States, does not adequately cover the issue of profiling that is based on mere inferences rather than personal data, as described above. Ad Competition Many AI-applications are developed and deployed by a handful of private actors. These actors are also present in multiple market segments that are related to AI or use AI, such as finance, insurance, etc. If too much market power in these different markets is concentrated in a few companies, this could lead to unfair competition and difficulty for new (smaller) players to enter the market. 8 Purtova ( 9 Koops ( 10 Ad Transportation While autonomous driving has not yet fully materialized, the autonomous vehicles industry is well underway to making vehicles behave more autonomous and needing less 'active involvement' from their drivers. These developments call for a critical review of the various EU regulations and directives that deal with transportation, such as Regulation (EC) 561/2006 and Regulation (EEC) 3821/85 regarding driving and resting times and digital tachygraphy (for truck platooning), Directive 2014/45/EU on Roadworthiness, Directive 2010/40/EU on Intelligent Transport Systems in the field of road transport and for interfaces with other modes of transport and Directive 2003/59/EC on training and initial qualifications of professional drivers10. Ad Trade of dual-use technology Following an impact assessment in 2016, a new reform process was started to (a.o.) future-proof the export control regime for rapidly developing emerging technologies. In sum, the main goal of this reform is to control the export of information technologies that can be used for the suppression of human rights, thus increasing the scope and scale of dual-use governance. In 2016 the focus was on the inclusion of so called cyber-surveillance technologies 11, but recently AI applications such as facial recognition have also entered this debate as a possible next step for dual-use regulation. Ad Consumer protection At this point, consumers do not receive adequate protection against unacceptable impact.12 Many of these protections would need to be better covered in different existsing legislative instruments. As an example, the EU legal protection of consumers against unfair AI-driven personalized pricing is mostly principle-based, leading to uncertainty on its interpretation as long as there is no clarification in case law.13 Ad Health In the Medical Devices Regulation software is considered a medical device, making AI-driven applications in healthcare subject to certain requirements and a mandatory labelling scheme. Alignment of this regulation with a new legislative instrument on AI is important. 10 Also: Directive 2009/103/EC on motor vehicle insurance; Directive 2007/46/EC on vehicle approval; Directive 2006/126/EC on requirements for driving licences 11 Immenkamp, 2019 12 BEUC, EU Rights for Consumers 13 de Streel, Alexandre; Jacques, Florian ( : Personalised pricing and EU law, 30th European Conference of the International Telecommunications Society (ITS): ""Towards a Connected and Automated Society"", Helsinki, Finland, 16th-19th June, 2019, International Telecommunications Society (ITS), Calgary 11 Also, now that the Medical Devices Regulation has been postponed to enter into effect in May 2021, existing legislation, and the current practice of assessment and certification of AI-diven healthcare devices, should be critically looked at. Ad Energy & Environment The High Level Expert group on AI (HLEG AI) has argued that sustainability and ecological responsibility of AI systems should be encouraged and has made ""Societal and Environmental Well-being"" one of the 7 requriements for Trustworthy AI of its Ethics Guidelines for Trustworthy AI. According to the HLEG AI, it must be ensured that AI-systems operate in the most environmentally friendly way possible. The system s development, deployment and use process, as well as its entire supply chain, should be assessed in this regard, e.g. via a critical examination of the resource usage and energy consumption during training, opting for less harmful choices. Measures securing the environmental friendliness of AI systems entire supply chain should be encouraged. This could trigger a necessery review and possible adaptation of EU engery regulations such as Directive 2010/30/EU on the indication by labelling and standard product information of the consumption of energy and other resources by energy-related products. AI stress test for EU regulation The foregoing calls for a much broader AI stress test for existing EU regulation. Three questions need to be answered in particular: To what extent are the policy and legal objectives underlying these regulations affected by AI systems and in what ways? What are the existing monitoring, information gathering and enforcement frameworks capable of providing meaningful and effective oversight to ensure that policy and legal objectives are still effectively achieved? To what extent does existing legislation work in a way that they promote and safeguard the ethical principles and requirements (as described in the AI HLEG's Ethics Guidelines for Trustworthy AI)? ALLAI points out that the AI HLEG has already advised the European Commission to perform this 'stress test' at EU level, in order to arrive at an unambiguous regulatory framework across Europe. It should be noted that EU legislation does not affect all national legislative areas. Where the EU has no legislative competence, ALLAI advises the Dutch legislator to initiate the stress test at national level. Think of elements of labor law, social security law, administrative law and criminal law. 12 Basis for EU intervention ALLAI fully agrees with the Inception Impact Assessment's reasoning that the objectives described cannot be reached effectively by Member States alone, but can be better reached at Union level. We agree that it is important that fragmantation would prevent the free circulation of goods and services containing AI and thus negatively affect the Digital Single Market. Fragmentation would also lead to divergence in levels of protection of citizens and society against the abovementioned harms and risks, which could lead to an unacceptable ""race to the bottom"" of AI regulation and protections in an effort to attract more AI investment. B. Objectives and Policy Options Objective ALLAI supports the overall objective of the instrument, i.e. to ensure the development and uptake of lawful and trustworthy AI across the Single Market through the creation of an ecosystem of trust. ALLAI would like to suggest the following additional aims of the initiative: To fill any legal lacunae either regarding the effectivenes, applicabilty or enforceability of existing EU law and where no EU law exists, so as to ensure that overall EU policy and legal objectives as regards trustworthy AI are promoted and safeguarded; In addition to what is expressed in aim (c) we strongly recommend to broaden this aim so as to include risks for people and society. As part of aim (c) and aim (e) it is important to be able to effectively monitor the future developments of AI within and outside the Single Market, so as to make sure that new opportunities for trustworthy AI are identified and promoted, but also that new challenges are adequately and timely addressed. In addition to aim (e), we suggest to set up a structure that includes not only the relevant authorities in the Member States, but also all relevant stakeholders, such as workers' and business' representatives, other civil society organisations, NGO's, academia (various disciplines), policy makers, etc. As an additional aim we reccommend to set up a European AI Authority as part of a global framework of AI Authorities. Such a framework could be set up as folows: o A global AI Authority; o Several regional sub-authorities (e.g. EU, the Americas, Asia, Oceanea, Middle East); o National executive authorities (either existing or new), for the EU to be appointed or set up by the Member States. 13 The roles and responsibilities of these authorities should be carefully considered, but they should have broad expertise of the different elements and impact domains of AI, including but not limited to technical, legal and ethical expertise, as well as knowledge of behavioural effects, labour market effects, economic and societal effects of AI. Policy Options On Option ""Baseline ALLAI agrees that the current ""Baseline"" or Option ""0"" does not suffice to adequately address the risks and potential harms connected to AI and thus considers this option not viable. On Option ""EU soft law"" The option of EU soft law should only apply to AI-systems or uses that have no adverse impact on people, society or the environment nor on our (fundamental) rights, democracy and the rule of law. The two factor approach that was suggested by the European Commission in its Whitepaper on AI does however not suffice to determine these types of AI-systems. As an example consider targeted online advertising. The Commission will likely qualify advertising as a low-risk sector, and AI-driven targeted adds as a low risk AI-application. Targeted advertising however, has shown to have a potential segregating and dividing effect. This is the reason why we recommend looking at the level of societal or personal impact of an AI-system or use to determine the risk level of the system. On Option ""EU legislative instrument setting up a voluntary labelling scheme"" The labelling scheme is not a standalone option, but could be split into two options, the first to be joined with option 1 and second to be joined with option Where EU soft law would suffice (option , a voluntary labelling scheme could be an interesting addition for industry players to gain competitive advantage, or to feel confident to define or explore a niche area of application. Where an EU legislative instrument establishing mandatory requirements for AI is necessary (option , a mandatory labelling scheme (like the CE-marking) could be considered to avoid any untrustworthy AI being deployed on the Digital Single Market. Current practices around certification of AI, such as those already executed in the healthcare sector, should be carefully reviewed, to see if those practices are sufficient, or should be amended or replaced by a new mandatory labelling scheme to avoid overlap with possible new labelling scheme. 14 Labels should not merely refer to the technical characteristics of the system, but more importantly also to the effects and impacts of the system. On Option ""EU legislative instrument establishing mandatory requirements for AI"" ALLAI is in favour of introducing an EU legislative instrument, in the form of a combination of sub-options (a) and (b). Exceptionally impactful AI The following AI-systems or uses that are considered to be too impactful could give rise to the necessity of a ban, moratorium, strong restrictions or conditions for exceptional and controlled use: Indiscriminate use of facial recognition and other forms of biometric recognition either by state actors or by private actors; AI-powered mass surveillance (using facial/biometric recognition but also other forms of AI-tracking and/or identification such as through location services, online behaviour, etc.); Personal, physical or mental tracking, assessment, profiling, scoring and nudging through biometric and (online) behaviour recognition in violation of fundamental rights (AI-enabled Social/Citizen Scoring); Covert AI systems and deep fakes; Implanted human-AI interfaces; Exceptional use of these technologies, such as for national security purposes or medical treatment or diagnosis, could be allowed but should be evidence based, necessary and proportionate and only be executed in controlled environments or cleary identified contexts and (if applicable) for limited periods of time. As regards biometric recognition14 (including facial recognition) we recommend that any use of biometric recognition only be allowed under the followoing cumulative conditions: i) there is a scientifically proven effect; (ii) it is used in controlled environment (e.g. a hospital); (iii) it is used under strict conditions (e.g. limited in time, for a specific purpose, etc.). Widespread and/or public use of AI-driven biometric recognition to surveil, trace, track, assess or categorise humans or human behaviour or emotions should not be allowed. As regards AI-driven mass surveillance we refer to the recommendation of the HLEG AI in its Ethics Guidelines for Trustworthy AI that automatic identification raises strong concerns of both a legal and ethical nature, as it may have an unexpected impact on many psychological and sociocultural levels. 14 Biometric recognition of micro-expressions, gait, (tone of) voice, heart rate, temperature, etc. is being used in various ways, one of which is to assess or even predict our behaviour, mental state, and emotions. As Barret et al. (Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements, have shown however, no sound scientific evidence exists to suggest that a person's inner emotions or mental state can be accurately 'read' from their facial expression, gait, heart rate, tone of voice or temperature, let alone that (future) behaviour could be predicted by it. 15 A proportionate use of control techniques in AI is needed to uphold the autonomy of European citizens. Clearly defining if, when and how AI can be used for mass surveillance both by public or private actors, differentiating between the identification of an individual versus the tracing and tracking of individual, will be crucial for the achievement of Trustworthy AI. As regards AI-enabled Social Scoring we refer to the recommendation of the HLEG AI in its Ethics Guidelines for Trustworthy AI, that 'any form of citizen scoring should only be used if there is a clear justification, and where measures are proportionate and fair. Normative citizen scoring (general assessmentof moral personality or ethical integrity ) in all aspects and on a large scale by public authorities or private actors endangers these values, especially when used not in accordance with fundamental rights, and when used disproportionately and without a delineated and communicated legitimate purpose.' The HLEG AI also argues 'that citizen scoring on a larger or smaller scale is already often used in purely descriptive and domain-specific scorings (e.g. school systems, e-learning, and driver licences). Even in those more narrow applications, a fully transparent procedure should be made available to citizens, including information on the process, purpose and methodology of the scoring.' It also notes that 'mere transparency cannot prevent non-discrimination or ensure fairness, and is not the panacea against the problem of scoring and that ideally the possibility of opting out of the scoring mechanism without detriment should be provided otherwise mechanisms for challenging and rectifying the scores must be given. This is particularly important in situations where an asymmetry of power exists between the parties. Such opt-out options should be ensured in the technology s design in circumstances where this is necessary to ensure compliance with fundamental rights and is necessary in a democratic society.' As regards covert AI systems and deep fakes we again refer to the recommendation of the HLEG AI in its Ethics Guidelines for Trustworthy AI that 'human beings should always know if they are directly interacting with another human being or a machine, and it is the responsibility of AI practitioners that this is reliably achieved. AI practitioners should therefore ensure that humans are made aware of or able to request and validate the fact that they interact with an AI system (for instance, by issuing clear and transparent disclaimers ). Note that borderline cases exist and complicate the matter (e.g. an AI-filtered voice spoken by a human). It should be borne in mind that the confusion between humans and machines could have multiple consequences such as attachment, influence, or reduction of the value of being human.' As regards Implanted human-AI interfaces such as Elon Musk's Neuralink brain implant give rise to a plethora of legal (in particular as regards to (fundamental) rights that protect physical and psychological integrity, autonomy, free will, privacy etc.) and ethical concerns that should be carefully assessed and addressed in order to establish the appropriate regulatory framework. 16 Common denominators for determining the level of impact of AI For all other AI applications, mandatory requirements on issues such as robustness, accuracy and reproducibility, traceability, transparency, human oversight and data governance15 could be set, relative to the degree of impact of the AI-application. We would like to stress again that these mandatory requirements should not be aimed merely at the data used to train and feed the AI-system, but also at the model(s) and algorithm(s) that comprise the system. These requirements should be met prior to the deployment of the system and be maintained throughout the use of the system. We already expressed our concerns on the suggested 'two-factor' approach to identify 'high risk AI' (high risk application + high risk sector) that would then be subject to mandatory requirements. We also expressed our concerns on the list based approach of AI-applications that would be considered high risk 'as is', i.e. irrespective of the sector. For both structures, we fear that multiple AI applications or uses could fall (or actively be kept) outside the scope of the legislative initiative.16 In stead of the two-factor approach, we recommend to determine common denominators for AI applications or uses that are to be considered high risk, or rather medium or high impact, and would fall within the scope of the legal instrument. The following elements could serve as a guidance to set such common denominators: Common denominators should be determined based on the impact of the system on people and/or society, rather than (merely) on the technical characteristics of the system, or the sector in which the system is being used; 'AI impact' is to be considered both at individual and at societal/collective level whereas AI can impact both the individual as well as larger parts of our collective society; Context, severity, scale and likelihood of the impact is important to determine the appropriate and proportionate mandatory requirements, which could result in different requirements for different levels of impact; For high impact AI applications that generate unacceptable risks or pose threats of harm or systemic failure that are substantial, a precautionary and principle-based regulatory approach should be adopted; For medium impact AI applications a risk-based approach could be more appropriate. On Option ""A combination of either or all of the previous options"" As can be concluded from the above, ALLAI would be most in favor of a combination of the policy options described in the Inception Impact Assessment. In short we would envisage the following structure: Soft law for low impact AI applications, including voluntary labelling EU instrument with mandatory labelling covering: 15 We have elaborated on these requirements in our Final Analysis of the EU Whitepaper on AI, ALLAI ( 16 Final Analysis of the EU Whitpaper on Artificial Intelligence, ALLAI ( 17 o Clear restrictions, conditions, safeguards and/or boundaries for a limited number of exceptionally impactful AI-applications or uses; o Mandatory requirements for medium to high impact AI based on common denominators to determine the level of impact. On: ""Ex-ante and/or ex-post enforcement mechanisms"" While ex-ante and ex-post enforcement mechanisms are necessary, in our opinion, trustworthy AI primarily needs an ex-durante mechanism to ensure a continuous, systematic, socio-technical governance approach, looking at the technology from all perspectives and through various lenses. This requires a multidisciplinary approach where policy makers, academics from a variety of fields (AI, data-science, law, ethics, philosophy, social sciences, psychology, economics, cyber security), social partners and NGO s work together on an ongoing basis. It also requires socio-technical governance processes and structures that facilitate: Understanding of the potential legal, ethical and social effects of the AI-system and improve the governance/legislative choices based on that understanding; Monitoring of the workings and developments of AI to address and mitigate any new ill effects. We thus call for a European AI Authority, as part of a global stucture of AI Authorities as mentioned above under B.C. Preliminary Assessment of Expected Impacts We agree that SME's should not be excluded from the application of the regulatory framework, precisely because of the high scalability of AI-systems. As regards societal impact, we would like to draw attention to the fact that is still not clear whether AI will cause major loss of jobs or that this loss outweighs the number of new jobs it could bring. The maintenance or acquisition of AI skills is necessary in order to allow people to adapt to the rapid developments in the field of AI is therefore important. But policy and financial resources will also need to be directed at education and skills development in areas that will not be threatened by AI systems (i.e. tasks in which human interaction is vital, such as public interest services related to health, safety and wellbeing of people and based on trust, where humans and machines cooperate, or tasks we would like human beings to continue doing). As for environmental impacts, we expect that if energy regulation would be adapted to also cover AI, legislation could have a positive environmental impact. 18 Authors Virginia Dignum is professor of Artificial Intelligence at Ume University, program director of the Wallenberg AI, Autonomous Systems and Software Program Humanities and Society (WASP-HS), co-founder of ALLAI, member of the European High Level Expert group on AI and of the World Economic Forum AI Board, and currently working as an expert advisor for UNICEF. Catelijne Muller is co-founder and president of ALLAI, member of the European High Level Expert group on AI, Rapporteur on AI for the European Economic and Social Committee, and currently working as an expert advisor for the Council of Europe on AI & Human Rights, Democracy and the Rule of Law. Andreas Theodorou is postdoctoral researcher on Responsible AI at Ume University, member of the AI4EU consortium, member of the external ethics board of the ROXANNE project, and committee member on the IEEE Standards Association P70xx series of standards on AI. ALLAI refers to Stichting ALLAI Nederland, a foundation under Dutch Law. No entity or person connected to ALLAI, including its Board Members, Advisory Board Members, employees, experts, volunteers and agents, is responsible or liable for any direct or indirect loss or damage suffered by any person or entity relying wholly or partially on this communication. 2020 ALLAI, The Netherlands",en,"As for existing legislation, we call for a broad legal AI stress test, because we see a large number of additional legal lacunae where it comes to AI, that were not mentioned in the Inception Impact Assessment, such as the GDPR, law enforcement, competition law, transportation, trade of dual use technology, medical devices, energy and the environment, to name a few. 8 GDPR Law Enforcement Competition law Transportation Trade of dual-use technology Consumer protection Healthcare Energy & Environment Ad GDPR A much cited existing EU regulation in the context of AI is the GDPR. Ad Energy & Environment The High Level Expert group on AI (HLEG AI) has argued that sustainability and ecological responsibility of AI systems should be encouraged and has made ""Societal and Environmental Well-being"" one of the 7 requriements for Trustworthy AI of its Ethics Guidelines for Trustworthy AI. According to the HLEG AI, it must be ensured that AI-systems operate in the most environmentally friendly way possible. via a critical examination of the resource usage and energy consumption during training, opting for less harmful choices. Measures securing the environmental friendliness of AI systems entire supply chain should be encouraged. This could trigger a necessery review and possible adaptation of EU engery regulations such as Directive 2010/30/EU on the indication by labelling and standard product information of the consumption of energy and other resources by energy-related products. As for environmental impacts, we expect that if energy regulation would be adapted to also cover AI, legislation could have a positive environmental impact.",risk
European Tech Alliance (Belgium),F550909,10 September 2020,Business association,Micro (1 to 9 employees),Belgium,"European Tech Alliance Feedback on The European Commission's Inception Impact Assessment ""Artificial intelligence - ethical and legal requirements"" September 2020 1In February 2020, members of the European Tech Alliance (EUTA) joined forces to publish the EUTA High Level Principles on AI ahead of the publication of the European Commission s White Paper on AI. EUTA members strongly believe the EU has the potential to become a world leader in AI. Europe benefits from a vibrant ecosystem of top academic talent, leading AI research labs and an ever growing number of AI-driven start-ups. This fruitful ecosystem is supported by industry best practices and the strong fundamentals of the EU's regulatory architecture.Against this backdrop and our contribution to the public consultation on the AI White Paper last Spring, we welcome the opportunity to share the following comments on the European Commission s inception impact assessment Artificial Intelligence Ethical and Legal requirements Our key recommendations 2We advocate for a soft-law policy response combined with a targeted, risk-based regulatory framework focused on clearly defined high-risk applications. We agree with the Commission that any new rules governing AI should not impede on the freedom to conduct a business, right to property or freedom of science on concerned entities. We agree with the Commission s assessment that mandatory requirements would entail some costs depending on the nature of the requirements, both in terms of changed business practices and ex-ante or ex-post compliance. We have strong reservations about extending legal requirements to all AI applications, including low-risk applications, as this could over-regulate and harm innovation and investment into AI technologies. We urge the EU institutions to refrain from regulation of low-risk AI. High-risk AI applications should be clearly and narrowly defined by legal standards based on an impartial, regulated and external assessment.Regarding the possibility of a requirement on businesses to conduct a new risk assessment to define whether they are using low- or high-risk AI applications, we support self-regulatory principles for accountability and transparency to help businesses tread market opportunities and the possibility for unfair bias and discrimination. We share the Commission s concerns about significant knock-on effects on SMEs if heavy AI requirements were introduced. We call on the Commission to provide a proportionate legal framework that is nurturing for scale-ups. Imposing a one-size-fits-all regulation on all businesses would seriously endanger innovation and European competitiveness in this field, especially for low-risk AI applications where development costs would become prohibitive. The EU is well-placed to lead on AI by providing EU-wide coordination and guidance. A harmonised, well-targeted EU framework would reduce compliance costs for businesses operating across the EU and avoid a plethora of divergent AI rules at the national level (for instance, the recent French CNIL Algorithm report included some proposals around AI ethical requirements which could conflict with GDPR rules and cover both high-risk and low-risk applications). We also need to ensure that the future EU framework is sufficiently flexible so that it is not outdated in a short span of time We disagree with the Commission s assessment that the social impact of the future AI framework will be limited: not all algorithms are created equally, with many bringing tangible social benefits to consumers and citizens. In addition, human developers will always develop algorithms with some degree of bias. Therefore, we suggest that the EU encourages self-regulation to mitigate negative algorithmic bias, with businesses maintaining the flexibility to choose measures that will deliver the best outcomes. We agree to some extent with the Commission s initial assessment that no direct significant environmental impacts are expected from the proposed AI measures. Yet it is important to flag that any additional requirements to control bias or risk could lead to additional data being collected, requiring further storage and energy consumption from our data centers - expanding CO2 emission levels. At the same time, the potential for significant positive effects environmentally and socially should not be discounted, especially as part of the Commission s Impact Assessment underpinning legislation.The European Tech Alliance (EUTA) brings together and gives a voice to the major European digital champions, scaleups and leading startups. We believe that Europe is good at tech and our sector is driving jobs and growth across the continent. With an overarching goal of fostering innovation in Europe, EUTA members are keen to provide expert insights to the EU institutions and promote the EU competitiveness in the global tech space. This paper has been developed at a preliminary stage in the policy discussions in order to share our members expertise and inform the debate. It is not directly attributable to any individual member and we invite you to contact our members, should you like to better understand their specific situation. 34",en,"We agree to some extent with the Commission s initial assessment that no direct significant environmental impacts are expected from the proposed AI measures. Yet it is important to flag that any additional requirements to control bias or risk could lead to additional data being collected, requiring further storage and energy consumption from our data centers - expanding CO2 emission levels. At the same time, the potential for significant positive effects environmentally and socially should not be discounted, especially as part of the Commission s Impact Assessment underpinning legislation.The European Tech Alliance (EUTA) brings together and gives a voice to the major European digital champions, scaleups and leading startups.",risk
Keidanren (Japan),F550832,10 September 2020,Business association,Medium (50 to 249 employees),Japan,"1 Proposal for a legal act of the European Parliament and the Council laying down requirements for Artificial Intelligence Comments on Inception Impact Assessment September 10, 2020 AI Utilization Strategy Task Force Committee on Digital Economy Keidanren Page 2 At the same time, AI may generate new safety risks for users and third parties, which are not yet explicitly tackled clearly by the product safety legislation. For example, in principle stand -alone software is not explicitly covered by EU product safety legislation, with the consequence that the risks generated by the probabilistic nature of AI are not yet clearly and specifically addressed by existing safety rules. Additionally , such legislation focuses on safety risks present at the time of placing the product on the market and presupposes static products, while AI systems can evolve. In addition to generating new safety risks for user and third parties, the lack of clear saf ety provisions tackling such risks may give rise to: legal uncertainty for businesses that are marketing their products involving AI in the Union, as well as for those using such products in their own processes, and challenges for market surveillance and supervisory authorities which may find themselves in a situation where they are uncertain whether they can intervene, because they may not be empowered to act and/or may not have the appropriate tools and means to inspect AI -enabled systems. Specific c hallenges on product safety are currently also being addressed by other ongoing initiatives, such as the revisions of the Machinery Directive and of the General Product Safety Directive. The Commission will ensure coherence and complementarity between thos e initiatives and this initiative. When considering the introduction of new regulations, full attention must be given to existing regulations and system s, and they must not be extend ed to AI technology embedded in hardware, including software and services . Changes in the 2 targets of regulation will mean AI system developers be ing held accountable for issues they cannot be directly involved with, and this may undermine the development and utilization of AI systems. Since existing safety regulations have not envisioned application to AI technology, a solution to the issue of lack of explicit rules to address new safety risks will take time. Discussion s on this issue need to take into account a balance between the benefits for society as a whole including developers, providers, deployers, and users of AI systems and the possible risks. Page 3 More specifically, the aims are: (a) to ensure the effective enforcement of rules of existing EU law meant to protect safety and fundamental rights and avoid illegal discrimination by ensuring the relevant documentation for the purposes of private and public enforcement of EU rules; (b) to provide legal certainty for businesses that are marketing their AI -enabled products or using such solutions in the EU as regards the rules applicable to such products and services; (c) to prevent where possible or to minimise significant risks for fund amental rights and safety; (d) to create a harmonised framework in order to reduce burdensome compliance costs derived from legal fragmentation, which could jeopardise the functioning of the Single Market; For business operators, it is desirable that the legal position of their businesses is clear, that the credibility of AI -related products and services is enhanced by legal underpinning, and that compliance cost is reduced through the creation of a harmonized framework. We, therefore, welcome the discussi on in the inception impact assessment. A discussion on accountability, which is inevitable in the process of introducing regulations, should give consideration to the areas of AI utilization, social impact, and other background factors. Imposing uniform accountability rules for businesses operating in diverse areas may hinder AI utilization in multiple areas and is, therefore, undesirable. 3 There are cases where i t may not be possible to sort out all AI -related rights and obligations with legal provision s and cases where comprehensive categorization will be difficult. Even in such cases, the legal stability of businesses and the safety of users can be ensured through the clarification of rights and obligations by the parties in a contract. It is desirable to hold discussions on new rules while paying due attention to security and safety guaranteed by voluntary and flexible free contracts by the parties involved. Page 4 Alternative options to the baseline scenario Striking a balance between innovation and regulation is important for the realization of the interests of an advanced and highly credible data -driven society. Likewise, it is desirable to ensure the harmony of international rules to prevent unnecessary fractionaliza tion. Lack of clarity on what is allowed and not allowed under the regulations is one obstacle to the development and deployment of AI solutions in Europe. Concerns about proof of full compliance with regulations have undermin ed many business negotiations with both government and the private sector. Basically, it is desirable not to complicate the legal systems developers must comply with, such as by regulating areas that can be regulated by existing rules and systems under these frameworks (e.g. regulatin g AI used in medical equipment through regulations on medical equipment) . New regulations should be limited to the minimum required, and when considering the introduction of such regulations, compatibility with existing rules and systems should be ensured, and particular attention needs to be paid to prevent ing overlapping admini stration. Page 4 ( Option EU soft law (non-legislative) approach to facilitate and spur industry - led intervention (no EU legislative instrument) With regard to areas that existing rules and systems are unable to regulate, 4 considering policies with the soft law approach is effective. Under the current situation where it is still premature to define and regulate high -risk AI, imposing prior regulations on AI -enabled products and services without any explicit basis may hinder innovation that will co ntribute to industrial development and help resolve social issues in Europe. Therefore, it is better to promote industry -led measures and enhance the credibility of AI in the market by developing a joint government -private sector scheme for the appropriate evaluati on of the voluntary steps taken by businesses. Compatibility with international standards is necessary for the voluntary steps initiated by the industrial sector to be widely recognized and adopted. Continuous investigation, discus sion, and improvement for all stakeholders are necessary in the process of determining policies to address the risks relat ing to the development and utilization of AI applications. Corporate self -governance is particularly important for the protection of fundamental rights. Page 5 ( Option EU legislative instrument establishing mandatory requirements for all or certain types of AI applications (see sub -options below). a. As a first sub -option, the EU legislative instrument could be limited to a speci fic category of AI applications only, notably remote biometric identification systems (e.g. facial recognition). Without prejudice to applicable EU data protection law, the requirements above could be combined with provisions on the specific circumstances and common safeguards around remote biometric identification only. A consensus has yet to be reached on the definition and correct understanding of biometric data, such as face recognition and dactyloscopy (fingerprint ) data, which has been identified as high-risk AI application, and legal systems pertaining to its utilization have not been establish ed. It is necessary to clarify the definition of the scope and uses of systems such as remote biometric identification and biometric authentication , as well as the difference between the two , ensuring that this issue is discussed carefully, in order not to restrict utilization by the private sector 5 unnecessarily. The creation of practical guidelines is necessary to clarify the conditions and operational requirements for using remote biometric identification systems. The guidelines must also define remote biometric identification systems and classify their utilizati on methods. Page 5 b. As a second sub -option, the EU legislative instrument could be limited to high-risk AI applications, which in turn could be identified on the basis of two criteria as set out in the White Paper (sector and specific use/impact on ri ghts or safety) or could be otherwise defined. Evaluation as high-risk must be based on the current discourse and definition of risk at institutions deliberating international standards. Uniform regulations must not be imposed on diverse sector -specif ic definitions and methods for risk assessment and management, and measures based on existing regulations in each sector should be considered. While the criteria for defin ing high-risk AI set out in the European Commission s white paper are indispensable for the realization of credible AI as an issue for the future , technical validation o f these criteria is still very difficult at present . It must also be noted that in cases where the risks of systems using high -risk AI can be fully eliminat ed or mitigat ed through physical safety measures and operations, the criteria defining high -risk AI must not be applied uniformly. Therefore, it is desirable to adopt a step -by-step approach to arrive at a realistic timetable and criteria based on a road m ap to be drawn up after consultations with experts. The substance of the road map and the standards must be consistent with and conform to European as well as world standards. Page 5 c. In a third sub -option, the EU legislative act could cover all AI applications. There are many AI applications, such a s those relating to the optimization of 6 production proce sses and energy use , that do not pose any risks to the fundamental rights and safety being considered in this assessment. The imposition of uniform criteria on all applications will result only in demerits, such as increased cost, for such applications . Therefore, Sub -option 3 under Option 3 is inappropriate. Page 5 ( Option combination of any of the options above taking into account the different levels of risk that could be generated by a particular AI application. The standards for fairness, safety, and quality differ for each country, culture, sector, user, and so forth. Therefore, it is necessary to make appropriate choices of measures to be adopted for each AI technology and sector, such as the soft law approach and measures ba sed on existing regulations, in order to be able to adapt flexibly to AI and other rapidly evolving new technologies. In the introduction of regulations, consistency and conformity with laws recognized as a vital framework for the privacy and security of European citizens personal data, which are also regarded as models by many countries, particularly the General Data Protection Regulation ( GDPR ), is very important. In order to promote consistency and conformity , cooperation with the European Data Protection Board (EDPB ) and national and international data protection bodies in the implementation of such regulation s is advisable. Page 5 The public intervention may however impose additional compliance costs, in so far as the development of some AI sy stems may have to account for new requirements and processes. If compliance costs outweigh the benefits, it may even be the case that some desirable AI systems may not be developed at all. The imposition of conditions on companies developing AI applications that constitute a burden on them may have economic consequences , including weakened international competitiveness of EU companies due to the delay in the launch of products and services in the EU market compared to other economic zones, or even failure to launch them. It must also be noted that the result may impede innovation. 7 It must be noted that even with uniform rules for EU members, lack of conformity with international regulations , which are important for businesses engaged in global operations, will increase compliance cost . Page 6 The assessment will also have to consider which measures a responsible economic operator would take even without explicit public intervention. The extent of the economic benefits depends, all other thing s being equal, on the increase in trust. Other things being equal, users will have more trust when they can rely on legal requirements, which they can enforce in courts if need be, than if they have to rely on voluntary commitments Regardless of whether there is explicit public intervention, s ince AI undergoes model changes through learning after the start of service, even if measures that need to be taken by the responsible operators at the start of service are anticipated, these operators may still face unforeseen litigation risks as a result of subsequent model changes, thus compromising the social benefits AI would have generated . For this reason, the terms that suppliers of products and services need to abide by should be clarified to ensure legal cer tainty. Page 6 Due to the high scalability of digital technologies, small and medium enterprises can have an enormous reach, potentially impacting millions of citizens despite their small size. As stated elsewhere in this document, the extent of the impact of AI on society, the environment, basic human rights, and so forth cannot be measured by the capital or number of employees of the corporate developers but is largely determined by the number of users and the utilization methods of the products and services. Therefore, it is not necessarily appropriate to use the size of the corporate developer or whether it is a small or medium -sized enterprise as a criterion for determining whether it is subject to regulation. Page 6 Likely social impacts 8 We hope for greater credibility of AI applications in society and greater social acceptance of these applications. It must be noted that t he imposition of conditions on companies developing AI applications that constitute a burden on them may prevent users in EU nations from enjoying the benefits of cutting -edge technology due to delay in the launch of products and services in the EU market compared to other economic zones, or even failure to la unch them , with the result of jeopardizing innovation. Page 7 Impact assessment The completion of the impact assessment is scheduled for December The start time for the impact assessment of this initiative should be specified. Furthermore, th is assessment should be implemented with ample lead time, taking into account the effect of the COVID -19 pandemic, and with sufficient opportunities to engage in dialogue with the industrial sector.",en,"There are many AI applications, such a s those relating to the optimization of 6 production proce sses and energy use , that do not pose any risks to the fundamental rights and safety being considered in this assessment.",risk
EnBW Energie Baden - Wrttemberg AG (Germany),F550748,09 September 2020,Company/business,Large (250 or more),Germany,"As EnBW Energie Baden-W rttemberg AG we welcome the approach of European Commission to conduct an Impact Assessment concerning ethical and legal requirements for AI. For several years already we have developed AI solutions in the energy sector. As operator and service provider in the field of critical infrastructures, we at EnBW operate multiple self-developed AI services already productively. These include AI-assisted overhead power line inspection by using drones, predictive maintenance of offshore wind turbines, securing public places by means of AI-assisted barrier systems, automatic detection of road damage and detection of damage to gas and water pipes. Lately we have been awarded AI-Champion of Baden-W rttemberg - as EnBW we would like to share our expertise in an ecosystem of excellence in the EU. Our view on the outlined alternative options to the baseline scenario: Option 3c would most probably be an overall inhibitor as from the start. 3b puts up heavy hurdles for some selected sectors. As outlined in the white book on AI, the energy sector would suffer under this form of general suspicion of being a high-risk sector (white paper on AI, page . The first criterion already gives rise to fears of bureaucracy, while being too broad at the same time. In the coming years we need room for innovation instead of discrimination of individual sectors. For a risk-assessment of particular applications, a clear definition of high-risk would be very much needed from an industrial perspective. Rather soft measures (e.g. guidelines or other approaches which leave room for innovation) should be preferred before introducing bureaucratic certification processes. Concerning the outlined alternative options to the baseline scenario, we would advocate option A voluntary labelling scheme would complement the existing EU legislation, which already sets high standards for example concerning liability (e.g. the product safety directive for products) or personal data (GDPR). A good example to operationalise AI ethics using labels can be found under: Similarly, page 19 of the German report on the data ethics commission lines this out. ( Because of the diversity of products, services, customers and processes, it would be best to have individual labels and industry-inspired processes to qualify. Common guidelines might be helpful. For customers and employees it is important to understand the path of getting to a label transparency is important, explainability creates trust.",en,"For several years already we have developed AI solutions in the energy sector. As outlined in the white book on AI, the energy sector would suffer under this form of general suspicion of being a high-risk sector (white paper on AI, page .",risk
Enel SpA (Italy),F550740,09 September 2020,Company/business,Large (250 or more),Italy,"Dear Members of DG CNECT .A.2, Enel SpA, a multinational company in the energy sector, highly appreciates the EC proposal for a legal act laying down requirements for Artificial Intelligence. At Enel, we use Artificial Intelligence and technology to make the energy and power systems more efficient, more predictable and more sustainable, making easier for our customers to interact with us and to play a more active role in the energy system, mainly in the liberalised energy market. The European digital strategy released on February this year, stresses that citizens need to be able to trust the technology itself as well as the way in which it is used. The European Commission White Paper on AI, maps out various policy options and reveals that clear and mandatory requirements would in principle apply only to AI systems or applications which are considered high-risk , i.e. employed in sectors where significant risks can be expected to occur, or used in such a manner that significant risks are likely to arise. In line with the non-binding Ethics Guidelines by the AI HLEG, the White Paper on AI suggested that the mandatory requirements for high-risk systems could cover the following aspects: Training data; Data and record-keeping; Information to be provided; Robustness and accuracy, with an ex-ante consideration of the potential risks; Human oversight; Specific requirements for certain AI applications, such as remote biometric identification. Considering this and welcoming a risk-based approach, Enel advices the European Commission that the risks are in the infringement, not in the technology or in the sector and, recommends to assess and establish the characteristics of different types of risks and threats with a sector-by-sector approach, requiring inter alia a sectoral data protection assessment. Within this view, the different classes of use-cases must be integrated. It is worth noting that in the energy sector, which is considered high-risk according to section C of the White Paper, all its AI based applications cannot systematically be categorised such as high-risk . For instance, an application utilised to interact online with clients about customer care activities will normally not pose risks of such significance to justify, at this time, legislative intervention. Possible mandatory legal requirements to be imposed on high-risk applications should be carefully evaluated before establishing them as (ex-ante) obligations for businesses. Enel believes also necessary to better define high-risk and the methodology to assess it. Poor categories and definitions might deter private investments and become a competitive disadvantage to European companies. Given the actual economic crisis together with the financial risk nature of research and innovation activities, Enel urges the Commission to build a financial structure to back organisations involved in the mentioned high-risk sectors. If the EU system is not supportive towards innovation, the opportunity for innovations to come from Europe will be reduced. Therefore, the Enel Group promotes the development of long-term European action plans on digital policy, which should be evidence-based, promote security by design, guarantee high data quality standards, promote ethical evaluation for different use cases and enable new technologies and business models, without imposing unnecessary burdens and costs nor obstacles to innovation.",en,"Dear Members of DG CNECT .A.2, Enel SpA, a multinational company in the energy sector, highly appreciates the EC proposal for a legal act laying down requirements for Artificial Intelligence. At Enel, we use Artificial Intelligence and technology to make the energy and power systems more efficient, more predictable and more sustainable, making easier for our customers to interact with us and to play a more active role in the energy system, mainly in the liberalised energy market. It is worth noting that in the energy sector, which is considered high-risk according to section C of the White Paper, all its AI based applications cannot systematically be categorised such as high-risk .",risk
BEUC - The European Consumers Voice (Belgium),F550609,09 September 2020,Consumer organisation,Small (10 to 49 employees),Belgium,"0 Contact: Ernani Cerasaro digital @beuc.eu BUREAU EUROP EN DES UNIONS DE CONSOMMATEURS AISBL | DER EUROP ISCHE VERBRAUCHERVERBAND Rue d Arlon 80, B -1040 Brussels Tel. +32 (2 743 15 90 EC register for interest representatives: identification number -45 Co-funded by the European Union Ref: BEUC -X-2020-049 - 12/06 /2020 BEUC S RESPONSE TO THE EUROPEAN COMMISSION S WHITE PAPER ON ARTIFICIAL INTELLIGENCE The Consumer Voice in Europe 1 Why it matters to consumers Artificial Intelligence (AI) and Algorithmic -based Decision Making (ADM) applications are already shaping consumers lives. For example, online video platforms use algorithms to personali se users content and recommendations ; banks use them to track suspicious activities and prevent fraud ; public authorities make use of AI and ADM to process an d answer citizen requests ; smart phones integrate virtual personal assistants and social media applications organi se the feeds that con sumers see in their timeline on the basis of automated analysis of their past behaviour, online activities and interactio ns. We are still just at the beginning of the digital transformation of our societ y. While AI may offer many innovative opportunities for consumers, its widespread use brings profound social, legal and economic challenges affecting consumers and the entire society. A strong regulatory framework is necessary to ensure that the use of AI is adequately r egulated and controlled . It should facilitat e innovation and guarantee that consumers can fully reap the benefits of the digital transformation of our societie s but are protected against the risks posed by AI . A REGULATORY FRAMEWORK FOR AI AND ADM ................................ The scope o f the EC proposal and its risk -based approach ................................ Specific requirements for high -risk applications ................................ THE WAY FORWARD ................................ ................................ A precautionary approach and more gradual establishment of risks and corresponding legal requiremements ................................ ................................ ................................ Approach to data management and control must favour consumers and public interest 12 AI must not further entrench digi tal commercial surveillance ................................ Consumers must have a strong set of rights ................................ ................................ A strong and streamlined approach to sustaina bility and environmental protection is needed ................................ ................................ ................................ ................................ Liability rules must be updated to ensure compensation in case of harm arising out from AI-powered products ................................ ................................ ................................ Existing legislation must be updated to ensure consumers are adequately protected A coherent oversight, enforcement and redress system is necessary Con trol and oversight ................................ ................................ Accountability and transparency ................................ ................................ Enforcement ................................ ................................ Remedies ................................ ................................ ................................ . 18 VOLUNTARY LABELLING SYSTEM AND LOW -RISK APPLICATIONS BIOMETRIC TECHNOLOGIES ................................ ................................ 2 Summary of recommendations In response to the European Commission s White Paper on Artificial Intelligence, BEUC make the following recommendations to desig n a regulatory framework for AI and ADM which responds to consumers needs and expectations: The definition of AI provided in the White paper should be refined and align ed with the one agreed by the AI H igh Level Expert Group ( HLEG )In addition, we recommend the use of terms such as Algorithmic -based Decision Making (ADM), robotics or algorithmic systems, depending on the context and on the technology. The proposed risk-based approach for the development of the new legal framework on AI and ADM should be revise d and broaden ed: New rules should not only cover applications considered to be high -risk . A broader, more inclusive, approach should be envisaged. Legal obligations should gradually increase alongside the identified level of risk, starting fro m the principle that some basic obligations (e.g. regarding transparency) should be applicable to all AI applications. From there, the greater the potential of algorithmic systems to cause harm, the more stringent the legal requirements. The n ew rules should apply to algorithmic systems, including AI, machine learning, deep learning, ADM and robotics regardless of the level of risk . The new framework should be applicable where consumers are users of or subject to an algorithmic system, irrespective of the p lace of establishment of the entities developing and/or depl oying the system. The n ew rules should also encompass provisions on th e admissibility and design of algorithmic systems ; organisational and technical safeguards ; and establish an institutional str ucture for effective supervision and enforcement . The process which determines the level of risk of an application (in form of a n impact assessment) must be trustworthy , verifiable and object ionable. Such impact assessment should take into account the pos sible risks arising throughout the whole life cycle of the system for both individuals and society at large . Enforcement authorities should be tasked to propose and update valid methodologies for assessing the level of risks and potential harms . When prop osing legislation on AI and ADM, the Commission should adopt a precautionary approach . We consider this to be essential to ensure that technologies that pose significant harms for individuals and society are not deployed until they are tested and certified . As an ultima ratio measure, it sho uld be possible to ban the use of certain AI or ADM systems. Self-assessment of compliance with the 1 BEUC is a member of the European Commission s High Level Expert Group on AI . To download the definition: 3 new rules by operators should be in principle avoided, at least for the application presenting a high level of risk. Consumers should have control of their data when it is used by AI and ADM products and services. In particular, consumers must know how their data is processed through enhanced transparency provisions and should be able to manage the processing through user -friendly interfaces . The Commission should enshrine a set of AI rights for consumers in any future regulation. This set of rights should at least include: right to transparency, explanation, and objection; right to accountability and control; right to fairness; right to non- discrimination; right to safety and security; right to access to justice; right to reliability and robustness. While highlighting the need to protect consumers, the white paper lacks specific initiatives for mitigating the negative consequences of the widespread use of algorithmic systems on consumers fundamental rights and wellbeing. In particular, we urge the Commission to specifically address in any future regulation the negative effects of businesses large-scale commercial surveil lance of consumers and its potential influence on their online and offline choices and behaviours. AI has the potential to help achieve the green transition but also comes with a big environmental footprint. We urge the European Commission to explore the opportunities offered by AI but also to consider the environmental harms such as carbon -dioxide emissions and electronic waste resulting from the data-driven infrastructures needed to power the large -scale deployment of AI and ADM powered products and services. We recommend incentivising the use of greener infrastructures for the development and deployment of these technologies so that they support the achievement of sustainable development, climate neutrality and circular economy goal s. Any future reg ulation should envisage a coherent and efficient compliance and enforcement system which: Obliges b usiness es to ensure built -in control mechanism s for the development and use of ADM systems . Ensure s a high level of protection for consumers via a combinatio n of independent ex -ante verification mechanisms and continued ex -post compliance checks by authorities in presence of high -risks applic ations . Ensures a coherent structure and harmonised procedures for authorities to deal with pan -European/cross -border i nfringements. Guarantees the active cooperation among the relevant enforcement authorities, as well as between public and private enforcement bodies, including consumer organisations. Ensures that enforcement authorities are equipped with the necessary fi nancial, technical, and human resources, as well as the necessary legal powers, to do their job efficiently. Provides the e nforcement authorities with the necessary powers (e.g. right to obtain information, the right to inspect and access) so that they can scrutinise and evaluate these ADM systems and impose penalties i n case of la w infringements. 4 Ensure s that companies are transparent about their use and expected results of ADM systems and processes and build in specific interfaces in order to allow author ities to exercise meaningful oversight and ultimately enforce the rules (compliance by design ). Ensures the availability of effective remedies for consumers and the accessibility of procedures to claim the violations of their rights through the use of ADM systems and AI technologies. An updated liability framework for digital goods and services is urgently needed to ensure effective access to justice for consumers when things go wrong with their products . In particular , we call for a sound revision of the Product Liability Directive2. An updated legal framework on consumer protection , including safety legislation , is equally needed to ensure that consumers are fully protected against the risks create d by AI products and services. In particular, we urge the Commission to moderni se the General Product Safety Directive (GPSD) . 2 For more info on our position on product liability, please refer to our recent position paper Product Liability EU rules fit for consumers in the digital age , published in May 2020 . 5 Introduction On 19 February 2020, the European Commission published a White Paper on Artificial Intelligence: A European approach to excellence and trust . The aim of the Commission is to launch a European strategy promoting the uptake of AI and addressing the risks associated with certain AI applicat ions. Europe s ambition is to become a global leader in innovation in the data economy and its applications by fostering a development of an AI ecosystem which profits citizens, business and public sector. The White Paper identifies two main elements allowing for such an ecosystem to arise : excellence and trust . For the creation of an ecosystem of excellence , the Commission focuses on concrete actions to support research, development and uptake of AI across the EU economy and public administration . For the creation of an ecosystem of trust , the Commission builds on the AI High Level Expert Gro up ""Ethics Guidelines for Trustworthy Artificial Intelligence "", published in April These guidelines identify seven key requirements for the development of trustworthy AI applications: human agency and oversight ; technical robustness and safety ; privacy and data governance ; transparency ; diversity, non-discrimination and fairness ; societal and environmental wellbeing ; accountability. The Guidelines, however, are not legally binding . In light of this, and in line with the Commission President s political guid elines , the White Paper recognises th e need for a European regulatory framework which would build trust among consumers and businesses, and therefore speed up the uptake of the concerned technolog ies. The Commission s White Paper sets out a first outline of a possible new regulatory framework which is based on a risk -based approach. BEUC agrees that a risk -based approach is appropriate. However, we are concerned that an approach which, as envisaged by the Commission, focuse s solely on high risk app lications would significantly reduce the scope of the new rules and ultimately inadequately protect consumers. While AI applications are already subject to European legislation inter alia on data protection, privacy, non -discrimination, consumer protectio n, product safety and liability, the existing regulatory framework is not fit for purpose to address the risks posed by AI . Therefore additional measures are needed3. Consumers expect effective protection and respect of their rights whether or not a product or service relies on AI. Recent develop ments following the COVID19 pandemic have brought AI to the spotlight once again, highlighting its potential to improve health treatments for exam ple. While we recognise that AI has a lot of positive potential , we would like to highlight that the current situation does not change the fact that AI comes with many challenges and risks which require the use of this technology to be properly regulated. 3 We already expressed some of our concerns in other position papers: AI rights for consumers ; Automated decision makin g and Artificial Intelligence ; AI must be smart about our health ; Access to consumers' data in the digital economy ; When innovation means progress - BEUC s view on innovation in the EU . 6 PROBLEM DEFINITION: WHAT IS AI? Before addressing the practical and regulatory implications of AI, we must agree on its definition. As a starting p oint, it is worth highlight ing that there is no common nor legal definition of AI , and that the definition provided by the Commission in its White Paper should be considered overly simplistic. For the sake of straightforwardness, we often refer to ""AI syst ems"", AI applications , uses of AI , and similar. The concept of AI is blurry and can embrace different perspectives. The reason is quite simple: there are thousands of different techniques currently used to develop very complex and (partially) autonomous technologies that can fall into the artificial intelligence basket . At a regulatory level there are different definit ions of AI being used4. In the European regulatory framework, a first definition was provided in the ""Ethics guidelines for trustworthy AI"", published by the European Commission AI High-Level Expert Group5 (AI HLEG) in April Based on thi s definition, AI can be either a system (software or possibly hardware) or a scientific discipline. In the first case, such a system should have: a human mind that designs the technology; a given dataset (structured or unstructured); a complex goal to be achieved aut onomously; a reasoning on the knowledge acquired from the dataset; a scientific technique to be applied; the capability to behave ; the capability to adapt to external reaction on its previous actions. However, i n an attempt albeit underst andable to provide a simple and straightforward definition, in its White Paper, the Commission states that: Simply put, AI is a collection of technologies that combine data, algorithms and computing power . As is evident from the abovementioned definiti on adopted by the AI HLEG, this wording cannot be considered sufficient to define AI and the Commission should take utmost account of this. The White Paper s definition, in fact, could almost be applied to any software ever written. This definition lacks crucial components of what AI is. By only referring to data and algorithms in combination with computing power it does not explain the behavioural characteris tics of AI and it overlooks the social and human context where AI technology is created7. It does not explain its purpose: Is AI modelling human behaviours? Is it modelling human thoughts? Is it a model that can behave intelligently? Is it a mix of all of this? Having a solid definition is crucial and has major regulatory consequences . If we were to follow the definition provided in the White Paper at regulatory level , all the concepts that descend from it could be questioned by simply arguing that a specific application cannot be considered as AI. Such a definition would allow organi sations to easily bypass and circumvent future regulations and would ultimately lead to lack of accountability. 4 See, for example, US FUTURE of Artificial Intelligence Act of 2017 . 5 To download the definition: 6 A first draft of the Guidelines then subject to public consultation - was published in December -single -market/en/news/ ethics -guidelines -trustworthy -ai 7 Yoshua Bengio, one of the godfathers of AI, defined it as: [AI is] about making computers that can help us that can do the things that humans can do but our current computers can t . Although it is not adaptable to a re gulatory instrument, this definition allows us to understand why and how AI was born and developed. 7 Then, if we want to use AI as a term, it is first of all appropriate to debate on a valid and comprehensive definition which could encompass present and future applications creating risks for individuals (persons or legal entities), for society at large and for specific social groups. For the time being, our proposal is to use more specific terms depending on the context, not reducing everything to AI. Thus , it seems that the use of algorithmic based decision making (ADM hereinafter) is more aligned with the regulatory objectives HLEG of the white paper and more suitable to define its action field. ADM is a technology neutral term, that includes the technologies that the AI HLEG and the public generally referred to as artificial intelligence. In the same line, the German data ethics commission chose to focus on algorithmic systems , rather than artificial intelligence . An ADM system comprises much more than just program code or an algorithm. It refers to the entire process from data acquisition and data analysis to the interpretation of the results and the derivatio n of a decision or recommendation from the results8. ADM systems are characteri sed by the fact that they contain an algorithmic component (control system) which produces an output (decision) on the basis of an input and outputs it in the form of a (numeric al) value. As such ADM -Systems also include learning systems that derive decision rules from data by means of machine learning and can adapt them over time. The systems discussed under the keyword artificial intelligence (AI) usually fall under this defi nition. The key element of the term ADM-System is its relevan ce from a policy point of view , as i t stresses the element that the system produces an output that is used to prepare or make a decision that has an impact on people or legal entities. A REGULA TORY FRAMEWORK FOR AI AND ADM The scope of the EC proposal and its risk -based appr oach Consumers concerns in relation to AI and ADM systems range from the lack of transparency, to concerns about safety, unintended consequences and malicious uses. For exam ple, as shown in a survey commissioned by our German member Verbraucherzentrale Bundesve rband (vzbv) automated decisions are regarded as a risk for 75% of consumers if the underlying data and principles applied are unclear9. The new regulatory framework fo r AI and ADM must properly address the whole set of consumer concerns and ensure th at this technology is developed and deployed in a manner that embeds strong and tangible safeguards during its whole lifecycle. Such safeguards should be ensured to anyone w ho is affected by an ADM system . To ensure a trustworthy development of AI technologies, the White Paper refers to the non - legally binding requirements stipulated by the Ethical Guidelines of the AI HLEG: human agency and oversight ; technical robustness a nd safety ; privacy and data governan ce; transparency ; diversity, non -discrimination and fairness ; societal and environmental wellbeing ; accountability. Although these points certainly contribute to shape more trustworthy technologies, the se guidelines are not legally binding. To date, there is no specific legal framework at EU level aimed at regulating AI. That being said, AI applications are in certain instances already subject to a range of existing laws (e.g. data protection and consumer protection legis lation), as it happens with any other products or services falling into the scope of such laws. For example, 8 Vieth, Kilian; Wagner, Ben: Teilhabe, aus gerechnet (, URL: - stiftung.de/de/publikationen/publikation/did/teilhabe -ausgerechnet [2019]. 9 8 if the use of a chatbot for customer support in the EU is processing personal data for delivering solutions to the co nsumer ( which means that it ne eds process the customer s information, communications, etc .), such a technology should respect the General Data Protection Regulation (GDPR) . In its White Paper, t he European Commission acknowledges that existing legislation may not be effective or might otherwise be difficult to apply in the case of AI technologies. This principally because of the inner opaqueness of such technologies (so-called ""black box - effect"") , their complexity, volatility and autonomy . The White Paper therefore sets out the possibi lity to adapt existing legislation and add new rules. BEUC welcomes that the European Commission wishes to examine how to adapt existing legislation such as EU legislation on product safety and product liability to ensure an effective consumer protecti on and re -think the allocation of responsibilities between those actors involved in the development and deployment of technologies (developers, business, etc). As BEUC has repeatedly highlighted the need to update the EU s current rules and bring them up t o speed with technological development s. The Commission also i dentifies a potential need for additional regulation to address the risks inherent to the use of AI. For th e purpose of designing these new rules and obligations , the Commission puts forward a r isk-based approach . BEUC considers that new legislation is necessary to address the risks posed by AI and ADM and also that such legislation should adopt a risk -based approach. In this sense, we welcome the direction envisaged by the Commission. We are ho wever concerned about the risk assessment methodology , the risk management a nd the narrow scope of the new legal regime envisaged by the Commission , as explained further below. In particular, we underline that the mere fact that certain applications pose a higher risk than others, doesn t mean that only such riskier applications should be further regulated. The main risks identified by the Commission are related to fundamental rights (in particular , data protection and non -discriminations) and to safety an d the effective functioning of the EU liability regime (e.g. safety risks related to autonomous vehicles an d allocating liability if such car causes an accident). On the basis of these main risks, the White Paper draws a clear -cut line between high -risk AI applications and all other AI applications . The new legal obligations envisaged by the Commission would only apply to high-risk applications . Such applications would face stricter legal requirements , including for example technological conformity assessme nts and, in some cases, mandatory regulatory pre-approval before market deployment. AI applications not considered high risk would be exempted from these new legal requirements, the only additional measure envisaged for such applications would be a volunta ry label ling scheme awarding those which meet certain EU -wide, yet undefined, standards. According to the White Paper , for an AI application to be classified as high -risk two cumulative elements should be present: High -risk sector : the technology is devel oped in a sector where significant risks can be expected . Such high-risk sectors should be specifically and exhaustively individuated by the new legislation and m ight initially include healthcare; transport; energy and parts of the public sector . Such a list should be periodically reviewed and amended where necessary . In addition to these sector -based high -risk applications, the Commission expects exceptional instances [where] the use of AI applications for certain purposes is to be considered as high-risk as such[.] as the use of AI applications for recruitment processes as well as in situations impacting 9 workers rights, specific applications affecting consumer rights and facial recognition technology. High -risk use : high -risk sector techn ologies are used in suc h a manner that significant risks are likely to arise . Such uses include uses of AI applications that produce legal or similarly significant effects for the rights of an individual or a company; that pose risk of injury, death or significant material or immaterial damage; that produce effects that cannot reasonably be avoided by individuals or legal entities . According to the White Paper a sum of the two abovementioned conditions would ensure a narrow scope of application while, at the same time, provid ing the maximum level of legal certainty. Although BEUC share s the view that new additional regulat ion is necessary , we think that the risk-based approach envisaged by the Commission is too narrow in scope and lacks nuance : First, the definition of high -risk provided in the paper is tautological. To define whether there is a high-risk, the elements to be taken into account are still high -risks which remain undefined. We encourage the Commission to redefine such a concept, specifying and elaborating on the concrete and precise factors that would cause risks for individuals and society . In this sense, we would recommend to follow the example of the opinion of the German Data Ethics Commission , according to which a risk -based approach addresses AI applications which are associated with regular or significant potential for harm . Second ly, regulati ng by sectors is confusing and unsuitable . While we acknowledge that some AI applications present higher risks than others, if the binary approach put forward in the White Paper is accepted , we believe that the scope of the regulation is too narrow render any future measures ineffe ctive. It would, in fact, contradict the obligation to provide for a high level of protection for consumers across all sectors . In our opinion , first there is a need for a horizontal intervention that covers all sectors and which possibly takes into accoun t the single specificities of a given sector. When introduced as a cumulative requirement, t he ex -ante identification of high -risk sectors could caus e a dangerous illusion for consumers subject to technologies classified as not risky . For example, AI too ls present in everyday consumers lives such as smart cars, home assistants, drones for delivering , algorithmic selection of social media feeds, music and media streaming would fall outside the scope but also financial services . Although it is true that al l these services may present a different level of risk, it is equally true that it is not possible to exclude that they will fit into a lo w or high risk category only because they are part of a given sector. Furthermore, technologies often do not differ from each other as per sector. We wonder why a certain decision in sector X, adopted on the same parameters and with the same techniques as a decision adopted in sector Y, must be assessed differently if the damage is caused by a biased or inaccurate algorit hm. Lastly, a bifurcation of AI into high and low/no risk as envisaged by the Commission fails to capture the essence of the risks in AI technologies . The starting point should be that AI deployment s can be risky as such but, as they have different intensi ties, can be mitigated. A careful analysis of the actual digital world demonstrates that the deployment of AI technology should be approached cautiously , keeping i n mind the real and present dangers 10 brought by this technology10. Generally speaking, over-stating the potential benefits of AI or having a too narrow approach towards its risks can have very negative consequences . The Commission itself states that AI can generate harm and that such harm can be material (safety and health of individuals, including loss of life, damage to property) and immaterial (loss of privacy, limitations to the right of freedom of expression, human dignity, discrimination for instance in access to employment). We should not lose sight of all these risks and ensure regulation br ings a high level of protection against them. Specific r equirements for high -risk applications The distinction between high -risk and non -high-risk applications envisaged by the Commission has consequences in terms of legal requirements which would be appl icable , depending on which category the application in question falls into. Only h igh-risk AI solutions would be subject to specific requirements in relation to training data , keeping records of data , transparency requirements, r obustness & accuracy , human oversight . In addition, the Commission suggests a prior conformity assessment, po ssibly including procedures for testing and inspection of certification of algorithms and data sets. BEUC welcomes the provision of such requirements which are certainly capa ble of contributing to a more transparent and responsible development of new techn ologies. While waiting for the se requirements to be better specified by the Commission in a clearer regulatory framework, we underline that , we need a gradual approach to r isk assessment and corresponding mandatory requirements for each risk -level. These requirements, should thus not be limited to high risk applications . Even more so if the risk is identified through inaccurate or unclear facto rs. THE WAY FORWARD The future EU law on AI is a crucial test for Europe s digital policy. Europe must design a regulatory framework that ensures that innovation can flourish in a way that is respect ful of individual s fundamental and consumer rights and of our societal values . The Comm ission must ensure it captur es and properly considers all the various aspects relating to the sustainable development and deployment of powerful and intelligent technologies in our markets and society . The narrative throughout the White Paper indicates th at the Commission is aware of th e challenges we are facing and takes them seriously. The two building blocks of the White Paper , an ecosystem of excellence and an ecosystem of trust, are well -grounded. However, there are several elements where greater nuance and a more concrete approach would be required. One of the shortcomings of the White Paper is that while consumers are mentioned in different parts, there is no focus on the specific risks arising for them when using AI and ADM technologies. Algorithmic advanced systems might cause adverse impacts for co nsumers which are not properly addressed in the White Paper, such as discrimination or social and economic exclusion. For example, in the case of big data analytics algorithms are used to create cre dit scores and inform loan screening also via the incorpor ation of non -financial data 10 For example, the U.S. National Institute of Standards and Technology recognises biases in AI facial recognition tools. Recently, a Dutch court stated that bias in systems to detect welfare fraud determined a violate human rights. Racial bias in a health care deli very algorithm was discovered by researchers after the algorithm being used for years. Alike, unfortunately, many other examples could be quoted. 11 sets (such as where people live, internet browsing habits and purchasing decisions). The decisions taken on the basis of the algorithmic reasoning of these systems are la rgely unregulated while they are often discriminatory. How ever, the White Paper does not seem to consider them in its regulatory proposal. Similarly, while the intention behind the human - centric approach of the Commission seems well -defined , there is a lac k of critical socio - political analysis surrounding diversity, equal ity and environment which constitute indispensable aspects for such humanistic development to take place . Finally, we underline the importance of building a regulatory spectrum that enabl es a fairer access to technolog y. Such belief is the building block for ensuring that the interests of marginalised and vulnerable consumers are adequately taken into account. In this sense, we would encourage the Commission to introduce a duty of care for developers in order to deploy consumer -oriented systems. In particular, by incentivising a consumer -centric approach we could safeguard access to all intended users avoiding exclusions especially for those consumers who are currently left behind . In the following sections we describe some of the main elements where we expect the Commission to develop a more detailed and ambitious approach. A precautionary approach and more gradual establishment of risks and corresponding legal requiremements One of the major weakness of the Commission s White Paper is the absence of a precautionary approach to the development and use of AI and ADM systems. It would seem that the intention of the Commission is to allow the development and use of the ADM service s/product s regardless of their riskiness. For example, the word 'ban' is never mentioned in the White Paper except when referring to the work done by the German Data Ethics Commission. We consider it is essential that the assessments with regards to the risks for individuals and society are developed in the form of a preventive impact assessment , as to allow the blocking of highly dangerous technolog ies. The causes of risk must be assessed from t he conceptual phase of the system and must, to the fullest extent po ssible and according to the state of the art knowledge, foresee possible harms during its whole lifecycle. Furthermore, the risks should be considered as a non -exhaustive list and should t ake into account numerous factors, including: the type and nature of the data used (e .g. personal/non personal data); the type of algorithmic model; the types of logical reasoning carried out by the system; the security measures put in place; the methods u sed to test and maintain the system; sectors; harms for the environm ent; the desirable dissemination of the product/service; the business model . The process which determines the level of risk of an application (in a form of a preventive impact assessment) must be trustworthy , verifiable and objectionable. Such preventive i mpact assessment should take into account the possible harms arising throughout the whole life cycle of the system for both individuals and society . Authorities should be task ed to develop a methodology and to set the criteria needed to define the level o f risk of an application. The authorities could refine this set of criteria and the methodology depending on different sectors. The intensity of the risk can go from a low level up to irreversible harm for the individual or the society , in which case the technology should be banned. For example, some companies are investing more and more resources into very intrusive and potentiall y very harmful technologies such as those meant to figure out how to objectively read emotions in people by detecting facial expressions . However, 12 researchers demonstrated that the science of emotion is ill -equipped to support any of these initiatives.11 For this reason we think that business es intending to use such form of emotion recognition should not have access to markets (as already happens) until it is demonstrated that such practices are not harmful and can fulfil the respective risk mitigating requirements . Legal obligations should gradually increase alongside the identified level of potential harm, as described in the German data ethics commission opinion . Approach to d ata management and control must favour consumers and public interest Data is a crucial resource of th e digital economy and it can be indispensable for the development of services and products powered by algorithmic technologies. A future regulatory framework for algorithmic systems should take outmost account of various aspects relating to data and ensure a consumer centric approach to data access and control . Consideration should be given to power asymmetries between institutions, businesses and individuals arising from the growth of digital devices and systems and the rapid expansion of digital data that they generate. Where appropriate, f or instance in situations of individual or collective harm, consideration should be given to whether additional regulatory measu res to those stipulated by the GDPR may be needed to address this (please, also refer to point 4 below) . Second, it is importa nt to ensure that consumers are fully aware of what happens with their personal data, as also required by the GDPR. Consumers should always be informed in a timely clear and intelligible manner about the existence, process and rationale of algorithmic systems. Third, consumers should be fully in control of their personal data. While it is important to ensure competition among busi nesses also by allowing them to access essential data, personal data sharing always has to be done under full control by consumers and in compliance with the GDPR s right, obligations and principles . AI must not further entrench digital c ommercial survei llance The development of algorithmic technologies is directly linked to the widespread growth of intelligent services and products on the market . Such technologies are rapidly changing the way that consumers search and shop for produc ts. For example, a current trend is the use of devices such as smart assistants allowing consumers to search and order products using voice commands (e.g., Amazon s Alexa) . While these new AI powered products and services may have many attractive features and could provide benefits like customi sed services , they also provide the possibility to monitor and analyse consumers behaviour in detail and therefore to greatly influence their choices . AI and ADM technology elevate s the levels of pervasiveness and power of the so -called commercial surveillance ecosystem that has come to dom inate the online world, further endanger ing consumer s autonomy and freedom of choice. Digital services rely on algorithm powered technologies for processing consumer data and for targeting consume rs with ads and other messages. Al gorithms can be used to exploit 11 Pag. Barrett, L. F., Adolphs, R., Marsella, S., Martinez, A. M., & Pollak, S. D . (. Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements. Psychological Science in the Public Interest , 20(, 1 . 13 consumer s weaknesses and biases in order to convince them to purchase products limiting their choices. Via massive data collection , companies are able to oversee consumers online activity , record it and use it to discover possible correlation s that may be useful in influencing consumers through the most effective ads12. The wide diffusion of smart products and services is also able to incentivise price discriminations among consumers. It has been demonstrated that it is efficient for a brand operating in a competitive environment to price discriminate less tech -friendly consumers across di stribution channels13. Consumers with clearer preferences find discounts and reduced prices that are oft en unavailable to those who are less customary to using online marketplaces. In this sense , we highlight that the re should be no price differentiation by means of personalised and non -personalised automated assessments. Moreover , it is important to bear in mind that services are often provided 'for free' in order to maximi se the number of users and therefore the amounts of profit -generati ng consumer data collected for AI and ADM systems. The scope, invasiveness and potential consequences of this commercial surveillance is difficult, if not impossible, for consumers to comprehend. This happens, for example, in the context of digital adverti sing or social networks . In our opinion , in presence of such phenomena , the potential damage suffered by the individual consumer or by a group of consumers can be very high. However, the Commission White Paper seems not to embrace everyday activities su ch as shopping activities within its definition of high risks. We therefore call on the Commission to restrict the use of systems building on consumers commercial surveillance and to encourage the deployment of consumer - centric systems based fair and non-discriminatory practices . Consumers must have a strong set of rights In our 2019 position paper14 AI rights for consumers , we outlined a non-exhaustive list of AI rights ensuring a fair, safe, and just society and set to guarantee a high level of consumer protection. We urge th e Commission to concretise these rights in its future legislative proposal by translating them into enforceable rul es so that ADM powered technologies serve consumers and does not harm them. In particular, at least the following rights should be guaranteed : Right to transparency, explanation, and objection : consumers should have a right to get a clear picture of how decisions that affect them are made and be able to oppose wrong or unfair decisions and request human intervention. In particular consumers s hould be able to object automated decisions independently of the restrictions individuated by article 22 GDPR , i.e. that such decisions are taken solely on automated data processing , that the data processed is qualified as personal data , and that such decisions should have legal effects or similarly signif icantly affect them . We have concerns that if such right to object is aligned to the GDPR article 22 as far as its scope is concerned, commonly used automated decision - making processes that have an impact on consumers would not be able to be objected . Right to accountability and contro l: consumers should have a right that appropriate technical and organisational systems as well as measures are put in place that ensure l egal compliance and regulator y oversight. 12 Sartor, G., New aspects and challenges in consumer protection, Study for the committee on the Internal Market and Consumer Protection, Policy Department for Economic, Scientific and Quality of Life Policies, Eur opean Parliament, Luxembourg, 2020 . 13 Liu, Yi & Yildi rim, Pinar & Zhang, Z. (. Artificial Intelligence and Price Discrimination. 14 tions/beuc -x-2019-063_ai_rights_for_consumers.pdf . 14 Right to fairness : consumers should have a right that algorithmic decision making is done in a fair and responsible way. Right to non-discrimination : consumers should have a right to be protected from illegal discrimination and unfair diff erentiation. Right to safety and security : consumers should have a right that A DM-powered products are safe and secure throughout their lifecycle. Right to access to justice : consumers should have a right to redress and public enforcement if risks asso ciated with ADM materialise. Right to reliability and robustness : consumers should have a right that A DM-powered products are technically reliable and robust by design. A strong and streamlined approach to s ustainability and environmental protection is needed Digitalisation and AI can help the urgently needed green transformation and the move towards more global sustainability . But it can also act as a fire accelerant if not managed properly. To this end, t he connection between the carbon footprint an d computer processing is another of the essential considerations to be made when regulating ADM and AI. Empirical findings have shown that digital t echnologies contribute to 4% of overall greenhouse gas emissions, a number expected to double by Othe r studies show that training a single AI model emits carbon dioxide in amounts comparable to that of five cars over their lifetimes16. This problem must not be underestimated , particularly in the context of the European Green Deal. In this sense, a general rethink of political strategies is needed to ensure coherence between sustainability and digital policy objectives . For example , it is contradictory to push for a massive use of IT systems that require infrastructures that are potentially very energy/carbo n intensive without adequate safeguards . The Commission must provide more clarity how it intends to ensure that generali sed dev elopment and extended access to algorithmic technologies is carried out in compliance with environmental requirements. In parti cular it is key to ensure that the development of innovative technological solutions will address sustainability challenges , by for example incent ivising companies to reduce the carbon footprint of data centres and IT devices (including smartphones). While certain measures have already been taken such as addressing the energy efficiency of servers through Ecodesign measures and by proposing a new strategy on ICT products as part of the second Circular Economy Action Plan , much more needs to be done in relat ion to the scale of the problem. Besides handling the infrastructure which underpins the internet and AI in a sustainable manner it will be crucial to shape digitalisation in a way that it can serve a fundamental transition towards sustainability. 15 Maxime Efoui -Hess, Climate Crisis: The Unsustainable Use of Online Video , Shift Project ( 16 Karen Hao, Training a Single AI Model Can Emit as Much Carbon as Five Cars in Their Lifetimes, MIT Tech. Rev., -a-single -ai-model -can-emit-as-much -carbon -as-five-cars- in-their-lifetimes/ ; Emma Strubell, Ananya Ganesh & A ndrew McCallum, Energy and Policy Considerations for Deep Learning in NLP, Ann. Meeting Ass n Compu tational Linguistics (2019 ). 15 Liabili ty rules must be update d to ensure compensation in case of harm arising out from AI -powered products As the Report on the safety and liability implications of Artificial Intelligence, the Internet of Things and robotics17 (hereafter the Report ) accompanyi ng the AI White Paper h ighlights, AI and ADM technologies nowadays importantly disrupt liability rules18. In parallel, EU and national liability rules have been designed with traditional business models and traditional products in mind. For example, product s that the drafters of the Product Liability Directive had in mind in the 1980s are a far cry of those surrounding consumers nowadays. The existing framework established by the Product Liability Directive in 1985 is no longer adapted to the multiple challe nges brought by new te chnologies in This situation creates legal uncertainty for both businesses and consumers, multiplies risks of unequal treatment of consumers in the Single Market, prevents redress and ultimately hinders trust in digital goods in general. An updated l iability framework for digital goods is urgently needed to ensure effective access to justice for consumers when things go wrong with their products.19 We therefore call on the Commission to take outmost account of the following recomm endations: Liability rules for digital goods should ensure a higher level of protection for consumers and should be fair and cost -effective . In order to mitigate the existing informational asymmetries, we notably call for a reversal of the burden of proof : it should be up to the party that has access to the relevant information to investigate the cause of the problem when problems arise. The li ability framework should be clear and enforceable. The fact that multiple actors may potentially intervene in the product supply chain (e.g. manufacturer, app developer, programmer, designer, etc .) should not prevent consumers from obtaining compensation. All professionals involved in the supply of digital goods should be held jointly liable in case of harm . Liability rules should provide the right set of incentives to all actors involved in the supply chain. Actors should be required to fully internalise t he risks of their products and to take the precautionary measures that would prevent harmful situations from occur ring in the first place. Among others, the notion of product , defect , producer and damage should be revised and adapted to the digital context. Targeted changes in national liability based on a risk-approach rules should be carefully assessed. In any event, these targeted changes should by no means replace a sound revision of the Product Liability Directive . Finally, if special li ability rules were to be introduced for some categories of digital products, it will be essential to clarify the interpl ay between the upgraded EU product liability framework and the special liability rules applying for certain AI products. Again, 17 European Commission, COM(64 final. 18 This situation precludes consumers from obtaining compensation when things go wrong . Digital goods relying on algorithms are overly complex, opaque, data -driven, may evolve in directions that were not initially expected and vulnerable to cyberattacks . For example, in February 2020, a research conducted by the Dutch consumer organisation Consumentenbond revealed that many smart products are vulnerable to hacks. Consumentenbond tested 10 products and found security issues with two sex toys, two children's GPS watches and two baby cameras. In total, the investigations and hack tests reveal ed 27 vulnerabilities: - laks-met-veiligheid -slimme -apparaten 19 In May 2019, the European Parliament alr eady regretted that no legislative proposal was put forward during [the last] legislature, t hereby delaying the update of the liability rules at EU level and threatening the legal certainty across the EU in this area for both traders and consumers ( Europ ean Parliament, P8_TA(0081 , 12 February 2019, pt . 16 such a c onsistency and coherence are necessary for ensuring a clear and enforceable liability framework for all stakeholders. We have further detailed the necessary changes in our recent position paper Product Liability EU rules fit for consumers in the digital age published in May Existing legislation must be updated to ensure consumers are adequately protected In addition to liability rules, also safety and consumers rights legislations need to be updated. In particular, in our forthcoming position paper Views for a modern regulatory framework on products safety we call for a moderni sation of the General Pro duct Safety Directive (GPSD) adopted in 2001 . Although the Directive constitutes a key piece of consumer protection policy especially by creati ng a general obligation for producers to place only safe products on the market , it is now outdated and unable to grasp the challenges arising from technological developments such as AI and ADM technologies. For example, t he current legal definition of pr oduct does not explicitly include software that may be incorporated in a connected product or downloaded after its placing on the market. In this sense, we are concerned that if a safety issue arises due to a software update or inefficiency consumers would not be able to be sufficiently protected. Similarly, t he current definition does not offer clarity about who is responsible for the safety of self-learning AI products. We have therefore issued some recommendations for an update of the rules . Among other s, we underline the need to establish a principle of security by design and by default which constitutes a priority for connected products . This would for instance require manufacturers of such products to respect minimum cybersecurity requirements (e.g. strong authentication features; encryption) from an early stage of and throughout their design process, before putting their products on the market. Regarding the EU consumer law aquis, it is necessary to assess whether horizontal legislation regarding unfair commercial practices, unfair contract terms, and consumer rights when buying on -line (just to name a few central pieces of consumer protection legislation) are still fit for purpose. For example, as outlined in a previous position paper the law on unfair commercial practices has its roots in the idea that consumers must be given essential information so that they can make an informed decision. Is essential information still a valid concept when nobody can retrace why and how a specific decision has been taken? A targeted REFIT exercise should be undertaken to evaluate whether these directives can still effectively meet their legislative objectives in an ADM environment. A coherent oversight, enforcement and redress system is necessary In order to ensure the trustworthy deployment of algorithmic based technologies, the Commission recogni ses the need for the applicable legal requirements to be complied in practice and be effectively enforced both by competent national and Euro pean authorities and affected parties at national and European level. The Commission also underlines that competent authorities should be in a position to investigate individual cases, but also to assess the impact on society. The proper functioning of any future regulation will depend on the effectiveness of its provisions and therefore on strong, clear and sound public enforcement . But it will also depend on providing the necessary me ans to civil society , and in particular consumer organisations , 17 to fulfil their role as ma rket watchdogs, either via testing of products and services, private enforcement, such as collective redress actions , or via the collaboration with public authorities by providing them with alerts about illegal practices or market failures . For this to happen , it is firstly crucial to disclose information about each automated decision system, including details about its purpose, design features , potential use and implementation timeline in order to ensure that ADM systems are comprehensible for consumer s and supervisory authorities . This way, consumers trust will increase and authorities will be able to scr utinise systems and consequently minimise the harms by imposing modifications, restrict ing or prohibiting the use of the syste m. Strong oversight by supervisory authorities should be ensured regardless of the level of risks . We also highlight the need for closer cooperation among the authorities, and for an enforcement system that can deliver EU wide results for EU wide infringements/challenges, ensuri ng a harmonised and effective approach. Our recommendations can be summari sed as follows: Control and oversight ADM technologies should as a matter of principle be subject to independent control and oversight . Whether an algorithm -based decision is accu rate, fair, or discriminative can only be assessed if an appropriate control system is in place. As a general principle, companies and operators should be able to demonstrate that they comply with the law, such as rules on c onsumer or data protection, as w ell as non -discrimination rules. In the pre-marketing phase , at least for high risk applications, it should be mandatory for a producer/service provider to involve independent third -parties which assess legal compliance and can for example request design changes before a product goes into mass production or a service can be brought to the market. For applications that present the highest levels of risk, ex-ante scrutiny procedures by authorities (e.g. regulatory pre -approval before market deployment, publ ication of impact assessments) should be put in place. For the post-marketing phase , it will be important to ensure that the compliance of a certain product or service with the legal requirements of any future regulation will be assessed during its whole lifecycle , establishing a principle of continued conformity . This concept is particularly impor tant in an advanced technological environment as recurring software updates and self-learning algorithms may change the properties of products and services over the time and consequently have an impact on how the system respects the legal requirements. Therefore , continuous internal and external control and oversight will be crucial to keep consumers protected . Accountability and transparency Depending on the level of risk, accountability measures should comprise ADM impact assessments, documentation, internal audits or transparency measures for the users . Operators must be transparent about their business model and use interfaces which will allow authorities to exercise meaningful oversight and ultimately enforce the rules. To enable both ex -ante and ex -post assessments, independent third -party testing and enforcement measures by Member States, companies need to be accountable and must put in place measures t o allow for external control of their ADM systems. Rules for an effective auditing system should be put in place so that authorities are able to check the compliance of relevant ADM processes. This would also reveal which 18 potentially unintended conseq uences the processes have for consumers everyday lives20. The Commission should also evaluate the possibility of impos ing the use of mandatory standards for technical design, logging, documentation and description of ADM systems (transparency by design )Enforcement Enforcement authorities should have the necessary powers (e.g. right to obtain information, the right to inspect and access) so that they can scrutinise and evaluate AI and ADM systems and , in case of law infringements , stop illegal practices , impose remed ies and award damages wher e relevant as well as impose penalties . Authorities must also be capable of conducting ex post checks on relevant ADM systems at any time. It must be possible, in particular for the competent supervisory authority, to review and verify the tests performed by the operators. As an ultima ratio measure, authorities should be able to ban the use of certain A I or ADM systems upfront . Remedies We also recall the importance of ensuring that effective remedies for consumers a re easily available. Consumers should have the concrete possibility to interact with a human able to handle and explain the processing activities of the system and its decisions. It should be guaranteed a minimum level of human oversights s o that the syste m s decisions can be checked, timely contested and corrected. VOLUNTARY LABELLING SYSTEM AND LOW -RISK APPLICATIONS The Commission envisages the possibility of introducing a voluntary labelling scheme for those applications that would n ot fall under the h igh-risk category and therefore into the scope of the new regulation . In our view, such a scheme is not suitable to provide meaningful protection to consumers, even if it is just envisaged for low-risk applications. Labels are fruitful only in relation to the requirements and enforcement systems they are based on. Once clear legal rules and enforcement mechanisms will be in place, the role of a trustworthy label could be considered. It is important to also bear in mind that the inherent information asymmet ry in complex and evolving algorithmic learning system s, mak es the role of a label very complex and different from other sectors such as environmental , fair trade or food labels for products and services which do not change properties constantly . BIOMETRI C TECHNOLOG IES Companies are increasingly using c onsumers biometric data for different purposes . All over the world facial recognition is used for tagging people on social media platforms, to unlock 20 See BEUC s German member (vzbv) factsheets ARTIFICIAL INTELLIGENCE: TRUST IS GOOD, CONTRO L IS BETTER . 21 REGULATION OF ALGORITHMIC DECISION MAKING FOR THE BENEFIT OF CONSUMERS by BEUC s German member (vzbv) . 19 smart phones or to authenticate/identify customers in the context of financial services. Retailers can leverage facial recognition to identify a premium customer. Biometrics are particularl y sensitive data, and their illegitimate processing can have very serious consequences. For example, o ne aspect which is very worrying for consumers is the use of biometrics for emotion recognition (e.g. real time facial recognition that analyses feelings and adapts what consumers see/or are offered accordingly). This can lead to serious infringements of consumers privacy a s well as to their manipulation . Due to their inner intrusiveness, the White Paper assigns to the use of biometrics a specific role in the regulatory landscape, considering the need to provide with specific requirements for their use. The White Paper ri ghtly distinguishes biometric data processed for the purpose of customer authentication, from those used to identify them. This second category22 certainly raises more concern s than the first (which is however not harmless) and pose greater risks of harm . Biometric identification systems are already covered by the General Data Protection Regulation (GDPR). The processing of biometrics data for uniquely identifying purposes , such as facial recognition, is forbidden pursuant to Article 9( of GDPR , unless it falls under the scope of one of the exemptions listed in such article. Thus, an effective implementation o f GDPR may ensure that facial recognition is used in a duly justified manner and does not excessively interfere with the right to privacy. However, when these systems are used especially in public spaces ethical and social questions arise. For example: Is it socially acceptable that facial recognition is used in public places? For what purposes? Who and how can guarantee a use that respects the pr inciples of necessity and proportionality? Which is the role of Governments, society and the other stakehold ers? These questions are not answered by the GDPR. Well aware of these dilemmas, the Commission determine s that it will launch a broad European deb ate on the specific circumstances, if any, which might justify such use, and on common safeguards . BEUC w elcomes the fact that the Commission wishes to have a broad and inclusive debate on the use of these systems. However, in our view, t his debate must not to be limited only to the specificities identified by the Commission (remote identification in public spaces). There are actually many other uses of biometric data that should be subject to a public debate as their effect can be irreversible for socie ty and future generations. For example, more attention should be paid to the risks arising from the use of biometric data of vulnerable subjects, such as children. A recent study took the processing of children s biometric data as paradigmatic example unde rlying that that closer attention must be paid to the actual social contexts in which data relating to children comes to affect their lives through AI practices In this sense, we emphasi se that all biometric systems and all collections of biometric dat a, regardless of being for identification or for authentication purposes, can lead to problematic outcomes . We would therefore recommend to apply heightened safeguards also to seemingly less intrusive biometric techniques. 22 In fact, there are sim ple biometric identification systems which compare individual physical or behavioural characteristics with the information stored in the system in order to find a match for the purpose of identifying the person. Such systems which seem to be out of the s cope of the White Paper - differ from those which embed machine learning techniques which in addition to data collection and combination - allow the system to learn, react and adapt its endeavours on the basis of its findings. These latter technologies a re defined in the White Paper as remote biometric identification . 23 Velislava Hillman, Nick Couldry, Elettra Bietti, Gretchen Greene, Response to the European Commission s communication to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions on Artificial Intelligence (White Paper COM 2020 -. 20 In order to tackle these pro blems, we recommend the Commission to : Ensure full compliance with the GDPR and strong application of key principles such as, transparency, data minimisation and purpose limitation. Ensure that individuals can remain in control and exercise their rights wh en they are not directly interacting with the technology (e.g. when a consumer is walking down the street and is inadvertently captured by a facial recognition system installed in a shop or an interactive billboard). Develop a risk assessment system and en able possibilities for independent testing for accuracy and unfair bias. Biometric technology should never be deployed without a prior impact assessment (not only limited to data protection impact) and consultation with the comp etent supervisory authority. Clear red lines must be established to limit or, where appropriate, prohibit the use of biometric technology for specific purposes or in specific situations (e.g. to monitor children in schools) where the risk for people s ri ghts and freedoms would be t oo high and the impact of this technology would be detrimental to the individual or to society as a whole. When it comes to consumer identification via biometric data (e.g. to enter a venue for a concert, to access an online ba nking account, or to unlock a mobile device) the consumer should, as a general rule and taking into account security risks, be provided with the possibility to choose an other identification system instead that does not require the processing of biometric d ata. The use of biometric id entification technology, such as facial recognition, should not be normalised and widely deployed given the serious data protection and privacy implications and risks of such technology. END 3 This publication is part of an activity which has received funding under an operating grant from the European Union s Consumer Programme (2014 -. The content of this publication represents the views of the author only and it is his/her sole responsibility; it cannot be considered to reflect the views of the European Commission and/or the Consumers, Health, Agriculture and Food Executive Agency or an y other body of the European Union. The European Commission and the Agency do not accept any responsibility for use that may be made of the information it contains.",en,"Approach to data management and control must favour consumers and public interest 12 AI must not further entrench digi tal commercial surveillance ................................ Consumers must have a strong set of rights ................................ ................................ A strong and streamlined approach to sustaina bility and environmental protection is needed ................................ ................................ ................................ ................................ AI has the potential to help achieve the green transition but also comes with a big environmental footprint. We urge the European Commission to explore the opportunities offered by AI but also to consider the environmental harms such as carbon -dioxide emissions and electronic waste resulting from the data-driven infrastructures needed to power the large -scale deployment of AI and ADM powered products and services. We recommend incentivising the use of greener infrastructures for the development and deployment of these technologies so that they support the achievement of sustainable development, climate neutrality and circular economy goal s. Any future reg ulation should envisage a coherent and efficient compliance and enforcement system which: Obliges b usiness es to ensure built -in control mechanism s for the development and use of ADM systems . For the creation of an ecosystem of trust , the Commission builds on the AI High Level Expert Gro up ""Ethics Guidelines for Trustworthy Artificial Intelligence "", published in April These guidelines identify seven key requirements for the development of trustworthy AI applications: human agency and oversight ; technical robustness and safety ; privacy and data governance ; transparency ; diversity, non-discrimination and fairness ; societal and environmental wellbeing ; accountability. To ensure a trustworthy development of AI technologies, the White Paper refers to the non - legally binding requirements stipulated by the Ethical Guidelines of the AI HLEG: human agency and oversight ; technical robustness a nd safety ; privacy and data governan ce; transparency ; diversity, non -discrimination and fairness ; societal and environmental wellbeing ; accountability. Such high-risk sectors should be specifically and exhaustively individuated by the new legislation and m ight initially include healthcare; transport; energy and parts of the public sector . A strong and streamlined approach to s ustainability and environmental protection is needed Digitalisation and AI can help the urgently needed green transformation and the move towards more global sustainability . To this end, t he connection between the carbon footprint an d computer processing is another of the essential considerations to be made when regulating ADM and AI. Empirical findings have shown that digital t echnologies contribute to 4% of overall greenhouse gas emissions, a number expected to double by Othe r studies show that training a single AI model emits carbon dioxide in amounts comparable to that of five cars over their lifetimes16. This problem must not be underestimated , particularly in the context of the European Green Deal. In this sense, a general rethink of political strategies is needed to ensure coherence between sustainability and digital policy objectives . For example , it is contradictory to push for a massive use of IT systems that require infrastructures that are potentially very energy/carbo n intensive without adequate safeguards . The Commission must provide more clarity how it intends to ensure that generali sed dev elopment and extended access to algorithmic technologies is carried out in compliance with environmental requirements. In parti cular it is key to ensure that the development of innovative technological solutions will address sustainability challenges , by for example incent ivising companies to reduce the carbon footprint of data centres and IT devices (including smartphones). While certain measures have already been taken such as addressing the energy efficiency of servers through Ecodesign measures and by proposing a new strategy on ICT products as part of the second Circular Economy Action Plan , much more needs to be done in relat ion to the scale of the problem. Besides handling the infrastructure which underpins the internet and AI in a sustainable manner it will be crucial to shape digitalisation in a way that it can serve a fundamental transition towards sustainability. 15 Maxime Efoui -Hess, Climate Crisis: The Unsustainable Use of Online Video , Shift Project ( 16 Karen Hao, Training a Single AI Model Can Emit as Much Carbon as Five Cars in Their Lifetimes, MIT Tech. Rev., -a-single -ai-model -can-emit-as-much -carbon -as-five-cars- in-their-lifetimes/ ; Emma Strubell, Ananya Ganesh & A ndrew McCallum, Energy and Policy Considerations for Deep Learning in NLP, Ann. It is important to also bear in mind that the inherent information asymmet ry in complex and evolving algorithmic learning system s, mak es the role of a label very complex and different from other sectors such as environmental , fair trade or food labels for products and services which do not change properties constantly . 23 Velislava Hillman, Nick Couldry, Elettra Bietti, Gretchen Greene, Response to the European Commission s communication to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions on Artificial Intelligence (White Paper COM 2020 -.",risk
European Digital Rights (EDRi) (Belgium),F550551,09 September 2020,Non-governmental organisation (NGO),Small (10 to 49 employees),Belgium,"Use cases: Impermissable AI and fundamental rights breaches This briefing has been compiled to assist policymakers in the context of the EU s regulation on artificial intelligence. It outlines several cases studies across Europe where artificial intelligence is being used in a way that compromises EU law and fundamental rights, and therefore requires a legal prohibition or ban . August 2020 Grave concerns remain as to how artificial intelligence (AI) will impact people, communities and society as a whole. Some AI systems have the ability to exacerbate surveillance and intrusion into our personal lives, reflect and reinforce some of the deepest societal inequalities , fundamentally alter the delivery of public and essential services, vastly undermine vital data protection legislation, suppress freedoms of expression and assembly, and disrupt the democratic process itself. In European Digital Rights explainer, EDRi details some of the implications of AI on fundamental rights. In EDRi s recommendations for a fundamental rights-based regulation on artificial intelligence, EDRi outlines the need for clear legal limits on the uses of AI , legal criteria for, democratic oversight, and the need for a prohibition on impermissable uses of AI. EDRi recommends the European Commission draw red-lines for AI, in particular in these areas: Indiscriminate biometric surveillance and biometric capture and processing in public spaces, including public facial recognition; use of AI to determine access to or delivery of essential public services (such as social security, policing, migration control); uses of AI which purport to identify, analyse and assess emotion, mood, behaviour, and sensitive identity traits (such as race, disability) in the delivery of essential services; predictive policing; use of AI systems at the border or in testing on marginalised groups, such as undocumented migrants; autonomous lethal weapons and other uses which identify targets for lethal force (such as law and immigration enforcement); general purpose scoring of citizens or residents, otherwise referred to as unitary scoring or mass-scale citizen scoring. European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | Further information on EDRi s recommendation to the Commission on fundamental rights-based regulation on artificial intelligence: More information on biometric processing and capture in publicly accessible spaces can be found in EDRi s position paper on Biometric Mass Surveillance , where EDRi calls for an immediate and indefinite ban on biometric mass surveillance. European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | DENMARK: Gladaxe system UNITED KINGDOM: Harm Assessment Risk Tool ( HART ) DENMARK: Tv rspor research project NETHERLANDS: SyRI Case POLAND: Random Allocation of Cases SPAIN: BOSCO and Bono Social de Electricidad GERMANY: SCHUFA system AUSTRIA: Employment Agenc y AMS II. Uses of AI to identify, analyse and assess emotion, mood and sensitive identity traits in the delivery of essential services FRANCE: Behaviour prediction in Marseille FRANCE: Emotion recognition programs (several) FINLAND: DigitalMinds GERMANY: Affective Computing ITALY: Redditometro III. Predictive policing NETHERLANDS: ProKid 12-SI DENMARK: Gladaxe system ITALY: KeyCrime SWITZERLAND: Precobs, Dyrias and ROS UNITED KINGDOM Gangs Violence Matrix NETHERLANDS: Crime Anticipation System ITALY: Video surveillance and the prediction of abnormal behaviour UNITED KINGDOM: Offender Group Reconviction Scale BELGIUM: Zennevallei BELGIUM: iPolice UNITED KINGDOM: National Data Analytics Solution (NDAS) UNITED KINGDOM: Origins Software IV. Use of AI systems at the border, in migration control or in testing on marginalised groups EUROPE: Common Identity Repository (CIR) and Visa Information System (VIS) EUROPE: Military Drones at borders EUROPE: Frontex scales up AI uses in border control HUNGARY, GREECE: iBorderCtrl UNITED KINGDOM: UK Home Office Visa Algorithms SLOVENIA: BORDER AI GREECE: SPIRIT Project V. Biometric Surveillance ITALY: SARI facial recognition GREECE: Smart policing European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | UK: Covert facial recognition SERBIA, UGANDA: Government surveillance ARGENTINA: Public facial recognition FRANCE: School facial recognition GLOBAL: dignity, trauma, bias, function creep I. Use of AI to determine access to or delivery of essential public services DENMARK : Gladaxe system In Denmark in 2018 three local authorities asked for exemption from data protection rules to run an experiment to trace children with special needs from a very early stage. The purpose was to trace children who were vulnerable due to social circumstances even before they showed actual symptoms of special needs. Based on previous use of statistics, the authorities decided to combine information about risk indicators , to determine neighbourhoods to be characterised as ghettos , based on an automated algorithmic assessment. Indicators included unemployment levels, crime rates, educational attainment and other risk indicators , as well as whether the levels of first and second-generation migrants in the population is more than 50%. Individuals from neighbourhoods which meet these criteria are classified as ghettos . The model used a points-based system, with parameters such as mental illness (3000 points), unemployment (500 points), missing a doctor s appointment (1000 points) or dentist s appointment (300 points). Divorce was also included in the risk estimation, which was then rolled out to all families with children. These neighbourhoods are then subject to special measures, including higher punishments for crimes, putting children into public day care at an early age, lifting the protection of tenants in order to privatise public housing, tearing down entire building blocks and indeed applying the automated risk assessment system for families with children. This program poses clear discrimination risks and a lack of equal treatment on the basis of race, ethnicity and migration background. Source: Algorithm Watch, Automating Society (, society-denmark/ European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | UNITED KINGDOM : Harm Assessment Risk Tool ( HART ) Harm Assessment Risk Tool ( HART ), used by Durham Constabulary in the United Kingdom is based on a machine-learning algorithm to assess a suspect s risk of reoffending, using over thirty variables that characterise an individual s criminal history and socio-demographic background. The risk assessments conducted by HART are used by the local police to determine whether an individual should be charged, or diverted into a rehabilitation programme. HART s assessment can trigger a chain of events that can result in the deprivation of liberty, and/or a criminal conviction. Rather than basing such charging decisions on individual cases, HART creates profiles for entry into diversion programs on the basis of sensitive and personal information . Clear examples of racial discrimination in this system include the use of the Mosaic code developed by a consumer credit reporting company, that categorised individuals into various groups according to inter alia ethnic origin, income, and education levels. Some socio-demographic categories used by Mosaic were highly racialised, including, for example, Asian Heritage , which stereotyped individuals of Asian origin as being unemployed or having low-paid jobs, and living with extended families. Source: Marion Oswald et al., Algorithmic risk assessment models: lessons from the Durham HART model and Experimental proportionality Information & Communications Technology Law, Vol 27, Issue 2 ( DENMARK : Tv rspor research project This automated risk assessment experiment in the field of social welfare is a project that measures chronically ill patients behaviour in order to estimate when or how further efforts are necessary, namely whether patients should be admitted to hospital with severe conditions. The aim of this project is that it creates a tested model for assessing the individual patient's risk profile and for offering a cross-sectoral effort that can be extended to several clusters. Source: Algorithm Watch, Automating Society (, society-denmark/; patientadfard-skal-gore-sundhedsvasenet-mere-proaktivt/ NETHERLANDS: SyRI Case SyRI is a risk profiling system used by the Dutch government, that linked and analysed large amounts of personal data of citizens, such as data on identity, labour, movable and immovable property, education, pension, business, income and assets, pension and debts. SyRI was used to prevent and combat abuse of social security provisions, tax and contribution fraud and non- compliance with labour laws. European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | The profiling of citizens by SyRI created risk reports: so-called surprise addresses with an increased risk of fraud. These people were registered, after which they could be subject to criminal and administrative investigations and sanctions. Every inhabitant of the Netherlands was suspected in advance by the government s use of SyRI. In 2014, a coalition of civil society organizations initiated strategic litigation on SyRI against the Dutch State. The manner in which the government used SyRI against its citizens and thus processed large amounts of data was unprecedented, undemocratic and subject to serious human rights objections. On 5 February 2020, the District Court of The Hague ruled that SyRI was in violation of the European Convention on Human Rights. According to the Court, SyRI constituted a disproportionate invasion of the private lives of citizens. This did not only apply to people who were identified by SyRI as being at increased risk, but to everyone whose data was analysed by SyRI. According to the District Court, SyRI was not transparent and therefore not verifiable. The invasion of privacy was unforeseeable for citizens and they could not defend themselves against it. The Court also mentioned the actual risk of discrimination and stigmatization of citizens, based on socio-economic status and possible migration background, in so-called problematic neighbourhoods , where SyRI has already been deployed. According to the Court, the deployment of SyRI is accompanied by a risk of prejudice, but this risk cannot be controlled. Source: POLAND: Random Allocation of Cases In Poland, since the beginning of the 2018, the system of Random Allocation of Cases to Judges (also as System ) began to be used in all common courts to randomly assign judges to cases. The use of this System means that the algorithm determines which judge will receive a specific case to be heard. The Foundation, recognizing that the use of algorithms in the judiciary should be used extremely carefully, wanted to know the rules of its functioning. In December 2017, the Foundation filed an application for access to public information to the Minister of Justice, and asked for access to an algorithm on the basis of which the Random Allocation of Cases System operates. The Minister refused to provide information covered by the Foundation's application and pointed out that the algorithm consists of technical information, is not public information within the meaning of the Polish Act on Access to Public Information, and therefore is not subject to disclosure. Then, in December 2017 the Foundation filed an action for failure to act of the Minister before the Court, considering that the algorithm that determines how individual judges are assigned to hear cases is public information and should be available to citizens. European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | Source: ; SPAIN : BOSCO and B ono Social de Electricidad The Bono Social de Electricidad is a discount on energy bills to at-risk individuals and families. The complexity of the application process and the lack of information from the administration were preventing disadvantaged groups from applying. Energy companies use BOSCO, a software created by the Spanish Ministry for Green Energy Transition, to decide who is entitled to the subsidy. Both the government and the Council of Transparency and Good Governance denied Civio access to the code by arguing that sharing it would incur in a copyright violation. However, according to the Spanish Transparency Law and the regulation of intellectual property, work carried out in public administrations is not subjected to copyright. Source: algorithms-should-never-be-allowed-in-a-social-and-democratic-state-under-the-rule-of-law/ GERMANY: SCHUFA system Germany s leading credit bureau, SCHUFA, can determine acceess to housing, as landlords may refuse to rent an apartment, banks may reject credit card applications and network providers will say no to a new contract. The scoring procedure of the private company SCHUFA is highly intransparent and not accessible to the public. According to the campaign OpenSCHUFA, Spiegel Online and BR who investigated this program, the determines the creditworthiness of 67 million Germans. The investigation has found a number of flaws, uncluding that this system may reinforce discrimination and that it has violated the GDPR s data access provisions . Source: AU STRIA: Employment Agency AMS The Austrian employment Agency is using an algorithm to determine potential jon opportunities of unemployed people. In the recently published paper (""Algorithmic Profiling of Job Seekers in Austria: How Austerity Politics Are Made Effective""), the authors express strong concerns about the AMS algorithm. The program will divide the unemployed into those with good, medium and bad job opportunities. People with good job market opportunities are those who are 66% likely to be able to find European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | employment for three months within seven months. Many variables are included in the assessment of the opportunities: for example, the place of residence of the job seeker, the previous career, the highest level of education completed, but also gender. When calculating short-term job opportunities, the individual variables mentioned have a different weight than in long-term assessment. Depending on how long someone has been unemployed, the weighting changes further. As investigated by NGO Epicenter Works, there is not full transparency and so the impact assessment is not possible in some areas. However, the algorithm is shown to explicitly discriminate on the ground of gender . e.g. women with children are negatively weighted but men with children are not. In addition the system rates women 's job opportunities worse than those of men. Source: discriminatory-algorithm/ ESTONIA: Following a reform of the work ability support system, machines and algorithms were used to automatically re-evaluate incapacity levels. Reportedly, the incomplete data in the e-health platform, coupled with a lack of in-person interviews, resulted in loss of social benefits for certain persons with disabilities and older persons with disabilities. Source: Governing the Game Changer Impacts of artificial intelligence development on human rights, democracy and the rule of law . II. Uses of AI to identify, analyse and assess emotion, mood and sensitive identity traits in the delivery of essential services FRANCE: Behaviour prediction in Marseille The City of Marseille is developing a system in testing phase to automatically alert the police to any ""abnormal behavior"" detected by its CCTV cameras. Invetsigated by La Quadrature du Net, the city revealed that the system is being designed to detect and analyse behaviors that may be considered abnormal in the public space (crowd starting to run, crowd linked to an event - accident, fight -, individual walking repeatedly in a space). In an internal memo dated October 29, the City nevertheless explains that this tool is not functional to date. According to the same document, two other functions are ""pending regulatory framework"": a tool to reconstruct a posteriori ""the journey of an individual from the archives of several cameras"" and a sound detection module. European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | Such a system linking biometric capture and processing with the delivery of public services poses severe risks to fundamental rights, data protection and often does not meet standards of justification in law, necessity or proportionality. FRANCE: Emotion recognition programs (several) Two-I. Initial project: on trams in Nice (but not conducted due to backlash). Sold to Alain Behm (Mobil Security, a crow control gate which will be used in the 2024 French Olympics, and was used for the Environment Ministers 2019 G7 in Metz). Sold to a casino (for identifying compulsive gamblers). Partnership in Dubai to rank neighbourhoods according to happiness levels , and try to bring the features conducive to happiness to the unhappy neighbourhoods. Trial with volunteer patients in private hospitals in Metz, France. Source: i-mesurera-votre-douleur F INLAND: DigitalMinds DigitalMinds aims to eliminate the human participation in the recruiment process, in order to make the personality assessment process faster and more reliable , according to the company. Since 2017 it has used public interfaces of social media (Twitter and Facebook) and email (Gmail and Microsoft Office to analyse the entire corpus of an individuals online presence. This results in a personality assessment that a prospective employer can use to assess a prospective employee. Measures that are tracked include how active individuals are online and how they react to posts/emails. Such techniques are sometimes complemented with automated video analysis to analyse personality in verbal communication. Source: GERMANY: Affective Computing Some companies and scientists present Affective Computing, the a lgorithmic analysis of personality traits also known as artificial emotional intelligence , as an important new development. But the methods that are used are often dubious and present serious risks for discrimination. European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | Companies that use Precires s software can select the characteristics their applicants should have to be considered for a position. If a company uses characteristics based on their existing senior management staff however they are measured to create a profile for future managers, there is a real risk that only people with comparable characteristics are hired or promoted . Yet the company uses precisely these sorts of speech profile analyses of people in leadership positions to promote the company. Source: ITALY: Redditometro The Italian Revenue Agency, using a tool called Redditometro, created profiles, which w ere based, amongst others, on assumed expenses made by taxpayers deduced, according to statistical parameters, from their allocation in specific family categories or geographical areas. This profiling tool was investigated by the Italian DPA, the Garante. One of the main issues was the low quality of the data and the resulting high error rate based on unreliable inferences drawn from the data. On the basis of its investigation, the Garante prescribed that a taxpayer s real income could only be calculated from actual, documented expenses, and not deduced from statistically-based assumptions of levels of expenses. Source: European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | III. Predictive policing NETHERLANDS : ProKid 12-SI In the Netherlands, the government has used an algorithmic risk assessment tool, ProKid 12- SI, which purports to assess the risk of criminality of 12-year-old children since ProKid uses existing police data on these children, such as reports of where children have come into contact with the police, their addresses, information about their living environment , even including whether they are victims of violence, to identify them as being in one of four categories of risk of committing crimes in future. The system assesses children based on their relationships with other people and their supposed risk levels, meaning that individuals can be deemed higher risk by being linked to another individual with a high risk assessment, such as a sibling or a friend. Parents assessed risk can also impact a child s risk level. ProKid s algorithms assess risks in relation to future actions that the children have not yet carried out, and judges them on the basis of the actions of others close to them. These risk assessments result in police registering these children on their systems and monitoring them, and then referring them to youth care services. ProKid frames children as potential perpetrators even when they are registered as victims of violence; which has serious implications on their presumption of innocence. As such, the ProKid 12-SI raises severe concerns relating to rights of the child, the right to non-discrimination based on a number of protected characteristics, the presumption of innocence and data protection rights . S ource : K La Fors-Owyczynik, Profiling Anomalies and the Anomalies of Profiling: Digitalized Risk Assessments of Dutch Youth and the New European Data Protection Regime (, ) DENMARK : Gladaxe system See above (Use of AI systems to determine access to public services) Source: Algorithm Watch, Automating Society (, society-denmark/ ITALY: KeyCrime KeyCrime is predictive policing software based on an algorithm of criminal behaviour analysis. It was designed to automatically analyse criminal behaviour, help identify trends and suggest ways of thwarting future crimes. European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | Reportedly, the system can now sift through some 11,000 variables for each crime. These range from the obvious (time, location and appearance) to the less obvious (reconstructions of witnesses and suspects during subsequent interviews, and even the modus operandi of the criminal). Video feeds are included in the analysed data. The idea behind the software is to rationalise the use of the police force and automatically deploy officers exactly where they are needed. Source: SWITZERLAND : Precobs, Dyrias and ROS Precobs: The Precobs system purports to predict where burglaries will occur from past data, based on the assumption that burglars often operate in small areas. The focus is on the detecting a cluster of burglaries, and directing police resources into those neighbourhoods. Dyrias: The dynamic system for the analysis of risk program purports to predict the likelihood that a person will harm their intimate partner. Using police perceptions data, this algorithm outputs a likelihood score. 3,000 individuals were labeled dangerous in 2018 (but the label might not be derived from using Dyrias). ROS: German-speaking cantons in Switzerland use ROS (an acronym for Risikoorientierter Sanktionenvollzug or risk-oriented execution of prison sentences) to label prisoners into categories when based on likelihood of recidivism. These classificatons generally cannot be changed and determine privileges and other decisions in the criminal justice system. These programs disproportionately target people from working class and other marginalised communities, although there is a lack of data relating to race and ethnicity. According to Algorithm Watch, [v]ery little public information exists on Precobs, Dyrias and ROS. The people impacted, who are overwhelmingly poor, rarely have the financial resources needed to question automated systems, as their lawyers usually focus on verifying the basic facts alleged by the prosecution. Algorithm Watch, Automating Society (, forthcoming, UNITED KINGDOM Gangs Violence Matrix The Gangs Matrix was launched the Metropolitan Police in 2012 as a database of suspected gang members in London. It purports to be a risk-management tool focused on preventing serious violence by identifying potential gang members. However, according to a series of vague indicators, the database collects information of individuals who have never been involved with violent crime. European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | Even being a victim of a crime that the police link to a gang is viewed as an indicator of a likelihood of subsequently becoming drawn in to involvement in serious crime and can result in the individual being placed on the Matrix. The police also share the Matrix with other agencies, such as job centres, housing associations, and educational institutions, leading to discrimination against individuals on the basis of their supposed gang affiliation. Depending on the nature of the way this information is shared, this poses an opportunity for possible violations of the right to privacy and may affect housing and employment rights on a discriminatory basis . Those whose names are on the Matrix experience multiple stop and search encounters which seemingly lack any legal basis. Some report that police have stopped and searched them 200 times, others report up to as many as 1,000 times, with some reporting multiple stops everyday. This has an impact on individuals rights to freedom from interference with their privacy and their freedom from arbitrary arrest on an ethnically discriminatory basis. Matrixes like the gang matrix violate the right to non-discimination in that generally, racial, ethnic minorities are overrepresented, and vastly disproportionate to corresponding crime figures. As such, they are likely to codify racialised policing practices alongside infringements on data protection rights . 78 per cent of individuals on the Matrix are black, and an additional 9 per cent are from other ethnic minority groups, while the police s own figures show that only 27 per cent of those responsible for serious youth violence are black. Source: Amnesty UK What is the Gangs Matrix? matrix-metropolitan-police NETHERLANDS : Crime Anticipation System In Amsterdam, the Crime Anticipation System is a place-based predictive policing tool which attempts to predict where specific crimes, such as burglary, muggings and assaults will take place within a two-week period. Amsterdam Police developed the system to predict more at-risk areas in a city, and improve efficient distribution of their workforce. The system uses machine learning to analyse three sources of data: socio-economic data from the Central Bureau of Statistics which includes people s age, incomes and the amount of social benefits in an area; historical crime data, originally gathered by the police, focusing on previous crimes, locations and known criminals; Geo- data from the Municipal Administration which consists of streets and addresses. The aim of the analysis is to grade different areas of Amsterdam into red, orange and yellow. Areas graded red are considered high-risk and have increased police surveillance deployed to prevent predicted crimes from occurring. European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | Amongst the indicators used by CAS to predict crimes in a particular area was the number of non- Western allochtones in the area in other words, non-Western individuals with at least one foreign-born parent. This presupposes the existence of a correlation between ethnicity and crime , and singles out a category of ethnicities to be of particular concern, given that the presence of Western , autochtone individuals were not included in the indicators. Furthermore, given that Western was defined somewhat subjectively (for example, including individuals of Japanese or Indonesian origin, and including all European nationalities, apart from Turkish), CAS incorporated highly questionable societal categorisations and biases. Although not directed at individuals, place-based predictive policing systems still present a range of data protection risks and a clear cases of discrimination on the grounds of ethnic origin. In particular, the use of automated decision making systems to target policing to certain areas deemed high crime can further overpolicing of certain communities and further embed existing bias and inequalities in policing data which already profile individuals from racial ethnic and religious minority communities, working class and other marginalised groups as posing a higher risk of committing crime. Source: European Network Against Racism (. Data Driven Policing: Hardwiring Discriminatory Policing Practices in Europe: ; Oosterloo, S. at al. ( The Politics and Biases of the Crime Anticipation System of the Dutch Police . Available at: ITALY: Video surveillance and the prediction of abnormal behaviour In 2020, Hermes Center investigated the deployment of a so-called innovative video surveillance system in Tokamachi Park, Como. This advanced and potentially very intrusive system has been designed to perform facial recognition on all passers-by, including predicting when they are loiter - ing or trespassing . Hermes Center obtained the Data Protection Impact Assessment (DPIA) for the system and discovered that it was poorly written; did not risk-assess the enhanced fundamental rights issues implicated by facial recognition compared to video surveillance; and showed a lack of awareness for the fact that the system was already unlawful within the Italian legal framework . Subsequently, the Italian Data Protection Authority (DPA), the Garantie Privacy, struck down the sys - tem as having no legal basis, showing that the Como authorities had wasted public money . Source: (investigated by Hermes Center) European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | UNITED KINGDOM : Offender Group Reconviction Scale The Offender Group Reconviction Score (OGRS) is a predictor of re-offending based on static risk such as age, gender and criminal history to calculate individual predictions. The tool is employed by probation and prison services across Europe and uses an algorithm to calculate the likelihood of reoffending (which is expressed as a percentage score). The use of key variables such as age at first sanction (including warnings, (never) cautions, etc) and age at first conviction is likely to create discriminatory effect on minority ethnic groups namely, age at first sanction (including warnings, (never) cautions, etc) and age at first conviction. Given the effects of suspicion which result in increased levels of police stops, it is logical that the calculation of their risk of reconviction will be higher, not as a consequence of criminal activity, but as a consequence of the increased likelihood of being stopped by the police and law enforcement agencies. Source: European Network Against Racism (. Data Driven Policing: Hardwiring Discriminatory Policing Practices in Europe: ; Research summary 07/09, OGRS the revised Offender Group Reconviction Scale: BELGIUM : Zennevallei The Zennevallei police zone will be working on a project on 'predictive policing' with Ghent University over the next two years. In 'predictive policing' it comes down to the fact that the police zone knows, on the basis of predictions, which location in the zone is a risk area for burglaries and other crimes. This program will be developed by a doctoral student at Ghent University. First and foremost, she conducts a consultation round in which she collects 'big data' about all kinds of facts. That information goes much further than just a location and time, but also, for example, what the weather was like at the time. All that information will then be mapped. A question has been tabled in the Belgian senate about this case. In the government s written response it was stated that The federal police also aims to improve their analytical capacity to move towards predictive policing. Later, my administration considers it important above all to properly build the basis of what will make the ""predictive policing"" , Ie the data. Too often, the data exploited are police statistics and only these. By doing so, we instill a bias in the system and we force the algorithms to reproduce prejudices and misinterpretations of the facts. Police statistics primarily reflect police activity. They do not reflect the criminal reality of a given territory. Source: European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | BELGIUM: iPolice Predictive policing becomes possible thanks to the introduction of a new police computer system in iPolice. Ipolice uses databanks and algorithms help to identify times and spots when and where there is a heightened chance of criminal activity. Belgian police are currently creating the tools and building the system. Data will then be introduced to allow links to be made. Data will be supplied by the police but also by outside agencies like the Met Office. Police spokesman Theyskens stresses that predictive policing is an aid and no magic ball. He argues that Ethnic profiling will not be allowed! We have no intention to come to a Big Brother databank . Source: ; UNITED KINGDOM : National Data Analytics Solution (NDAS) The National Data Analytics Solution ( NDAS ) risk assessment tool uses statistical analysis and machine-learning to inform policing decisions, develop a list of individuals with a future likelihood to commit crimes, and to facilitate early interventions where appropriate. The sources of data that the system uses to conduct its risk assessments raise concerns that the system will be built to profile individuals on the basis of very sensitive, personal information , including stop and search data, data from social services, and the National Health Service. Where this data is used to indicate the likelihood of individuals criminality, it will inevitably flag up people whose profiles fit those who are over-represented in that data as being higher risk, prima facie infringing on certain individuals right to non-discrimination and equal treatment . It is particularly worrying that an individual might be profiled for policing purposes on the basis of their health conditions or their access to essential services, such as welfare or benefits. S ource : Sarah Marsh, Ethics committee raises alarm over predictive policing tool , The Guardian (20 April entrench-bias-ethics-committee-warns UNITED KINGDOM: Origins Software This tool deployed by the London Metroplitan Police Force purports to profile perpetrators and victims of crime. In official statements, designers of the software said the program is intended to redirect policing and other community policing services to enable safer neighbourhood teams to better understand the communities they serve. European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | However, there has been clear linkages between the use of this software to create ethnicity based crime profiles based on racialised assumptions and stereotypes of criminal behaviour. The designer of the program wrote that it is likely that different cultures do foster differences in behaviour among their members . This system was used since 2015 by the metropolitan police, amongst other police forces in the UK. Campaigners have warned of the links between racist stereotypes, algorithmic profiling, and overpolicing of racialised communit ies. Source: ethnic-groups-specialise-profile?CMP=share_btn_tw IV. Use of AI systems at the border, in migration control or in testing on marginalised groups EUROPE: Common Identity Repository (CIR) and Visa Information System (VIS) These systems are included insofar as they facilitate AI and other automated systems which profile, make decisions about, and potentially discriminate against migrants and other people on the move. These interoperable databases are used alongside predictive policing, security, migration control and anti-terrorism programs to profile already marginalised communities. Through these programmes, the EU will significantly extend the collection and use of biometric and biographic data taken from visitors to the Schengen area. Statewatch, reporting on the Visa Information System (VIS) notes that Data will be gathered on travellers themselves as well as their families , education, occupation and criminal convictions. Fingerprints and photographs will be taken from all travellers, including from millions of children from the age of six onwards . This data will not just be used to assess an individual s application, but to feed data mining and profiling algorithms. It will be stored in large-scale databases accessible to hundreds of thousands of individuals working for hundreds of different public authorities. This system, the Common Identity Repository (CIR), is being introduced as part of the EU s complex interoperability initiative and aims to facilitate an increase in police identity checks within the EU. It will only hold the data of non-EU citizens and, with o nly weak anti-discrimination safeguards in the legislation, raises the risk of further entrenching racial profiling in police work . [ ] Furthermore, the last decade has seen numerous states across the EU turn their back on fundamental rights and democratic standards, with migrants frequently used as scapegoats for society s ills . In a climate of European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | increased xenophobia and social hostility to foreigners, it is extremely dangerous to assert that intrusive data-gathering will counterbalance a supposed threat posed by non-citizens . Source: Statewatch Automated Suspicion: The EU s new travel surveillance initiatives ( available at: initiatives/ E UROPE: Military Drones at b order s Frontex, the European Border and Coast Guard Agency, has been testing unpiloted military-grade drones in the Mediterranean for the surveillance and interdiction of migrant vessels hoping to reach European shores to file asylum applications. While smart-border technologies have sometimes been called a more humane alternative to physical barriers, using invasive surveillance technologies can push migration routes towards more dangerous terrains, potentially resulting in more deaths in the Mediterranean as more migrant boats are prevented from reaching the shores of Europe. Source: Raluca Csernatoni, Constructing the EU s High-Tech Borders: FRONTEX and Dual-Use Drones for Border Management ( 27 European Security 175 . E UROPE: Frontex scales up AI uses in border control Frontex announces plans to scale up various projects involing the use of AI in migration control. These include: intuitive user interfaces and wearables supported by Artificial Intelligence and with Augmented Reality capabilities, 3D facial and iris verification technology for real-on-the-move border crossing experience, digital identity based on blockchain technology, highly accurate and cost effective handheld devices for drug and precursors detection on the field. The projects in question include: ANDROMEDA, ARESIBO, BorderSens, COMPASS2020, D4FLY, MIRROR and PERCEPTIONS. These address a wide spectrum of technological capabilities critical for border security, including unmanned platforms, document fraud detection, situational awareness, artificial intelligence, augmented reality, integrated systems and identification of illicit drugs and their precursors. Source: to-future-border-control-VetIX5 European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | HUNGARY, GREECE: iBorderCtrl In airports in Hungary and Greece, a pilot project by a company called iBorderCtrl, funded by Horizon2020 in the EU for 4 million Euro, introduced AI-powered lie detectors at border checkpoints. The company alleged that people s faces would be monitored for signs of lying, and if the system becomes more skeptical through a series of increasingly complicated questions, the person will be selected for further screening by a human officer. Various groups including EDRi and Amnesty International have challenged the iBorderCTRL project, outlining the major fundamental rights abuses, including discrimination, data protection and the infringement on the right to dignity. Most notably, the Greek NGO Homo Digitalis, member of EDRi, filled a petition to the Greek Parliament, underlining the lack of transparency and calling for a full data protection impact assessment, echoed by Member of the European Parliament Patrick Breyer from the Green Party, who launched a legal challenge to the EU Commission s Research Agency refusal to disclose ethical assessments of the iBorderCTRL system. Source: ; the-future-of- fortress-europe/ ; with full text of petition here https:// Further research into iBorderCTRL and similar systems at EU borders has emphasised the profound human rights ramifications and real impacts on human lives in the use of biometric and other bor - der technologies, such as the EU s iBorderCTRL , on vulnerable and/or marginalised groups such as refugees and people on the move. The project emphasises the significant human rights threats posed by the discriminatory outcomes that can arise when culturally-specific actions or behav - iours are interpreted by an algorithm , as well as the privacy and security threats to asylum seekers by the use of these technologies at borders. The work foregrounds the lived experiences of people on the move, as these perspectives are often left out of policy conversations on these far reaching technological experiments, including the coercive collection of biometric information in humanitar - ian settings such as refugee camps. The project also examines the responsibility of various actors, including state entities, the private sector, and international organizations such as UN agencies in sharing of data about refugees without appropriate safeguards . Sources: ( ( European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | ( ( ( 337780154_Technology_on_the_margins_AI_and_global_migration_management_from_a_human_rig hts_perspective UNITED KINGDOM: UK Home Office Visa Algorithms In 2019 the Joint Council for the Welfare of Immigrants (JCWI) and the Foxglove legal team, have launched a legal case challenging the discriminatory nature of the secretive visa algorithms used by the UK Home Office in what they are calling a digitally hostile environment. In the submission to the UK High Court which has granted judicial review, the group alleged that the algorithms has created three separate streams or channels for applications, including a so-called fast-lane that could lead to speedy boarding for white people to enter the country. The case alleges that applications from people whose nationalities may be flagged under various categories receive a higher risk rating , becoming subjected to far more intensive scrutiny by Home Office officials, taking longer to reach a decision and were much more likely to be refused. The case alleges that this type of risk streaming results in racial discrimination and therefore breaches the 2010 Equality Act. On the 7th August 2020, the Home Secretary Priti Patel announced the intention to end the use of the streaming algorithm. The government has pledged a full review of the system, including for issues of unconscious bias and discrimination. Source: hostile-environment ; ; nbsp-after-we-sued-them SLOVENIA: BORDER AI The Delo newspaper report stated that the police have acquired information about almost 800,000 airline passengers (so-called Passenger Name Records, PNR ) since October An algorithmic system tagged 8,928 passengers who were then thoroughly inspected before entering the country. The police stated that 40 per cent of those passengers were tagged as not suspicious and will not be reported next time they come to the border. Airline companies provided the data. European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | A police spokesperson explained they are not using algorithmic decision-making systems in this process. The system automatically matches a passenger to other police data such as criminal files. If a match is positive, a border police officer is informed and required to manually verify the information before inspecting a passenger. The police system also flags passengers who are using unusual or illogical flights . The Slovenian Human Rights Ombudsman and the Information Commissioner stated that such a system is not constitutional and filed a formal complaint in [The Information Commissioner claimed that the adopted changes of the amended law on the duties and powers of the police, which gave the police the power to gather the PNR, have legalised some excessive and inadmissible measures for gathering personal data without sufficient protection of citizens that have not been accused or suspected of any wrongdoings , e.g. terrorism or organised crime. They argued that all passengers should not be routinely scanned at the airport just because they are entering the state or landing during the transfer. The Human Rights Ombudsman supported their claims and the Slovenian Constitutional Court will therefore be required to rule on the constitutionality of the latest revision of the law on the duties and powers of the police. Source: GREECE: SPIRIT Project Greece and other EU borders: social media-scraping SPIRIT project funded by Horizon2020 does experiments with genuine end-users (investigation by Homo Digitalis) The example of the Horizon 2020-funded SPIRIT project reinforces the lack of fundamental rights compliance, transparency and accountability in a social media scraping use-case. Five law enforce - ment-related stakeholders participate in this research project: the Hellenic Police (GR), the West Midlands Police (UK), the Police and Crime Commissioner for Thames Valley (UK), the Serbian Min - istry of Interior (RS), and the Police Academy in Szcytno (PL). According to the website, the project aims to use tools such as face extraction and matching, to correlate information from social media data, which constitutes a form of mass surveillance, and to continuously initiate complex associa - tive searches over all sources relevant to criminal investigation. According to freedom of information requests, trials are planned for 2020 and 2021, including with genuine end users (i.e. potentially vulnerable migrants at EU borders) - despite the project purportedly being only an experiment. Source: %20signed.pdf?cookie_passthrough=1 European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | V. Biometric Surveillance ITALY: SARI facial recognition Facial recognition system threatens fundamental rights and fails to demonstrate lawful basis (investigated by Hermes Center) In 2017, Hermes Center reported on the Italian Ministry of Interior s purchase of a facial recognition system called SARI . They found that according to the Ministry s contract, the system would be used in both Enterprise (at rest) and Real-Time (live) modes, including at public demonstrations , which could lead to a chilling effect on freedom of expression and assembly , supported by a mass database of 10 million images. At the time of purchase, a lawyer confirmed that Italy had no war- ranty provision that specifies the ways and limits of capture and related database management . Hermes Center s investigation also found a lack of transparency/information, and a contractual requirement to use an algorithm approved by the NIST committee despite the committee not having tested any algorithms for accuracy of video sequencing, creating a risk of mistaken identity. ( Further investigations by Hermes Center in 2019, including using freedom of information (FOI) requests, found inconsistencies in the details provided by authorities about the system, including the size of the database and when the system entered into use, as well as a disproportionate use of the system against migrants and foreigners . The Italian data protection authority (Garante Privacy) is still investigating the Real-Time mode of SARI, although FOI requests show that the Ministry of the Interior has since stopped replying to the DPA . ( Sources: ( a54016211ff2 ( GREECE: Smart policing Smart policing project suspected of violating data protection rights; Hellenic police fail to provide information to the contrary (investigated by Homo Digitalis) European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | In 2019, the Hellenic Police signed a 4 million contract with Intracom Telecom, for a smart policing project, majority funded by the European Commission s ISF fund. Based on the provisions of the Directive 2016/680 (LED) and the Greek Law 4624/2019 implementing it, Homo Digitalis asked the Minister of Citizen s Protection whether or not the Hellenic Police had consulted the Hellenic Data Protection Authority (DPA) on this matter and/or conducted a related Data Protection Impact Assessment (DPIA) and what the applicable safeguards are, as well as to clarify the legal provisions that allow for such data processing activities by the Hellenic Police. In February 2020, the Hellenic Police replied but neither confirmed nor denied that a prior consulta - tion with the Hellenic DPA took place or that a DPIA was conducted . Moreover, Homo Digitalis claims that the Hellenic Police did not adequately reply about the applicable safeguards and the legal regime that justifies such data processing activities . With this request, Homo Digitalis claims that the processing of biometric data, such as the data described in the contract, is allowed only when three criteria are met: it is authorised by Union or Member State law, it is strictly necessary, and it is subject to appropriate safeguards for the rights and freedoms of the individual concerned. None of the above mentioned criteria is applicable in this case. Specifically, there are no special legal provisions in place allowing for the collection of such biometric data during police stops by the Hellenic police . Moreover, the use of these devices cannot be justified as strictly necessary since the identification of an individual is adequately achieved by the current procedure used . Nevertheless, such processing activities are using new technologies, and are very likely to result in a high risk to the rights and freedoms of the data sub - jects. Therefore, the Hellenic Police is obliged to carry out, prior to the processing, a data protection impact assessment and to consult the Hellenic DPA. Source: UK: Covert facial recognition Covert, unlawful public-private partnership in King s Cross covered up by police (investi- gated by Privacy International ) In 2018, civil society organisations in the UK discovered that a secretive facial recognition system in London s Kings Cross was the result of a 2-year partnership between the Metropolitan Police and the private owner of the site . They found evidence of the police unlawfully sharing images of people with a private company , and a failure to define what a wanted or suspicious person would be under law. The Police also dishonestly denied knowledge of the system until investigations revealed their confirmed involvement. ( European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | Further civil society reports showed that UK s deployments of biometric surveillance systems were wrong in 9 out of 10 cases . ( There are additionally examples of people in the UK being stopped for covering their face in public in order to avoid the systems, undermining the idea that passers-by can be considered to consent to being surveilled by the system. ( Sources: ( police-helped ( technology-failure ( london- trial-met-police-face-cover-man-fined-a8756936.html SERBIA, UGANDA: Government surveillance Huawei enters into opaque and undemocratic government surveillance partnerships (inves- tigated by Privacy International and SHARE Foundation ) EDRi members Privacy International (PI), SHARE Foundation and PI s partner in Uganda, Unwanted Witness, have been following the use of Huawei facial recognition technologies in undemocratic ways. In Serbia, the Ministry of Interior have refused to answer freedom of information (FOI) requests about the deployment of 1000 cameras across Belgrade , and have also failed to meet their legal obligation to perform a data protection impact assessment (DPIA) . SHARE Foundation have found evidence that footage from these cameras has at times been leaked to the public and at other times gone missing when it has been needed . ( Early reports from protests in Belgrade in July 2020 also suggest that the facial recognition systems may have been used to identify and prosecute lawful protesters. In Uganda, the facial recognition surveillance contract between the government and Huawei has been kept secret, with evidence that it has already been used to spy on political opponents . There are plans to expand the system and connect it to other forms of sensitive data, which is very con - cerning given the lack of transparency and the fears of human rights violations. ( Sources: ( European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | biometric-surveillance ( ARGENTINA: Public facial recognition Professor mistakenly stopped as a result of flawed facial recognition technology is arbitrar - ily detained by police (investigated by EDRi) In 2019, a professor and small business owner called Leo Colombo in Buenos Aires, Argentina was wrongfully incriminated by a facial recognition system which had matched him to a police database that mistakenly identified him as a suspected robber. ( Despite the police knowing that he was not their suspect, their lack of control over the system meant that they had to take Leo to the police station for a long, distressing and wasteful process. Whilst it was eventually resolved for Leo, a fel- low citizen with darker skin faced the same issue, and was imprisoned for 7 days as a result of suspected technological reinforcement of unconscious bias and the reluctance of humans to over - ride algorithms. Th ese are socio-technological phenomena which are evidenced in a growing body of academic research. ( While this example occurred in Argentina, the fundamental issues ( police inability to override machine decisions ; the power and influence of private companies to control systems and set parameters; abuse of data and databases ; false arrests; and more) are just as relevant in the EU. Sources: ( ( For example, see McGregor, L., Murray, D. & Vivian Ng. (. International Human Rights Law as a Framework for Algorithmic Accountability . The International and Comparative Law Quarterly, 68(, 309-FRANCE: School facial recognition France: court rules that facial recognition in schools violates data protection rights (investigated by La Quadrature Du Net ) European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | In July 2019, the Provence-Alpes-C te d Azur (PACA) regional authority asked France s data protection authority, the CNIL, for permission to use a facial recognition system for managing entry at Amp re high school in Marseille. This trial was intended to be a year-long experiment and was also being carried out at another school in the region (the Lyc e les Eucalyptus in Nice) and was said to be held on the basis of students and parents consent. The intention of the system was to facilitate the job of the schools security agents, helping them to spot identity theft and to prevent access of unauthorised persons to the school. This was designed to increase the security of students and staff and to speed up the time it took for students to enter the school premises. As the CNIL emphasised, a school facial recognition system is not necessary when there is the less intrusive alternative of using identity badges . Furthermore, this use of facial recognition is disproportionate as it brings in a large-scale, intrusive data surveillance pro gram against minors simply for the objective of school entry. ( Under the GDPR, there are legal requirements for consent and for the minimisation of data . As con- firmed by the CNIL and the Marseille regional Court, the Amp re facial recognition trial significantly violated both of these criteria , gathering data when it was unjustified, and being fundamentally unable to obtain legitimate consent due to the power dynamics between the public authority and students. Across EU law, young people are given enhanced protections (cf. Article 8 GDPR re infor - mation society services). Under GDPR, biometric data is considered highly sensitive (Article 9(). The biometric data of minors therefore requires the highest level of protections , which Amp re did not meet. ( ( Sources: ( deux-lycees-la- cnil-precise-sa-position ( ( GLOBAL: dignity, trauma, bias, function creep Various locations: facial recognition is violating rights to dignity, non-discrimination, due process, data protection and privacy (examples collected by Privacy International ) Researchers scraped videos of t ransgender vloggers off YouTube without their knowledge [or consent] to train facial recognition software ( European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | Study: Emotion-reading facial recognition displays racial bias ( In this study, the faces of black men were consistently interpreted as displaying more negative emotions / anger than white men by a facial recognition algorithm. Study: Facial recognition research erases trans and nonbinary people ( Facial recognition systems display inherent bias ( This includes a facial recognition algorithm labeling non-white people as gorillas , ani - mals , or apes and telling Asian users their eyes were closed when taking photographs . Amazon s facial recognition software, Rekognition, incorrectly matched 28 US lawmakers to [criminal] mugshots ( One Ring to watch them all ( This piece reveals that Amazon have been training law enforcement to circumvent inves- tigatory warrants by using their Ring technology, despite the fact that the technology consistently targets black people . There is evidence that this technology is now being used in the UK. Further examples (collected by EDRi): Increasing evidence of facial recognition leading to traumatic wrongful arrests Black man in US wrongfully arrested due to facial recognition system, highlighting seri - ous failure of due process and investigatory standards by police. Lack of evidence that biometric surveillance works, challenging its grounds of necessity and proportionality Many studies have shown that despite claims by law enforcement and private compa - nies there is no link between surveillance and crime prevention. Even when studies have concluded that at best CCTV may help deter petty crime in parking garages , this has only been with exceptionally narrow, well-controlled use, and without the need for facial recognition. ( ) Take for example iBorderCtrl, a Horizon 2020-funded project that aims to create an auto - mated border security system to detect deception based on facial recognition technology and the measurement of micro-expressions. In short, the EU spent 5 million on a project that detects whether a visitor to the continent is lying or not by asking them 13 questions in front of a webcam. [ ] The historical practice of lie detection is lacking in substantial scientific evidence and the AI technologies being used here to analyse micro expressions are just as questionable . [ ] To make matters worse, the Commission is ignoring the transparency criteria outlined in the Ethics Guidelines by refusing to publish European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | certain documents, including an ethics assessment, on the grounds that the ethics report and PR strategy are commercial information of the companies involved and of commercial value . ( funding-dystopian-artificial-intelligence-projects/ ) Significant evidence of function creep and combining databases Assistant Professor at Leiden University reports on the increasing use of social media and other data in border assessments, including algorithms which systematically assign higher risk ratings to ethnic minorities ( passports-biometric-risk-profiles-the-codes-we-carry/ ) EDRi member CCC have collected information relating to the use of facial recognition by police to analyse images and videos collected during the G20 meeting in Hamburg by the police, by public transport authorities and by various third parties through a reporting platform. The Hamburg data protection authority ordered the police to delete the 17 ter - abytes of data that they analysed, but the police ignored the order and then took the data protection authority to court ( loeschung-biometrischer- g20-datenbank/ ) Examples of unlawful or problematic uses stopped by national data protection authorities (DPAs) Sweden ( Swedish DPA finds school facial recognition unlawful, issues fine ( swedens-first-gdpr-fine_en ) France ( French DPA finds two school facial recognition systems unnecessary and disproportionate ( dans-deux-lycees-la-cnil-precise-sa-position ) UK ( UK DPA finds use of covert facial recognition by private/public partnership deeply concerning ( blogs/2019/08/statement-live-facial-recognition-technology-in-kings-cross/ ) UK ( UK DPA tells law enforcement to slow down and justify the use of facial recognition in public spaces ( and-blogs/2019/10/live- facial-recognition-technology-police-forces-need-to-slow- down-and-justify-its-use/ ) Poland ( Polish school fined for defacto mandatory fingerprint scanning system for distributing school lunches ( ) Sweden ( Swedish DPA launches investigation into Clearview AI ( anledning-av-clearview-ai/ ) European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 | As of July 2020, the Hamburg data protection authority is also considering opening an investigation against Clearview AI, according to investigations by CCC. As of July 2020, CCC have also started investigations into PimEyes, a Polish company with similar functionality to Clearview AI, for clear breaches of the GDPR. CCC have submitted a complaint to the Hamburg data protection authority. For more information on EDRi s work across artificial intelligence and fundamental rights, please contact Sarah Chander, Senior Policy Officer, at sarah.chander@edri.org For more information on EDRi s work on biometrics and fundamental rights, please contact Ella Jak - ubowska, Policy and Campaigns Officer, at ella.jakubowska@edri.org European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. +32 2 274 25 70 |",en,"+32 2 274 25 70 | Source: ; SPAIN : BOSCO and B ono Social de Electricidad The Bono Social de Electricidad is a discount on energy bills to at-risk individuals and families. Energy companies use BOSCO, a software created by the Spanish Ministry for Green Energy Transition, to decide who is entitled to the subsidy. In a climate of European Digital Rights | 12 Rue Belliard, 1040 Bruxelles, Belgium | Tel. Most notably, the Greek NGO Homo Digitalis, member of EDRi, filled a petition to the Greek Parliament, underlining the lack of transparency and calling for a full data protection impact assessment, echoed by Member of the European Parliament Patrick Breyer from the Green Party, who launched a legal challenge to the EU Commission s Research Agency refusal to disclose ethical assessments of the iBorderCTRL system.",risk
CEA (France),F550327,08 September 2020,Academic/research Institution,Large (250 or more),France,"Commissariat l nergie atomique et aux nergies alternatives Atomic Energy and Alternative Energies Commission (CEA) 1 Artificial intelligence ethical and legal requirements CEA Feedback on the roadmap / inception impact assessment Feedback period: 23 July 2020 - 10 September 2020 Date of issue : September 8th 2020 CEA welcomes the opportunity to provide feedback on the Commission s Inception Impact Assessment for a European legal act aimed at addressing the ethical and legal issues raised by AI . We welcome the initiative objectives, in particular the intention to create a harmonised framework in order to reduce burdensome compliance costs derived from legal fragmentation. It is worth noting , a trustworthy AI as explained in HLEG on A I is at the same time a lawful, an ethical, and a robust AI. It takes into account all the concerns reflected in the trustworthy AI assessment list such as technical robustness, safety, security, privacy, traceability, explainability, auditability, in addi tion to diversity, non -discrimination, fairness , and respectability of fundamental rights. Actually, there is no need for an entirely new legislation, as current European legislation and standards already address many of these concerns. However, they may have some gaps that need to be filled. Below are our comments on the outlined legislative options. - Option 1 - non-legislative approach to facilitate and spur industry -led intervention: option 1 is probably convenient for low-risk AI solutions. However, there is a need of a minimum legislation that classifies the solutions according to their level of risk. Self-reporting of complianc e with the HLEG ethical guidelines must remain verifiable and auditable by law. The Commission's support and encouragement of industry -led intervention towards trustworthy AI is very important, however in no case can this replace the enforcement of legislations that define the responsibilities and obligations of AI solution stakeholder s. - Option 2 - legislative instrument setting up a voluntary labelling scheme : the diversity and multiplicity of uses of AI applications may make it very hard to have a simple labelling for applications with variable risks. On the other hand, a multitude of labelling depending on the use cases could be difficult to understand by the consumer. Particular attention should be paid to the concept of labelling that may lead to dangerous issues : labelling must always be carried out against a charter verifiable b y a third party. Self -labelling involves too much risk with regard to its verification. CEA is not in favour of this option. - Option 3a: legislation for a specific category of AI applications only, notably remote b iometric identification systems: biometric identification systems must be subject to strong legislation. However, CEA considers it is more efficient to put in place a legislative instrument that applies to all AI applications and that modulates according to the AI application risk aspects. - Option 3b: legislation establishing mandatory requirements for high -risk AI applications. High-risk AI applications should not be binary; there must be a gradation of risks as in all sectors with a safety or security aspect. Different rules adapted t o the application risk levels (like those in the IEC 61508 standard) are needed . Commissariat l nergie atomique et aux nergies alternatives Atomic Energy and Alternative Energies Commission (CEA) 2 - Option 3c: the EU legislative act could cover all AI applications . CEA supports option 3C. Laws and standards cover all human activity , but this does not prevent creativity , innovation and freedom of expression. On the contrary, the absence of laws and therefore an increase of incidents combined with a legal vacuum may impede the AI development and deployment . In summary, CEA supports option 3C; however, legislation should consider an AI application like any software application. We cannot blame the AI : following the use case, the application providers, issuers, operators, or users are responsible for a malfunctioning of the applications. Finally, changes in the system may regularly occur, thus the requirements of trustworthiness will be too light without a continuous risk assessment. There is a real need for more R&D in the risk area in order to conceive and create continuous risk assessment processes.",en,Commissariat l nergie atomique et aux nergies alternatives Atomic Energy and Alternative Energies Commission (CEA) 1 Artificial intelligence ethical and legal requirements CEA Feedback on the roadmap / inception impact assessment Feedback period: 23 July 2020 - 10 September 2020 Date of issue : September 8th 2020 CEA welcomes the opportunity to provide feedback on the Commission s Inception Impact Assessment for a European legal act aimed at addressing the ethical and legal issues raised by AI . Commissariat l nergie atomique et aux nergies alternatives Atomic Energy and Alternative Energies Commission (CEA) 2 - Option 3c: the EU legislative act could cover all AI applications .,risk
Digitalcourage e.V. (Germany),F2665649,06 August 2021,Non-governmental organisation (NGO),Small (10 to 49 employees),Germany,"European Commission adoption consultation: Artificial Intelligence Act Brussels, 3 August 2021 European Digital Rights (EDRi) outlines the following analysis of the Proposal for a Regulation Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) 2021/0106(COD) .1 This input is intended for the European Commission consultation on the adoption of the Artifi cial Intelligence Act. It builds on previous EDRi positions on the EU s approach to artificial intelligence regulation, including Recommendations for a Fundamental rights- based artificial intelligence regulation 2 and Ban Biometric Mass Surveillance .3 Section A summarises an initial analysis of the Artificial Intelligence Act, and Section B begins to chart recommendations for improvement and adaption to ensure fundamental rights are duly protected: A)Analysis: Artificial Intelligence Act B)Recommendations for a fundamental rights-based Artificial Intelligence Act EDRi will publish a full response to the Artificial Intelligence Act in autumn 2021, outlining the network s recommendations toward the European Parliament and the Council of the EU. 1 European Commission (, Proposal for a Regulation laying down harmonised rules on artificial intelligence (Artificial Intelligence Act): %3A52021PC0206 . 2 EDRi (, Recommendations for a Fundamental rights-based artificial intelligence regula - tion: . 3 EDRi (, Ban Biometric Mass Surveillance: ; for more EDRi resources on artificial intelligence, consult the EDRI AI and fundamental rights document pool: document-pool/. 1 Summary of recommendations: EDRi recommends that the European Parliament and the Council of the EU implement the following improvements to the Artificial Intelligence Act (AIA). For the full recommendations, see section B. Ensure effective protection against prohibited practices and ad - dress the full scope of unacceptable risks through AI: a.Strengthen existing prohibitions in article 5 to provide meaningful pro - tection against fundamental rights violations and individual and col - lective harms; b.Comprehensively prohibit the use of remote biometric identification in publicly accessible spaces for any purpose, and implement a general ban on any use of AI for an automated recognition of human features in publicly accessible spaces; c.Include new prohibitions on the following practices which are incompatible with fundamental rights and democracy, and thus pose an unacceptable risk: i.Uses of AI in the field of law enforcement or criminal justice that purport to predict future behaviour; ii.Uses of AI in the field of migration control in ways that undermine the right to claim asylum; iii.Uses of AI that implement invasive surveillance, monitoring and algorithmic management in an employment and educational context; iv.The use of AI to categorise people on the basis of their human features, which can pose a grave and disproportionate threat to all human rights, in particular equality and non-discrimination; v.Placing on the market, putting into service or use of AI to infer, predict, analyse or assess a person s emotions, feelings, emotional state, beliefs, preferences, intentions or otherwise inner thoughts, as well as to use human features, behaviours or expressions to predict future actions or behaviours; vi.Uses of AI that constitute mass surveillance. Adapt the AIA to ensure a holistic, democratic and future-proof framework: a.Introduce a democratic, inclusive and accessible process by for the insertion of new prohibitions. Include criteria for unacceptable risk and the addition of future prohibitions into the AIA; b.Ensure the potential to update the high risk use case areas in the future (amending article in addition to updating the use case sub- areas ; 2 c.Respond to gaps in regulation with respect to economic and environmental impact, structural forms of inequality and AI and migration control, law enforcement and worker surveillance, mass surveillance, and exports of high-risk or prohibited AI outside the EU; d.Remove loopholes in articles 2( and 83 which currently leave out of scope of the AIA AI systems used as part of international law enforcement agreements; e.Remove the broad exemption to forgo the duty to conduct a conformity assessment on grounds of public security in article 47; f.Remove the exemption to the principle of purpose limitation contained in article 54((a) for innovative AI within the regulatory sandbox provisions for uses in the criminal justice context. Ensure responsibility to those subjected to AI systems with en - hanced obligations on users of all AI systems: a.Mandate users to conduct and publish an ex ante human rights impact assessment before putting a high risk AI system into use, clearly outlining the stated purpose for which the system will be implemented; b.Implement on users a duty to cooperate with national competent authorities investigating AI systems for potential threats to fundamental rights or safety under articles 65 and 67 for all AI systems, regardless of risk designation; c.Implement a duty on users to meaningfully consult with institutions, civil society and social partners representing affected groups before deploying high risk AI systems; d.When the user of any AI system is a public authority, implement a notification requirement to all those impacted by a decision made by the system. Implement meaningful public transparency for high risk AI sys - tems: a.Ensure meaningful public transparency by requiring registration in the EU database (article of all high risk AI systems (and potentially also all AI systems to which people are subject) that are put into use. This would enable individuals and civil society to access information about AI systems in operation; b.Ensure the inclusion of instructions for use for AI systems in law enforcement and migration, asylum and border control management in the public database as per Annex III, points 1, 6 and Remove the exemption contained within Annex VIII, point 11; c.Require providers to include access to the conformity assessment alongside the instructions for use as per article 13(-( in the public database under article 60; d.Require providers to provide more thorough details about the system to the users as part of article 13(; 3 e.Remove the exemptions in article 52 relating to the transparency of AI systems used for detection and prevention of criminal offences, (as argued by the EDPB and EDPS) and for the prosecution of people. When AI systems under article 52 are used for investigation, suspects should be notified post factum . Facilitate accountability: Include oversight and enforcement in - frastructures that work for people: a.Ensure a cohesive national enforcement structure; b.Include flagging and redress mechanisms allowing individuals and collectives to contest and seek redress for all AI systems that cause harm and threaten fundamental rights; c.Implement a more democratic governance infrastructure, with greater independence for the European AI board. 4 EDRi is the biggest European network defending rights and freedoms online. The EDRi network is a dynamic and resilient collective of 45 NGOs, as well as experts, advocates, and academics working to defend and advance dig - ital rights across Europe and beyond. T ogether, the EDRi network builds a movement of organisations and individuals pushing for robust and enforced laws, informing and mobilising people, and promoting a healthy and accountable technology market. Acknowledgments With thanks to the EDRi network for their contributions. In particular: Access Now ARTICLE 19 Bits of Freedom Chaos Computer Club (CCC) Digitale Gesellschaft Schweiz Free Software Foundation Europe (FSFE) Panoptykon Foundation Electronic Frontier Norway (EFN) Electronic Privacy Information Center (EPIC) epicenter.works Homo Digitalis IT-Political Association of Denmark (IT-POL) Statewatch Thanks also to conversation partners: Amos T oh, Fieke Jansen, Jill T oh, Griff Fer - ris, Jeremias Adams-Prassl, Reuben Binns, Aislinn Kelly-Lyth, Petra Molnar, Alyna Smith, Mher Hakobyan, Marlena Wisniak, Agathe Balayn, Seda G rses, Mute Schimpf, Jascha Galaski, Mark Brakel and members of the Digital Dignity Coali - tion for their thoughtful insights. 5 (A) Analysis: Artificial Intelligence Act EDRi welcomes the European Commission s globally significant step towards regulating the development and deployment of artificial intelligence (AI) systems. Uses of AI systems have the ability to enable mass surveillance and intrusion into our personal lives, reinforce some of the deepest societal inequalities, fundamentally alter the delivery of public and essential services, shift more power into corporate hands and disrupt the democratic purpose. The proposal thus takes a notable step to acknowledge that some uses of AI are simply unacceptable and must be prohibited. However, we would like to make a number of suggestions to ensure that the AIA is in line with the Charter of Fundamental Rights of the EU (CFEU), future proof , and a role model for other rights-protective future AI legislation around the world. Overall, EDRi raises a number of concerns relating to the AIA as a regulatory framework, specifically in relation to the extent to which it protects fundamental rights and is able to address broader structural, political and economic issues as a result of the widescale promotion and adoption of AI systems in various areas of life. We have argued that any approach which assumes benefits from the widescale uptake of AI will be problematic from a fundamental rights perspective and should not be a policy objective in itself.4 Further, we highlight throughout that there are structural concerns relating to the extent to which the use of AI systems can systematically target, harm and exclude marginalised communities, exacerbating existing power imbalances in society. Additionally, there are broad questions as to how far the AIA as a framework is sufficiently comprehensive to address these structural harms, due to its tendency toward de-regulation of all but the most narrowly-defined unacceptable of AI systems, the lack of obligations on users and the lack of provisions for individual or collective redress for those subjected to AI systems. As such, the following outlines EDRi s main analysis of the proposal for an Artificial Intelligence Act. 1Inconsistencies with stated objectives of the AIA The Explanatory Memorandum to the Commission s proposal establishes that the primary purpose of the AIA is to implement the second objective [of the Commission s White Paper on AI] for the development of an ecosystem of 4 EDRi (, Recommendations for a Fundamental rights-based artificial intelligence regula - tion: . 6 trust by addressing the risks associated with certain uses of such technology based on EU values and fundamental rights (. The Memorandum continues that this is separate from the Commission s aim of promoting the uptake of AI ( which while important - is not the main goal of this Act. The four specific objectives which the Memorandum describes are also aligned with the ambition of trustworthy AI: safety and respect for fundamental rights; legal certainty; enhanced governance and enforcement of safety and fundamental rights; and a lawful, safe and trustworthy single market (. Despite this reassurance, the AIA proposal seems, at its core, designed to enable AI uptake rather than to limit or mitigate its harms. The fact that the vast majority of rules apply only to the narrowest sub-set of high risk AI - which the Commission explicitly admits is a minimum necessary approach ( - is at odds with EU fundamental rights obligations. It also contradicts the precautionary principle , which civil society has warned is necessary, given the vast contextual harms which may arise from the use of AI systems to which people are subject. Furthermore, the Memorandum describes that the proposed rules cover the development, placement on the market and use of AI systems [italics for emphasis] ( when in fact, the use of AI receives insufficient attention in the proposal. Even the prohibition of certain forms of remote biometric identification (RBI) under article 5 is recognised in the Memorandum as not being a real ban: the RBI rules, the Commission explains, constitute only specific restrictions and safeguards (. Whilst the AIA seeks to classify the risk level of an AI system based on the intended purpose of the AI system (, it has created a set of rules and obligations which are largely unable to achieve this aim. As critics are increasingly pointing out, the AIA proposal attempts to transplant a typical product safety framework into an often novel AI context. By failing to account for the specificities of artificial intelligence, for example the variety of ways in which it can be applied, the importance of context, the fact that it is often sold as a service (not a product), and its self-learning nature, the proposal falls short of what would be needed to anticipate, prevent or at the very least mitigate the myriad ways in which it can cause harm. There is a broader concern as to the extent to which the AIA s primary objective of harmonising the single market for AI products is compatible with the other objective of safeguarding fundamental rights, and the broader need to mitigate the societal impacts of AI. The promotion of AI s uptake and the push for a harmonised single market, via the act s maximum harmonisation function, may preclude Member States from introducing higher fundamental rights standards than those contained in the AIA.5 Despite the protection of 5 Veale and Zuiderveen Borgesius (, Demystifying the Draft EU Artificial Intelligence Act : . 7 personal data being one of the treaty bases of the proposal, it is clear that the proposal goes nowhere near far enough to ensure the protection of fundamental rights, and in doing so, contradicts its own stated aims and objectives to ensure trustworthy AI. 2Prohibited practices: Incomplete coverage of unacceptable AI and fundamental rights threats Whilst it is positive that the AIA proposal foresees that some uses of artificial intelligence pose an unacceptable risk to fundamental rights and therefore must be prohibited under article 5, the AIA s approach to unacceptable risks falls short in two main ways. 1 Proposed prohibitions are too wide and vague Firstly, article 5 leaves a wide scope for interpretation, broad exceptions and in some cases unreasonably high thresholds for systems to be prohibited. As such, there is a risk that this provision fails to prevent the worst excesses of potential fundamental rights abuses arising from AI systems. Those shortcomings are: Subliminal and exploitative uses Physical or psychological harm: Articles 5((a) and (b) are unduly narrow leading to significant concerns that they will fail to prevent against manipulative or exploitative uses of AI. In particular, that both prohibitions only apply when there is or likely to be physical or pyschological harm foresees a burden of proof on invididuals to demonstrate future or actual harm (without creating a legal path to flag or contest such systems, see section 5 below). Whilst the background to the AIA acknowledges the opacity and unpredictability of AI systems, this provision does not incorporate these concerns into the drafting of this provision. The requirement that the use is in order to materially distort behaviour adds an unreasonably high threshold; Individual harm: Both provisions are drafted in narrow, individualistic terms, not foreseeing that many such systems are unlikely to target specific persons, but rather whole groups of people in society; Limited vulnerabilities: Article 5((b) attempts to prevent only such uses of AI that may exploit people on the basis of specific vulnerabilities age and physical or mental disability. It is unclear why the AIA limits only to these vulnerabilities rather than taking a comprehensive approach and prohibiting uses of AI that exploit vulnerabilities based on the full range of protected characteristics under EU law. 8 Social scoring Limitation to public uses: The prohibition of social scoring systems contains a number of limitations suggesting an extremely high threshold for its application. Firstly the prohibition is limited to uses in a public context, by public authorities or on their behalf, thus excluding commercial uses, such as scoring of customers on online platforms leading to different service options; Trustworthiness : The provision is limited to those systems which evaluate or classify trustworthiness, without providing a definition of trustworthiness in the act. This could be potentially limiting for those systems that have a parallel impact but do not purport to map trustworthiness per se; General purpose score: The implicit grounding of the prohibition in the notion of a single score to be used for general purposes (as indicated in Recital suggests that many of the examples of risk scoring used in specific governmental contexts (such as risk-scoring for welfare fraud in the notorious Dutch SyRI case) are to be excluded from the scope of the prohibition, unless they deploy data collected in one context to be used in another or have an unjustified or disproportionate impact ; Temporal limit: The provision also includes a temporaral limitation, thus applying to systems which evaluate or classify the trustworthiness of natural persons over a certain period of time , another limiting threshold; Added conditions : The prohibition is limited to uses which lead to detrimental or unfavourable treatment in contexts other than that in which the data was collected, or in a nature that causes unjustified or disproportionate harm. Such conditions suggest that the central principle is not the harm caused, otherwise these conditions would not be relevant. Arguably, there are reasons to contest social scoring systems regardless of the presence of proof of unfavourable outcomes insofar as they reduce the complexity of human experience to a combination of limited, measurable indicators, with potential negative implications for fundamental rights to good administration and human dignity. Remote biometric identification The AIA s prohibition of real-time remote biometric identification (RBI) in publicly accessible spaces for the purpose of law enforcement addresses only a small range of the many practices that can constitute biometric mass surveillance.6 As we will recommend further in Section B, the AI Act must be amended to ensure it does not undermine existing fundamental rights standards; furthermore all remote biometric identification and the use of AI for the automated recognition of human features must be prohibited without exceptions . 6 EDRi (, Ban Biometric Mass Surveillance : . 9 Despite accepting that real-time RBI can unduly restrict people s fundamental rights (Recital , and noting that the majority of respondents to the Commission s Consultation were in favour of new rules, article 5 of the AIA contradictorily risks creating a blueprint for conducting biometric mass surveillance, instead of a substantive prohibition of these practices. In its approach to RBI, the Act requires a lot of improvements to bring it in line with existing standards of fundamental rights and data protection: Wide exceptions with low thresholds: Despite recognising the severe undue fundamental rights risks of real-time RBI, the AI Act allows Member States to adopt three broad and highly discretionary exceptions to the prohibition (d). In article d.i, the exception for potential victims of crime suggests that there need only be the potential of a crime, creating a dangerously wide and potentially arbitrary scope which may be easily misused to justify perpetual and untargeted use; furthermore, the fallacious reference to targeted search fails to recognise that remote biometric identification is by definition always mass / indiscriminate.7 d.iii sets a potentially very low bar to permit RBI to search for perpetrators or suspects of crimes under the European Arrest Warrant, which is a long list of crimes including non-violent ones like counterfeiting currency, forging administrative documents or trafficking endangered plants. This exception is further problematic because it is based on the assumption that facial recognition or other RBI is useful for the prosecution of a perpetrator or a suspect of a criminal offence . However, due to its inherent probabilistic nature (sometimes referred to as the base rate fallacy phenomenon), biometric identification can never and will never provide conclusive identification or inferrence.8 Thus it cannot be relied upon in a court of law, as shown in a 2019 case in the Netherlands where the defendant was acquited because a facial recognition match could not meet the burden of proof.9 The Act claims that these exceptions are narrowly defined (Recital , but these examples show just how wide the exceptions and how low thresholds are to permit the mass infringement of fundamental rights; Safeguards in name only: The exceptions to the prohibition are furthermore subject to a series of purported safeguards (articles 2 and , including temporal and geographic limitations, and judicial or administrative authorisation. Given that the ex ante authorisation safeguard can be waived in the event of each Member State s 7 Garante per la Protezione dei Data Personali (Italian data protection authority) (, Ri - conoscimento facciale: Sari Real Time non conforme alla normativa sulla privacy: https:// . 8 EDRi (, Why EU passenger surveillance fails its purpose: eu-passenger-surveillance-fails-its-purpose/ . 9 EDRi (, The Rise and Rise of Biometric Mass Surveillance in the EU: A legal analysis of biometric mass surveillance practices in Germany, the Netherlands, and Poland, [65]: https:// edri.org/wp-content/uploads/2021/07/EDRI_RISE_REPORT.pdf . 10 interpretation of the vague and discretionary threshold of a duly justified situation of urgency (, it is possible that in its current form, the AIA may not be able to substantively prevent any law enforcement use of real-time RBI. This is especially pertinent in the context of systemic threats to democracy and the rule of law across the EU, evidenced for example in the pending European Court of Human Rights case brought by Panoptykon Foundation against the Polish government for the non-existence of effective supervision over the government s surveillance activities.10 The authorisation process is thus vulnerable to government pressure and even further weakens the already deficient RBI prohibition ; The authorisation process : Dr N ra Ni Loideain has further noted that the authorisation is a flawed process which does not meet the standards of Charter of Fundamental Rights of the European Union (CFEU): firstly, in the current proposal, prior authorisation is permissible on the basis of a the low evidentiary threshold of objective evidence or clear indications presented to it [the authorising authority] (italics for emphasis) (. This is a low bar in which the decisive factor in whether or not to authorise RBI can be clear indications provided by the very entity with a vested interest in using RBI. Furthermore, this authority is compelled to assess the seriousness, probability and scale of the harm caused in the absence of the use of the system (italics for emphasis) (a). This coercive and speculative approach seems at odds with fundamental rights principles of necessity and proportionality (CFEU article which require that the burden of proof is on demonstrating that a use case or action does not unduly restrict fundamental rights;11 Incompatibility with requirements of necessity and proportionality: In addition to the failure of the proposed safeguards to comply with existing fundamental rights law, the AI Act has further been criticised for its misapplication of the tests of necessity and proportionality for conducting RBI. The EDPS and EDPB Joint Opinion, for example, states that: [t]he reasoning behind the Proposal seems to omit that when monitoring open areas, the obligations under EU data protection law need to be met for not just suspects, but for all those that in practice are monitored (paragraph .12 As demonstrated in EDRi s Ban Biometric Mass Surveillance position paper, real time and post RBI (both of which constitute indiscriminate biometric surveillance) are inherently unnecessary and disproportionate under the CFEU and should be fully prohibited. Conversely, in its current form, the AIA creates the conditions for law enforcement agencies to unduly restrict the 10 Panoptykon Foundation, No control over surveillance by Polish intelligence agencies. ECHR demands explanations from the government , December - ernment-surveillance-echr-complaint . 11 Ni Loideain, N., University of London, a rticle forthcoming, August 12 EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act): edps-proposal-regulation-european_en 11 fundamental rights of whole populations through biometric mass surveillance practices, contravening the CFEU; Threat to existing data protection laws : Recital 23 clarifies that this Regulation is not intended to provide the legal basis for the processing of personal data . Therefore, it needs to be clarified that the lex specialis status of the prohibition on real-time RBI does not provide a legal basis for law enforcement under one of the exceptions (d), nor does it weaken existing protections of biometric data under the Data Protection Law Enforcement Directive (LED) or national implementations of the LED; The post RBI loophole: The AIA draws a fundamentally arbitrary distinction between real-time and post uses of remote biometric identification by virtue of a significant [temporal] delay between collection and processing which is, in fundamental rights terms, irrelevant. By doing so, the Act creates a loophole which permits law enforcement agencies to retrospectively apply biometric identification to CCTV footage or photographs. This form of biometric mass surveillance can unduly restrict people s rights equally as profoundly as real-time methods and sometimes even more invasively so, due to the potential to pool data from many different sources across place and time. This erroneous distinction also leaves the deployment of equally harmful post RBI systems free from the restrictions in time, place and authorisation that apply for the exceptional uses of real-time RBI deployments (, meaning that the potential for mass surveillance from post RBI is even further strengthened. Similarly, the definition of remote ( is overly narrow in ways that may also create loopholes, for example arbitrarily and illogically linking the system definition to the (lack of) prior knowledge of the user. This must be corrected, as we will discuss further in the recommendations laid out in Section B; No ban on other purposes ( i.e. other governmental or private purposes): The AIA limits the RBI prohibition to law enforcement purposes, on the basis that other purposes are already sufficiently prohibited under the General Data Protection Regulation (GDPR). By doing so, the AIA fails to acknowledge the existing wide exemptions under the GDPR, which EDRi has demonstrated have already led to the systematic and sustained violations of people s rights and freedoms across the EU.13 In this manner, the proposal misses the opportunity to bring in complementary rules which will reinforce and strengthen the provisions on the processing of biometric data in the GDPR and align with the fundamental rights enshrined in the CFEU. Furthermore, by addressing only the use of these systems, EU providers may still be able to develop and sell rights-violating RBI systems outside of the EU; No ban on other types of processing: By limiting the prohibition to real-time RBI, the AIA fails to address other biometric mass surveillance practices such as singling out individuals based on their biometric characteristics. The protection of fundamental rights need a broader 13 EDRi ( The Rise and Rise of Biometric Mass Surveillance in the EU: A legal analysis of biometric mass surveillance practices in Germany, the Netherlands, and Poland, https:// edri.org/wp-content/uploads/2021/07/EDRI_RISE_REPORT.pdf 12 prohibition of biometric mass surveillance practices than just remote biometric identification; Infrastructural enablement : The AIA fails to address the underlying issue of biometric mass surveillance infrastructures and enabling practices. Such infrastructures and practices can proliferate under the Act because post uses of RBI, real-time exceptional uses (under d.i - iii), and uses for non-law enforcement purposes will all ensure that the required databases, software and hardware remain readily accessible. In essence, because only the use is prohibited, and not the development, sale, purchase or deployment, the implication is that biometric devices, software and databases can be bought, installed and maintained, and may be turned on with a simple authorisation (which we have already demonstrated is highly flawed). This doesn t just fail to stop biometric mass surveillance: it may even enable and encourage it by incentivising governments to make greater use of the costly, convenient infrastructures that are already in place; Online spaces out of scope: The exclusion of online spaces from the definition of publicly-accessible spaces which are subject to the prohibition - contrary to recommendations from EDRi14 and more recently, the EDPS and EDPB15 - suggests that the AIA may not prevent the scraping of online sources to develop commercial databases and software such as those offered by Clearview AI to many European law enforcement agencies. This is despite a number of EU data protection authorities (DPAs), including the Hamburg DPA, confirming the inherent rights-violating nature of such practices. The COVID-19 pandemic has made this even more urgent, as large parts of people s everyday lives have necessarily moved online. Online spaces must be included in the definition of publicly-accessible spaces, and the data scraped from online spaces (such as from social media) included in the prohibition;16 Vague and complicated wording : The wording relating to RBI is unnecessarily vague and complicated, creating risky grey areas and making it overly onerous for civil society as well as AI providers and users to apply the rules in a consistent and rights-respecting manner. 2Lack of mechanism to add unacceptable uses Secondly, the proposal does not introduce a mechanism by which unacceptable uses of AI may be added in the future , unlike the process outlined in article 7 for updating the list of stand alone high risk use cases. 14 EDRi (, Ban Biometric Mass Surveillance: 2020/05/Paper-Ban-Biometric-Mass-Surveillance.pdf . 15 EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act): edps-proposal-regulation-european_en . 16 noyb ( Clearview AI s biometric photo database deemed illegal in the EU, but only par - tial deletion ordered: . 13 The lack of more general criteria to establish unacceptable risk is an inconsistency in the framework, leaving a lack of clarity as to why the article 5 prohibitions were included (to the exclusion of others below), whilst not providing a framework for future unacceptable uses cases to be added as the AI market evolves. As outlined in the recommendations below, such criteria might include the impact on fundamental rights, structural power imbalances around the context of deployment (including potential for enhanced discrimination, marginalisation, inequality), lack of capacity for individuals, groups or civil society to contest the usage, etc. 3Unacceptable use cases missing from list of prohibited AI Thirdly, the proposal does not put forward a holistic set of prohibitions covering the full range of unacceptable uses of AI . As highlighted by EDRi alongside 62 human rights organisations,17 116 MEPs18 and the European Data Protection Supervisor and European Data Protection Board,19 there are further use cases of AI that pose unacceptable risks to fundamental rights and democracy, and therefore must be prohibited under the AIA. These are the following: Predictive policing and uses of AI to risk assess for future criminality, offending or re-offending. The use of predictive modelling to forecast where and by whom certain crimes are likely to be committed, alongside uses of AI to detect risk in the context of criminality, unduly and unnecessarily inpinge on a number of fundamental rights. In particular, the rights to dignity,20 to an effective remedy and a fair trial,21 to good administration22 as well as the presumption of innocence23 are compromised by practices that attempt to automate the prediction and characterisation of the future behaviour of individuals and groups, with potentially harmful consequences for their liberty and privacy. In addition, systems designed to assess risk or predict crimes have been demonstrated to repeatedly score poor, working class, racialised and 17 EDRi ( Open letter: Civil society call for the introduction of red lines in the upcoming Commission proposal on artificial intelligence: red-lines-in-the-european-unions-artificial-intelligence-proposal/ . 18 MEP letter to President von der Leyen, 8th March . 19 EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act): edps-proposal-regulation-european_en . 20 Charter of Fundamental Rights of the European Union, article 21 Charter of Fundamental Rights of the European Union, article 22 Charter of Fundamental Rights of the European Union, article 23 Charter of Fundamental Rights of the European Union, article 48; See also EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act), [34]. 14 migrant communities with a higher likelihood of presumed future criminality,24 therefore unreservedly compromising the rights to equality before the law and non-discrimination.25 The use of historical data in practice serves as a proxy for race and other protected characteristics, as well as socio-economic status, reflecting patterns of over-policing of certain communities, exacerbating racial biases, the criminalisation of poverty and affording false objectivity to patterns of racial and other types of profiling. Insofar as such practices reflect ongoing surveillance priorities, it is highly likely that such practices will amplify existing patterns of institutionalised discrimination insofar as they reify presumptions of criminality on the basis of individual or group characteristics, behaviour, or location. The use of AI systems at borders and in migration control. The proliferation of tests, trials and deployments in the context of migration control is a particular fundamental rights concern, which is not systematically addressed in the AIA. Specific conditions relating to the migration context warrant a higher level of scrutiny and limitations on the use of AI, in particular: the heightened conditions of vulnerability placed on people on the move, including refugees, migrants, non-status individuals, and other categories; the lower procedural safeguards and protection of rights afforded to migrants;26 and that the migration context has been used as an opportunity to experimenton an already highly marginalised category of persons.27 It is vital that particular limitations are drawn and higher safeguards applied to ensure that the rule of law and fundamental rights cannot be overridden by national security or other vaguely-defined policy priorities, and that the principles of necessity and proportionality are upheld. In particular, myriad uses of AI in the migration control context pose severe risks to fundamental rights of people on the move, as well as comprising potential violations of international refugee and human rights law. The increasing datafication of the migration management process, use of AI systems and big data to predict migration controls in combination with an expansive surveillance infrastructure28 to detect, intercept and prevent entry into Europe, is an impermissable use of AI systems. It also amounts to mass surveillance and is in contravention of 24 European Network Against Racism (, Data Driven Policing: The Hardwiring of Discriminatory Policing Practices Across Europe: profiling-web-final.pdf , 25 Charter of Fundamental Rights of the European Union, articles 20 & 26 UN Special Rapporteur on contemporary forms of racism, racial discrimination, xenophobia and related intolerance (, Racial discrimination and emerging digital technologies: a hu - man rights analysis A/HRC/27 EDRi, Petra Molnar ( T echnological T esting Grounds: Migration Management Experi - ments and Reflections from the Ground Up: ech - nological-T esting-Grounds.pdf . 28 European Parliament Research Service (. A rtificial Intelligence at EU borders: Overview of applications and key issues . PE 15 obligations under the Geneva Convention as well as a fundamental violation of the right to asylum enshrined in the Refugee Convention and domestic legislation. Further, the growing resort to biometric identification, verification and analysis of migrants sensitive data in the the context of migration management is deployed in the context of significant power imabalance, particularly given that immigration and border administrative decision-making is already an area rife with opacity and discretion without adequate oversight and accountability. The use of individual risk assessments and predictive systems to classify security or health risks also pose particular consequences for human dignity, equality and non-discrimination, privacy and data protection risks, as well as due process and good administration rights. The extent to which these systems are used to facilitate processes such as detention and deportation present particular risks to fundamental rights and with vast potential for abuse. In addition, emotion recognition systems (explored further in general below) are particularly harmful in the migration context due to the power imbalance and deep reliance on generally flawed and un-scientific premises,29 leading to potentially innacurate and discriminatory decision-making processes. Invasive monitoring, surveillance (including of biometric and other human features) and algorithmic management in employment and educational contexts. As highlighted by unions, there are particular concerns with the deployment of AI to monitor, measure and manage employees, tasks and resources in employment and educational contexts. Firstly, we see a growing resort to invasive monitoring practices, predicated on a vast scale of data collection in extreme power imbalance and subordination,30 fundamentally undermining notions of consent in data processing, given the contractual subordination of employees to their employers. Many such systems are combined with algorithmic assessments of performance and other forms of algorithmic task management, which are not only very likely to infringe on data protection and privacy rights of workers, but also likely diminish well-being, pose serious physical and psychological harm,31 limit work autonomy and maintain greater distance and opacity between managers and workers. Further, as demonstrated by a number of cases contested by app-workers, algorithmic management and ranking systems used by large platforms have had severe consequences on the economic situation 29 ARTICLE19, (. Emotional Entanglement: China s emotion recognition market and its implications for human rights : ech- China-Report.pdf. 30 ETUI ( The AI Regulation: entering an AI regulatory winter? Why an ad hoc directive on AI in employment is required: - tory-winter. 31 Wood, A. J., Algorithmic Management: Consequences for Work Organisation and Working Conditions, Seville: European Commission, 2021,JRC124874. 16 of workers as a result of specific decisions, including discriminatory treatment and violation of statutory rights,32 and major decisions, such as termination, taking through substantively automated means.33 The classifcation of such systems as only high risk , subject to primarily technical requirements to be fulfilled by providers of AI systems, is wholly insufficient to mitigate these threats to fundamental rights and severe harms to individuals and groups. Rather than restricting these systems, the AIA in its current form rather deems such practices permissable, exacerbating the burden on civil society and affected individuals to seek redress in the event of harm. As outlined below in the recommendations, such practices must instead be prohibited. 4Insufficiency of the limited risk approach The following practices are generally considered to have only a limited risk profile under the AIA, despite their va st capacity for harm and violations of fundamental rights . In practice, at least for uses of AI which process personal data, it is hard to see how such rules go further than existing requirements under the General Data Protection Regulation (GDPR). Instead, biometric categorisation must be prohibited wherever it may unduly restrict fundamental rights, most notably equality and non- discrimination , as set out more extensively in our recommendations in Section B. Emotion recognition must be fully prohibited due to the fundamentally and unmitigably flawed assumptions on which emotion recognition rests, and its incompatibility with human dignity and many fundamental freedoms . Biometric categorisation Ignoring the evidence of harms: The AIA proposal puts biometric categorisation systems in the category of limited risk, entailing only a small number of mandatory transparency requirements (article . It further sets up the possibility that some biometric categorisation use cases could in future be considered high risk under Annex III heading 1, but does not at this point include any such use cases, on the grounds that there is not sufficient evidence. However, the EDRi network and other civil society groups have repeatedly demonstrated that in fact, 32 T ech crunch, 4th January 2021, Italian court rules against discriminatory Deliveroo rider- ranking algorithm: - tory-deliveroo-rider-ranking-algorithm/ . 33 Personnel T oday, 27th October 2020, Uber sued for automated dismissals: h ttps:// - sonneltoday.com/hr/uber-sued-for-automated-dismissals/ . 17 such categorisations can create severe and undue fundamental rights restrictions;34 Threats to equality and non-discrimination: Biometric categorisation can gravely threaten rights to equality and non- discrimination, in particular when they relate to special categories of data as enshrined in the GDPR and protected under the CFEU and the broader EU equality and non-discrimination acquis. By definition, biometric categorisation is a process that seeks to put people into (often arbitrary, discretionary and stereotyped) boxes, and then to make predictions or decisions about them on that basis. Biometric categorisation has historical roots in systems of oppression and injustice, including the control of enslaved people in the US through the so-called lantern laws , the suppression of Indian people under British colonial rule, and even Nazi eugenics.35 For these reasons, its use in a rule-of-law-respecting society is exceptionally hard to justify; Links with mass surveillance: Biometric categorisation often forms the technical foundation of other forms of biometric data processing which can lead to mass surveillance, such as in remote biometric identification. Recital 18 of the AIA acknowledges the particularly intrusive and chilling nature of law enforcement performing such RBI practices (albeit only in real-time); Law enforcement exemption: However, when it comes to the practice of biometric categorisation which is often inextricable from RBI article 2 contradictorily exempts its use in criminal investigations, detection and prevention from the already very limited transparency requirements that are established in the AIA. Given that law enforcement uses of biometric categorisation can be associated to severe and extensive harms (loss of liberty, denial of access to procedural rights, denial of the presumption of innocence etc) this risks creating a get-out clause for some of the most harmful biometric categorisation practices ; Things that cannot be inferred: Additionally, the biometric categories proposed in article 35 treat as equivalent categories that may be predicted with a relatively high degree of accuracy based upon visible human features (such as predicting hair or eye colour, although even these are never absolute ) with those that simply cannot be determined on the basis of human features (such as sexual or political orientation) and instead, risk perpetuating scientifically-discredited and discriminatory theories like phrenology and physiognomy .36 The self- learning nature of some AI systems could make it even harder to identify 34 EDRi (, Ban Biometric Mass Surveillance: 2020/05/Paper-Ban-Biometric-Mass-Surveillance.pdf ; All Out ( Ban automated recognition of gender and sexuality, . 35 Najibi, A (, Racial Discrimination in Face Recognition T echnology : - vard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/ ; Sengoopta, C ( Imprint of the Raj: How Fingerprinting was Born in Colonial India. London: Macmillan. 36 Access Now (, Ban Biometric Surveillance: - loads/2021/06/BanBS-Statement-English.pdf . 18 when prima facie non-sensitive biometric features are in fact being used as proxies for sensitive characteristics like sexual orientation. Emotion recognition A scientifically invalid process: except in the cases of law enforcement or migration uses of polygraphs and similar tools (which are designated high risk under Annex III, b and a) , the AIA proposal classifies emotion recognition as only limited risk, despite vast evidence of its harms as well as its complete scientific invalidity such as via EU- funded projects like the much-criticised iBorderCTRL.37 The Civil Liberties Committee in the European Parliament has already called for the use of emtoion recognition in law enforcement to be discontinued ;38 Fundamentally incompatible with fundamental rights : EDRi member ARTICLE19 has demonstrated that, as a practice, emotion recognition is incompatible with international human rights principles and rules.39 As an incredibly intrusive practice, it can infringe on people s dignity, is often used in discriminatory contexts, and intrudes into people s cognitive liberty by coercing not just how people express themselves, but even how they think. Despite this, emotion recognition is becoming increasingly common in employment contexts, education, border and migration experiments and advertising. It risks infringing on people s rights and freedoms, and has particularly grave impacts on human dignity when important decisions relating to people s free movement, employment and other rights are made upon the basis of an inherently probabilistic and flawed system, which no amount of improvements to accuracy or performance can ever fix. The problem of definitions within the limited risk category: The definitions in the AIA of an emotion recognition system ( and a biometric categorisation system ( both limit the application of rules for these processes to when it is performed on the basis of biometric data. However, given that the AIA s definition of biometric data ( applies only if it allows or confirms the unique identification of a natural person, there is a risk that certain emotion recognition and biometric categorisation practices could be performed using data sets which avoid or even evade the threshold for being considered biometric data, for example through anonymisation (despite growing scepticism about the credibility of supposedly anonymised biometric 37 Wired (, The science behind the EU's creepy new border tech is totally flawed:, ; Jakubowska, E., ( Mass facial recognition is the apparatus of police states and must be regulated: ronews.com/2021/02/17/mass-facial-recognition-is-the-apparatus-of-police-states-and-must-be- regulated. 38 LIBE Committee (, Artificial Intelligence in policing: safeguards needed against mass surveillance: intelligence-in-policing-safeguards-needed-against-mass-surveillance . 39 ARTICLE19 (, Emotional Entanglement: China s emotion recognition market and its im - plications for human rights: ech- China-Report.pdf. 19 data). This could allow providers and users to circumvent the already unacceptably low requirements on emotion recognition or biometric categorisation. The Act should therefore widen the scope for emotion recognition and categorisation to include biometric, physiological and behavioural signals (in a new definition under article 3 of human features , as elabourated in section B) in order to ensure that equally harmful uses of data about human features are in scope of the prohibitions even when unique identification may not occur. Evasive processing practices (e.g. edge or transient processes) may also be employed in attempts to avoid the technical processing threshold for data to be considered biometric ( although it is important to note that the Italian Data Protection Authority (DPA) has confirmed that even if discarded almost immediately, the practice of scanning the biometric features of everyone in view of a camera is still considered unlawful mass surveillance.40 The inadequacy of notification rules to prevent harms arising from biometric categorisation or emotion recognition : For both biometric categorisation and emotion recognition, the AIA fundamentally falters in its presumption that the disclosure of their use to those who are subject to them is a solution to the harms and violations of rights that these practices can entail; and that such notification constitutes genuine transparency and accountability. Rather, harms such as a trans person being mis-gendered in public, a racialised person being shown (or not shown) adverts on the basis of their predicted ethnicity, or an employee facing disciplicary procedures due to not showing the right emotions at work, are just three of many examples of how the negative impacts of biometric categorisation and emotion recognition will remain just as real, regardless of whether or not the use of AI is disclosed to the subject. 3The AIA s scope overlooks broader structural harms and impact of AI It is positive that the AIA proposes a broad definition of artificial intelligence to include in scope a wide range of potentially harmful AI systems. However, despite this broad definition, the AIA has an extremely narrow list-based approach to regulation, which narrowly specifies use cases to be classified as high risk, alongside a process (article for the future inclusion of high risk use cases that fit within existing areas outlined in Annex III. 40 Garante per la Protezione dei Data Personali (Italian data protection authority) (, Ri - conoscimento facciale: Sari Real Time non conforme alla normativa sulla privacy: https:// . 20 Yet the following limitations in the AIA s scope are particularly challenging due to the act s primary objective, which is to promote a harmonised single market for AI within the EU. As highlighted by Veale and Zuiderveen Borgesius,41 this maximum harmonisation function requires that Member States must disapply any conflicting rules with those in the act. Thus, Member States are to be potentially precluded from introducing higher fundamental rights standards than those contained in the AIA. 1A lack of future-proofing for high-risk requirements The specific challenges as to the scope of the AIA are as follows. Firstly, the core requirements of the AIA apply to a very narrowly defined list of high risk AI systems, as outlined in article As such, with respect to stand alone use cases based on fundamental rights risks, the AIA limits from the start the range of high risk areas, determined solely by the European Commission, which cannot be updated in the future. The limited and caveated nature of the pre- defined areas (for example the processing of biometric data is high risk only if it leads to identification or categorisation, despite the fact that some authentication uses may entail significant risks) casts doubt over how comprehensive and future-proof this Annex can possibly be. Furthermore, new sub-areas can be included only insofar as they are compatible with the criteria outlined in article This falls far from the precautionary principle or rights- based approach called for by civil society.42 In addition, currently the European Commission has centralised power to update the list of high risk sub-areas in Annex III. 2Exclusion of many harmful use cases from the requirements Secondly, the risk-based approach, with requirements primarily limited to the narrow list of high risk AI systems, necessarily means that a number of systems with potentially harmful impacts remain unregulated under this act. There is particular concern as to the extent to which the following types of harms are (not) addressed: AI systems which exacerbate structural inequalities and power imbalance: As outlined above and specifically with respect to deployments of AI in the contexts of law enforcement, migration control and workplace surveillance, uses of AI in certain contexts will necessarily perpetuate structural power imbalances and fundamental rights risks. 41 Veale and Zuiderveen Borgesius ( Demystifying the Draft EU Artificial Intelligence Act , . 42 Access Now (, The EU should regulate AI on the basis of rights, not risks: https:// ; EDRi (, Recommendations for a fundamental rights-based Artificial Intelligence Regulation: addressing collective harms, democratic oversight and impermissible use: AI_EDRiRecommendations.pdf . 21 Further, as outlined by the European Disability Forum (EDF), there are no provisions in the Act to ensure that all AI systems (regardless of risk level) meet international legal obligations relating to the accessibility of persons with disabilities.43 By promoting the notion that AI systems can be primarily regulated through a series of technical measures, (documentation, human oversight in design and data quality standards), the AIA provides no response to the structural harms outlined in the previous section.44 This techno-centric framing does not adequately deal with how AI as socio-technical systems become embedded in broader processes of structural discrimination, which the examination of possible biases (article , purported improvements in accuracy (Article and more documentation (article will simply not address.45 Environmental impact: The AIA wholly underestimates the vast impact of a policy agenda designed to promote the widescale uptake of AI, underpinned by the exponential collection of data and focusing on the presumed benefits, without sufficient regards to the broader implications of the greater resort to AI systems on the environment. In particular, consequences on the environment relating to the vast environmental resources (including the exploitation of natural resources for the hardware required to underpin AI systems) as well as the energy consumption46 required for many of such systems to be trained and functional, as well as for data to be stored, find no place in the proposed regulatory framework. Economic and infrastructural c onsequences of AI systems : By defining high risk primarily with respect to fundamental rights and product safety, the AIA leaves little room for broader political and economic impacts of AI that fall outside of these frameworks. Also overlooked are the labour implications of the AI production pipeline, which often rely on labour exploitation of people in the Global South, but also how the resort to algorthmic management is reshaping the labour market toward crowd work and other more precarious, flexibilised forms of work.47 In addition, broad scale economic and political impacts following from the increased uptake of AI, including the increased 43 European Disability Forum (. Disability Perspective of AI of excellence and of trust (forthcoming). 44 EDRi (, EU s AI law needs major changes to prevent discrimination and mass surveil - lance: mass-surveillance/ . 45 EDRi (, Beyond De-biasing: Automated decision making and structural discrimination, authored by Agathe Balayn and Seda G rses, Delft University of T echnology, the Netherlands (forthcoming, September . 46 Emma Strubell, Ananya Ganesh and Andrew McCallum (. Energy and Policy Considera - tions for Deep Learning in NLP , accessed via: . 47 Valerio De Stefano (, The rise of the just-in-time workforce : On-demand work, crowd work and labour protection in the gig-economy : - stract_id=2682602 . 22 dependencies on centralised computational infrastructures,48 as well as the re-structuring of organisations and democractically accountable institutions49 are entirely overlooked in the AIA framework, with no foresight of the need for users of AI systems to take such factors into account. Enabling mass surveillance : The AIA proposal addresses neither the processes nor the infrastructures that may contribute to and normalise mass surveillance in its many forms (including, but not limited to, biometric mass surveillance). By promoting structures for gathering, inferring or predicting ever-more information about people, and connecting it across entities and services, the capacity for states and companies to unjustifiably surveil part or whole populations becomes ever- present and inescapable. Exporting rights-violating AI to the rest of the world : Concerns are also relevant in the context of export, as the scope of the AIA in article 2 establishes that the Act covers only AI that is put on the market/into service or used within the EU. This means that companies based in the EU may nevertheless be able to develop high-risk AI in an unrestricted manner, and even prohibited AI. It is a contradiction of EU rights and values that companies or entities based in the EU should be allowed to develop and then sell systems to states or companies outside of the EU, despite such systems being deemed to pose an unacceptable risk to fundamental rights and safety within the EU. 3 Loopholes enabling high risk uses Military and international law enforcement : Further, a number of loopholes limit the scope of the AIA in areas with crucial fundamental rights implications. article 2( leaves out of scope uses of AI developed or used for military contexts, and 2( leaves out of scope of the legislative proposal international organisations using AI systems in the framework of international agreements for law enforcement. This poses an unwarranted loophole for uses of AI for organisations such as EUROPOL, yet which still operates with significant fundamental rights implications for individuals in the European Union.50 Large scale migration databases: Further, article 83 leaves out of scope AI systems which are components of large scale IT systems, including the Schengen Information system, Visa Information System, Eurodac, the Entry/Exit 48 For an explanation of computational infrastructures, see - mable-infrastructures . 49 EDRi (, Beyond De-biasing: Automated decision making and structural discrimination, authored by Agathe Balayn and Seda G rses, Delft University of T echnology, the Netherlands (forthcoming, September . 50 EDRi (. Recommendations on the revision of Europol s mandate: content/uploads/2021/06/Recommendations-on-the-revision-of-Europols-mandate.pdf . 23 system, ETIAS, the European Criminal Records Information System on third-country nationals and stateless persons, and the Interoperability framework (Annex IX). This is major loophole for AI uses within the EU s migration control framework, with significant and severe consequences on the fundamental rights of people on the move should the AI systems that form part of these controls be excluded from the AIA s scope. Public security exemption : In addition, article 47 provides for a concerning ability for market surveillance authorities to authorise and provide exemptions to the conformity assessment procedure for reasons of public security or the protection of life and health of persons, environmental protection and the protection of key industrial and infrastructural assets. This provides an overly broad basis for market surveillance authorities to eradicate the already limited safeguards provided for in the Act, potentially compromising the principles of necessity and proportionality if not duly respected. Further processing of data exemptions: Further, article 54 of the proposal sets out a dangerous exemption to the principle of purpose limitation for innovative uses of AI. Specifcially, it allows for further processing of personal data for uses of substantial public interest , such as particular uses for law enforcement, public health, and environmental reasons. As argued by the EDPB and EDPS, this provision, alongside others in the proposal, presents a potential disconnect with the underlying principles contained in the GDPR regarding the grounds for further processing. These loopholes along with exclusions from rules for certain law enforcement purposes throughout the proposal also pose an additional risk that certain AI- based processes currently dealt with by administrative authorities may be pushed to law enforcement agencies in order to avoid regulatory scrutiny. The impact of such moves is that certain people most likely from marginalised groups, for example people on the move could be criminalised as a result. 4Focus on providers; limited obligations on users The core assumption of the AIA is that providers of AI systems are best placed to forecast, identify and mitigate the main harms that may emanate from AI systems. Following this, the AIA centralises the provider in the regulatory framework, with the bulk of requirements in the Act falling on those developing AI systems (articles 8-. The regulation allows a very wide scope for self-regulation by companies developing high risk AI. For the majority of high-risk AI uses contained in Annex III, the rules in article 43( mean that compliance with the Regulation s requirements is primarily ensured through self-assessment by the providers themselves. It is concerning that AI providers (those with a financial interest in 24 securing compliance and without the expertise to assess the implications on people s rights) themselves to judge if they have sufficiently met the requirements set out on data governance, transparency, accuracy, and more. Further, as higlighted by Veale and Zuiderveen Borgesius51, the conformity assessment process is likley to be highly influenced by European standardisation organisations such as CEN (European Committee for Standardisation) and CENELEC (European Committee for Electrotechnical Standardisation). The involvement of such entities in setting broad standards for the fulfilment of the essential requirements on providers further abstracts and weakens the process, creating a presumption of conformity . This process is particularly inappropriate for AI systems falling under article 6( of the act relating to fundamental rights impact, which may be incredibly complex, intangible and thus difficult to standardise for, but also have incredibly high potential impact on peoples rights. Furthermore, article 43( establishes a potential loophole, enabling providers to evade the requirement for external conf ormity assessments of biometric identification or categorisation systems if they comply with the standards. This means that a key purported safeguard may in practice have no effect. Another crucial flaw of the AIA s approach is that it overlooks the complexity of AI systems and the importance of context to be able to asess impact on fundamental rights, people and society . Whilst the provider- led conformity assessment process may identify the core technical shortcomings of the system, the mechanism is fundamentally ill-suited to identify the risks in the context of deployment. For example, a facial recogni - tion system may meet the technical requirements specified in the Act, yet still pose significant fundamental rights violations, compromise data protection and non-discrimination law, and enable mass surveillance in the context of deploy - ment (i.e. in a shopping centre). Because AI systems also by definition learn over time, there are intrinsic limitations to any snapshot-in-time conformity as - sessment. As such, ensuring that there are greater (and ongoing) obligations on users, in addition to providers, is crucial in order to address the fundamental rights issues that will arise in the use of AI. Further, we see that the requirements contained in the AIA placed on providers are highly technical in nature, thus largely inappropriate as a mechanism to prevent or mitigate potential risks to fundamental rights or other structural harms, or economic or environmental shifts engendered by the introduction of AI systems in context. Such considerations are inherently better assesed by the users in light of the context of deployment. 1Embedding dominance of AI providers 51 Veale and Zuiderveen Borgesius ( Demystifying the Draft EU Artificial Intelligence Act : . 25 This focus on providers potentially sets up a situation in which users of an AI system, such as a government agency, will be legally bound to follow the guidelines set out by a private company relating to the use of the AI system which they have procured (article . Whilst this article is intended to avoid misuse of a system, an unintended consequence of it may be that technology companies developing AI, whereby many already command disproportionate power over people and markets as established in EDRi s work on platform power, will thus be able to dictate the application of rules to which users including governments - are bound. 2Insufficient transparency With respect to the transparency framework, the Regulation (article largely imposes limited transparency obligations on providers toward users, as opposed to transparency requirements directly to people affected by or subject to AI systems (the exception to this is for limited risk uses cases under article 52, however we have explained on page 18-20 the serious shortcomings in the limited risk approach in the AIA). As such, the proposal will have a severely limited effect on people s ability to understand and challenge harmful and opaque AI systems deployed against them. Whilst the inclusion of an EU database of high risk AI systems as outlined in ar - ticle 60 is welcomed, currently the provision focuses on registration of high risk applications being put on the EU market. Full public transparency necessitates that this database registers high risk systems being put into use, including de - tails on which actors are deploying them and for which purpose. In addition, Annex VIII, s.11 contains an exception for public transparency for uses of AI in law enforcement and migration control, limiting the efficacy of the tool for pub - lic transparency in these sensitive contexts and the extent to which this data - base provides the necessary democratic checks and balances.52 Further, whilst the information currently included in article 13 to users provides a good basis of transparency, there is little obligation on providers to disclose to users information relating to the political a ssumptions and specific decisions related to the fundamental goals and assumptions of the system , weightings, parameters and standards resulting from these decisions. Articles 13 and 15 re - fer to accuracy of AI systems, however the definition and standards for accu - racy are left to the discretion of the providers. This provides little by way of guarantee to the user as to the validity or efficacy of the AI system for the pur - pose of use. This self-regulatory approach gives very little certainty with re - spect to the potential impact on fundamental rights. There is a concern that, if performance metrics conveyed to the user under article 13 are not sufficiently 52 EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act): [69]-[70]. 26 detailed and substantive, the broader human rights implications of de - ploying such a system may not be evident or discoverable by the user. 3Structural discrimination through AI With respect to discrimination exacerbated by AI systems, the AIA makes the assumption that data quality can solve the harms emenating from high risk AI systems. However, for many of the applications listed in Annex III, whilst AI developers may be able to predict and prevent some negative biases, for the most part such systems will inevitably exacerbate structural inequalities. This is because AI systems are deployed in a wider context of structural discrimination.53 By relying on technical checks for bias as a response to discrimination, the proposal risks reinforcing a harmful suggestion that removing bias from such systems is even possible, potentially obfuscating the need for structural solutions, such as limitations on certain uses, but also the need for intensive governance related responses. 4Impact on marginalised communities There are no specific requirements on users intending to put into use an AI sys - tem to measure the potential impact of such system on marginalised communi - ties, nor take steps to mitigate those impacts (including ensuring inclusive ac - cess, or halting deployments should they have an harmful impact on certain groups). The AIA fails to impose specific requirements on users to ensure the accessibility of AI systems or services that are operational through AI sys - tems.54 This is a systemic oversight in this Regulation, particularly in light of the broad promise that AI systems are likely to bring benefits to all in society. 5No consultation with affected groups Futher, the AIA foresees no specific duty on users to consult with affected groups or social partners before deploying AI systems, or modes of democratic oversight of AI systems deployed in contexts vital to the public interest. As already highlighted by Unions, this may serve to dilute existing consultation requirements with social partners in the employment context.55 53 EDRi (, Beyond De-biasing: Automated decision making and structural discrimination, authored by Agathe Balayn and Seda G rses, Delft University of T echnology, the Netherlands (forthcoming, September . 54 European Disability Forum (, Disability Perspective of AI of excellence and of trust (forthcoming). 55 ETUI ( The AI Regulation: entering an AI regulatory winter? Why an ad hoc directive on AI in employment is required: - tory-winter; UNI Global European Commission consultation response, June 27 5Limited enforcement and governance framework without actionable redress for subjects of AI The AIA pays insufficient attention to the fundamental interaction between the user and the subject of AI. This relationship is key to any fundamental rights based analysis and regulation of AI systems, and to the crucial question of how harms can be prevented and mitigated. Aside from article 52 outlining notification requiremented for a few narrowly defined limited risk AI systems (which are in themselves insufficient) the Act does not foresee notification requirements for high risk systems; duties to explain the reasoning behind automated decision making processes; nor, crucically, mechanisms for flagging or contestation of violations or harms as a result of interaction with AI systems. Whilst the AIA foresees the need for coordination between relevant national authorities supervising the regulation (articles 63(; 64; 65 and it provides no mechanism by which affected individuals or groups may flag to authorities potential harms, breaches of the Act or fundamental rights issues with an AI system. In addition, there is no mechanism for individual or collective redress for harms in scope of the AIA. This is a particularly fundamental ommission considering the limitations of other legal frameworks to provide effective redress with respect to AI systems, including the limits of article 22 of the GDPR, which is likely to be insufficient as a means to provide a right to explanation for many AI systems.56 Further, the limits of non-discrimination law, namely the focus on a limited set of protected characterstics, its requirement of a comparator, the focus on individual instances of discrimination as well as the high burden of proof on individuals in practice (exacerbated by the opacity of AI systems) reiterates the need for the inclusion of a redress mechanism in the AIA.57 Whilst the Explanatory Memorandum to the proposal claims that all major stakeholders were consulted in the course of developing the proposal (, many civil society organisations and communities have challenged the accuracy of this claim, citing their exclusion from this process. It is crucial that going forward in the legislative process, engagement with more affected groups (especially marginalised groups that are most likely to be subjected to AI decisions) is prioritised, in particular to consider the need for meaningful redress measures. 56 Wachter, Mittelstadt, and Floridi, (, Why a Right to Explanation of Automated Deci - sion-Making Does Not Exist in the General Data Protection Regulation International Data Pri - vacy Law, Volume 7, Issue 2, May 2017, Pages 76 57 T etyana Krupiy (, Why the proposed Artificial Intelligence Regulation does not deliver on the promise to protect individuals from harm European Law Blog: - blog.eu/2021/07/23/why-the-proposed-artificial-intelligence-regulation-does-not-deliver-on-the- promise-to-protect-individuals-from-harm/ . 28 Lastly, there is a significant concern with centralised nature of the proposed governance framework for the AIA. Article 56 establishes the European AI Board, however removes the competence of this Board (which appeared in previous verisons) to present additions to the list of high risk AI systems. In the AIA in its current form, this function is centralised with the European Commission only, presenting a significant concern as to the democratic nature of this process. Further, due to the power of the European Commission in the European AI Board, it raises significant questions relating to the independence of national supervisory authorities, which report to the AI board. However, as it is likely that these authorites should be Data Protection Authorities (and following the recommendation of the EDPS and EDPS it is clear that they should), that they should report to the European Commission potentially compromises the independence of these entities. 29 (B) Recommendations for a fundamental rights- based Artificial Intelligence Act In light of this analysis, EDRi recommends that the European Parliament and the Council of the EU implement the following improvements to the Artificial Intelligence Act (AIA): 1Ensure effective protection against prohibited practices and address the full scope of unacceptable risks through AI Imperative to the goal of a fundamental-rights respecting artificial intelligence regulation is the need to implement meaningful mechanisms geared toward the prevention of harm on individuals, groups and wider society. Civil society has been clear on the need to prevent, rather than to mitigate after the fact, impermissible or unacceptable risks to fundamental rights.58 a)Strengthen existing prohibitions in article 5 to provide meaningful protection against fundamental rights violations and individual and collective harms: i.Ensure that the prohibition on subliminal manipulative techniques in article 5((a) extends to harms which target groups of people as well as individuals; ii.Remove the caveat that AI systems that deploy subliminal techniques in order to materially distort a person s behaviour (article 5((a)) or exploits vulnerabilities of a specific group due to their age, physical or mental disability (article 5((b)) must cause or be likely to cause psychological and physical harm in order to be prohibited. Extend the list of vulnerabilities in article 5((b) to at least the protected characteristics outlined in the Charter of Fundamental Rights in the EU, with the explicit inclusion of gender identity; iii.Ensure wide application of the prohibition on social scoring59 (article 5((c)). Remove narrow framings, such as the temporal limitation over a certain period of time , the limitation to public authorities, 58 EDRi (, Open letter: Civil society call for the introduction of red lines in the upcoming Commission proposal on artificial intelligence: red-lines-in-the-european-unions-artificial-intelligence-proposal/ . 59 EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act), [29]. 30 the narrow framing of a singular score and replace the reference to trustworthiness to one of risk . Remove references to general purpose in recital b)Comprehensively prohibit the use of remote biometric identification in publicly-accessible spaces for any purpose , and implement a general ban on any use of AI for an automated recognition of human features in publicly accessible spaces - such as of faces but also of gait, fingerprints, DNA, voice, keystrokes and other biometric or behavioral signals - in any context , as per the EDPS- EDPB Joint Opinion.60 These prohibitions must apply for all purposes and in any context, including online spaces, and without exception: i.Furthermore, the putting on the market or placing into service of remote biometric identification software and hardware should be restricted in order to prevent biometric mass surveillance infrastructures being rolled out, and to ensure that EU companies cannot sell products and services which are designed for biometric mass surveillance outside the EU. The purpose limitation principle in the GDPR (article b) should be reiterated here, as it already stipulates that CCTV footage, for example, should not be used for other purposes, for example training AI software or for performing re-identification; ii.The definition of remote in RBI ( should add that RBI occurs not just with reference to watchlists but also to general databases. The provision that it applies only if there is no prior knowledge of the user about whether the person of interest will be present and identifiable should be fully removed, in order to avoid creating loopholes; iii.Human features should be defined under article 3 to include but not be limited to - biometric, physiological, behavioural and neurological signals; iv.As called for by the Civil Liberties Committee in the European Parliament, there must be a ban on the use of private facial recognition databases in law enforcement such as Clearview AI due to the likely incompatibility of such uses with EU data protection law.61 EDRi s analysis has shown that many of the same incompatibilities will apply also for databases developed by law enforcement agencies themselves. 60 Ibid [11]. 61 LIBE Committee, Artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters 2020/2016(INI), awaiting Plenary vote. 31 c)Include new prohibitions on the following practices which are incompatible with fundamental rights and democracy, and pose an unacceptable risk: i.Uses of AI in the field of law enforcement or criminal justice that purport to predict future behaviour, including analysing the risk that individuals will offend or re-offend, and predicting the likelihood that criminal or unfavourable conduct will occur on the basis of personality traits, individual or group characteristics or location; ii.Uses of AI in the field of migration control in ways that undermine the right to claim asylum, including but not limited to those: to risk assess inviduals for factors that do not relate to the substance of their immigration claim, such as risk of terrorism, public health threats, etc; to collect data and / or predict patterns in migratory movements for the purpose of preventing the exercise of the right to claim asylum; AI systems to assess eligibility for asylum, refugee or visa claims; iii.Uses of AI that implement invasive surveillance, monitoring and algorithmic management in an employment and educational context; iv.(Biometric) categorisation, which can pose a grave and disproportionate threat to all human rights, in particular equality and non-discrimination, by comprehensively prohibiting the use of AI to categorise people, on the basis of their human features, to the special categories of data as defined in article 9 of the GDPR;62 or to categories based on the grounds for unlawful discrimination in article 21 of the Charter of Fundamental Rights of the European Union;63 or on the basis of mental health status, migration status or gender identity: This prohibition must also include the use of potentially non- special or non-personal data, as well as data that does not meet the threshold to be considered biometric, captured from human features when used to categorise people according to proxies of 62 Racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade union membership, and the processing of genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health or data concerning a natural person's sex life or sexual orientation (GDPR, article . 63 Sex, race, colour, ethnic or social origin, genetic features, language, religion or belief, politi - cal or other opinion, membership of a national minority, property, disability, age or sexual ori - entation (CFEU, article . 32 special or protected categories (e.g. by combining eye and hair colour to predict ethnicity or using the wearing of a headscarf to predict religion). In the event that the self-learning nature of some AI systems makes it difficult to know whether people are being assigned to categories that could lead to discrimination, the precautionary principle dictates that such uses should also be prohibited; As already explained above, the definition of biometric data (article must be complemented with a definition of human features to ensure that it includes all data relevant to biometric or other human feature categorisation (without loopholes for types of data or methods of processing that don t meet the current threshold);64 v.Emotion recognition, which is scientifically invalid and can unduly infringe on all human rights, in particular human dignity and free expression, by comprehensively prohibiting the placing on the market, putting into service or use of AI to infer, predict, analyse or assess a person s emotions, feelings, emotional state, beliefs, preferences, intentions or otherwise inner thoughts, as well as to use human features, behaviours or expressions to predict future actions or behaviours; vi.Uses of AI that constitute mass surveillance should be prohibited. Mass surveillance means the surveillance of, or potential for surveillance of, whole or part populations (including specific groups), and is thus inherently unnecessary and disproportionate. Note that outside of the proposed prohibitions of biometric mass surveil - lance, biometric categorisation on the basis of special or protected categories and emotion recognition in general, there are additional AI ap - plications which use human features and which can pose a high risk to funda - mental rights. Therefore we additionally recommend that: Heading 1 of Annex III is changed to Physiological, behavioural, biomet - ric and neurological authentication, identification and categorisation . If human features are defined in article 3 according to our recommenda - tion, then an alternative heading could be Authentication, identification and categorisation of human features ; Under heading 1, the following use cases are added in addition to the ex - isting use case (a), but are not necessarily exhaustive at this point: i.Physiological, behavioural or biometric authentication, identification or categorisation (i.e. of human features) for law enforcement purposes; 64 See Access Now s Submission to the Consultation on the AI White Paper for a deeper engagement with issues of the definition of (biometric) categorisation. 33 ii.Physiological, behavioural or biometric authentication, identi - fication or categorisation (i.e. of human features) by private actors for surveillance or security purposes (such as security companies); iii.Physiological, behavioural or biometric authentication, identification or categorisation (i.e. of human features) for any purpose, where it can determine, solely or in part, people s access to: Public services (e.g. getting benefits payments); Private or privatised services which are necessary for people to ex - ercise or enjoy their fundamental rights and freedoms (e.g. using e- border gates, entering supermarkets, going to work). Furthermore, the Act must guarantee that providers of high-risk uses of AI under Annex III paragraph 1 should not be able to circumvent the obli - gation for an ex ante third party conformity assessment simply by meeting harmonised EU standards (as proposed in article 43(. 2Adapt the AIA to ensure holistic, democratic and fu - ture-proof framework Noting that the impact of AI systems extend far beyond impact on individual rights and product safety, but also are highly transient and susceptible to rapid change, the following proposals are designed to democratise the framework set out in AIA as well as better respond to structural and infrastructural harms. a)Introduce a democratic, inclusive and acccessible process by for the insertion of new prohibitions. Include criteria for unacceptable risk and the addition of future prohibitions into the AIA to ensure the enduring relevance of this regulatory instrument: i.Such criteria might criteria might include the impact on fundamental rights, structural power imbalances around the context of deployment (including potential for enhanced discrimination, marginalisation, inequality), lack of capacity for individuals, groups or civil society to contest the usage, etc. b)Ensure the potential to update the high risk use case areas in the future (amending article in addition to updating the use cases sub-areas . Further, this process must not be centralised with the European Commission only, but include a range of actors including civil society. 34 c)Respond to gaps in regulation with respect to economic and environmental impact, structural forms of inequality and AI and migration control, law enforcement and worker surveillance, mass surveillance, and exports of high- risk or prohibited AI outside the EU. d)Remove loopholes in articles 2(, and 83 leaving out of scope of the AIA AI systems used as part of international agreements on law enforcement and large scale IT systems in the migration control context. e)Remove the broad exemption to forgo the duty to conduct a conformity assessment on grounds of public security in article f)Remove the exemption to the principle of purpose limitation contained in article 54((a) for innovative AI within the regulatory sandbox provisions for uses in the criminal justice context. 3Ensure responsibility to those subjected to AI systems with enhanced obligations on users of all AI systems In the current AIA framework, the majority of the requirements fall on providers to implement a series of technical measures designed to mitigate harm in the deployment of systems. However, many of these harms are likely to be contextual and are best evaluated and addressed by the user, who has ultimate responsibilty to those subjected to the AI system. T o ensure the use of AI systems is accountable to and compliant with fundamental rights, we recommend that the requirements on providers are complemented with obligations on users geared toward greater responsibility to those subjected to AI systems. a)Mandate users to conduct and publish an ex ante human rights impact assessment before putting a high risk AI system into use, clearly outlining the stated purpose for which the system will be implemented: i.The impact assessment must be published on registration of use of the system in the public database under article 60; ii.This impact assessment must involve prior consultation with relevant national authorities, including equality bodies, consumer protection agencies, and data protection agencies. If other impact assessments are also required, these impact assessments must be published together; 35 iii.The impact assessment must also carry out meaningful consultation with social partners, civil society groups and individuals and groups affected by the use case; iv.The impact assessment must include full assessment of the fundamental rights that are likely to be impacted by the AI system, in addition to broader, social, political and economic consequences of deploying the AI system in the particualr context for the particular use. This should include indirect consequences of deploying the system, beyond impacts on those directly impacted by a decision generated; v.The impact assessment must include clear steps as to how the harms identitified will be mititgated, and how effective this mitigation is likely to be. If adequate steps for mitigation cannot be cannot be outlined, the system ought not to be deployed. b)Implement on users a duty to cooperate with national competent authorities investigating AI systems for potential threats to fundamental rights or safety under articles 65 and 67 for all AI systems, regardless of risk designation. c)Implement a duty on users to meaningfully consult with institutions, civil society and social partners representing affected groups before deploying high risk AI systems: i.Documentation of the results of this consultation should be included in the publicly acessible impact assessment. d)When the user of any AI system is a public authority, implement a notifcation requirement to all those impacted by a decision made by the system: i.This should include communicating how and why the decision was made, and how other available information or alternative outcomes were considered in reaching a decision.65 4Implement meaningful public transparency for high risk AI systems 65 This proposal was originally made by Melanie Fink (, The EU Artificial Intelligence Act and Access to Justice : to-justice-by-melanie-fink/ . 36 T o ensure greater public oversight of AI systems, the existing frame - work must be complemented by substantive mechanisms for transparency, such that AI systems in use are discoverable by oversight bodies, civil society and individuals. a)Ensure meaningful public transparency by requiring registration in the EU database (article of all high risk AI systems, and potentially all AI systems to which people are subject that are put into use. This would enable individuals and civil society to access information about AI systems in operation: i.The responsible authority or entity for deploying the high risk AI system should be listed with a contact point; ii.This should include information as to the stated purpose of the AI system in clear terms for individuals to understand. b)Ensure the inclusion of instructions for use for AI systems in law enforcement and migration, asylum and border control management in the public database as per Annex III, points 1, 6 and Remove the exemption contained within Annex VIII, point c)Require providers to include access to the conformity assessment alongside the instructions for use as per article 13(-( in the public database under article d)Require providers to provide more thorough details about the system to the users as part of article 13(. This must include: i.Information relating to the weightings and criteria relevant to choices in automated decision making systems; ii.An explanation of the fundamental assumptions and decisions informing the design of the AI system; iii.Ensure that information regarding the accuracy of the system under article 13((ii) is precise, allowing the user to objectively assess whether the AI sytem is fit for purpose. e)Remove the exemptions in article 52 relating to the transparency of AI systems used for detection and prevention of criminal offences (as argued by the EDPB and EDPS) and for the prosecution of people. When AI systems under article 52 37 are used for investigation, suspects should be notified post factum . 5Facilitate accountability: Include oversight and enforce - ment infrastructures that work for people Lastly, the following proposals are designed to ensure that those harmed by the systems regulated under the AIA are able to contest and seek remedies. Further, there must be more independence for the European AI board and more distributed scope of governance funtions. a)Ensure a cohesive national enforcement structure: i.Following the recomemndation of the EDPS and EDPB, national Data Protection Authorities (DPAs) should be the designated national supervisory authorities under the act, with a stated duty to work with other relevant enforcement authorities in evaulation and monitoring; ii.Ensure sufficient resources for national supervisory authorities in order to evaluate AI systems but also to respond and administer complaints. b)Include flagging and redress mechanisms allowing individuals and collectives to contest and seek redress for all AI systems that cause harm and threaten fundamental rights: i.This could include a flagging mechanism for those potentially impacted by an AI system to trigger national supervisory authorities evaluative action under article 67; ii.This duty to evaluate for fundamental rights risks should not be limited only to high risk systems, but any AI system once the national supervisory authority has received a complaint; iii.An explicit individual and collective redress mechanism must be introduced specifically to apply to those subjected by all AI systems, in particular noting that many such stand-alone systems are not covered by consumer mechanisms for collective redress. c)Implement a more democratic governance infrastructure, with greater independence for the European AI board: 38 i.Ensure that the mandate to make substantive updates to the legal framework (updates to high risk use cases, prohibitions) is held by a representative and democractically accountable European AI Board, not solely with the European Commission; ii.Include within the structure of the AI Board representatives of social partners and civil society, in particular those representing marginalised groups. 39",en,"Include criteria for unacceptable risk and the addition of future prohibitions into the AIA; b.Ensure the potential to update the high risk use case areas in the future (amending article in addition to updating the use case sub- areas ; 2 c.Respond to gaps in regulation with respect to economic and environmental impact, structural forms of inequality and AI and migration control, law enforcement and worker surveillance, mass surveillance, and exports of high-risk or prohibited AI outside the EU; d.Remove loopholes in articles 2( and 83 which currently leave out of scope of the AIA AI systems used as part of international law enforcement agreements; e.Remove the broad exemption to forgo the duty to conduct a conformity assessment on grounds of public security in article 47; f.Remove the exemption to the principle of purpose limitation contained in article 54((a) for innovative AI within the regulatory sandbox provisions for uses in the criminal justice context. 21 Further, as outlined by the European Disability Forum (EDF), there are no provisions in the Act to ensure that all AI systems (regardless of risk level) meet international legal obligations relating to the accessibility of persons with disabilities.43 By promoting the notion that AI systems can be primarily regulated through a series of technical measures, (documentation, human oversight in design and data quality standards), the AIA provides no response to the structural harms outlined in the previous section.44 This techno-centric framing does not adequately deal with how AI as socio-technical systems become embedded in broader processes of structural discrimination, which the examination of possible biases (article , purported improvements in accuracy (Article and more documentation (article will simply not address.45 Environmental impact: The AIA wholly underestimates the vast impact of a policy agenda designed to promote the widescale uptake of AI, underpinned by the exponential collection of data and focusing on the presumed benefits, without sufficient regards to the broader implications of the greater resort to AI systems on the environment. In particular, consequences on the environment relating to the vast environmental resources (including the exploitation of natural resources for the hardware required to underpin AI systems) as well as the energy consumption46 required for many of such systems to be trained and functional, as well as for data to be stored, find no place in the proposed regulatory framework. Energy and Policy Considera - tions for Deep Learning in NLP , accessed via: . Public security exemption : In addition, article 47 provides for a concerning ability for market surveillance authorities to authorise and provide exemptions to the conformity assessment procedure for reasons of public security or the protection of life and health of persons, environmental protection and the protection of key industrial and infrastructural assets. Specifcially, it allows for further processing of personal data for uses of substantial public interest , such as particular uses for law enforcement, public health, and environmental reasons. Further, we see that the requirements contained in the AIA placed on providers are highly technical in nature, thus largely inappropriate as a mechanism to prevent or mitigate potential risks to fundamental rights or other structural harms, or economic or environmental shifts engendered by the introduction of AI systems in context. 34 c)Respond to gaps in regulation with respect to economic and environmental impact, structural forms of inequality and AI and migration control, law enforcement and worker surveillance, mass surveillance, and exports of high- risk or prohibited AI outside the EU.",risk
BETTER FINANCE (Belgium),F2665632,06 August 2021,Non-governmental organisation (NGO),Micro (1 to 9 employees),Belgium,"Ref: EU Commission consultation on Artificial Intelligence (AI) - Ethical and Legal requirements Link to consultation : -regulation/have -your -say/initiatives/12527 -Artificial - intelligence -ethical -and-legal -requirements_en BETTER FINANCE s feedback on the EU Commission proposal on Regulation laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts About BETTER FINANCE BETTER FINANCE, the European Federation of Investors and Financial Services Users, is the public interest non -governmental organisation advocating and defending the interests of European citizens as financial services users at the European level to lawmakers and the public in order to promote research, information and training on investments, savings and personal finances. It is the one and only European -level organisation solely dedicated to the representation of individual investors, savers and ot her financial services users. BETTER FINANCE acts as an independent financial expertise and advocacy centre to the direct benefit of European financial services users. Since the BETTER FINANCE constituency includes individual and small shareholders, fund and retail investors, savers, pension fund participants, life insurance policy holders, borrowers, and other stakeholders who are independent from the financial industry, it has the best interests of all European citizens at heart. As such its activities are supported by the European Union since Introduction The European Commission has proposed a Regulation laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts. The purpose of this regulation is to provide a uniformed legal framework fo r the internal market regarding the marketing and the use of Artificial intelligence (AI) . Artificial intelligence has been deployed in several sectors and economic activities, in particular services and products for consumers. Therefore, it is crucial to provide rules for the application of artificial intelligence in order to ensure safety and compliance with fundamental rights. The intent of the regulation is to provide a technolog y neutral definition of artificial intelligence system s that can provide legal certainty and at the same time is future -proof, gives enough flexibility in regard to future technological developments. The proposal establishes requirements for the use of AI in several activities and sectors . Regarding financial services, the proposal tackles the use of AI on the assessment of creditworthiness and credit scores. Individual s should be informed that they are interacting with an AI . These transparency requirements also include chatb ots. 1 The cre dit institution will need also to observe a series of requirements based on high risk criteria and to complement AI deployment with human oversight. 2 1 -law/analysis/what -eu-ai-act-means -financial -services 2 Ibid. However, the requirements for the deployment of AI address only in part financial services and do not cover the use of AI in the entire financial sector. The proposal only encourages financial firms to establish (voluntarily) and apply a code of conducts on the use of AI in other financial services.3 In terms of supervision the Commission proposal define s a new entity named Artificial Intelligence Board in order to enhance coordination among national authorities. AI deployed in financial services could also be subject to market supervision . The proposal gives the following indication: [ ] the authorities responsible for the supervision and enforcement of the financial services legislation, including where applicable the European Central Bank, should be designated as competent authorities f or the purpose of supervising the implementation of this Regulation, including for market surveillance activities, as regards AI systems provided or used by regulated and supervised financial institutions. 4 Also in terms of financial supervision the EU Commission refers to the application of AI in the context of credit firms and their internal governance. BETTER FINANCE Feedback BETTER FINANCE strongly supports the proposal for an AI framework that allow s to provide legal and ethical clarity on the use of Artificial Intelligence . We have raised on several occasion s in our policy recommendations and research on Robo advisors that in order to regain the trust of consu mers it is necessary propose a legislative framework for Artificial Intelligence and to ensure that the use of algorithm is fair, transparent and accountable to consumers and do es not harm EU citizens fundamental rights . In addition, we have for long advised to undertake an in -depth fitness check of all relevant EU legislation s in the ins urance and financial sector in order to propose legislative updates where necessary . However, we regret that the requirements on the use of AI do not cover entirely the EU financial services. The p rovision on the voluntary creation and application of codes on the use of AI in other financial services ( other than credit directive and credit institutions ) will not be enough to address potential risks and consumer detriment caused by the use of AI in the retail financial market . Our research on Robo -advisors5 shows that there are persistent issues in terms of reliability of the algorithms when used to propose investments to financial services users. Consumers very often complain about the high fees charged for the investment product , higher than those explain ed during the advice process. New fintech platforms as Robo -advisors , operate as an alternative to more traditional financial advisors, with comparatively lower fees and offering access to simpler and cheaper products such as ETFs. However, the use of algo rithm and Automated Decision Making (ADM) may cause risks to consumers concerning e.g. the level of suitability of the investment advice: several platforms provide investment advice that seems inconsistent with the investor and risk profile of the mystery shoppers . 3 Ibid. 4 -regulation/have -your -say/initiatives/12527 -Artificial -intelligence - ethical -and-legal -requirements_en 5 nce.eu/wp -content/uploads/Robo -Advice -Report -2020 -pdf strong discrepancy in terms of investment gains and high dispersion of asset allocation for the same investor profile . Summary bo x: BETTER FINANCE research on Robo -advic e Robo -advice online platforms : Following four consecutive years of research on Robo -advice by BETTER FINANCE (link to the report ), four main areas of concern stand out: (i) investor protection awareness (i i) investment advice (iii) disclosure and (iv) sustainable investing. I. Investor protection awareness: Our research suggests that the propensity of retail investors to seek advice and take financial action (invest) is determined by the level of financial literacy and trust in capital markets. These two factors act more as complements and can reduce the vulnerable position of retail savers and their perceived lack of protection. II. Investment advice: For the third time in a row, the findings our Robo -Advice report show that: several platforms provide investment advice that seems inconsistent with the investor and risk profile of the mystery shoppers . strong discrepancy in terms of investment gains and high dispersion of asset allocation for the s ame investor profile . This may stem from how the investor questionnaires are designed or how the background information of the mystery shoppers is analysed. the recommended equity exposure ranges from 9% to 95% for exactly the same investor profile. Annual returns vary from + 80% to + 8% for the Millennial profile, and from +60% to +40% for the Baby Boomer . III. Disclosure: the responsibility to provide clear and non -misleading information falls squarely on the suppliers of financial services. However, we have observed that the information provided to individual investors is somehow scattered across the website or even missing in certain cases: 14 out of 17 platforms include best - and worst -case scenarios but only 10 platforms include past performance scenario in their investment advice. only 5 platforms (28%) specify that the past/future performance scenario are not reliable indicators of the actual performance . Only 33% of the platforms clearly provid e a warning stating that the investment may lose value. 67% of platforms clearly disclose the risk level of the portfolio in question, though the underlying details of what the risk level contains in practise varies greatly and leaves much to be desired. IV. Sustainability: Only 6 of the 18 platforms analysed in this year s research also propose sustainable investing options to their clients. However, it is quite disappointing to note that none of the platforms ask about the sustainability preferences of the ir clients during the questionnaire . Only a few platforms ask whether the client wants to invest sustainably at the beginning of the questionnaire, but most of the 6 platforms in scope allow for tweaking their portfolio from traditional to sustainable once the investment advice is provided. Full research report available here Also , the EIOPA expert group on digital ethics published in June 2021 the report on Artificial Intelligence Governance Principles : Towards Ethical and Trustworthy Artificial Intelligence in the European Sector6. In the context of Robo advisors t he report also underlines the need for consumers to be provided with meaningful and timely information about the system s capabilities and limitation s, and to the extent possible, consumers should be allowed to request the intervention of an employee at some point of the process . In addition, financial consumers should re ceive necessary information in order to gain a basic understanding of the algorithm that is behind the investment recommendations provided by the Robo advice such as the level of risk appetite, green preferences, and type of assets . Also , a human advice should be always recommended by these platforms.7 Therefore , additio nal pieces of legislations on financial services should be included in order to provide requirements on AI in a systematic manner . In order to avoid any legislative incongruences , the EU Commission proposal should cover additional EU legislations such as Mifid II and Solvency II . The proposal should cover transparency and explainability issues derived by the deployment of AI in the insurance sectors and AI -automated financial advice . Complementary governance aspects are essential in th ese financial services when it co mes to ensure good outcomes for consumers. The AI High Level Expert Group (HELG) has also recognised that expla inability of the algorithms and their transparency are the most important aspects that should be required to p rovide trust and accountability of AI systems deployed in financial services.8 9 Better finance 10is a contributor of the Human -Centric Digital Manifesto for Europe, How the digital transformation can serve the public interest (September 2019 )11 It is fundamental to ensure the human centric and fair application of Artificial Intelligence and the creation of an appropriate institutional framework on AI governance. Ethical codes should be developed an d implemented for the development and the application of Algorithm and Artificial Intelligence. Ethical codes and principles should be at the basis for fair, non -discriminatory and non -harmful use of AI. Specific rules should be also developed to address t he pricing problem in the insurance sector. The use of algorithm may generate substantial risks to consumer as discrimination or unfair practices. Some group of customers may be directly excluded by the algorithm being determined as too risky (too costly). Application of a segmentation of customers could result in strong differences of pricing for group of customers thus going against the fundamental rights of citizens that should be treated equally. Therefore, a code of conduct and AI governance framework should prevent unfair and discriminatory practices. 6 -publishes -report -artificial -intelligence -governance - principles_en 7 -publishes -report -artificial -intelligen ce-governance - principles_en 8 Ibid. 9 -strategy.ec.europa.eu/en/policies/expert -group -ai 10 11 -x-2019 -053-a-human -centric -digital -manifesto -for-europe.pdf",en,"Sustainability: Only 6 of the 18 platforms analysed in this year s research also propose sustainable investing options to their clients. However, it is quite disappointing to note that none of the platforms ask about the sustainability preferences of the ir clients during the questionnaire . In addition, financial consumers should re ceive necessary information in order to gain a basic understanding of the algorithm that is behind the investment recommendations provided by the Robo advice such as the level of risk appetite, green preferences, and type of assets .",risk
ALLAI (Netherlands),F2665629,06 August 2021,Non-governmental organisation (NGO),Micro (1 to 9 employees),Netherlands,"ARTIFICIAL INTELLIGENCE ACT ANALYSIS & RECOMMENDATIONS Catelijne Muller Virginia Dignum August 6, 2021 Submission to the public consultation on: Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS For (press) enquiries: welkom@allai.nl 2021 ALLAI, The Netherlands 2 Executive Summary 5 Objective, Scope & Definition 7 Objective 7 Scope 7 Definition 8 Prohibited AI practices 10 Materially distorting behaviour 10 Social scoring 11 Real time remote biometric recognition by law enforcement 12 New prohibition - Indiscriminate on- and offline tracking 15 High-risk AI 17 The criteria for risk of harm to health, safety and fundamental rights 17 Stand-alone high-risk AI (ANNEX III) 18 Harmonised products with AI (ANNEX II) 21 Delegated acts 22 Do the requirements mitigate the risks? 22 The prerogative of human decision making 24 Accountability, Governance & Enforcement 25 Complex self-assessment structure 25 Inclusivity and multi-disciplinarity 25 Complaints and redress 26 Legacy high-risk AI 26 3Introduction In its highly anticipated legislative proposal for AI, the European Commission projects a clear message: fundamental rights and European values are at the core of Europe s approach to AI. Europe is basically saying: when it comes to this technology, anything goes is no longer the norm. We will not allow everything, just because it can be done. And we don t just regulate bits and pieces of the technology, we set EU wide rules that will resonate across the globe. ALLAI welcomes the ambitious proposal for a Regulation for Arti cial Intelligence (also referred to as the Arti cial Intelligence Act ( AIA )). We particularly welcome the fact that the European Commission took up the courage to actually prohibit certain AI practices and that it draws heavily on the Ethics Guidelines for Trustworthy AI. By Catelijne Muller and Virginia Dignum Over the past couple of years AI has found its way into our societies in many forms and shapes. It has shown promise, but also posed serious risks to health, safety and fundamental rights. Apart from the many examples where AI adversely impacts society by interfering with our (fundamental) rights, ethical principles and social values, there are more and more signs that the current state of the art of AI does not live up to the human like capabilities that are attributed to it. In many instances AI has overpromised and underdelivered. ALLAI welcomes the fact that the Commission proposal for the Artificial Intelligence Act (the AIA ) not only addresses the risks associated with AI. The combination of prohibited AI practices, requirements for high and medium-risk AI plus the measures to promote trustworthy AI innovation including the voluntary compliance model, will help improve the quality, performance and trustworthiness of AI and improve the chances that AI can live up to its promises, so that the benefits of AI can be reaped and the risks be mitigated. This paper contains an analysis of the main elements of the AIA and, where relevant, proposes textual or conceptual changes. Amsterdam/Ume , August 6, 2021 EXECUTIVE SUMMARY While we welcome the AIA, we do see areas for improvement of the text in its current form; in particular regarding the scope and de nition, the clarity of the prohibited AI practices, the implications of the categorisation choices made in relation to the risk pyramid , the risk-mitigating effect of the requirements for high-risk AI and the relation to existing regulation and other recent regulatory proposals. Below is a summary of the most important areas for improvement. ALLAI welcomes the fact that the AIA puts health, safety and fundamental rights at the centre of the AIA. We also welcome the external effect of the AIA, making sure that AI that is developed outside of the EU, has to meet the same legal standards if deployed or having an impact within the EU. For any law to be effective, there must be a clear legal definition of what it is, it attempts to regulate. While the proposal aims to regulate AI-practices rather than the technology itself, it focuses heavily on AI-technologies or methods, and as such runs the risk of organisations evading the regulation, simply by classifying their applications differently. It is more effective to focus on the characteristics, or properties of a system, that are relevant to be regulated. The AIA lacks more general notions such as the prerogative of human decision making, the need for human agency and autonomy, the strength of human-machine collaboration and the full involvement of stakeholders. AI does not operate in a lawless world and the AIA should be clear(er) on the fact that existing laws and regulations (beyond the GDPR) apply to AI and the way we use it. The two first prohibitions centre around distorting a person s behaviour , but in their current form only capture rare cases. They however provide a grand opportunity to address one of the most worrying and widespread capabilities of AI: harmful conditioning and manipulation. It is important that the AIA halts the current trajectory of public and private actors using ever more information to assess, categorise and score us. AIA should attempt to draw a clear line between what is considered social scoring and what can be considered an acceptable form of evaluation for a certain purpose, for example at the point where the information used for the assessment is not reasonably relevant or related to the assessment. Broadly in line with the EDPB and EDPS, ALLAI calls for a ban on biometrics recognition (which includes biometric identification, but also all forms of emotion/behaviour/affect/intent/trait recognition with biometric recognition, which is now categorised as medium-risk and high-risk in some areas) both by private organisations and by or on behalf of (semi-)public authorities. 5ALLAI thinks that the ubiquitous tracking of our entire behaviour through our online behaviour, our location data and our IoT data serves no obvious social benefit. While prohibiting these practices might be challenging, ALLAI believes that the further proliferation of these forms of widespread tracking of our entire lives by public and private actors should at least be curbed. The high-risk approach, consisting of criteria for risk of harm to health, safety and fundamental rights , a selection of high-risk AI that supposedly serve a social benefit and a number of requirements that are aimed at mitigating their risks, can normalise and mainstream quite a number of AI practices that are still heavily criticised, often due to their lack of sufficient social benefit. Moreover, this approach assumes that the risks high-risk AI systems pose, also future ones, can be sufficiently mitigated by the requirements, which unfortunately is not always the case. The AIA lays down the criteria for what can be considered a risk of harm to health, safety and fundamental rights . This limits the broad interpretation of our fundamental rights framework, that allows for the consideration of all relevant circumstances (or criteria ). Analysis of the high-rise AI uses on the stand-alone high-risk AI list of ANNEX III shows that the benefits of a number of them not necessarily outweigh the risks and that for some, clear benefits are simply lacking. Moreover, the risks these uses pose cannot necessarily always be sufficiently mitigated by the current requirements for high-risk AI. We also think that a number of AI uses such as in election and voting processes and for content moderation should be added to ANNEX III. We welcome the fact that the requirements for high-risk AI strongly reflect the requirements for trustworthy AI of the Ethics Guidelines for Trustworthy AI. A number of requirements are however not included in the AIA, which we think is a missed opportunity. (i) Human agency (ii) privacy (iii) diversity, non-discrimination and fairness, (iv) explainability and (v) environmental and social well being are particularly important to avoid the risks AI poses, which are those of privacy, bias, exclusion, inexplicability of the outcomes of AI decisions, the undermining of human agency and the environment. If one takes a deeper look at the high-risk AI uses of ANNEX III, one will see that once the requirements for high-risk AI are met, AI can potentially largely replace human decision making in law enforcement, the judiciary, enjoyment of essential services, migration and asylum, recruitment, hiring, firing and worker-assessment, education, democratic processes. Most AI is technically incapable of making these decisions at a human level and it is questionable if it is even desirable. Ultimately, not all decisions can or should be simplified to ones and zero s . In line with our long advocated human-in-command approach to AI, ALLAI strongly recommends the AIA to arrange for certain decisions to remain the prerogative of humans, particularly in domains where these decisions have a strong moral component as well as legal implications or a societal impact such as in the judiciary, law enforcement, social services, healthcare, housing, financial services, labour relations and education. OBJECTIVE SCOPE & DEFINITION ALLAI welcomes the fact the AIA puts health, safety and fundamental rights at the centre of the AIA. We also welcome the external effect of the AIA, making sure that AI that is developed outside of the EU, has to meet the same legal standards if deployed or having an impact within the EU. We do however see limitations in the chosen instrument and the de nition of AI. Objective There are a number of challenges but also opportunities when regulating AI from a human (or fundamental) rights perspective. To date, there here has been much focus on the the impact of AI on the human rights regarding non-discrimination and (data) privacy, but many other human rights are at play with AI, and sometimes more than one at the same time. AI Regulation should make sure to address this. 1The fact that the human rights impact of AI is often 'hidden' or unknown and only discovered after the fact necessitates AI regulation to contain ex ante, ex Durante and ex post assessment and monitoring obligations. AI can have both a negative and a positive impact on the same or 2different human rights at the same time. This requires AI regulation to leave room to balance 3these impacts. Many AI applications are developed and/or used by private actors or used in private settings (workplaces, shops, cars, the internet), who cannot be held directly to account where it comes to human rights violations. In the same manner as the GDPR, AI regulation bring the human rights protections into these private spheres as well. Scope The choice to deal with a complicated, general purpose technology, that impacts virtually all aspects of society, by way of a product regulation, makes the proposal very complex and hard to The overview of human rights at play with AI contains multiple citations of the paper by C. Muller for the CAHAI: The 1impact of AI on Human Rights, Democracy and the Rule of Law"" Ibid.2 Ibid.37navigate. In this attempt, it normalises and likely even legitimises a number of highly invasive AI-systems and uses and it leaves a number of risks unaddressed. On a higher level, the proposal lacks more general notions such as the prerogative of human decision making, the need for human agency and autonomy, the strength of human-machine collaboration and the full involvement of stakeholders. AI is not merely a technology neither is it just the sum of its software components. It comprises the entire socio-technical system around it. Limiting the legal framework to a product regulation ignores these important notions. The Ethics Guidelines for Trustworthy AI state that AI systems do not operate in a lawless world. A number of legally binding rules at European, national and international level already apply or are relevant to the development, deployment and use of AI systems today. Legal sources include, but are not limited to: EU primary law (the Treaties of the European Union and its Charter of Fundamental Rights), EU secondary law (such as the General Data Protection Regulation, the Product Liability Directive, the Regulation on the Free Flow of Non-Personal Data, anti-discrimination Directives, consumer law and Safety and Health at Work Directives), the UN Human Rights treaties and the Council of Europe conventions (such as the European Convention on Human Rights), and numerous EU Member State laws. Besides horizontally applicable rules, various domain-specific rules exist that apply to particular AI applications (such as for instance the Medical Device Regulation in the healthcare sector). This is why the Ethics Guidelines for Trustworthy AI specifically mention lawfulness as the first pillar of trustworthy AI. To clarify this, ALLAI recommends to amend Recital ( or even be insert into the body of the regulation: Whereas: ( ) ( This Regulation should not be interpreted as indicating that the use of any AI-system is necessarily lawful under other acts of Union law or under national law compatible with Union law. Any use of AI should continue to comply with the the European Charter on Fundamental Rights, secondary Union law and national law. This Regulation should not be understood as providing the legal ground for unlawful AI development, deployment or use. Definition For any law to be effective, there must be a clear legal definition of what it is, it attempts to 4regulate. While the proposal aims to regulate AI-practices rather than the technology itself, it contains a technical definition of AI in art. 3 jo. ANNEX I, the latter listing a number of broad AI-techniques and approaches that a system should incorporate to fall within the scope of art. 3 sub (. Moreover for any of these techniques to fall within the scope of the definition, it should be able to (paraphrased) generate outputs that influence the environments they interact with, when given a set of human-defined objectives . A legal definition, in particular when regulating such a complicated technology with such broad and deep impact, 4should meet a number of requirements such as inclusiveness, preciseness, comprehensiveness, practicability and permanence. 8Listing AI-techniques could easily create confusion, legal uncertainty and loopholes. First of all, the list of AI-techniques and approaches (ANNEX I) it lacks a number of relevant AI-techniques and approaches such as decision trees, random forests, fuzzy logics, game theory etc. More importantly however, AI-techniques and approaches (and their names for that matter) are constantly evolving and new techniques are being developed as we speak. Moreover, while the regulation should of course be interpreted based on its spirit rather than its letter, adding a list of AI-techniques and approaches limits the room for such interpretation and opens the door to a contrario reasoning, where it is argued that any AI-technique that is not listed, falls outside of the scope of the AIA. The Commission does retain the right to update the definition and AI-techniques by delegated act, but can only do so when these new techniques have similar characteristics as the ones already on the list. While this approach promotes flexibility in lawmaking, we also fear that it could lead to a game of whack a mole for the Commission along with discussions and disagreements of what is considered to be a similar AI-technique. It is better to focus on the characteristics, or properties of a system, that are relevant to be regulated. By focusing on technologies, or methods, i.e. by regulating systems that are based on machine learning, logic, or statistical approaches , we run the risk of organisations evading the regulation, simply by classifying their applications differently. 5Borrowing from the definition given in the seminal textbook on AI, AI is the discipline of developing computer systems that are able of perceiving its environment, and to reason about how to best act on it in order to achieve its goals, assuming that the environment contains other agents similar to itself. As such, AI is about the autonomy (or better automation) to decide on how to act, the adaptability to learn from the changes affected in the environment, and the interactivity required to be sensitive to the actions and aims of other agents in that environment, and decide when to cooperate or to compete. 6A responsible, ethical, approach to AI goes further than the technology used, and needs to include the social, organisational and institutional context of that technology. It is this socio-technical that needs to ensure transparency about how adaptation is done, responsibility for the level of automation on which the system is able to reason, and accountability for the results and the principles that guide its interactions with others, most importantly with people. In addition, and above all, a responsible approach to AI makes clear that AI systems are artefacts manufactured by people, for some purpose, and that people are responsible for the use and development of AI. We propose to consider the following alternative definition of AI, and 7removing ANNEX I from the AIA entirely: Article 3 Definitions For the purpose of this Regulation, the following definitions apply: ( Artificial intelligence system (AI system) means software that can, in an automated manner, for a given set of human-defined objectives, generate outputs such as content, predictions, recommendations or decisions, influencing the environment it interacts with. Ibid.6 Ibid.PROHIBITED AI PRACTICES The escalating risk pyramid (from limited and medium-risk, to high-risk, to unacceptable risk) used to categorise a number of general AI practices as well as a number of domain speci c AI use cases, acknowledges that not all AI poses risks and not all risks are equal. The descriptions of the various prohibited AI practices and medium and high-risk AI use cases, however, are at times unclear, multi-interpretable and could lead to legal uncertainty and create loopholes. Moreover, ALLAI questions a number of categorisation choices made and sees a great number of still heavily criticised AI uses listed as medium and high-risk, potentially further mainstreaming them. Materially distorting behaviour It is not entirely clear what it is the AIA aims to prohibit in article 1 paragraphs (a) and (b). What stands out is that they centre around distorting a person s behaviour and that the distorted behaviour should lead to physical or psychological harm for the prohibition to kick in. While these prohibitions likely cover a very narrow set of AI practices, they provide a grand opportunity to address one of the most worrying and widespread capabilities of AI: harmful conditioning and manipulation. With regards to the right to receive and impart information and ideas, AI used in media and news curation, bringing ever more personalised online content and news to individuals, raises concerns. Search engines, video recommendation systems and news aggregators often are opaque, both where it comes to the data they use to select or prioritise the content, but also where it comes to the purpose of the specific selection or prioritisation. Many business models 8are based on online advertising revenue. In order to have people spend as much time on a platform or website as possible, they might be selecting and prioritising content that will do only that: keep people on their platform, irrespective of whether this content is objective, factually true, diverse or even relevant. Beyond commercial motives, political or other motives might lead to AI-systems being optimised to select or prioritise particular content in an effort to coerce and influence individuals towards certain points of view, for example during election processes. 9 Burrell, J. (. How the machine thinks : Understanding opacity in machine learning algorithms. Big Data & Society, 83(, Cambridge Analytica, Netflix Documentary: The Great Hack 910Moreover, AI is becoming more capable of producing media footage (video, audio, images) resembling real people s appearance and/or voice (also known as deep fakes ), enabling the deceptive practices for various purposes. All this can give rise to filter bubbles and proliferation of fake news, disinformation and propaganda, and affects the capacity of individuals to form and develop opinions, receive and impart information and ideas and thus impact our freedom of expression. 10According to the Ethics Guidelines for Trustworthy AI, every human being possesses an intrinsic worth , which should never be diminished, compromised or repressed by others nor by new technologies like AI. This means that all people are to be treated with respect, as moral subjects, rather than merely as objects to be surveilled, sifted, sorted, scored, herded, conditioned or manipulated. 11In order to capture what the AIA should prohibit, which is condition, manipulate and exploit people into harmful behaviour, ALLAI recommends to combine the two current prohibitions under article 1 (a) and (b) as follows: Article 5 The following artificial intelligence practices shall be prohibited: (a) The placing on the market ( ) of an AI system deployed, aimed at or used for materially distorting a person s behaviour or exploit a person s vulnerabilities, in a manner that causes or is likely to cause harm to that person s, another person s or group of persons fundamental rights, including their physical or psychological health and safety, or to democracy and the rule of law (b) DELETE Social scoring ALLAI welcomes the prohibition of social scoring of article 1 (c). We do however feel that the prohibition leaves the door open to a great number of social scoring mechanisms that in our opinion deserve scrutiny. First of all, in the current wording the scoring should be aimed at evaluation or classification of trustworthiness of people. While we like the term trustworthiness for obvious reasons, in this context it is vague. What is considered the trustworthiness of a person? AI used to determine creditworthiness, an element of trustworthiness, is considered high-risk and thus allowed. Secondly, the existence of detrimental or unfavourable treatment , which is a requirement in the current prohibition, will be difficult if not impossible, to prove. We have seen many instances of social scoring by public and private actors alike. While they do not consist of generalised schemes of citizen scoring, examples of which we have seen in China, UN Report of the Special Rapporteur on the promotion and protection of the right to freedom of opinion and 10expression, A/73/348 Ethics Guidelines for Trustworthy AI, High Level Expert Group on AI (1111they do consist of the scoring of people for various purposes, such as social benefits fraud detection, loan or mortgage eligibility etc. For social scoring to be effectively prohibited in Europe, the AIA should attempt to draw a clear line between what is considered social scoring and what can be considered an acceptable form of evaluation for a certain purpose. ALLAI believes that this line can be drawn where the information used for the evaluation should no longer be deemed relevant or reasonably related to the goal of the evaluation. We realise that this will lead to discussions as to what is relevant or reasonably related, but we do not see this as a problem, as many legal norms are open to interpretation. We do however feel that it is important that the AIA halts the current trajectory of using ever more information to assess our every move. A solution could be for the AIA to incorporate a definition of social scoring and adjust the prohibition as follows: Article 3 Definitions For the purpose of this Regulation, the following definitions apply: ( social scoring means the evaluation or categorisation of EU citizens based on their behaviour or (personality) characteristics, where one or more of the following conditions apply: -the information is not reasonably relevant for the evaluation or categorisation; -the information is generated or collected in another domain than that of the evaluation or categorisation; -the information is not necessary for or proportionate to the evaluation or categorisation; -the information contains special categories of personal data, including biometric data. Article 5 The following artificial intelligence practices shall be prohibited: (c) the placing on the market, putting into service or use of AI systems by or on behalf of (semi-)public authorities or by private actors for the purpose of social scoring. Real time biometric recognition for law enforcement The AIA bans real time remote biometric identification (with for example facial recognition) for law enforcement and categories it as high risk when used for other purposes, except in some specified circumstances. The AIA puts some strict limitations on what kind or biometric recognition it aims to prohibit, whereas it should: -is real time 12-is remote - identify - used for the purpose of law enforcement This leaves post and near biometric recognition allowed. It also leaves biometric recognition not aimed at identifying a person, but rather assessing a person s behaviour from their biometric features (micro-expressions, gait, temperature, heart rate, etc.) allowed. The limitation to law enforcement allows biometric identification, as well as al other forms of biometric recognition not aimed at identification of an individual, including all mentioned forms of emotion recognition for all other purposes, by all other actors, in all public and private places, including at the workplace, shops, stadiums, theatres etc. (as long as it does not serve law enforcement purposes.) This leaves the door wide open to a world where we are constantly being emotionally assessed for whatever purpose the actor assessing us deems necessary. Biometric recognition has a broad and deep impact on the right to privacy. Privacy discussions around AI currently tend to focus primarily on data privacy and the indiscriminate processing of personal (and non-personal) data. It should however be noted that, while data privacy is indeed an important element, the impact of AI on our privacy goes well beyond our data. Art. 8 of the ECHR encompasses the protection of a wide range of elements of our private lives, that can be grouped into three broad categories namely: (i) a person s (general) privacy, (ii) a person's physical, psychological or moral integrity and (iii) a person's identity and autonomy. Different 12applications and uses of AI can have an impact on these categories, and have received little attention to date. Facial recognition, involves the capture, storage and processing of personal (biometric) data (our faces), but it also affects our 'general' privacy, identity and autonomy in such a way that it 13creates a situation where we are (constantly) being watched, followed and identified. As a psychological chilling effect, people might feel inclined to adapt their behaviour to a certain norm, which shifts the balance of power between the state or private organisation using facial recognition and the individual. In legal doctrine and precedent the chilling effect of surveillance 14can constitute a violation of the private space, which is necessary for personal development and democratic deliberation. Even if our faces are immediately deleted after capturing, the 15technology still intrudes our psychological integrity. Other forms of AI-driven biometric recognition have an even greater impact on our psychological integrity. Recognition of micro-expressions, gate, (tone of) voice, heart rate, temperature, etc. are currently being used to assess or even predict our behaviour, mental state and emotions. These AI practices are commonly known under multiple definitions such as emotion recognition or affect recognition and more specifically as deception recognition or behaviour recognition , but are also often presented as supposedly being able to predict a person s personality traits and even leadership skills, future job performance etc. The AIA only defines emotion recognition (as AI systems that can identify or infer emotions or intentions ) and categorises these systems generally as medium-risk, with the exception of a few areas where they are categorised as high-risk. Guidance to art. 8 ECHR, Council of Europe.12 The jurisprudence of the European Court of Human Rights (ECtHR) makes clear that the capture, storage and 13processing of such information, even only briefly, impacts art. 8 ECHR. Examined Lives: Informational Privacy and the Subject as Object, Julie E. Cohen, 14 The chilling effect describes the inhibition or discouragement of the legitimate exercise of a right. It has been shown 15that once people know that they are being surveilled they start to behave and develop differently; Staben, J. (. Der Abschreckungseffekt auf die Grundrechtsaus bung: Strukturen eines verfassungsrechtlichen Arguments. Mohr Siebeck.13It should be noted upfront that no sound scientific evidence exists corroborating that a person's inner emotions or mental state can be accurately 'read' from a person's face, gate, heart rate, tone of voice or temperature, let alone that future behaviour could be predicted by it. In a recent meta-study, a group of scientists concluded that AI-driven emotion recognition could, at the 16most, recognise how a person subjectively interprets a certain biometric feature of another person. An interpretation does not align with how that person actually feels, and AI is just labelling that interpretation which is highly dependent on context and culture. Far-fetched statements, that AI could for example determine whether someone will be successful in a job based on micro-expressions or tone of voice, are simply without scientific basis. The GDPR restricts the processing of biometric data only to some extent. Biometric data according to the GDPR is personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, which allow or confirm the unique identification of that natural person. The last part of the sentence is crucial, because if biometric recognition is not aimed at identification (but for example at categorisation, profiling or affect recognition), it might not fall under the GDPR-definition. In fact, recital 51 of the GDPR says that 'the processing of photographs [is considered] biometric data only when processed through a specific technical means allowing the unique identification or authentication of a natural person. Many biometric recognition technologies are not aimed at processing biometric data to uniquely identify a person, but merely to assess a person s behaviour (for example in class) or to categorise individuals (for example for the purpose of determining their insurance premium based on their statistical prevalence to health problems). These uses might not fall under the definition of biometric data (processing) under the GDPR. Apart from the right to privacy, biometric recognition also affects other human rights and freedoms such as the freedom of opinion and expression. When the protection of group anonymity no longer exists, if everyone in the group could potentially be recognise, this could lead to people no longer partaking in peaceful demonstrations. 17In stead of categorising these AI practices as high and medium-risk, (as the AIA now does), we strongly call for a blanket ban on biometrics recognitions (which includes biometric identification, but also all forms of emotion/behaviour/affect/intent/trait recognition) both by private organisations and (semi-)public authorities. In the exceptional instances already mentioned in art. 1 (d) subparagraphs (i), (ii) and (iii), biometric identification might be considered acceptable, however, there is a risk of misuse because the systems need to be in place, even if they are only allowed to be switched on in these circumstances. In any event, the conditions set in the same article should apply. Another situation where some of these AI practices could be allowed is in controlled environments such as for example hospitals, where the technology could serve a scientific purpose (i.e. predicting autism, Alzheimer's etc.). Here also, we warn that these uses should at a very minimum be evidence based, limited in time, proportionate and necessary. Barrett, L. F., Adolphs, R., Marsella, S., Martinez, A. M., & Pollak, S. D. (. Emotional Expressions Reconsidered: 16Challenges to Inferring Emotion From Human Facial Movements. Psychological Science in the Public Interest, 20(, 1 Privacy Impact Assessment Report for the Utilization of Facial Recognition Technologies to Identify Subjects in the 17Field, 30 June 2011, p. 14Broadly in line with the call of the EDPS and EDPR, ALLAI calls for the following amendments of 18article and additions to 5 AIA: Article 5 The following artificial intelligence practices shall be prohibited: (d) the placing on the market, putting into service or use of AI systems by or on behalf of (semi-)public authorities or by private actors for the purpose of automated recognition of human features in publicly and privately accessible spaces, such as recognition of faces, gait, fingerprints, DNA, voice, keystrokes and other biometric or behavioural signals, in any context (e) A ban on AI systems using biometrics to categorise individuals into clusters based on ethnicity, gender, political or sexual orientation, or other grounds on which discrimination is prohibited under Article 21 of the European Charter of Fundamental Rights; (f) A ban on the use of AI to infer emotions, affect, behaviour or intent, of a natural person, except for very specified cases, such as some health purposes, where the patient emotion recognition is important. New prohibition - Indiscriminate on- and offline tracking Many public and private actors partake in AI-driven online on- and offline tracking of our entire behaviour, interests, activities, locations, and so on. This is not only done through our online behaviour, such as our social media channels or web visits and search queries, but also through our IoT data and location data. Also this creates a world where are constantly being followed, tracked, profiled, categorised, analysed, sifted and sorted. We already argued for a prohibition on the use of unrelated information to, for example, detect welfare fraud or to receive benefits, a credit, a mortgage, a loan, a job, a promotion, etc., as we find this unacceptable forms of social scoring . This however still leaves room to track us for many other purposes. The GDPR and the proposals for the Digital Services Act proposal ( DSA ) and the Digital Markets Act ( DMA ) only partially deal with this where it comes to private actors. The European Parliament and the 19European Data Protection Supervisor have already urged to regulate advertising more strictly 20in favour of less intrusive forms of advertising that do not require any tracking of user interaction with content . Amnesty International has asked to end the use of cross-site tracking, prohibit the use of special categories of data listed under Article 9 of the GDPR for ad targeting, and prohibit further uses of personal data that could expose people to discrimination (e.g. data to infer a person s social or financial situation or mood). The DSA and DMA propose elements of this, but rely on end-users to be provided a specific choice whether or not to consent with these practices. It is however questionable whether such consent will truly meet the GDPR s requirements of being freely given, European Parliament resolution of 20 October 2020 with recommendations to the Commission on a Digital Services 19Act: adapting commercial and civil law rules for commercial entities operating online (2020/2019(INL)) ; European Parliament resolution of 20 October 2020 with recommendations to the Commission on the Digital Services Act: Improving the functioning of the Single Market, (2020/2018(INL)) European Data Protection Supervisor, Opinion 1/2021 on the Proposal for a Digital Services Act, 10 February 20212015specific, informed and unambiguous. Current practice has shown that breaches of basic GDPR obligations are common and difficult to rectify once they occur and has led to lengthy legal proceedings. 21ALLAI thinks that these AI practices do not serve society and have no obvious social benefit. While prohibiting these practices might be challenging, ALLAI believes that the further proliferation of these forms of widespread tracking of our entire lives by public and private actors should at least be curbed. Article 5 The following artificial intelligence practices shall be prohibited: (g) AI-driven indiscriminate surveillance and tracking by public and private actors of, including but not limited, online behaviour, location data and IoT-data. noyb, GDPR: noyb.eu filed four complaints over forced consent against Google, Instagram, WhatsApp and 21Facebook, May HIGH-RISK AI In deciding whether an AI practice or use that poses a risk to health, safety or fundamental rights should nevertheless be allowed under strict conditions, the AIA contains a 3 step approach: (i) it determines the criteria for the risk of harm by AI to health, safety and fundamental right s, (ii) it decides on those AI areas and uses that are high-risk but nevertheless pose social bene ts and (iii) it sets a number of requirements that should mitigate this risk. The chosen approach first and foremost poses the risk of normalising and mainstreaming quite a number of AI practices that are still heavily criticised, often due to their lack of sufficient social benefit. Moreover, this approach assumes that the risks high-risk AI systems pose, also future ones, can be sufficiently mitigated by the requirements (which are (paraphrased: datasets of high quality, documentation, transparency, human oversight, accuracy, cybersecurity and robustness). This is however not always the case. Not for the fundamental rights that are most often mentioned as impacted by AI (the right to non-discrimination and the right to privacy), but neither for the many other fundamental rights of the EU Charter that can be impacted by AI. But perhaps the most pertinent question this approach raises is: are we ready to allow high-risk AI to largely replace or heavily influence human decision making, even in critical processes such as law enforcement or the judiciary? The criteria for 'risk of harm to health, safety and fundamental rights' In art. 2 the AIA lays down the criteria to establish whether an AI system s use poses a risk of harm to health, safety and fundamental rights and should be (and likely were) put on the high-risk list. Codifying a number of criteria and leaving out others, prioritises certain circumstances over others, which might nevertheless be relevant. Setting the intended purpose of the AI system as a criterion, limits the possibility to consider the foreseeable use of the system. Setting the ability of a system to affect a plurality of persons as a criterion undermines the need to consider the effect AI can have only on a small number of persons or even one person. As such the provision limits the broad interpretation of our fundamental rights framework, that allows for the consideration of all relevant circumstances. 17Stand-alone high-risk AI (ANNEX III) Annex III (standalone high-risk AI) lists a number of AI uses in a total of 8 areas that are considered high-risk, but nevertheless bring enough social benefit to justify their use, provided that a number of requirements are met. A brief analysis of these uses shows that the benefits of a number of them not necessarily outweigh the risks and that for some, clear benefits are simply lacking. BIOMETRIC IDENTIFICATION AND CATEGORISATION As already argues above, we see no societal benefit in the broad acceptance of biometric identification and categorisation and call for moving this AI practice to the prohibitions. MANAGEMENT AND OPERATION OF CRITICAL INFRASTRUCTURE Missing from this area are AI systems used in the management and operation of the telecom and internet infrastructure: Management and operation of critical infrastructure: (a) AI systems intended to be used as safety components in the management and operation of telecom, internet, road traffic and the supply of water, gas heating and electricity The management and operation of the telecom and internet infrastructure. EDUCATION AND VOCATIONAL TRAINING 22AI systems to determine access to education and evaluate students, in particular where these applications use biometrics and behaviour recognition, pose a number of risks of harm to student health, safety and fundamental rights. Algorithmic grading used during the pandemic caused serious uproar in the UK as it incorrectly downgraded students having detrimental effects on their life opportunities. Online proctoring 23tools track multiple things such as eye movement, mouse movement, key board strokes, sound scapes, copy and paste behaviour, online search behaviour, body movement, etc. to supposedly flag suspicious behaviour and indications of cheating during online exams. Scientific evidence of the effectiveness of these techniques in correctly indicating suspicious behaviour is generally absent, while students have experienced the use of these systems as truly invasive and having a negative effect on their ability to conduct the exams in a dignified manner. 24 EESC INT/940 Artificial Intelligence Act, Rapporteur Muller C. (22 WORKERS MANAGEMENT AND ACCESS TO SELF-EMPLOYMENT 25The use of AI systems for monitoring, tracking and the evaluation of workers, has been causing serious concerns as regards workers fundamental rights to work, to fair and just working conditions, to information and consultation and to justified dismissal. The addition of AI systems used in labour relations to the high-risk list is likely to cause conflicts with national labour laws aimed at (un)fair dismissal, healthy and safe working conditions and the right to worker information. Particularly in labour relations, where there is a power imbalance, these types of 26systems should not be implemented without proper worker consultation and the involvement of social partners. ACCESS AND ENJOYMENT OF ESSENTIAL PRIVATE SERVICES AND PUBLIC SERVICES AND BENEFITS 27Depending on the kind or information used, we consider the use of AI systems in relation to access and enjoyment of public and private services a form of unacceptable social scoring. For assessments with AI in these domains, the description of the use of AI systems in relation to the access and enjoyment of public services is more broad than the use of AI systems in relation to access and enjoyment of essential private services, where for the latter, only credit(worthiness) scoring by AI is considered high risk. We propose the following amendments: Access to and enjoyment of essential private services and public services and benefits: (a) AI systems intended to be used by or on behalf of (semi-)public authorities or private parties to evaluate the eligibility of natural persons for public assistance, benefits and services and essential private services, as well as to grant reduce, revoke, or reclaim such benefits and services; (b) DELETE LAW ENFORCEMENT Many of the listed AI uses in the area of law enforcement pose a risk of harm to human dignity, privacy, the right to non-discrimination, the presumption of innocence, the right of defence and the right to a fair trial. 28Lie detection and emotion detection with biometric recognition is scientifically flawed and truly invasive. Many AI systems that make individual risk assessments merely seek correlations between characteristics found in other cases , on characteristics that a person happens to share with other convicted criminals (such as address, income, nationality, debts, employment, behaviour, behaviour of friends and family members and so on). Moreover, it is often impossible for legal professionals to understand the reasoning behind the outcomes of the system, which affects the right to the right to a fair trial. Predictive policing has led to undesirable feedback loops where the same communities are surveilled more often invading the right to privacy. Ibid.25 EESC INT/940 Artificial Intelligence Act, Rapporteur Muller C. (2727 Open letter by 61 organisations calling for legal limits on AI risk assessment systems in the criminal justice context2819 We strongly recommend to consider strongly limiting the use of these types of AI uses by law enforcement. Exceptional uses could be considered, however these should be evidence based, necessary and proportionate, limited in time and have a legal basis. MIGRATION, ASYLUM AND BORDER CONTROL MANAGEMENT AI used in migration, asylum and border control management for making individual (criminal or security) risk assessments, pose a risk of harm to the presumption of innocence, the right of defence and the right to asylum. According to a recent paper by the EPRS: The EU s 29centralised information systems for borders and security are increasingly incorporating 30biometric technologies for the purpose of identity verification or identification, automated fingerprint identification. Facial recognition is expected to be used by all systems except one in the near future. A number of EU-funded projects and initiatives have explored and piloted emotion recognition technologies at the EU border. Art. 83 of the AIA however excludes AI systems which are components of these systems from its scope, if they are put into service before the application of the AIA. This enables EU s centralised information systems for borders and security to implement (to be) prohibited and high-risk AI systems in the near future, only having to comply with AIA when the entire system is evaluated. We propose to move the AI uses listed in ANNEX III point 7 under (a) and (b) to the prohibitions. THE ADMINISTRATION OF JUSTICE AND DEMOCRATIC PROCESSES 31The use of AI in the administration of justice and democratic processes is particularly sensitive and should be approached with more nuance and scrutiny than it is done now. Merely putting systems to assist a judicial authority in researching and interpreting facts and the law and in applying the law to a concrete set of facts overlooks the fact that judging is so much more than finding patterns in historical data (which is in essence what current AI systems do). The text also assumes that these types of AI will only assist the judiciary, leaving fully automated judicial decision making out of scope. While the header mentions democratic processes as an area, there are no AI uses listed in the realm of elections. We recommend the following amendment to ANNEX III: Artificial Intelligence at EU borders , EPRS These systems include (a.o.): Schengen Information System (SIS), the Visa Information System (VIS), Eurodac, the Entry/30Exit System (EES), the European Travel Information and Authorisation System and the European Criminal Records Information System on third-country nationals and stateless persons (ECRIS-TCN) EESC INT/940 Artificial Intelligence Act, Rapporteur Muller C. (3120 Administration of justice and democratic processes: (a) AI systems intended to be used for or assist a judicial authority in researching and interpreting facts and the law and in applying the las to a concrete set of facts. (b) AI systems or uses in the realm of elections, voting and counting processes and election news aggregation and provision. MISSING HIGH-RISK AI DOMAINS/USES Missing from the High-Risk list is AI used for content moderation: X. AI systems intended to be used for online content moderation (as described in articles 12, 26, 27 of the DSA) Harmonised products with AI The interwoven system between the AIA and Union Harmonisation Legislation is a smart approach from a lawmaking point of view, however, only AI safety components of a product, or AI that is itself a product, that is covered by Union Harmonisation Legislation listed in ANNEX II, are currently covered by the AIA. AI however not only can pose risks when used as safety components of these products, nor is the AI system itself always a product. When AI is used for example as part of a diagnostic or prognostic tool in the medical field or an AI driven thermostat that regulates a boiler (both covered by Union harmonisation legislation listed in ANNEX II), this should also be covered by the AIA. ALLAI realises that this will cover many AI systems, also those that pose no risk whatsoever, but a case can be made that the fact alone that these products are covered by harmonised rules and already go through rigorous pre-market assessment, merits prior conformity assessment of all AI elements of these products as well. Article 6 Classification rules for high-risk AI systems Irrespective ( ) (a) the AI system is intended to be uses as a safety component ( ); (b) the product whose safety component is the AI system ( ). Delegated acts - limited to predefined areas Art. 7 of the AIA allows for the Commission to add AI uses to the high-risk list by delegated act, but only in the 8 areas that are already listed in ANNEX III. This excludes the possibility to add risky AI uses in other areas. Moreover, amendments to ANNEX II are not possible under the AIA, while potential new Union Harmonisation Legislation for products with AI is feasible. We propose the following amendments: 21Article 7 Amendments to ANNEX II and ANNEX III The Commission is empowered to adopt delegated acts in accordance with Article 73 to update the list in to make additions to the lists in ANNEX II and ANNEX III by adding high-risk AI systems where both of the following conditions are fulfilled: (a) DELETE Do the requirements mitigate the risks? ALLAI welcomes the alignment of the the requirements for high risk AI with elements of the Ethics Guidelines for Trustworthy AI (the Ethics Guidelines ), however, five important requirements of the Ethics Guidelines are not specifically dealt with in the AIA namely: (i) human agency (ii) privacy (iii) diversity, non-discrimination and fairness, (iv) explainability and (v) environmental and social well being. ALLAI considered this a missed opportunity, because many of the risks AI poses are those of privacy, bias, exclusion, inexplicability of the outcomes of AI decisions, the undermining of human agency and the environment, and are all reflected in our fundamental rights. While some of these missing requirements are (partially) dealt with in existing legislation such as the GDPR (privacy and explainability) and the European Charter on Fundamental rights (non-discrimination), this is not sufficient. Moreover, by adding the missing requirements, the AIA can bring many human rights protections into the private sphere as well. The GDPR offers some frameworks for the right to an explanation of AI decisions (in the recitals) 32and the right to human intervention. However, these frameworks are insufficient to ensure adequate explainability in automated decision-making. The right to human intervention in the GDPR for example only exists in the event of a fully automated decision, which is easy to circumvent. Moreover, the GDPR deals with situations where personal data is involved and it is well known that a decision based on non-personal data can also have a major impact. 33What is more important is that the AIA works from the premise that the requirements for high-risk AI will sufficiently mitigate the risks of harm to health, safety and fundamental rights. This is however expected not to be the case in all circumstances. As for discriminatory outcomes for example, it overlooks the fact that AI biases are the result of low-quality data, lack of human oversight or inaccuracy. The design of any artefact is in itself an accumulation of choices that can cause biases to creep in. And even with high quality data, documentation, human oversight, accuracy, robustness and cybersecurity, AI can still pose risks. Often where it comes to less mentioned human rights such as freedom of expression, freedom of assembly, presumption of innocence, right of defence, right to fair trial etc. Merely focussing on technical and procedural solutions does always not suffice to avoid this. Rather the socio-technical processes around AI need to be organised in such a way that that any Lokke Moerel and Marijn Storm, Law and Autonomous Systems Series, ""Automated Decisions Based on Profiling - 32Information, Explanation or Justification, That is the Question!"", Oxford University, Faculty of Law Artificiele Intelligentie - Human in Command ALLAI (3322discriminatory, unfair or otherwise harmful outcomes of AI can be adequately identified and addressed. ALLAI proposes to add the missing requirements from the Ethics Guidelines for Trustworthy AI to the requirements of Chapter 2 of Title III of the AIA, to improve the ability of the AIA to effectively protect our health, safety and fundamental rights from adverse impact of AI: Human Agency (part of EGTAI requirement Human Agency and Oversight ) Privacy (part of EGTAI requirement Privacy and Data Governance ) Diversity, Non-discrimination and Fairness Explainability (part of EGTAI requirement Transparency ) Societal and Environmental Well-being Finally, the AIA is unclear as regards the situation where the requirements for high-risk AI are not met. Art. 67 of the AIA deals with compliant AI systems which present a risk , but only after these systems have entered the market. The potential gravity of the adverse effects of high-risk AI, justifies a precautionary approach, rather than a post-hoc reparatory approach. This can be arranged for by adding a paragraph to art. 16 AIA: Article 16 Obligations of providers of high-risk AI systems Providers of high-risk AI systems shall: (h) refrain from placing a high-risk AI system on the market that: (i) is not in conformity with the requirements set out in Chapter 2 of this Title; or (ii) poses a risk fo harm to health, safety or fundamental rights despite its conformity with the requirements set out in Chapter 2 of this Title. A FIDUCIARY DUTY FOR PROVIDERS AND USERS The AIA could further mitigate the risks of high-risk AI by establishing a fiduciary duty on AI providers and users to acknowledge, anticipate and protect the fundamental rights of those at the receiving end of the AI system, accompanied by an obligatory human rights impact assessment . A known characteristic of high-risk AI is the imbalance of power, knowledge, economic or social circumstances, or age between those affected by the system and those designing or running it (see also art. 2 (f) AIA). It thus makes sense to place this special duty of care on providers and users of high-risk AI systems, where they prioritise the interests of those affected by the AI systems even over their own potential interests: Article [x] (new) The providers and users of high-risk AI systems have a fiduciary duty towards those affected by the the AI systems they are deploying or using to act in the interest of potentially harmed or adversely impacted persons. 23INCLUSIVITY AND MULTI-DISCIPLINARITY Another important tool to assess, address and mitigate potential risks of high-risk AI that is missing from the AIA is the involvement of relevant stakeholders, domain experts and representative bodies. We propose the following addition to art. 9 AIA: Article 9 Risk management system ( ) The risk management ( ) updating, continuously involving and consulting relevant stakeholders, including but not limited to domain experts, representative bodies and the social partners. The prerogative of human decision making In line with our long advocated human-in-command approach to AI, ALLAI strongly recommends the AIA to arrange for certain decisions to remain the prerogative of humans, particularly in domains where these decisions have a strong moral component as well as legal implications or a societal impact such as in the judiciary, law enforcement, social services, healthcare, housing, financial services, labour relations and education. If one takes a deeper look at the high-risk AI uses of ANNEX III, one will see that once the requirements for high-risk AI are met, AI can potentially largely replace human decision making in law enforcement, the judiciary, enjoyment of essential services, migration and asylum, recruitment, hiring, firing and worker-assessment, education, democratic processes. Most AI is technically incapable of making these decisions at a human level and it is questionable if it is even desirable. Ultimately, not all decisions can or should be simplified to ones and zero s . ALLAI proposes the AIA to arrange for certain decisions to remain the ultimate prerogative of humans, particularly in domains where these decisions have potentially severe legal implications (such as in the judiciary, law enforcement, social services, healthcare, housing, financial services, labour relations and education). ACCOUNTABILITY GOVERNANCE & ENFORCEMENT ALLAI welcomes the governance structure set up by the AIA. We do see areas for improvement though, in particular where it comes to inclusivity and multi-disciplinarity, complaints and redress, and the exclusion (for now) of the applicability of the AIA to legacy high-risk AI and components of large scale European IT systems in the realm of freedom, security and justice already put into service before the application of the AIA. Moreover, the accountability structure is quite complex. Complex self-assessment structure The AIA contains a complex accountability structure with a lot of text on third party conformity assessment by notified bodies, while only biometric recognition and categorisation outside law enforcement is subject to third party prior conformity assessment. Prior conformity of all other standalone high-risk AI on ANNEX III are self-assessed. The complexity of the AIA and the requirements in combination with self-assessment runs the risk of this process simplifying being reduced to check lists where a simple yes or no could suffice to meet the requirements. This would make the AIA fail to achieve its initial objectives of promoting and driving innovation of AI that is trustworthy and in line with our fundamental rights and societal values. ALLAI recommends to have all high-risk AI assessed by a third party. Inclusivity and multi-disciplinarity We recommend the AI Board to not only include representatives from the national authorities, but from wider society as well, including the social partners and NGO s. Also, the board should consist of or be advised by expert advisors from various fields such as AI, data science, the law, ethics, social sciences, psychology, economics, labour relations, healthcare and education. 25Complaints and redress A complaints and redress mechanism for organisations and citizens that have suffered harm from the use of any AI-system that falls within the scope of the AIA is lacking and should be arranged for in the AIA. Legacy high-risk AI Article 83 of the AIA excludes high-risk AI systems already placed on the market or put into service before application of the AIA (unless after that date the AI system undergoes significant changes). ALLAI stresses that these legacy high-risk AI systems should also be covered by the AIA, in order to avoid deployers to fast track any high-risk AI to avoid compliance requirements. * * * 26Authors Catelijne Muller is co-founder and president of ALLAI; former member of the European High Level Expert group on AI; Rapporteur on AI for the European Economic and Social Committee; expert advisor for the Council of Europe on AI & Human Rights, Democracy and the Rule of Law; member of the OECD Network of Experts on AI (ONE.AI); former Dutch lawyer, admitted to the Amsterdam Bar. Virginia Dignum is co-founder and board member of ALLAI; professor of Artificial Intelligence at Ume University; scientific director of the Wallenberg AI, Autonomous Systems and Software Program Humanities and Society (WASP-HS); former member of the European High Level Expert group on AI; member of the World Economic Forum AI Board; expert advisor on AI for UNICEF; member of GPAI s working group on Responsible AI. ALLAI refers to Stichting ALLAI Nederland, a foundation under Dutch Law. No entity or person connected to ALLAI, including its Board Members, Advisory Board Members, employees, experts, volunteers and agents, is responsible or liable for any direct or indirect loss or damage suffered by any person or entity relying wholly or partially on this communication. 2021 ALLAI, The Netherlands Prinseneiland 23A 1013 LL Amsterdam welkom@allai.nl 27",en,"(i) Human agency (ii) privacy (iii) diversity, non-discrimination and fairness, (iv) explainability and (v) environmental and social well being are particularly important to avoid the risks AI poses, which are those of privacy, bias, exclusion, inexplicability of the outcomes of AI decisions, the undermining of human agency and the environment. ALLAI welcomes the alignment of the the requirements for high risk AI with elements of the Ethics Guidelines for Trustworthy AI (the Ethics Guidelines ), however, five important requirements of the Ethics Guidelines are not specifically dealt with in the AIA namely: (i) human agency (ii) privacy (iii) diversity, non-discrimination and fairness, (iv) explainability and (v) environmental and social well being. ALLAI proposes to add the missing requirements from the Ethics Guidelines for Trustworthy AI to the requirements of Chapter 2 of Title III of the AIA, to improve the ability of the AIA to effectively protect our health, safety and fundamental rights from adverse impact of AI: Human Agency (part of EGTAI requirement Human Agency and Oversight ) Privacy (part of EGTAI requirement Privacy and Data Governance ) Diversity, Non-discrimination and Fairness Explainability (part of EGTAI requirement Transparency ) Societal and Environmental Well-being Finally, the AIA is unclear as regards the situation where the requirements for high-risk AI are not met.",risk
Hkan Burden (Sweden),F2665627,06 August 2021,EU citizen,,Sweden,"Regarding the Commission s propos al to regulat e Artificial Intelligence (COM/2021/206 final) The consultation response The response is perfo rmed by H kan Burden and Susanne Stenberg, senior researchers at the Research Institute of Sweden, RISE ( , working within software engineering, law and policy development. We welcome the opportunity to provide feedback on the European Commission s proposal on the regulation of Artificial Intelligence. The proposed regulation [1] covers several t opics and is intertwined with existing and coming regulations and frameworks (e.g. [8 -15]), making it a steep task to see the full width of the consequences. From our perspective it is still clear that numerous activities and organizations will be affected if the proposal is accepted. Thus, t he response has been prepared and drafted based on an initial analysis on our side. In our work, we have had the opportunity to obtain views from several others (e.g. researchers in computer science, regional authoritie s, start - ups, government agencies and private innovation actors). The views have been obtained through seminars organized by AI.se, RISE AI agenda and industry organizations. Our response tries to meet the challenge by both digging deeper into a few aspec ts as well as taking a broader perspective by relating the proposed regulation to overarching themes. Even if this is our formal response to the Commission, we do not claim to have a final analysis. To fully understand the complicated network of ongoing re gulation within the EU in relation to digitali zation and related artefacts is a moving target full of complexity, therefor this is to be seen as a first step towards an analysis of the consequences that will evolve together with the proposed regulations and in interaction with the relevant stakeholders . Structure of the response Our reply towards the proposed regulation of AI is organized as to one, relate the motivation for a regulation of AI to the content of the proposal, two, high -light some possible consequences of the regulation in terms of innovation and future investments in areas concerning AI, th ree, look at the effect of the regulation from a technical perspective in terms of obligations for different actors within the eco -system fo ur, illustrate how the proposed regulation could affect public governance, five, introduce the concept of sustainable development to show the limits of the proposed regulation, and .. six, place the proposed regulation in a broader context by the synergi es with on -going changes to existing regulations and the shift of who decides what within the EU. These topics are not orthogonal, there are common aspects shared among the topics. The up - side is that the topics raises questions worth considering when ass essing the proposed regulation how do we promote sustainable innovation in regard to AI and what do we perceive as sustainable innovation? Who should be responsible for what, both in terms of developing and using AI -systems but also in terms of governing those activities? Which role should AI play in relation to governance and digitalisation? And not least, will the proposed regulation resonate with the underlying motivation? Motivati ng AI regulation This section views the proposed regulation in the lig ht of how it is motivated and the definitions of AI and high -risk. Trust towards AI as technology The Explanatory Memorandum lists four objectives with the proposed regulation (: Ensure that the AI -systems placed on the market are safe and respect fundamental rights, Create a predictable legal foundation for innovations and investments, Broaden the governance and surveillance of existing regulations concerning fundamental rights and safety requirements to encompass AI -systems, and Facilitate a commo n market for legal, safe and trustworthy AI -systems as well as ensure that the playing field is not fragmentised along national borders. The importance of the triad health -fundamental rights -safety recurs throughout the proposal. Yet the chosen areas of regulation , deducted from the proposals wording, seems to build upon two, rather than three, thus prompting how come health does not need the same explicit emphasis ? For instance, w e can see how fundamental rights are covered by points and . Annex III lists areas where the usage of AI is regarded as introducing a high risk in terms of fundamental rights. We can also see how safety is covered by points , and , while Annex II lists products that pose a high risk in terms of safety if AI is a part of their safety components. It is to us unclear how the given triad motivates the chosen items listed in Annex III. For instance , there is no coverage of AI systems used for distributing health services while health is a key motivation for regulating AI systems . We are not arguing for or against regulating AI systems within the health sector, but from our perspective the motivation and the regulated activities are unaligned. Working environment is not mentioned either. The only mentioning of health is in a.ii where it says that exemptions from GDPR can be approved if it serves the purposes of training an AI for proactive, controlling and/or treating diseases. Are these areas already governed by existing or coming regulations , in the same way as the fi nancial sector (Memorandum 2 )? Is health to be seen in the light of physical and psychological harm (Article ? But health can be so much more than (the absence of) harm. Or are there other reasons why health is not regulated in the same way as fundamen tal rights and safety? Reading the proposal as it stands, it is unclear for us why health is used for motivating the regulation. Since infrastructure and water supplies are listed as fundamental resources in Annex III, it is interesting that other fundamental resources such as agriculture and forestry are not mentioned as well. AI -systems can be used for determining where and when t o fertilise, when to harvest or limit the waste from timbering [2]. So why are these areas not listed in A nnex III? If the machinery uses AI as part of the safety system , then the products would be regulated according to the proposal, but not the activity they are used for. Is this by purpose? It is difficult to see how the Commission motivates the scope to regulate. Is it products (Annex II) and areas (Annex III) that are to be regulated and thereby ensure that health, fundamental rights and safety are no t compromised? Since it is not clear on what basis which products and areas have been included or excluded from the annexes there is a risk that the regulation is seen as covering arbitrary usage of AI instead of being a step towards a systematic approach for sustainable digitalization. Defining AI and AI systems We know from talking to other actors in Sweden that the definition of AI as proposed in Annex I will be scrutinised in detail. We still want to raise a perspective , in relation to the ambition that regulation will improve the trust towards the technology. If the definition is ambiguous or does not resonate with that of the users and/or developers there is a risk that actors do not report their perspectives on the proposal, making the final regulation less effective since it only reflects a subset of interests in relation to AI. An ambiguous or vague definition of AI also raises the risk that relevant actors do not take the regulation seriously. The usage of different definitions of AI within th e EU s work on regulating AI does not make it easier to understand the scope and purpose of the proposal nor how different initiatives interact with each other. See for instance the definition used by the High -Level Expert Group [3] or in relation to ethic s [14]. The term stand -alone is not defined in Article It is a concept of importance in the proposed regulation , just as safety component of a product or system and high -risk , and the proposal would benefit from a clear definition of what is ref erred to when the concept is used. High -risk systems Article 6 defines what is to be regarded as a high -risk system. It contains two unclarities. The first unclarity is in Article 1 where a word -by-word reading of the proposal gives that an AI-system i s to be regarded as high -risk if it both a) Is a part of or is in its own right a safety component used in a product listed in A nnex II, and b) Is a part of or is in its own right a safety component used in a product listed in A nnex II and requires a third party in the certification process. Since all systems that fulfill b) also fulfill a), a) is either redundant or the intention is that a) or b) should be fulfilled. We would like to see that the formulation is clarified to avoid redundancy or to describe the distinction between a) and b). In Explanatory memorandum ( it says that an AI -system is seen as entailing high risk if it is used as part of a safety component that requires the participation of a third party during certif ication or within an area listed in Annex III. From that perspective it makes sense to delete the first sentence (a) (article a). The second unclarity regarding Article 6 derives from Article 60 that refers to Article 51, that refers to Article 2, t hat refers to Article 1 with the formulation In addition to the high- risk AI systems referred to in paragraph 1 . Is it just the systems defined under article 2 that are relevant for article 60, or does article 60 also refer to the systems defined und er article 1? Since the interpretation of article 1 is open for different interpretations it is both unclear if article 60 refers to the systems under article 1 and in turn which systems article 1 refers to. The activities listed under A nnex III are uncontroversial when performed by a human. The regulation should make it clear why the same activities are high -risk when performed by an AI, even if the outcome could be more reliable and consistent behaviour. Article 5 lists activities that are prohi bited to perform with an AI. The activities mentioned under Article 1 are already regulated when performed by a human. There are regulations prohibiting a human from using subliminal messages to manipulate someone to harm themselves or others, such as th rough commercials or the free forming of opinions. The question is then how come the activities under Article 1 need new (more or another) regulation, is it to clarify who is responsible for the effects or so that we can impose other penalties if the act ivities are performed through an AI rather than by other means? Article i states that providers of high -risk AI systems should affix the CE marking to their systems in accordance with Article There is an underlying assumption that the marking is visible in the same way it is for physical products. If the intention is that the marking will have the same effect as for instance on machines , it requires that the documentation is shown every time the high -risk AI system is launched which will have a n impact on the user experience. If the AI is embedded in a machine there will still only be one marking on the product so that it is not obvious for a user or other actors that the embedded high -risk AI is compliant with the regulations, perhaps not even that the product has an embedded high -risk AI system. While the CE marking has had success on the market for physical products it is not obvious that the concept can be applied to digital systems. Further investigations could be carried out on user experie nce and public perception before a straight application of regulations used for physical products is mandatory for digital systems. We have been in touch with appointed representatives from public organisations in Sweden that state that they have read the definition of AI and high -risk AI systems and do not understand which systems are affected and what the consequences will be. The risk is that the proposal will cause more uncertainty than it will clarify the role of AI and its usage in a harmonized market. The innovation system In terms of the proposed regulation s impact on the innovation system we have focused on how it will affect systems governed under the Old Approach Legislation (OAL) and the New Legislative Framework (N LF) as well as the role of the regulatory sandboxes. Old and new regulations In terms of product safety , Annex II lists both products regulated under OAL and NLF. At the same time, Article 2 states that products regulated under OAL are exempt from the proposed regulation in all respects except Article However, Article 84 says nothing about what is expected of actors in relation to products regulated under OAL. It is therefore unclear why they are listed in Annex II and why they should comply to Arti cle Since products regulated under OAL is out of scope in terms of the proposed AI regulation, a manufacturer of products within both OAL and NLF can choose to focus R&D within one segment to avoid the added investments needed for developing high -risk AI systems for products under NLF. That could in turn result in a shift from R&D in machine safety towards autonomous systems for vehicles, with a slow uptake of developed features in the NLF segment despite the AI systems already being in place. Not only could the difference in how the proposed regulation is implemented then have an impact on where R&D is conducted, it could also have an impact on how existing safety systems are deployed between different product segments. The exempt for OAL from the pr oposed regulation is a temporal solution. Ingress ( states that it is appropriate to amend those acts to ensure the mandatory requirements for high- risk AI systems laid down in this Regulation when adopting any relevant future delegated or implementi ng acts on the basis of those acts . A formulation with the same intent can be found in Memorandum 2 . This is not the first t ime that the EU has incorporated new harmonized regulation when adapting or implementing acts within OAL. It has been done both for the general product safety [4] as well as for the Evidence Reporting Devices (ERD) in relation to UNECEs acts on vehicle typ es and safety. The way this has been done previously has caused both uncertainty and tensions within the affected industry and actors. We have talked to automotive representatives that claim it could be easier to conduct R&D outside of the EU if this is th e initial steps of a long -term process where regulations imply more administration and market uncertainty. In terms of the ERD there are reports that when the initial US initiative was implemented in a EU setting, an additional 100 new functional requireme nts were added iteratively. New functional requirements imply new technical requirements which in turn drive new negotiations and procurement procedures. One source of disturbance is when compliance must be ensured in shorter loops than the manufacturers n ormal cadence for planning and designing new products since existing agreements with suppliers become obsolete and new ones have to be settled. One area of products that will be covered by the proposed regulation is machines and their safety systems. If you take the current version, Annex IV states that machines with devices for identifying the presence of persons need third -party involvement in t heir conformity assessment and therefore qualify for being high -risk AI systems according to Article 2 of the proposed AI regulation. In the proposal for a new machine directory, Annex I states that machines with devices for detecting the presence of per sons, logical units ensuring safety functions and/or AI systems ensuring safety functions need third -party involvement in the conformity assessment process. Similarly, these machines are also classified as having high - risk AI systems. This means that exist ing machines will need to be certified according to the proposed AI regulation if it is accepted as is. The question is to what extent actors on the harmonised market realise the extent of the proposed regulation and what the effect will be on what kind of safety systems we see on future machines. Reducing the number of safety systems, or their complexity, will probably also reduce the work needed for certification. The proposed regulation can therefore create different possibilities for similar market segments within the EU, uncertainty among the actors, unforeseeable costs for R&D, hamper market uptake of AI -based safety systems in one product segment which are available and used in other segments as well as cause tension with international bodies regula ting large product segments such as vehicle standards and aviation. Regulatory sandboxes and SMEs The Commission recognizes that the proposed regulation will result in more administration for the concerned actors. Small and medium sized enterprises (SMEs ) will probably be more affected since they lack the possibilities of larger organisations to handle the administration. We agree, not least because we have heard representatives from multinational companies state that regulation is beneficial since it mak es the market more stable and therefore more difficult to upset with innovations from SMEs. The proposed mitigation strategy is to create regulatory sandboxes that allow for SMEs to hand over some of the administration to a competent national authority. Ho w the authorities will balance that responsibility with their other assignments, such as market surveillance , or how to achieve a harmonized usage of sandboxes across the EU is not specified. From our perspective we wonder if there is evidence that regul atory sandboxes are an effective strategy for facilitating innovation. We would like to see a more elaborate report on the consequences of the regulation in terms of its effect on innovation, specifically in relation to regulatory sandboxes and SMEs. Another way forward would be for an SME to partner with a larger actor with the ability to take the full administrative work onboard. The deal could have an impact on IP rights and revenue streams which in turn could mean that the regulation serves to protect the established actors on the market. The benefit for SMEs would be that they can focus on their innovative ideas and make the administration of the AI system a question of commercial contracts instead of a negotiation with national authorities. The latte r demands another set of competences that not all SMEs have made a priority to recruit. The proposed regulation mentions another possibility in Article 2 that states that the quality management system ensuring compliance with the regulation should be i n relation to the size of the organization. This could be used by SMEs instead of regulatory sandboxes or difficult negotiations with large and established actors. But it could also be used by large organisations that want to reduce their administration by relating the quality management system to the size of the unit developing the AI system, not the whole organization. Regulatory sandboxes is a topic we will return to from the perspective of public governance further down in our response. Responsibiliti es in a system -of-systems In this section we will focus on AI systems and their development as systems -of-systems, but also on AI systems as component s in other systems and what effects the proposed regulation can have in terms of responsibilities among an d between actors. System boundaries A challenge for actors with the proposed regulation is that artefacts and systems developed for other purposes can be used for future development of high -risk AI systems and therefore covered by the regulation (Annex IV .. It could be services and platforms providing data regarding road conditions [5] or the placement of wastewater wells [6] (depending on the interpretation of the definition in Annex I they might be high -risk AI systems or mere data sources). If the sup plied data is used for developing AI systems which can be used for managing vital infrastructure like roads and water supply (Annex III) they will be part of the technology chain behind the AI system and need to be administered as such. The same goes fo r developers of models, such as digital twins, if they are used for developing AI systems that are classified as high -risk systems. Providers of data regarding populations face the same uncertainty if the data covers for instance taxable income, number of residents at a specific address or fluctuation of property prices since they can be used to determine strategies or decisions for social benefits or targeted interventions against social exclusion (Annex 5 and . In the l ong run all providers of digital artefacts face the probability that their service, data or technology will be used for developing high -risk AI and therefore need to have the right documentation to conform with the regulation and avoid fines . At the same time there are political initiatives that promote public authorities and actors to facilitate data sharing as well as a need for digital simulations and models for societal and business planning. These artefacts could be high -risk AI systems in themselves or become parts of such systems, either as components or in the product line, which could hamper public transparency and increase the risks across market segments as digital assets are withdrawn or not used. Regulatory scope There are two concrete markets worth discussing in more detail; social platforms that can be used for subliminal messages and digital artefacts used by employers and employees for promoting and responding to potential employments. In the case of subliminal messages, it is impor tant to recognize that the message itself does not have to be created by an AI for the regulation to be invoked. The message could be designed and posted by a human and then spread by social platforms through technology listed as AI in Annex I. Then that t echnology could fall under Article a with subsequent consequences for the responsible actor. In this way the proposed regulation can be used to regulate social media and platforms. It is not clear from the proposition if the intention is to regulate th e responsibilities of providers of social platforms or not. In the case of employments, the proposed regulation is relevant for two different scenarios. The first scenario is when employers and employees use digital platforms to inform of open positions if the distribution of the post is done by a technology listed in Annex I. The platform is then a high -risk AI system. The same would hold if a company uses a digital artefact, like an app, to distribute assignments among a set of job seekers if the distri bution is done by a technology listed in Annex I. The proposed regulation could then be used to regulate what is known as the gig economy but also networking platforms. It is not clear from the proposition if the intention is to regulate the gig economy or not, neither is it clear if digital networking platforms are to be seen as high -risk AI systems . Contracts and licenses One way for actors to reduce or transfer their responsibilities is through contracts and licenses. An open question is to what extent the proposed regulation will recognise such licenses or contracts. There is a risk that such agreements could have a negative effect on the willingness to use the resources or that actors enforce more restrictive terms of usage through their licenses and contracts to ensure that they are not within the scope of the AI regulation. Either way, the proposed regulation can result in actors being more careful in making their digital assets available for other actors, hampering both innovation and business. A specific concern is to what extent a supplier can negotiate their responsibilities towards a buyer or end user. The question is not only in terms of if the buyer or end user will agree to the terms, but also if it is within the intention of the regulation. As it is difficult for a supplier to see in advance the full scope of possibilities that their service or product can be used for in the future the question needs to be addressed to avoid unclear situations and expensive lawsuits determining who is to pay which fines if the documentation is not conforming to the proposition as it stands. As an employee it is also important to know to which degree you will be personally responsible for the system you are developing or using (developing in this case also ref ers to activities concerning the resources and tools used for designing, implementing, testing and maintaining the AI system). As an example, Annex g states that all test logs should be signed by a responsible person. Responsible for what is not made explicit; is it the test case, running the test, the AI system, or the organization? To what extent can an employee be responsible for the AI system and take the consequences if it does not conform to the proposed regulation? To what extent can a developer or manager of one part of the AI system be responsible for the system as a whole or its future usage? Public governance The proposed regulation will also affect public governance and could conflict with growing trends of digitalization of public adminis tration. Digitalisation Within public governance there are on -going attempts to make administration more effective, transparent, and deterministic through digitalisation, rule as code [7]. The aim is to automate repetitive and simple tasks so that train ed personnel can focus on more demanding issues and improvements. With the proposed definition of what is to be regarded as an AI system (annex I) and the list of activities perceived as high -risk (annex III), one plausible interpretation is that activities as the calculation of compensations for taking care of children or social benefits, deciding the right to housing allowance and/or administrating congestion taxes are high -risk when performed by a software system but mundane when a human carries them out. In the long run this could hamper the digitalization of public governance in general, and more specifically initiatives for open access and more informed decisions through digitalisation, as actors refrain from using and developing systems that need to be reported and certified according to the proposed regulation. Another area that might be affected is the usage of databases , both for national and international regulations , such as Eurlex . They could be seen as AI systems according to the defin ition, using statistical models for optimising search results and knowledge for listing the output according to relevance. So could popular search engines used to access the databases through browsers. All of t hese systems would then need to be compliant w ith the proposed regulation if they are to be used in public administration or judiciary . It is not given that developers of search engines will comply. It could be more economically sane to reduce the functionality of the systems and services within the h armonized market or challenge the applicability of the regulation for their products and services, leading to a public debate that not necessarily would increase the public trust in AI as a technology or how it is regulated within the EU. Regulatory sa ndboxes and national competent authorities Article 43 defines the concept of national competent authority as the national supervisory authority, the notifying authority and the market surveillance authority . It might be the case that all three functi ons will not be carried out by one and the same national authority . Specifically, since different authoritie s already are responsible for the product safety of toys and medical devices at a national level and both market segments will be affected by the pr oposed regulation in general, but also more specifically in relation to the regulatory sandboxes. It is an open question if the national authorities should be aligned so that they implement the surveillance in the same fashion, to the same level of detail for the same aspects regardless of the AI system being a safety component in a machine, a toy, radio equipment or medical devices. If they are to be aligned on national level, how will the same alignment be organized de facto between national authorities? If they are not aligned, it might be easier to place R&D in one member state rather than the others for quicker and easier compliance with the proposed regulation, which could have a negative effect on product safety and trust but yield positive effects i n terms of investments and job opportunities on national level. Since it is the national competent authorities that decide on the usage of regulatory sandboxes it is also important that each authority is aware of who can establish a regulatory sandbox for which AI system. Raising our heads The task of analy zing the proposed regu lation and its possible effects is daunting. There are interactions with other regulations, either as they stand today [4,8] or might be formulated in the future [11,12,13,15], across diverse activities such as the production of toys, social interaction on digital platforms and public governance. To grasp the full picture and describe the most important issues at hand requires both time and resources. It is therefore difficult to determine what the impact of the regulation will be in terms of innovation, public governance, and trust. There are aspects on how SMEs will fare in relation to large and established actors where the regulation could serve to hamper innovation but also lead to new kinds of partnerships and business opportunities. There are aspects of how the regulation can hamper open access to public assets, the usage of digital services and products in public governance as well as the relationship between national governance and the EU institutions. If these aspects are not resolved in a satisfac tory manner there can be implications on public trust, not necessarily towards AI as technology but towards the regulation as such and the intensions to regulate AI as technology. We have so far mainly focused on the content of the proposed regulation. If we instead look at what is not in the proposition, there is one global aspect missing sustainable development. The impact of AI on sustainable development is substantial. If one wants to, you can see an attempt to address some of the social aspects in the current proposal, such as in article 5 and annex III. Some of the economical sides can also be identified by a willing eye in terms of annex II and article There are of course more examples of where sustainable development can be induced from the pro position. However, it is far more difficult to see any attempt to regulate the climate impact of AI. Still, there is an opportunity to align the visions of leading responsible digitali zation and sustainable development. The question is why the opportunity has not been grasped? Is it because it is too difficult to determine the amount of resources used for AI development and how to regulate that consumption? Or is it too sensitive? There might be many , and more, reasons, so stating a clear intent in terms of how AI and sustainable development are to be aligned within the harmonized market would reduce the uncertainty . Regulating how AI is used within sustainable de velopment is a complex task but if the regulation is going to serve its objectives and fit in with the needs of today and future generations it needs to be addressed. If not now, when? Referen ces [1] Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT A ND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS COM/2021/206 final [2] Garske B, Bau A, Ekardt F. Digitalization and AI in European Agriculture: A Strategy for Achieving Climate and Biodiversity Targets? Sustainability . 2021; 13(: [3] High -Level Expert Group on Artificial Intelligence, A Definition of AI: Main Ca pabilities and Disciplines, Apr. 8, 2019 [4] Directive 2001/95/EC of the European Parliament and of the Council of 3 December 2001 on general product safety [5] Swedish Transport Administration , API, [6] Naturv rdsverket, Avloppsreningsanl ggningens utsl ppspunkter - utsl ppspunkter av avloppsvatten (Urban Waste Water Directive, 91/271/EEG), - details.html#esc_entry=105 63&esc_context=69&esc_ - org=http%3A%2F%2Fdataportal.se%2Forganisation%2FSE2021001975 [7] Mohun, J. and A. Roberts (, ""Cracking the code: Rulemaking for humans and machines"", OECD Working Papers on Public Governance, No. 42, OECD Publishing, Paris, -en. [8] Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency for Cybersecurity) and on information and communications technology cybersecurity certification and repealing Regulation (EU) No 526/2013 (Cybersecurity Act) (Text with EEA relevance) PE/86/2018/REV/1 [9] Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (Text with EEA relevance) [10] Directive (EU) 2019/1024 of the European Parliament and of the Council of 20 June 2019 on open data and the re -use of public sector information PE/28/2019/REV/1 [11] Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL on a Single Market For Digital Services (Digital Services Act) and amending Directive 2000/31/EC COM/2020/825 final. [12] Proposal for a Regulation on European data governance (Data Governance Act) COM/2020/767 [13] Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCI L on contestable and fair markets in the digital sector (Digital Markets Act) COM/2020/842 final [14] European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(INL). [15] European Commission, Proposal for a Regulation of the European Parliament and of the Council on machinery products, COM(202, 21 April 2021",en,"However, it is far more difficult to see any attempt to regulate the climate impact of AI. Referen ces [1] Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT A ND OF THE COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS COM/2021/206 final [2] Garske B, Bau A, Ekardt F. Digitalization and AI in European Agriculture: A Strategy for Achieving Climate and Biodiversity Targets? Sustainability .",risk
Avaaz Foundation (United States),F2665625,06 August 2021,Non-governmental organisation (NGO),Medium (50 to 249 employees),United States,"Avaaz Feedback on the Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence and Amending Certain Union Legislative Acts Introduction Avaaz is the world s largest online civic movement. Our 69 million members, including 22 million in Europe, campaign for urgent action on the key issues of our time - the climate crisis, ecological collapse, and the erosion of democracy. We have been at the forefront of research on online disinformation, and this has led us to a deep understanding of the role artificial intelligence ( AI ) can have in exacerbating risks of harm to fundamental human rights. AI can improve healthcare and even assist in the modelling needed to reduce carbon emissions but just as with any human tool, AI has the potential to exacerbate the harms and prejudices of any system of control and can be used as a tool for repression , surveillance , violation of privacy , and institutionalised bias . It is with this in mind that whilst we applaud the Commission's multi-sectoral approach, and think that its assessment system is a major step forward, we are concerned about its tiered approach to risk assessment that leaves mainstream AI development without any standardised ethical framework or structures. The approach that limits the assessment of AI to the relatively small section of perceived high risk categories in Annex III misses the fundamental point that AI, without human oversight, has the potential to usher in unparalleled risks to our human rights through myriad apparently low risk systems. We believe that every AI system provider whose AI system poses a risk to health and safety, or a risk of adverse impact on fundamental rights should conduct an assessment or audit of their proposed AI against the categories currently set out only for high-risk AI. That assessment should be openly available to national public authorities who will be established to oversee high risk activities. Our comments below provide the rationale for suggested amendments on the following topics: Section The right framework for AI risk assessment Our key suggestions in this section include: Measures to extend obligations of risk assessment, transparency and accountability across all AI systems that could pose risks of harm to health and safety or a risk of adverse impact on fundamental rights (Articles 7, 13, 14, 52, and ; Risk assessment : 1 Article compulsory assessment process for all new AI systems that could pose risks of harm to health and safety or a risk of adverse impact on fundamental rights; Article risk-based approach for providers of any AI system that distributes content online through any form of AI assisted content distribution algorithm and meets the test of posing a risk of harm to health and safety or a risk of adverse impact on fundamental rights. Transparency: Article transparency of operation on all AI systems that pose risks of harm to health and safety or a risk of adverse impact on fundamental rights. Article transparency in relation to the detection, prevention and investigation of crime. Article extension of the right to appeal to civil society organisations and external stakeholders when they have a legitimate interest in the decisions taken by the notified authorities designated by each member state to carry out third-party conformity assessment for AI systems. Accountability: Article human oversight for all AI systems. Measures to promote and protect human rights, democracy and the rule of law worldwide Scope of application : Article additional provisions for (i) the export of AI systems; and (ii) international cooperation with organisations or third-countries. Measures to enhance workers rights : Additional rights for workers subject to workplace AI surveillance or monitoring systems. Section Extension of protection for vulnerable groups and Artificial Intelligence practices which should be prohibited Our key suggestions in this section include: Extend protection for vulnerable groups: Article 5,1(b): extend the scope of protection to also include specific groups of persons due to their gender, sexual orientation, ethnicity, race, origin, including migrants, refugees and asylum seekers, and religion. Extend the ban on certain AI systems to private actors as well as public authorities, namely : Article 5,1(c): social scoring; Article 5,1(d): real-time remote biometric identification systems; Article 5,1(e): the use of AI systems categorising individuals from biometrics into clusters according to certain identitary criteria; and Article 5,1(f): AI systems to infer emotions of a natural person, except for special situations. 2Section 1 The right framework for AI risk assessment Introduction The system of risk assessments laid out in the Proposal for high risk systems must become industry-standard across AI development for all AI systems which could pose a risk of harm to health and safety or a risk of adverse impact on fundamental rights. Whilst it may be tempting to assume that large areas of AI development are of minimal or no risk , such as spam filters, that comfort collapses with scrutiny. Avaaz s work investigating and reporting on online bias, hate and disinformation has demonstrated the huge risks in the automated content and moderation systems of social media, which currently fall outside of Annex III. The pandemic has provided numerous stark illustrations of the effect of AI taking decisions previously supervised by human moderators. As the tech giants were forced to send human moderators home, they may have hoped that their AI could take on the vital work of safeguarding the information ecosphere they now run. But the AI didn t work as intended, with devastating impacts on human rights of free speech, freedom for discrimination and the right to life with reliable information on Covid-Reliable health information related to coronavirus was scrubbed 1 from the internet by the AI s overreaction to key words 2 whilst Avaaz s own reports detail how the platforms failed to curb the harm of disinformation accelerated by their own content recommendation AI. They tried valiantly to label it in many cases, but they just couldn't keep up. 3 In vital areas, such as child exploitation and self-harm, the number of removals fell by at least 40 percent in the second quarter of 2020 because of a lack of humans to make tough calls about what broke the platforms rules, according to Facebook s own transparency report. 4 Astonishingly, neither child protection nor suicide prevention are listed in Annex III as areas in which AI deployment is seen as high risk. The lockdown measures required by the pandemic also gave us a chance to see how AI without human moderation performed in dealing with sensitive questions of free speech for minorities. Between 2020 and 2021 scores of activists YouTube accounts in Syria, where citizens and the media rely on social media to document potential war crimes, were closed down overnight often with no right to appeal decisions made by the system s AI moderation. 5 This had a devastating effect on their fight for freedom and on the personal lives of those affected, ""It's not 5 France Activists in race to save digital trace of Syria war . ( 4 Politico. What happened when humans stopped managing social media content . ( 3 Avaaz. Disinformation Hub . ( 2 Politico. What happened when humans stopped managing social media content . ( 1 Kevin McKernan. Twitter post revealing how Twitter censored his scientific article . ( 3just videos that have been deleted, it's an entire archive of our life,"" said Sarmad Jilane, a Syrian activist and close friend of Al-Mutez Billah, one of the affected account holders, who was killed at the age of Sarmad said, Effectively, it feels like a part of our visual memory has been erased."" Furthermore, the bias phenomenon is known to the industries that use AI systems. For example, two studies in 2019 showed that AI trained to identify hate speech can actually amplify racial bias in failing to properly account for the context a human moderator might spot. Researchers have found that leading AI models for processing hate speech were one-and-a-half times more likely to flag tweets as offensive or hateful when they were written by African Americans, and 2 times more likely to flag tweets written in African American English (which is commonly spoken by black people in the US). 6 Another study found similar widespread evidence of racial bias against black speech in five widely used academic datasets for studying hate speech that totaled around 155,800 Twitter posts. 7 Finally, we would add in evidence this study which concluded bias is inevitable in AI coding of AI We conclude that discrimination can occur in any sociotechnical system in which someone decides to use an algorithmic process to inform decision-making . 8 Meanwhile in the same period, AI allowed hate speech to thrive online. In France, for example, during the period in which AI dominated moderation during the pandemic, reports were published indicating an increase of over 40% in antisemitic terms on Twitter was reported 9 and that less than 12 percent of those posts were removed. Despite Twitter s known problems with an AI bias, Twitter users uncovered a disturbing example of bias in its image-detection algorithm designed to optimise photo previews. This cropped out black faces in favour of white faces. Twitter apologised for this algorithmic prejudice, but the bug remains. 10 This kind of pervasive discrimination across multiple low risk applications has proven effects on mental health, particularly amongst our most vulnerable citizens - such as teenagers - who collectively form an experimental subject body for AI with their daily participation in social media. 11 The potential tragic effects of AI, such as seeding social unrest and hate crime, have been well-documented. In 2020, Avaaz reported on how Facebook's AI in Assam worked with inadequate datasets to recognise hate speech against Bengali Muslims, 12 the European Parliament reported on the disproportionate effect of misinformation on minorities and migrant communities in June 2021, 13 and the UN is still uncovering the full effect of Facebook's inadvertent role in fomenting the Myanmar massacres. 14 14 Reuters. U.N. investigators cite Facebook role in Myanmar crisis . ( 13 European Parliament. The impact of disinformation campaigns about migrants and minority groups in the EU . ( 12 Avaaz. Megaphone for Hate . ( 11 The Atlantic. How the Racism Baked Into Technology Hurts Teens . ( 10 The Atlantic. How the Racism Baked Into Technology Hurts Teens . ( 9 San Juan Partnership. What happened when machines took over social media . ( 8 JSTOR. How Algorithms Discriminate Based on Data They Lack: Challenges, Solutions, and Policy Implications . ( 7 Davidson et al. Racial Bias in Hate Speech and Abusive Language Detection Datasets . ( 6 Sap et al. The Risk of Racial Bias in Hate Speech Detection . ( 4These issues may be the unintended effects of apparently low risk AI like moderation AI but it will take regulation to focus the efforts of those deploying the AI systems to correct the risks of harm it poses. 15 AI used on online media platforms, specifically recommendation and ranking algorithms, as well as machine learning algorithms used to moderate content should fall within the ambit of this proposed regulation where it poses risks of harm to health and safety or a risk of adverse impact on fundamental rights And finally, if we turn to other areas of industry beyond online content, we see similar concrete risks to our fundamental human rights in systems excluded from in Annex III. For example, what could seem more low risk than the children s toy industry? But researchers on children's rights have demonstrated several risks that current applications pose such as the usage of emotional AI in children s toys and services. In a report entitled Emotional artificial intelligence in children s toys and devices: Ethics, governance and practical remedies 16 concerns about the evolution of human rights infringements regarding the datafication of childhood, hidden manipulation, increased parental vulnerability, the effect of synthetic personalities on child development were detailed. Even something innocuous, such as an AI spam filter, has been shown to be coded with hidden biases, 17 with biased datasets having a huge impact on our information we see and build our world view on - where those filters obscure and ignore minority and diverse datasets, they can only exacerbate societal division. From housing allocation 18 to our love lives 19 - the bias endemic in AI creates risks to our human rights . Industry players operating AI systems have neither the human rights perspective nor proper incentive to anticipate the effects their AI will have on humanity if they are left to self regulate, especially if their use of AI is motivated primarily by profit. It is crucial, then, that the system of risk assessments laid out in the Proposal for high risk systems become industry-standard across AI development for all AI systems which could pose risks of harm to the health and safety or a risk of adverse impact on fundamental rights. E ach assessment must be subject to external scrutiny by an appropriately resourced and qualified regulator. We know this is a concern, not just for Avaaz but for many civil society voices, and for a considerable number of Members of the European Parliament ( MEPs ) who presented an open letter on the issue to Ursula von der Leyen in March this year. 20 Bringing transparency and accountability to the black box 20 years ago, social media companies fought off transparency and accountability obligations that could have prevented the rise of disinformation with the argument that they were too small 20 European Parliament. MEP Letter on AI and fundamental rights . ( 19 For example t he dating app Coffee Meets Bagel tended to recommend people of the same ethnicity even to users who did not indicate any preferences. See UX Collective. How to mitigate social bias in dating apps . ( 18 Pivigo. AI in Social Housing: Use Cases . ( 17 Springer. Bias in algorithmic filtering and personalization . ( 16 Sage Journals. Emotional artificial intelligence in children s toys and devices: Ethics, governance and practical remedies . ( 15 Wired. Why is TikTok creating filter bubbles based on your race? ( 5an industry to cope with the burden of regulation. 21 The European Democracy Action plan and the Digital Services Act ( DSA ) now has to wrestle global giants that grew in this free for all environment, profiting from attention-driven economics with content acceleration algorithms that have wreaked havoc on our democracies, health and trust in each other. The lessons of online disinformation cannot be ignored. Transparency must be legislated to allow public oversight of the risks inherent in any AI system, and accountability to mitigate those risks should be backed up with regulation rather than left to self-regulation . As with the DSA, there must be a role for civil society in the identification of risks and the ability to contribute to any industry Code of Conduct. AI is as only good as its coding and the data that goes in and out of it. If the coding has a bias, intentional or not, these will shape and distort its outcomes. The adage of garbage in, garbage out is true of all coding, but the thing that makes AI different, and that requires regulatory intervention, is that it has the capacity to make decisions built on its own learning, machine learning. Humanity, the ethical counterbalance to the risks within any automated data driven system, can be removed from the equation. We believe that this ethical counterbalance should be present across all AI systems, not only those that have been defined as high risk. Too often, Avaaz has seen the effects of automated moderation that misses hate speech against minorities due to inadequate datasets. 22 Without human moderators to assess how the automated systems are functioning on a community level, to spot what is missing, and correct its biases as they emerge, the system continues to learn on its own parameters. Accordingly, Avaaz s other concern in terms of the overall framework is the restriction of the requirement for human oversight . Finally, we are also concerned about the lack of provision relating to the assessment of and transparency on AI s impact on workers rights and rights of consultation rights . A global approach The EU should aim to act as a norm-setter for AI in a hyper-connected world by adopting an efficient strategy towards its external partners, fostering its efforts to set global ethical norms for AI at international level in line with safety rules and consumer protection requirements, as well as with European values and fundamental rights . 23 Whilst not specifically responding to the AI Act, we believe that this quote from the Opinion of the Committee on the Internal Market and Consumer Protection ( IMCO ) expresses important EU values. However, the current draft Proposal falls short of setting global ethical norms for AI in line with European values and fundamental rights. It fails a basic test of ethics as it does not 23 European Committee on the Internal Market and Consumer Protection. Opinion on artificial intelligence: questions of interpretation and application of international law in so far as the EU is affected in the areas of civil and military uses and of state authority outside the scope of criminal justice . ( 22 Avaaz. Megaphone for Hate . ( 21 And we see this tactic repeating, see Center for Data Innovation. How Much Will the Artificial Intelligence Act Cost Europe? ( 6ensure that all AI systems exported from the EU cannot be used in contravention of those standards - irrespective of their intended civil or military use. It creates a loophole that could allow public authorities to circumvent the provisions of the AI Act if they rely on third-countries or international organisations operating high-risk or banned AI systems. Our amendments Avaaz has suggested various amendments to ensure the Proposal s basic framework is aligned with European fundamental rights. Measures to extend obligations of risk assessment, transparency and accountability across all AI systems that could pose risks of harm to health and safety or a risk of adverse impact on fundamental rights (Articles 7, 13, 14, 52, and ; a) Risk assessment (Article Ex ante regulatory procedures on AI systems should extend beyond those currently classified as high risk in Annex III. We are concerned about the limit the AI Act places on the categories of industry that are deemed as high risk. This would allow services that are not in Annex III, but which do pose significant risks of harm to the health and safety or a risk of adverse impact on fundamental rights, to slip out of any regulatory oversight. As we explained in our introductory remarks, we do not believe that such a powerful tool as AI can be allowed to grow outside of any public scrutiny. We argued in our first response that all content distribution through AI content acceleration systems should be deemed high risk, given its role in the dissemination of disinformation. We note the Commission did not adopt this perspective but has continued with a relatively narrow assessment of the risks of AI based on its use in certain industries. The Proposal itself lays out the framework for such assessment - namely the following criteria in Article 7,2 - but it fails to provide any transparency or accountability over such assessments unless the system appears in Annex III. We urge the Commission to make Article 7 s assessment process compulsory for all new AI systems that could pose risks of harm to health and safety or a risk of adverse impact on fundamental rights - with oversight of those risks assessments by national regulators, as proposed in the AI Act . Accordingly, all services should assess their AI systems in line with the Proposals framework for high risk services ie: (a) the intended purpose of the AI system 24 ; 24 This assessment should include whether there is a purpose limitation for the AI for the intended use. 7(b) the extent to which an AI system has been used or is likely to be used; 25 (c) the extent to which the use of an AI system has already caused harm to health and safety or adverse impact on fundamental rights, or has given rise to significant concerns in relation to the materialisation of such harm or adverse impact, as demonstrated by reports or documented allegations submitted to national competent authorities; (d) the potential extent of such harm or such adverse impact, in particular in terms of its intensity and its ability to affect a plurality of persons; (e) the extent to which potentially harmed or adversely impacted persons are dependent on the outcome produced by an AI system, in particular because - for practical or legal reasons - it is not reasonably possible to opt-out from that outcome; (f) the extent to which potentially harmed or adversely impacted persons are in a vulnerable position in relation to the user of an AI system, in particular due to an imbalance of power, knowledge, economic or social circumstances, or age; (g) the extent to which the outcome produced with an AI system is easily reversible, whereby outcomes having an impact on the health or safety of persons shall not be considered as easily reversible; (h) the extent to which existing Union legislation provides for: (i) effective measures of redress in relation to the risks posed by an AI system, with the exclusion of claims for damages; (ii) effective measures to prevent or substantially minimise those risks These assessments should be openly submitted to the relevant national regulator, in much the same way that Data Protection Impact Assessments ( DPIA s ) are made available to national data regulators. The Proposal also fails to provide a route for comment by either the victims of AI systems, or for civil society to provide context about the environment in which these risks are assessed. Currently, unless an AI service falls into the categories pre identified in Annex III all assessments are hermetically enclosed in the service s development teams - another case of leaving an industry to complete and then mark its own homework. We would specifically suggest that providers of any AI system that distributes content online through any form of AI-assisted content distribution algorithm should be required to take a risk-based approach by assessing and mitigating the distribution of content that may pose a risk to fundamental rights, public interests, public health, and security. 26 This would require a redraft of Chapter 2, removing where appropriate the restriction relating to risk assessments in Articles 8 and 9 as only required for high risk services. The Proposal should instead be applicable to all AI systems that pose a risk to health and safety, or a risk of adverse impact on fundamental rights. 26 This process would bring AI systems used in content distribution into line with Article 26 of the draft DSA t o carry out risk assessments at least once a year in relation to the functioning and use of their services. In particular, and in parallel with the provision of the DSA we suggest AI developers are, for example, required to identify systemic risks related to the dissemination of content, any negative effects for the exercise of certain fundamental rights, and the intentional manipulation of their service. 25 This assessment should include the extent to which individual data is obtained and used as a basis for the AI to function Transparency (i) Amending Article 13 to impose transparency of operation on all AI systems that pose risks of harm to health and safety or a risk of adverse impact on fundamental rights We noted in our introductory comments that, unlike the DSA, there is no general introduction of transparency into the operation of AI within systems. Transparency of computer operations is becoming a cornerstone of our democracies as our communications systems move into the digital space. We consider this to be a fundamental flaw in the Proposal and suggest that, ideally, Article 13 is amended to remove its restriction on transparency regulation to high risk services only. Please also see our specific suggestions for Article 52 below. (ii) Amending Article 52, 1 Users should have transparency of operations as of right, and it may be particularly important that such transparency is provided in relation to the detection, prevention and investigation of crimes. We see no rationale why the public should not be made aware of the use of AI systems by policing and immigration authorities, and in fact such disclosure seems vital to ensuring the proper balance between citizens rights to privacy and to protection from crime. Article 52 Transparency obligations for certain AI systems Providers shall ensure that AI systems intended to interact with natural persons are designed and developed in such a way that natural persons are informed that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use. This obligation shall not apply to AI systems authorised by law to detect, prevent, investigate and prosecute criminal offences, unless those systems are available for the public to report a criminal offence. Article 52 Transparency obligations for certain AI systems Providers shall ensure that AI systems intended to interact with natural persons are designed and developed in such a way that natural persons are informed that they are interacting with an AI system, of the data that has been used in order to generate any decision-making in relation to those natural persons and of the rights and processes to allow natural persons to appeal against the application of such AI to them unless this is obvious from the circumstances and the context of use. This obligation shall not apply to AI systems authorised by law to detect, prevent, investigate and prosecute criminal offences, unless those systems are available for the public to report a criminal offence . (iii ) Extending the right of appeal to civil society organisations and other external stakeholders when they have a legitimate interest in these decisions (Article 9Article 45 of the Proposal establishes that only parties having a legitimate interest can exercise the right of appeal against the decisions taken by the notified bodies that are designed by the competent national aut horities with the purpose of providing a third-party conformity assessment on AI systems. As the European Center for Not-For-Profit Law Stichting ( ECNL ) has also argued, this article misses an important opportunity to enable civic participation 27 . The current Proposal places business interests above fundamental rights, creating an imbalance between AI systems developers and people subjected to these AI systems. Avaaz advocates for relevant stakeholder engagement, in particular of marginalised and at-risk groups. Excluding them will exacerbate the inequality that AI systems already create. This right to appeal should be also extended to external stakeholders such as civil society organisations, where they have a legitimate interest for example with expertise on the impact of the AI on communities and or research as to the human rights impacts of the AI systems, as follows: Article 45 Member States shall ensure that an appeal procedure against decisions of the notified bodies is available to parties having a legitimate interest in that decision Article 45 Member States shall ensure that an appeal procedure against decisions of the notified bodies is available to parties having a legitimate interest in that decision. Civil society organisations should also have the right to appeal when they have a legitimate interest in these decisions. d) Accountability - Harmonising human oversight for all AI systems (Article Article 14 of the Proposal establishes human oversight only for high-risk AI systems. The overseeing duties can be extended to all AI systems that pose risks of harm to health and safety or a risk of adverse impact on fundamental rights in order to prevent or minimise such risks. The restriction of this article to high-risk systems should be removed as follows Article 14 High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use. Human oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its Article 14 High-risk AI s ystems that pose risks to health and safety or fundamental rights shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use. Human oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a 27 ECNL. Position Statement on The EU AI Act . ( 10intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter. Human oversight shall be ensured through either one or all of the following measures: (a) identified and built, when technically feasible, into the high-risk AI system by the provider before it is placed on the market or put into service; (b) identified by the provider before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the user. The measures referred to in paragraph 3 shall enable the individuals to whom human oversight is assigned to do the following, as appropriate to the circumstances: (a) fully understand the capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as possible; (b) remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system ( automation bias ), in particular for high-risk AI systems used to provide information or recommendations for decisions to be taken by natural persons; (c) be able to correctly interpret the high-risk AI system s output, taking into account in particular the characteristics of the system and the interpretation tools and methods available; (d) be able to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system; (e) be able to intervene on the operation of the high-risk AI system or interrupt the system through a stop button or a similar procedure. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons high-risk AI systems that pose risks to health and safety or fundamental rights or AI systems subjected to the transparency obligations ex article 52 are used in accordance with their intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter. Human oversight shall be ensured through either one or all of the following measures: (a) identified and built, when technically feasible, into any high-risk AI system s that pose risks to health and safety or fundamental rights by the provider before it is placed on the market or put into service; (b) identified by the provider before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the user. The measures referred to in paragraph 3 shall enable the individuals to whom human oversight is assigned to do the following, as appropriate to the circumstances: (a) fully understand the capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as possible; (b) remain aware of the possible tendency of automatically relying or over-relying on the output produced by an high-risk such AI system ( automation bias ), in particular for high-risk such AI systems used to provide information or recommendations for decisions to be taken by natural persons; (c) be able to correctly interpret the high-risk AI system s output, taking into account in particular the characteristics of the system and the interpretation tools and methods available; (d) be able to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system; (e) be able to intervene on the operation of the high-risk AI system or interrupt the system through a stop button or a similar procedure. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons Scope of application: Measures to promote and protect human rights, democracy and the rule of law worldwide Effective AI regulation in the EU must include exports and international cooperation (Article We do acknowledge the difficulties imposed by international law when regulating international cooperation and commend the efforts in the Proposal to protect EU Citizens rights even when providers of AI systems used in the EU are established in a third country. However, we are concerned about two loopholes evident in the Proposal. (i) Provisions for the export of AI systems The promotion of human rights worldwide is one of the two main streams of EU human rights policy and action 28 . The EU is based on a strong commitment to promoting and protecting human rights, democracy and the rule of law worldwide. Human rights are at the heart of EU relations with other countries and regions 29 . This strong commitment cannot be achieved if the EU continues to expose harmful technology to third-countries. The first loophole therefore we are concerned about is the lack of provisions addressing the export of AI Systems . According to Amnesty International 30 , a 2020 study in the EU evidenced that the current international voluntary due diligence framework is unsatisfactory to significantly change the way businesses manage human rights impacts . We recognise that the EU has been making an effort to deal with this scenario, and Avaaz commends all actions in that direction. However, in our opinion, the AI Act falls short in addressing the export of European products when there is a risk they may be used to violate human rights abroad. Our opinion is largely corroborated by studies and research in the area such as the recent paper Demystifying the Draft EU Artificial Intelligence Act 31 . Its authors analyse the initial draft proposed by the Commission and present their critique that, under the current draft of the AI Act, EU vendors can sell biometric systems which would be illegal to use in the EU to oppressive regimes in third countries . 31 Demystifying the Draft EU Artificial Intelligence Act published by (i) Michael Veale, from the Faculty of Laws, University College London, United Kingdom; and (ii) Frederik Zuiderveen Borgesius, from the Interdisciplinary Hub for Security, Privacy and Data Governance, Radboud University, The Netherland. 30 Amnesty International. EU companies selling surveillance tools to China s human rights abusers . ( 29 European Union. Human Rights and Democracy . 28 European Union. Human Rights and Democracy . 12This conclusion affirms an investigation into exports to one territory, by Amnesty Internationa l , which identified at least three cases of European companies exporting AI and surveillance technology to China: France (Idemia/Morpho) : in 2015, Morpho, which specialises in security and identity systems, including facial recognition systems and other biometric identification products, entered into a contract to supply facial recognition equipment directly to the Shanghai Public Security Bureau; The Netherlands (Noldus) : Noldus sold its FaceReader software, which is used for automated analysis of facial expressions that convey anger, happiness, sadness, surprise and disgust, to public security and law enforcement authorities in China. FaceReader was also used in Chinese universities with links to the police, and the Ministry of Public Security. Digital surveillance technology has also been sold by the company to universities in China; Sweden (Axis Communications): from 2012 to 2019, the company has been listed as a recommended brand in Chinese state surveillance tender documents. Axis Communications is specialised in security surveillance and remote monitoring, and has supplied technology to the surveillance programme of Guilin, a city in the south of China, in order to expand it from 8,000 cameras to 30,Amnesty International argues that the companies took insufficient steps to satisfy themselves as to whether sales to China's authorities were of significant risk. In doing so , Amnesty International concludes, those European companies totally failed in their human rights responsibilities . We agree with these concerns and encourage the Commission to include the export of AI Systems in the scope of application of the AI Act. Article 2 should be amended to include exporters based in the European Union, even if the AI systems they provide are deployed outside the Union. Additionally we believe the AI Act would be stronger if it incorporated the due diligence obligations set forth in the Regulation (EU) 2021/821 ( Dual Use Regulation ) 32 , as Article 2,As you will notice, the language we suggest is based on the wording of Article 5,2 of the Dual Use Regulation, which establishes due diligence obligations for the export of cyber-surveillance items, establishing: Prohibition of EU vendors to export AI systems that are prohibited by the AI Act; For all other AI systems, where an exporter is aware, according to its due diligence findings, that AI systems which the exporter proposes to export are intended, in their entirety or in part, for use in connection with internal repression 32 European Parliament. Regulation (EU) 2021/821 of the European Parliament and of the Council of 20 May 2021 setting up a Union regime for the control of exports, brokering, technical assistance, transit and transfer of dual-use items (recast). ( 13and/or the commission of serious violations of human rights and international humanitarian law, the exporter shall notify the competent authority; That competent authority shall decide whether or not to make the export concerned subject to authorisation. The Commission shall make available guidelines for exporters. (ii) International cooperation Our second concern is that the current draft of Article 2,4 creates a potential loophole in which public authorities in the EU could bypass the provisions of the AI Act by relying on third-countries or international organisations operating high-risk or banned AI systems. Our concern about this loophole can be put simply: no entity , public or priv ate, should be able t o accomplish in par tnership, with a pr ovider in a thir d countr y, what it could not achie ve within the EU. We are not alone in our opinion as other important organisations have already expressed their worries about the circumvention risks we outlined above. In particular, we draw attention to the Joint Opinion 5/2021 prepared by the European Data Protection Board ( EDPB ) and the European Data Protection Supervisor ( EDPS ) ( EDPB-EDPS Joint Opinion ). They express their (...) serious concerns regarding the exclusion of international law enforcement cooperation from the scope set out in Article 2( of the Proposal. This exclusion creates a significant risk of circumvention (e.g., third countries or international organisations operating high-risk applications relied on by public authorities in the EU) . This exclusion could severely impact the protection of fundamental rights that the AI Act hopes to safeguard. In this regard, we would like to refer to the IMCO Opinion on artificial intelligence concerning questions of interpretation and application of international law. EU human rights policy includes defending human rights through active partnership with partner countries, international and regional organisations, and groups and associations at all levels of society 33 . As evidenced above, the AI Act falls short in ensuring consistency with these values. We encourage the Commission to amend item 2( from the final draft of the AI Act to address this international cooperation loophole that poses serious circumvention risks to the scope of application of the AI Acts. In summary , we would propose to amend the scope of Article 2 to (i) include exporters based in the EU, even if the AI systems they provide are deployed outside the Union; (ii) amend subitem 4 from the final draft of the AI Act to address the international cooperation loophole; and (iii) incorporate due diligence obligations for exporters of AI Systems: Article 2 Article 2 33 European Union. Human Rights and Democracy . This Regulation applies to: (a) providers placing on the market or putting into service AI systems in the Union, irrespective of whether those providers are established within the Union or in a third country; (b)users of AI systems located within the Union; (c)providers and users of AI systems that are located in a third country, where the output produced by the system is used in the Union; (...) This Regulation shall not apply to public authorities in a third country nor to international organisations falling within the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in the framework of international agreements for law enforcement and judicial cooperation with the Union or with one or more Member States. This Regulation applies to: (a) providers placing on the market or putting into service AI systems in the Union, irrespective of whether those providers are established within the Union or in a third country; (b) users of AI systems located within the Union; (c) providers and users of AI systems that are located in a third country, where the output produced by the system is used in the Union; (...) (e) providers placing on the market or putting into service AI systems in a third country where the provider, distributor or operator of that AI system originates from the Union; (...) This Regulation shall not apply to public authorities in a third country nor to international organisations falling within the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in the framework of international agreements for law enforcement and judicial cooperation with the Union or with one or more Member States , provided, however, that no EU public authority nor any Member State shall obtain, or otherwise make use of, any data originated by an AI system that is prohibited under this Act when operated by any public authorities in a third country or international organisations. The use of data originated by high-risk applications operated by any public authorities in a third country or international organisations is not permitted for EU public authorities or Member States unless safeguards similar to the ones established in this provision for high-risk AI systems are adopted by those operators . 34 (...) Providers may not export AI systems set out in Article For all other AI systems, 34 We imagine this will happen inthe same way that the EU confirms the adequacy of third country data provisions Add comparison to agreeing inter country data sharing by ensuring reciprocity of data systems 15where an exporter is aware, according to its due diligence findings, that AI Systems which the exporter proposes to export are intended, in their entirety or in part, for use in connection with internal repression and/or the commission of serious violations of human rights and international humanitarian law, the exporter shall notify the competent authority. That competent authority shall decide whether or not to make the export concerned subject to authorisation. The Commission shall make available guidelines for exporters. Measures to enhance workers rights We acknowledge that, when it comes to workers' rights, employment is part of Annex iii and includes AI intended to be used for making decisions on promotion and termination of work-related contractual relationships, for task allocation and for monitoring and evaluating performance and behavior of persons in such relationships. However, we are not satisfied that the conformity assessment procedures are sufficient to safeguard workers rights in the workplace as an internal conformity assessment (or self-assessment) can be conducted without external oversight. Potential violations would only be discovered at a later stage by overburdened market surveillance authorities when damages have already occurred. Loosely regulating AI practices that potentially infringe on workers rights can be very risky, especially if we consider this regulation will apply to all AI developers targeting the EU market, including non-EU entities that might not share EU values. We therefore propose inclusion of the following additional rights for workers subject to workplace AI surveillance or monitoring systems (by workplace we mean any place in which an employee is contractually engaged in work for its employer so including the home, and any public spaces including transport systems in which the employee conducts their regular work), namely: i) A legal duty for employers to consult trade unions on the use of high risk and intrusive forms of AI in the workplace; ii) A legal duty for employers to ensure that workers are aware of the AI systems at the workplace, including their impact on data, digital footprint and work organisation. This could be achieved by extending the transparency and provision of information requirements ex article 13 to employers as well; iii) A legal right for all workers to have a human review of decisions made by AI systems so they can challenge decisions that are unfair and discriminatory; iv) An annual conformity assessment for work place based AI to guard against discrimination by algorithm; v) A legal right to ""switch off"" from work so workers can have proper downtime in their lives. 16Section Extension of protection for vulnerable groups and Artificial Intelligence practices which should be prohibited As detailed above, we believe the Commission needs to step back and review its overall framework for AI - but we do agree with the aspects of the Proposal that identify that urgent action that is needed to protect vulnerable groups and ban certain AI applications. Specifically, we believe that the EU should: a) Protection from the exploitation of vulnerabilities granted to a specific group of persons to include gender, sexual orientation, ethnicity, race, origin, including migrants, refugees and asylum seekers, and religion (Article 5,1(b)) The current wording of Article 5,1(b) prohibits AI systems that exploit any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability . This protection granted to certain groups is partially aligned with EU values and fundamental rights, which are the basis of the AI Actbut surprisingly, some groups protected by the Charter of Fundamental Rights ( Charter ) are not included in the scope of Article 5,1(b). In order to guarantee a greater harmonisation with the Charter, we suggest extending the scope of Article 5,1(b) to also include specific groups of persons due to their gender, sexual orientation, ethnicity, race, origin, including migrants, refugees and asylum seekers, and religion. Article 5 The following artificial intelligence practices shall be prohibited: (...) (b) the placing on the market, putting into service or use of an AI system that exploits any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability, in order to materially distort the behaviour of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm; Article 5 The following artificial intelligence practices shall be prohibited: (...) (b) the placing on the market, putting into service or use of an AI system that exploits any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability, gender, sexual orientation, ethnicity, race, origin ( including migrants, refugees and asylum seekers), and religion, in order to materially distort the behaviour of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm; Extending the ban on certain AI Systems to private actors as well as public authorities (Article 5,1(c),(d),(e)) We believe that some AI systems, which are currently approved by the AI Act under certain circumstances, should be banned in their entirety. Specifically, these systems are: social scoring; real-time remote biometric identification systems; systems that categorise individuals from biometrics into clusters according to certain identity criteria; and systems that infer emotions of a natural person, except for special situations. Please find our detailed rationale for each of the proposed bans below. (i) Social scoring (Article 5,1(c)) The current draft of Article 5,1(c) of the AI Act prohibits the use of AI systems by public authorities or on their behalf for the evaluation or classification of the trustworthiness of natural persons over a certain period of time based on their social behaviour or known or predicted personal or personality characteristics. It does so where this social score could lead to detrimental or unfavourable treatment of either individual natural persons or groups in social contexts which are unrelated to the contexts in which the data was originally generated or collected; or to detrimental or unfavourable treatment of certain natural persons or groups that is unjustified or disproportionate to their social behaviour or its gravity. It is our belief that these restrictions should be adopted by the AI Act in order to ban all such AI systems used by private actors as well. The EDPB-EDPS Joint Opinion is very clear that the use of AI for social scoring can lead to discrimination, stating Private companies, notably social media and cloud service providers, can process vast amounts of personal data and conduct social scoring. Consequently, the Proposal should prohibit any type of social scoring . AlgorithmWatch also expressed its concern that the prohibition of AI systems used for social scoring purposes is also limited to those deployed by public authorities. Again, private actors are kept out of the line of fire . Furthermore, the Demystifying the Draft EU Artificial Intelligence Act article also has interesting views on social scoring. Initially, the authors point out that the AI Act does not define trustworthiness, and relying on a 2000 paper on Political Trust and Trustworthiness, go on to define trustworthiness as a combination of attributes that indicate that an entity will not betray another due to bad faith such as misaligned incentives, lack of care, disregard for promise-keeping (commitment) or through ineptitude at a task (competence) . 18If this interpretation is correct, the authors conclude that several scoping practices are allowed by the AI Act. For the reasons explained above, and due to the risks that social scoring may represent to the fundamental rights of those residing in the EU, we urge the Commission to forbid social scoring by public authorities and private actors. If the Commission disagrees, and decides to maintain the current wording of Article 5,1(c), we encourage it to include a clear definition of trustworthiness in the AI Act to prevent misinterpretations of the provision. Article 5 The following artificial intelligence practices shall be prohibited: (...) (c) the placing on the market, putting into service or use of AI systems by public authorities or on their behalf for the evaluation or classification of the trustworthiness of natural persons over a certain period of time based on their social behaviour or known or predicted personal or personality characteristics, with the social score leading to either or both of the following: (i)detrimental or unfavourable treatment of certain natural persons or whole groups thereof in social contexts which are unrelated to the contexts in which the data was originally generated or collected; (ii)detrimental or unfavourable treatment of certain natural persons or whole groups thereof that is unjustified or disproportionate to their social behaviour or its gravity; Article 5 The following artificial intelligence practices shall be prohibited: (...) (c) the placing on the market, putting into service or use of AI systems by private actors, public authorities or on their behalf for the evaluation or classification of the trustworthiness of natural persons over a certain period of time based on their social behaviour or known or predicted personal or personality characteristics, with the social score leading to either or both of the following: (i)detrimental or unfavourable treatment of certain natural persons or whole groups thereof in social contexts which are unrelated to the contexts in which the data was originally generated or collected; (ii)detrimental or unfavourable treatment of certain natural persons or whole groups thereof that is unjustified or disproportionate to their social behaviour or its gravity; (ii) Real-time remote biometric identification systems in publicly accessible spaces (Article 5,1(d)) 19Avaaz, like Amnesty International , AlgorithmWatch , and Article 19 believes that the current draft of the AI Act is too weak to effectively protect human rights. One commonly held concern is that the AI Act has failed to provide for a total ban on real-time remote biometric identification systems in publicly or privately accessible spaces. AlgorithmWatch notes that the narrow applicatory scope of this prohibition of real-time biometric identification does not sufficiently consider that the wide-scale use of such systems may not only violate individuals fundamental rights but also pave the way for indiscriminate mass surveillance and undermine fundamental principles of democratic societies . According to Amnesty International , The EU s proposal falls far short of what is needed to mitigate the vast abuse potential of technologies like facial recognition systems. Under the proposed ban, police will still be able to use non-live facial recognition software with CCTV cameras to track our every move, scraping images from social media accounts without people s consent . Amnesty International also details how the use of real-time facial recognition on people who are suspected of irregularly entering or living in a European Member State, which is currently allowed by the AI Act, will be used as a weapon against migrants and refugees. The organisation also highlights that the AI Act allows predictive policing and the use of AI systems for border control purposes. Avaaz has noted reports on the sporadic unregulated use of real time surveillance techniques in environments as unexpected as supermarkets - for example the unlawful use of facial recognition systems in Mercadona, one of the leading supermarkets in Spain, which resulted in a 5 million fine imposed by the Spanish Agency for Data Protection 35 and the recent investigation in Holland of the indiscriminate use of facial recognition technologies in supermarkets 36 . Amnesty International also voiced itsconcerns about how the AI Act does not go far enough in addressing the risks of AI entrenching and exacerbating racism and discrimination . In its own words, Not only has research shown that facial recognition software is overwhelmingly less accurate with Black and Brown faces, but systemic racism in law enforcement means this technology can disproportionally be used against these communities and can lead to wrongful arrests. What s more, the safeguards and transparency obligations outlined in the proposal will fail to meaningfully protect the public . The organisation made a public call to the EU to to close the many loopholes in this regulation which leave the door open to rampant abuse and discriminatory practices, including banning the use of all facial recognition systems used for mass surveillance . 36 EDPB. Dutch DPA issues Formal Warning to a Supermarket for its use of Facial Recognition Technology . ( 35 Olive Press. Mercadona gets a 5 million fine for installing facial recognition cameras in Supermarkets in Spain . ( 20Amnesty s views are aligned with those of Article 19 , which publicly expressed itsdisappointment that the AI Act does not establish a ban on biometric mass surveillance in publicly accessible spaces. We agree with Article 19 s view that, real-time biometric identification systems remain available to police in some circumstances and other types of biometric systems could still be deployed and used for other purposes, such as migration control. These include the assignment of people into categories based on sex, age, eye colour, political or sexual orientation, among other groupings and these categorisations are not actually prohibited but merely flagged up as high risk . These concerns are also shared by AlgorithmWatch , which provides, (..) the prohibition of real-time remote biometric identification systems only applies to systems which are used for law enforcement purposes in publicly accessible spaces, thus neither to systems used by other public authorities nor to those used by private actors. Evidently, the major risks to fundamental rights such systems come with are not limited to law enforcement purposes a fact which the proposal does not sufficiently reflect. AlgorithmWatch explores in detail all the loopholes that authorities could try to exploit, despite the prohibitions in Article As examples, it lists the use of real-time biometric identification systems can be allowed for the prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons or of a terrorist attack , the interpretation of which leaves wide discretionary power to the authorities . The organisation also notes that due to emergency reasons, the judicial authorisations that may be used for such cases can be postponed. Even the UK s data protection regulator - the Information Commissioner s Office ( ICO ) expressed concern over the indiscriminate use of AI assisted facial recognition technology in its June 2021 report The use of live facial recognition technology ( LFR ) in public places. 37 This report cites numerous issues of concern regarding private companies use of facial recognition, including: The automatic collection of biometric data at speed and scale without clear justification; The lack of control for individuals and communities . The ICO stated that In most of the examples we observed, LFR deployed in public places has involved collecting the public s biometric data without those individuals choice or control. ; A lack of transparency . The ICO stated that Transparency has been a central issue in all the ICO investigations into the use of LFR in public places. In many cases, transparency measures have been insufficient in terms of the signage displayed, the communications to the public, and the information available in privacy notices . ; The technical effectiveness and statistical accuracy of LFR systems; The potential for bias and discrimination. The ICO cited several technical studies that have indicated that LFR works with less precision for some demographic groups, 37 ICO. The use of live facial recognition technology ( LFR ) in public places . ( 21including women, minority ethnic groups and potentially disabled people stating that These issues often arise from design flaws or deficiencies in training data and could lead to bias or discriminatory outcomes. Equally, there is a risk of bias and discrimination in the process of compiling watchlists (often manual) which underpin an LFR system . All these issues risk infringing the fairness principle within data protection law, as well as raising ethical concerns on AI systems design; The governance of LFR escalation processes; The processing of children s and vulnerable adults data. In many of the examples the ICO observed that LFR was deployed in locations likely to be accessed by children and vulnerable adults, such as retail or public transport settings. Data protection law provides additional protections for children and adults because they may be less able to understand the processing and exercise their rights. Given these concerns, we urge the Commission to adopt a total ban on AI systems that are incompatible with fundamental rights. We understand that this could be seen to fetter innovation, but given the concerns we share with the NGOs and ICO as detailed above, and many other civil society actors, we ask that at minimum the Commission establish a review in order to determine if the total ban on AI systems that use real-time remote biometric identification systems in publicly accessible spaces including online spaces should be extended to private actors as well as public authorities. To illustrate Avaaz s concerns with private actors, we again highlight the unlawful use of facial recognition systems in Mercadona, one of the leading supermarkets in Spain, which resulted in a 5 million fine imposed by the Spanish Agency for Data Protection 38 . Article 5 The following artificial intelligence practices shall be prohibited: (...) (d) the use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement, unless and in as far as such use is strictly necessary for one of the following objectives: (i)the targeted search for specific potential victims of crime, including missing children; (ii)the prevention of a specific, substantial and imminent threat to the life or physical safety Article 5 The following artificial intelligence practices shall be prohibited: (...) (d) the use of real-time remote biometric identification systems in publicly or privately accessible spaces or online spaces for the purpose of law enforcement, unless and in as far as such use is strictly necessary for one of the following objectives: (i)the targeted search for specific potential victims of crime, including missing children; (ii)the prevention of a specific, substantial and 38 Olive Press. Mercadona gets a 5 million fine for installing facial recognition cameras in Supermarkets in Spain . ( 22of natural persons or of a terrorist attack; (iii)the detection, localisation, identification or prosecution of a perpetrator or suspect of a criminal offence referred to in Article 2( of Council Framework Decision 2002/584/JHA 62 and punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least three years, as determined by the law of that Member State. The use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of the objectives referred to in paragraph 1 point d) shall take into account the following elements: (a)the nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harm caused in the absence of the use of the system; (b)the consequences of the use of the system for the rights and freedoms of all persons concerned, in particular the seriousness, probability and scale of those consequences. In addition, the use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of the objectives referred to in paragraph 1 point d) shall comply with necessary and proportionate safeguards and conditions in relation to the use, in particular as regards the temporal, geographic and personal limitations. As regards paragraphs 1, point (d) and 2, each individual use for the purpose of law enforcement of a real-time remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or by an independent administrative authority of the Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed rules of national law referred to in paragraph However, in a imminent threat to the life or physical safety of natural persons or of a terrorist attack; (iii)the detection, localisation, identification or prosecution of a perpetrator or suspect of a criminal offence referred to in Article 2( of Council Framework Decision 2002/584/JHA 62 and punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least three years, as determined by the law of that Member State. The use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of the objectives referred to in paragraph 1 point d) shall take into account the following elements: (a)the nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harm caused in the absence of the use of the system; (b)the consequences of the use of the system for the rights and freedoms of all persons concerned, in particular the seriousness, probability and scale of those consequences. In addition, the use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of the objectives referred to in paragraph 1 point d) shall comply with necessary and proportionate safeguards and conditions in relation to the use, in particular as regards the temporal, geographic and personal limitations. As regards paragraphs 1, point (d) and 2, each individual use for the purpose of law enforcement of a real-time remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or by an independent administrative authority of the Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed rules of national 23duly justified situation of urgency, the use of the system may be commenced without an authorisation and the authorisation may be requested only during or after the use. The competent judicial or administrative authority shall only grant the authorisation where it is satisfied, based on objective evidence or clear indications presented to it, that the use of the real-time remote biometric identification system at issue is necessary for and proportionate to achieving one of the objectives specified in paragraph 1, point (d), as identified in the request. In deciding on the request, the competent judicial or administrative authority shall take into account the elements referred to in paragraph A Member State may decide to provide for the possibility to fully or partially authorise the use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement within the limits and under the conditions listed in paragraphs 1, point (d), 2 and That Member State shall lay down in its national law the necessary detailed rules for the request, issuance and exercise of, as well as supervision relating to, the authorisations referred to in paragraph Those rules shall also specify in respect of which of the objectives listed in paragraph 1, point (d), including which of the criminal offences referred to in point (iii) thereof, the competent authorities may be authorised to use those systems for the purpose of law enforcement. law referred to in paragraph However, in a duly justified situation of urgency, the use of the system may be commenced without an authorisation and the authorisation may be requested only during or after the use. The competent judicial or administrative authority shall only grant the authorisation where it is satisfied, based on objective evidence or clear indications presented to it, that the use of the real-time remote biometric identification system at issue is necessary for and proportionate to achieving one of the objectives specified in paragraph 1, point (d), as identified in the request. In deciding on the request, the competent judicial or administrative authority shall take into account the elements referred to in paragraph A Member State may decide to provide for the possibility to fully or partially authorise the use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement within the limits and under the conditions listed in paragraphs 1, point (d), 2 and That Member State shall lay down in its national law the necessary detailed rules for the request, issuance and exercise of, as well as supervision relating to, the authorisations referred to in paragraph Those rules shall also specify in respect of which of the objectives listed in paragraph 1, point (d), including which of the criminal offences referred to in point (iii) thereof, the competent authorities may be authorised to use those systems for the purpose of law enforcement. (iii) ban the use of AI systems categorizing individuals from biometrics into clusters according to ethnicity, gender, as well as political or sexual orientation, or other grounds for discrimination (Article 5,1(e)) One of the aims of the AI Act is to protect fundamental rights. In order to effectively achieve this purpose, we suggest the inclusion of item (e) under Article 5,1, in order to expressly set forth a ban on the use of AI systems categorizing individuals from biometrics into clusters according to 24ethnicity, gender, as well as political or sexual orientation, or other grounds for discrimination under Article 21 of the Charter. This opinion is also shared by other organisations. Notably, the EDPB and the EDPS, in their EDPB-EDPS Joint Opinion expressly voiced their concerns and pushed for a ban, for both public authorities and private entities, on AI systems categorizing individuals from biometrics (for instance, from face recognition) into clusters according to ethnicity, gender, as well as political or sexual orientation, or other grounds for discrimination prohibited under Article 21 of the Charter, or AI systems whose scientific validity is not proven or which are in direct conflict with essential values of the EU (e.g., polygraph, Annex III, (b) and (a)) . We urge the Commission to take these suggestions into consideration and ban the clustering of individuals into aspects of their identity that can represent grounds for discrimination. Article 5 The following artificial intelligence practices shall be prohibited: (...) Article 5 The following artificial intelligence practices shall be prohibited: (...) (e) the use of AI systems categorizing individuals from biometrics into clusters according to ethnicity, gender, as well as political or sexual orientation, or other grounds for discrimination. (iv) AI systems that infer emotions of a natural person, except for health or research purposes or other exceptional purposes, and subject to full regulatory review and with full and informed consent at all times (Article 5,1 and 52, The current draft of the AI Act does not forbid the use of AI systems to infer emotions of a natural person, although it does establish under article 52,2 some transparency obligations towards users of those systems. We believe this is insufficient if the AI Act truly means to protect fundamental rights, and that the AI Act should establish a total ban on these emotion inferring systems, except for in certain specific circumstances, such as for health or research purposes and be subject to full and informed consent at all times. Our position is also aligned with other organisations. Article 19 also expressed its concerns about these AI systems, remarking that Worryingly, emotion recognition is also not prohibited, despite its discredited scientific basis and fundamental inconsistency with human rights. Instead, it is subject to weak transparency obligations that are largely ineffective for protecting human rights . 25AlgorithmWatch holds the same view and draws attention to the fact that these systems are likely to come with a high potential of severe harm on individuals and democratic societies . Moreover, the organisation stresses that the scientific basis of especially emotional recognition systems is highly disputed. Another point that is important to stress is that emotion-recognition systems are evidenced biased against minorities, specially against Black people. The findings of a study 39 showed that facial recognition software interprets emotions differently based on the person s race . Based on the comparison of the emotional analysis of two different facial recognition services, Face and Microsoft's Face API, the study found that services interpret black players as having more negative emotions than white players; however, there are two different mechanisms. Face consistently interprets black players as angrier than white players, even controlling for their degree of smiling. It is not hard to find other examples showing how emotion-recognition software is biased against minorities. Google, for instance, tagged Black people as gorillas 40 . Facial recognition programs have misidentified gender in 35 percent of darker-skinned females 41 . However, the use of AI to infer emotions should be allowed for very specific reasons, such as health, for example. The EDPB and the EDPS, in their EDPB-EDPS Joint Opinion express their views that the use of AI to infer emotions of a natural person is highly undesirable and should be prohibited, except for certain well- specified use-cases, namely for health or research purposes (e.g., patients where emotion recognition is important), always with appropriate safeguards in place and of course, subject to all other data protection conditions and limits including purpose limitation . We agree with their views on health purposes. Studies are being conducted, for instance, to determine how emotion recognition technologies can be used to teach children with autism spectrum disorder to identify and express emotions 42 . Similarly, research is being carried out to investigate the use of EEG-based brain-computer interface systems for emotion recognition in patients with disorders of consciousness, with promising results 43 . For these reasons, we urge the Commission to consider the ban of the use of emotional recognition systems, except for in specific circumstances such as for health or research purposes, and subject to full and informed consent at all times. 43 Emotion-Related Consciousness Detection in Patients With Disorders of Consciousness Through an EEG-Based BCI System . ( 42 Springer. Using emotion recognition technologies to teach children with autism spectrum disorder how to identify and express emotions . ( 41 The New York Times. Facial Recognition Is Accurate, if You re a White Guy . ( 40 The Wall Street Journal. Google Mistakenly Tags Black People as Gorillas, Showing Limits of Algorithms . ( 39 Lauren Rhue. Racial Influence on Automated Perceptions of Emotions . ( 26Article 5,1 The following artificial intelligence practices shall be prohibited: (...) Article 5,1 The following artificial intelligence practices shall be prohibited: (...) (f) the placing on the market, putting into service or use of AI systems to infer emotions of a natural person, except for health or research purposes or other exceptional purposes, and subject to full regulatory review and with full and informed consent at all times. Should the Commission not accept these arguments we consider it essential that citizens are informed about the use of these AI systems. Accordingly, we propose the following alternative changes to Article 52 in order to introduce greater transparency to users of AI in the event the ban is not accepted by the Commission. Article 52 (...) Users of an emotion recognition system or a biometric categorisation system shall inform of the operation of the system the natural persons exposed thereto. This obligation shall not apply to AI systems used for biometric categorisation, which are permitted by law to detect, prevent and investigate criminal offences. Users of an AI system that generates or manipulates image, audio or video content that appreciably resembles existing persons, objects, places or other entities or events and would falsely appear to a person to be authentic or truthful ( deep fake ), shall disclose that the content has been artificially generated or manipulated. However, the first subparagraph shall not apply where the use is authorised by law to detect, prevent, investigate and prosecute criminal offences or it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to appropriate safeguards for the rights and freedoms of third parties. Paragraphs 1, 2 and 3 shall not affect the requirements and obligations set out in Title III of this Regulation. Article 52 (...) Users of an emotion recognition system or a biometric categorisation system shall inform of the operation of the system the natural persons exposed thereto. This obligation shall not apply to AI systems used for biometric categorisation, which are permitted by law to detect, prevent and investigate criminal offences. Users of an AI system that generates or manipulates image, audio or video content that appreciably resembles existing persons, objects, places or other entities or events and would falsely appear to a person to be authentic or truthful ( deep fake ), shall disclose that the content has been artificially generated or manipulated. However, the first subparagraph shall not apply where the use is authorised by law to detect, prevent, investigate and prosecute criminal offences or it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to appropriate safeguards for the rights and freedoms of third parties. Users of any AI system that generates or distributes content, whether on a commercial or non commercial basis shall be fully informed of 27the data used and criteria adopted for that content s delivery. 5 . Paragraphs 1, 2 and 3, and 4 shall not affect the requirements and obligations set out in Title III of this Regulation. In conclusion We look forward to discussing these comments and amendments and urge the Commission, Parliament and Council to reconsider its overall framework in addition to these specific issues. If On artificial intelligence, trust is a must, not a nice-to-have "" 44 we must acknowledge that this law comes at a time when there is no standardised trusted AI assurance ecosystem 45 . If the current AI Proposal cherry picks a few egregious harms, and in the name of innovation allows non-human centered AI decision making to run rings around our fundamental human rights without any oversight, it will fail before it begins. Please contact us in the event of any queries at: Sarah Andrew at sarah.andrew@avaaz.org ; Ana Paula Rodrigues at anapaula@avaaz.org and Luana Lo Piccolo at luana@support.avaaz.org 45 Centre for Data Ethics and Innovation Blog. The European Commission s Artificial Intelligence Act highlights the need for an effective AI assurance ecosystem . ( 44 Margrethe Vestager, Executive Vice President of the European Commission for A Europe Fit for the Digital Age, for BBC. EU artificial intelligence rules will ban 'unacceptable' use . ( 28",en,"Our 69 million members, including 22 million in Europe, campaign for urgent action on the key issues of our time - the climate crisis, ecological collapse, and the erosion of democracy. AI can improve healthcare and even assist in the modelling needed to reduce carbon emissions but just as with any human tool, AI has the potential to exacerbate the harms and prejudices of any system of control and can be used as a tool for repression , surveillance , violation of privacy , and institutionalised bias . We therefore propose inclusion of the following additional rights for workers subject to workplace AI surveillance or monitoring systems (by workplace we mean any place in which an employee is contractually engaged in work for its employer so including the home, and any public spaces including transport systems in which the employee conducts their regular work), namely: i) A legal duty for employers to consult trade unions on the use of high risk and intrusive forms of AI in the workplace; ii) A legal duty for employers to ensure that workers are aware of the AI systems at the workplace, including their impact on data, digital footprint and work organisation.",risk
Climate Change AI (United States),F2665623,06 August 2021,Non-governmental organisation (NGO),Micro (1 to 9 employees),United States,"Feedback on the proposedHarmonised Rules on Artificial IntelligenceClimate Change AIAugust 6, 2021Key recommendations Explicitly involveclimate change mitigation and adaptationin the classificationrulesfor high-risk AI systems. Expand reporting requirements for high-risk AI systemstocollect data on greenhousegas impacts, including impacts through both computationalenergy use and theapplications for which these systems are used.IntroductionClimate change is one of the most urgent challengesof our time, and addressing it will requirerapid and concerted action across many sectors ofthe economy. As AI has increasinglytransformational effects on society, it is thereforecritical to holistically account for the effects both positive and negative that AI may have on climatechange.AI has a multi-faceted relationship with climate change(Kaack et al., 2020; Stein, , as itcan be used to help with climate change mitigationand adaptation (Rolnick et al., ; can bedeployed in ways that counteract such efforts (Greenpeace,, thereby potentiallyincreasing greenhouse gas emissions; and can directlyemit greenhouse gases throughcomputational energy consumption (Schwartz et al.,2019; Strubell et al., . As afast-developing new group of technologies with system-leveleffects, AI can add considerableuncertainty to the ability to reach climate targets.Climate Change AI welcomes that this nuanced perspectiveis reflected in the proposedregulation, which both recognizes the potential forusing AI to address climate change, as wellas the speed of technological change and possiblechallenges, and states that the EU iscommitted to strive for a balanced approach. The proposed regulation affects the intersection ofAI and climate change in different ways.Notably, climate change is mentioned prominently asan area where AI can support ""socially and environmentally beneficial outcomes."" Climate Change AI welcomes these provisions thatare key steps towards enabling deployment of AI technologiesthat are beneficial to climatechange mitigation and adaptation.We would like to suggest in addition, however, thatthe proposed regulation more explicitlyaccount for potential risks of AI systems to increasegreenhouse gas emissions or vulnerabilityto climate change. In addition, we believe that thelegislation provides an opportunity to collectmuch-needed information for assessing the greenhousegas emissions impacts of AI. ClimateChange AI proposes two additions that would be centralto appropriately accounting for andshaping the relationship of AI and climate change.More explicitly involving climate change mitigationand adaptation in the classificationrules for high-risk AI systems. In particular, moreexplicitly acknowledging environmentalprotection including reduction of greenhouse gasemissions to mitigate climatechange as one of the fundamental rights that, ifaffected negatively by the AI system,trigger a high-risk classification.Expanding reporting requirements for high-risk AIsystems to collect data on greenhousegas impacts, including impacts through both computationalenergy use and theapplications for which these systems are used. Thisapproach would leverage theopportunity of reporting requirements for high-riskAI systems to collect much-neededdata for decision-making on decarbonization strategies.Classification rules for high-risk AI systemsSome AI systems may, now or in the future, significantlycontribute to increasing greenhousegas emissions or vulnerability to climate change.For instance, they may be used to reinforcethe use of fossil fuels,1or induce economy-scalechanges with negative2or uncertain3climateimpacts that can be shaped by policy choices. Theclassification rules for high-risk AI systems,as they currently stand, do not sufficiently accountfor such risks. We propose this gap beaddressed by adapting the criteria laid out in Article7.The heart of this proposal rightly focuses on ensuringfundamental rights are protected whenusing AI. The proposal acknowledges environmentalprotection as one of the fundamental rightsthat affect whether an AI application should be consideredhigh risk in Recital However, thelanguage as written mentions environmental protectionmainly in reference to the immediate 3Some applications of AI may have uncertain but potentiallysignificant impacts on climate change,depending on how exactly these applications are executed.For instance, autonomous vehicles can beused to facilitate the use of low-carbon, public transportation,but depending on implementation choices,may equally serve to lock in individualized modesof transportation in a way that increases overallenergy consumption (Wadud et al., . Policy andregulation can play a significant role in shapingthese potential impacts.2For instance, uses of AI for advertising may causeincreases in consumption and resource use.1For instance, some uses of AI can change the economicviability and resource availability of fossil fuels(Greenpeace, .health and safety of individuals, and fails to explicitly mention some of the most pressingexamples where AI could negatively infringe on thefundamental right to environmentalprotection: greenhouse gas emissions and the vulnerabilityto climate change. We argue herethat climate change mitigation and adaptation shouldbe named more explicitly.The Commission could employ wording in the proposalto emphasize this point, for example byamending the sentence to read:""The fundamental right to a high level of environmental protection enshrined in theCharter and implemented in Union policies should alsobe considered when assessingthe severity of the harm that an AI system can cause,including in relation to the healthand safety of personsand the ability to appropriatelyaddress climate change.""We propose that other portions of Article 7 also beupdated to more explicitly acknowledge theright for environmental protection, including protectionagainst significant emission increases orother systemic effects resulting from AI that counteractdecarbonization efforts. The mostobvious example relates to impacts that are not easilyreversible, where wording could beamended as follows: the extent to which the outcome produced with anAI system is easily reversible,whereby outcomes having an impact on the health orsafety of persons,or anenvironmental impact such as the ability of meetinggreenhouse gas emissiontargets,shall not be considered as easily reversible; Including wording that explicitly accounts for potentialadverse effects of AI systems on climatechange can lower the barriers to regulating and monitoringAI systems on the basis of suchconcerns.Transparency and reporting of climate-relevantdataTo date, estimating AI s impact on greenhouse gasemissions has been difficult, and reliableaggregate numbers are scarce. Better impact assessmentrequires not just innovations inmeasurement methodologies, but also access to relevantinformation. This means key data onclimate impacts of AI will need to be released andsystematically gathered. The reportingrequirements for high-risk AI systems can be a uniqueopportunity in making this informationavailable at scale.While it is relatively straightforward to estimatethe compute-related greenhouse gas emissionsresulting from individual runs of AI systems, theusage patterns in practice (e.g., how often amachine learning model is used or re-trained) arelargely opaque. In addition, data centeroperators currently do not publish the shares of AIloads on their servers. These issues make itvery hard to obtain aggregate numbers on the emissionsassociated with the computationalenergy requirements of AI.The need for better data also extends far beyond compute-related emissions: it will be crucial tounderstand the broader effects of AI applications,whose impacts on their respective sectorscould be large but are often highly uncertain (asdescribed in footnote above). Knowledge aboutsuch impacts currently exists in the form of limitedcase studies, which points at a need toestimate impacts more broadly and systematically.The proposal contains provisions for reporting ofhigh-risk AI systems: as stated in theexplanatory memorandum at 1, AI providers willbe obliged to provide meaningful informationabout their systems. We propose to include the followingdata points in the scope of informationthat is requested: At a minimum, specifics on computing power neededfor system development,training/fine-tuning, and inference at appropriatetime resolutions, and information aboutthe type, time, and location of computing infrastructureused. Informative are alsospecifics about the model architecture and size, trainingrequirements for systemdevelopment (or pre-trained systems used), frequencyof training/retraining/fine-tuning,and as well as average number of inference uses perunit of time. An assessment of how the system affects or may affectclimate change mitigation oradaptation more broadly, including of the greenhousegas emissions resulting from theapplications of the AI system. The assessment shouldbe as quantitative as possible,and should describe the methodology and assumptionsused.Other considerations and recommendationsBelow we will provide short feedback on other aspectsof how the proposed legislationaddresses the intersection of AI and climate change,and where proposed rules intersect withclimate-relevant areas (even if not explicitly addressed). Climate Change AI welcomes the provisions on the establishmentof regulatorysandboxes. Research, development and demonstration(RD&D) support is key to drivinginnovation of AI systems, and to enable their meaningfulapplication to climate changemitigation and adaptation. In particular, we welcomethat climate change is identified asa priority area, and that relevant actions for promotingRD&D are discussed in detail inSection 11 of COM( 205 final ANNEX. AI applications in critical infrastructure are consideredhigh-risk AI systems; however,those sectors often hold great potential for applyingAI to help with climate changemitigation and adaptation (e.g. transportation, water,gas, heating and electricity;Creutzig et al., 2019; Rolnick et al., 2019; The RoyalSociety, . Barriers resultingfrom the legislation in these sectors must be appropriatelyaddressed to ensure they donot slow climate change mitigation and adaptationefforts that can benefit from AI. We welcome that the Commission and the Board shallencourage and facilitate thedrawing up of codes of conduct intended to fosterthe voluntary application to AI systemsof requirements related for example to environmentalsustainability, [...] on the basis ofclear objectives and key performance indicators tomeasure the achievement of thoseobjectives. We recommend that such codes of conducton environmental sustainabilitynot only focus on the direct impact of AI systems resulting from their computationalenergy and resource consumption, but also center the indirect impacts resulting fromthe use of the AI system.About Climate Change AIClimate Change AI (CCAI) is a volunteer-driven organizationof researchers and professionalswith the mission to catalyze impactful work at theintersection of climate change and machinelearning. Since it was founded in June 2019, CCAIhas led the creation of a global movement inclimate change and machine learning, encompassingresearchers, engineers, entrepreneurs,investors, policymakers, companies, and NGOs. Ouractivities include a foundationalreportdetailing where machine learning can have high leveragein addressing climate change,conferences and eventsat top machine learning venuesand the UN Climate ChangeConference, and various activities and resources developedtogether with partners fromgovernment and industry. Planned upcoming initiativesinclude grants programs, summerschools, and programs to bridge the gap between academicresearch and deployment.Contributors (Climate Change AI)Lynn Kaack (Hertie School, Germany)Priya Donti (Carnegie Mellon University, USA)Jesse Dunietz (MIT, USA)Konstantin Klemmer (University of Warwick, UK)Nikola Milojevic-Dupont (Technical University of Berlin,Mercator Research Institute on GlobalCommons and Climate Change, Germany)Contributors (external)Amy Stein (University of Florida, USA)ReferencesFelix Creutzig, Martina Franzen, Rolf Moeckel, DirkHeinrichs, Kai Nagel, Simon Nieland, and HelgaWeisz. Leveraging digitalization for sustainabilityin urban transport.Global Sustainability, 2, Greenpeace. Oil in the Cloud: How Tech Companiesare Helping Big Oil Profit from Climate Destruction,URL H Kaack, Priya L Donti, Emma Strubell, and DavidRolnick. Artificial Intelligence and ClimateChange: Opportunities, considerations, and policylevers to align AI with climate change goals, URL L Stein. Artificial intelligence and climatechange. Yale J. on Reg., 890, David Rolnick, Priya L Donti, Lynn H Kaack,Kelly Kochanski, Alexandre Lacoste, Kris Sankaran,Andrew Slavin Ross, Nikola Milojevic-Dupont,Natasha Jaques, Anna Waldman-Brown, et al.Tackling climate change with machine learning,URL Royal Society. Digital technology and the planet:Harnessing computing to achieve net zero.Technical report, The Royal Society, URL Strubell, Ananya Ganesh, and Andrew McCallum.Energy and policy considerations for deeplearning in NLP. In Proceedings of the 57th AnnualMeeting of the Association for ComputationalLinguistics, Florence, Italy, July Associationfor Computational Linguistics.Roy Schwartz, Jesse Dodge, Noah A. Smith, and OrenEtzioni. Green AI. CoRR, abs/10597, URL Wadud, Don MacKenzie, and Paul Leiby. Help orhindrance? The travel, energy and carbon impactsof highly automated vehicles. Transportation ResearchPart A: Policy and Practice, 1 18,",en,"Feedback on the proposedHarmonised Rules on Artificial IntelligenceClimate Change AIAugust 6, 2021Key recommendations Explicitly involveclimate change mitigation and adaptationin the classificationrulesfor high-risk AI systems. Expand reporting requirements for high-risk AI systemstocollect data on greenhousegas impacts, including impacts through both computationalenergy use and theapplications for which these systems are used.IntroductionClimate change is one of the most urgent challengesof our time, and addressing it will requirerapid and concerted action across many sectors ofthe economy. As AI has increasinglytransformational effects on society, it is thereforecritical to holistically account for the effects both positive and negative that AI may have on climatechange.AI has a multi-faceted relationship with climate change(Kaack et al., 2020; Stein, , as itcan be used to help with climate change mitigationand adaptation (Rolnick et al., ; can bedeployed in ways that counteract such efforts (Greenpeace,, thereby potentiallyincreasing greenhouse gas emissions; and can directlyemit greenhouse gases throughcomputational energy consumption (Schwartz et al.,2019; Strubell et al., . As afast-developing new group of technologies with system-leveleffects, AI can add considerableuncertainty to the ability to reach climate targets.Climate Change AI welcomes that this nuanced perspectiveis reflected in the proposedregulation, which both recognizes the potential forusing AI to address climate change, as wellas the speed of technological change and possiblechallenges, and states that the EU iscommitted to strive for a balanced approach. The proposed regulation affects the intersection ofAI and climate change in different ways.Notably, climate change is mentioned prominently asan area where AI can support ""socially and environmentally beneficial outcomes."" Climate Change AI welcomes these provisions thatare key steps towards enabling deployment of AI technologiesthat are beneficial to climatechange mitigation and adaptation.We would like to suggest in addition, however, thatthe proposed regulation more explicitlyaccount for potential risks of AI systems to increasegreenhouse gas emissions or vulnerabilityto climate change. In addition, we believe that thelegislation provides an opportunity to collectmuch-needed information for assessing the greenhousegas emissions impacts of AI. ClimateChange AI proposes two additions that would be centralto appropriately accounting for andshaping the relationship of AI and climate change.More explicitly involving climate change mitigationand adaptation in the classificationrules for high-risk AI systems. In particular, moreexplicitly acknowledging environmentalprotection including reduction of greenhouse gasemissions to mitigate climatechange as one of the fundamental rights that, ifaffected negatively by the AI system,trigger a high-risk classification.Expanding reporting requirements for high-risk AIsystems to collect data on greenhousegas impacts, including impacts through both computationalenergy use and theapplications for which these systems are used. Thisapproach would leverage theopportunity of reporting requirements for high-riskAI systems to collect much-neededdata for decision-making on decarbonization strategies.Classification rules for high-risk AI systemsSome AI systems may, now or in the future, significantlycontribute to increasing greenhousegas emissions or vulnerability to climate change.For instance, they may be used to reinforcethe use of fossil fuels,1or induce economy-scalechanges with negative2or uncertain3climateimpacts that can be shaped by policy choices. The proposal acknowledges environmentalprotection as one of the fundamental rightsthat affect whether an AI application should be consideredhigh risk in Recital However, thelanguage as written mentions environmental protectionmainly in reference to the immediate 3Some applications of AI may have uncertain but potentiallysignificant impacts on climate change,depending on how exactly these applications are executed.For instance, autonomous vehicles can beused to facilitate the use of low-carbon, public transportation,but depending on implementation choices,may equally serve to lock in individualized modesof transportation in a way that increases overallenergy consumption (Wadud et al., . Policy andregulation can play a significant role in shapingthese potential impacts.2For instance, uses of AI for advertising may causeincreases in consumption and resource use.1For instance, some uses of AI can change the economicviability and resource availability of fossil fuels(Greenpeace, .health and safety of individuals, and fails to explicitly mention some of the most pressingexamples where AI could negatively infringe on thefundamental right to environmentalprotection: greenhouse gas emissions and the vulnerabilityto climate change. We argue herethat climate change mitigation and adaptation shouldbe named more explicitly.The Commission could employ wording in the proposalto emphasize this point, for example byamending the sentence to read:""The fundamental right to a high level of environmental protection enshrined in theCharter and implemented in Union policies should alsobe considered when assessingthe severity of the harm that an AI system can cause,including in relation to the healthand safety of personsand the ability to appropriatelyaddress climate change. ""We propose that other portions of Article 7 also beupdated to more explicitly acknowledge theright for environmental protection, including protectionagainst significant emission increases orother systemic effects resulting from AI that counteractdecarbonization efforts. The mostobvious example relates to impacts that are not easilyreversible, where wording could beamended as follows: the extent to which the outcome produced with anAI system is easily reversible,whereby outcomes having an impact on the health orsafety of persons,or anenvironmental impact such as the ability of meetinggreenhouse gas emissiontargets,shall not be considered as easily reversible; Including wording that explicitly accounts for potentialadverse effects of AI systems on climatechange can lower the barriers to regulating and monitoringAI systems on the basis of suchconcerns.Transparency and reporting of climate-relevantdataTo date, estimating AI s impact on greenhouse gasemissions has been difficult, and reliableaggregate numbers are scarce. This means key data onclimate impacts of AI will need to be released andsystematically gathered. The reportingrequirements for high-risk AI systems can be a uniqueopportunity in making this informationavailable at scale.While it is relatively straightforward to estimatethe compute-related greenhouse gas emissionsresulting from individual runs of AI systems, theusage patterns in practice (e.g., how often amachine learning model is used or re-trained) arelargely opaque. These issues make itvery hard to obtain aggregate numbers on the emissionsassociated with the computationalenergy requirements of AI.The need for better data also extends far beyond compute-related emissions: it will be crucial tounderstand the broader effects of AI applications,whose impacts on their respective sectorscould be large but are often highly uncertain (asdescribed in footnote above). An assessment of how the system affects or may affectclimate change mitigation oradaptation more broadly, including of the greenhousegas emissions resulting from theapplications of the AI system. The assessment shouldbe as quantitative as possible,and should describe the methodology and assumptionsused.Other considerations and recommendationsBelow we will provide short feedback on other aspectsof how the proposed legislationaddresses the intersection of AI and climate change,and where proposed rules intersect withclimate-relevant areas (even if not explicitly addressed). Climate Change AI welcomes the provisions on the establishmentof regulatorysandboxes. Research, development and demonstration(RD&D) support is key to drivinginnovation of AI systems, and to enable their meaningfulapplication to climate changemitigation and adaptation. In particular, we welcomethat climate change is identified asa priority area, and that relevant actions for promotingRD&D are discussed in detail inSection 11 of COM( 205 final ANNEX. AI applications in critical infrastructure are consideredhigh-risk AI systems; however,those sectors often hold great potential for applyingAI to help with climate changemitigation and adaptation (e.g. Barriers resultingfrom the legislation in these sectors must be appropriatelyaddressed to ensure they donot slow climate change mitigation and adaptationefforts that can benefit from AI. We welcome that the Commission and the Board shallencourage and facilitate thedrawing up of codes of conduct intended to fosterthe voluntary application to AI systemsof requirements related for example to environmentalsustainability, [...] on the basis ofclear objectives and key performance indicators tomeasure the achievement of thoseobjectives. We recommend that such codes of conducton environmental sustainabilitynot only focus on the direct impact of AI systems resulting from their computationalenergy and resource consumption, but also center the indirect impacts resulting fromthe use of the AI system.About Climate Change AIClimate Change AI (CCAI) is a volunteer-driven organizationof researchers and professionalswith the mission to catalyze impactful work at theintersection of climate change and machinelearning. Since it was founded in June 2019, CCAIhas led the creation of a global movement inclimate change and machine learning, encompassingresearchers, engineers, entrepreneurs,investors, policymakers, companies, and NGOs. Ouractivities include a foundationalreportdetailing where machine learning can have high leveragein addressing climate change,conferences and eventsat top machine learning venuesand the UN Climate ChangeConference, and various activities and resources developedtogether with partners fromgovernment and industry. Planned upcoming initiativesinclude grants programs, summerschools, and programs to bridge the gap between academicresearch and deployment.Contributors (Climate Change AI)Lynn Kaack (Hertie School, Germany)Priya Donti (Carnegie Mellon University, USA)Jesse Dunietz (MIT, USA)Konstantin Klemmer (University of Warwick, UK)Nikola Milojevic-Dupont (Technical University of Berlin,Mercator Research Institute on GlobalCommons and Climate Change, Germany)Contributors (external)Amy Stein (University of Florida, USA)ReferencesFelix Creutzig, Martina Franzen, Rolf Moeckel, DirkHeinrichs, Kai Nagel, Simon Nieland, and HelgaWeisz. Leveraging digitalization for sustainabilityin urban transport.Global Sustainability, 2, Greenpeace. Oil in the Cloud: How Tech Companiesare Helping Big Oil Profit from Climate Destruction,URL H Kaack, Priya L Donti, Emma Strubell, and DavidRolnick. Artificial Intelligence and ClimateChange: Opportunities, considerations, and policylevers to align AI with climate change goals, URL L Stein. Artificial intelligence and climatechange. Yale J. on Reg., 890, David Rolnick, Priya L Donti, Lynn H Kaack,Kelly Kochanski, Alexandre Lacoste, Kris Sankaran,Andrew Slavin Ross, Nikola Milojevic-Dupont,Natasha Jaques, Anna Waldman-Brown, et al.Tackling climate change with machine learning,URL Royal Society. Digital technology and the planet:Harnessing computing to achieve net zero.Technical report, The Royal Society, URL Strubell, Ananya Ganesh, and Andrew McCallum.Energy and policy considerations for deeplearning in NLP. Green AI. The travel, energy and carbon impactsof highly automated vehicles.",risk
Fdration Franaise de l'Assurance (France),F2665617,06 August 2021,Business association,Medium (50 to 249 employees),France,"About the FFA The French Insurance Federation (FFA) represents 280 insurance and reinsurance companies operating in France, accounting for over 99% of the French insurance market. We represent the interests of insurers to national, European and international public aut horities; to institutions and to administrative or local authorities. We produce and make available statistical data essential to the indu stry and provide information for the general public and the media. The French Insurance Federation also contributes in raising the awareness and attractiveness of the industry by promoting insurance and risk management culture. FFA is a member of Insurance Europe and GF IA. In an ever -changing environment, faced with the emergence of new political, economic, social, techno logical and environmental risks, the French insurance industry constantly innovates to be more competitive, support the economy and extend the boundaries of insur ability. FEEDBACK OF THE FFA ON THE EUROPEAN COMMISSION S AI ACT PROPOSAL The use of AI offers key economic, societal, and competitive advantages to European citizens and businesses. An appropriate ethical and legal framework based on European values, and which is in line with the EU Charter of Fundamental Rights, will allow the roll out of a trustworthy and useful European and non -European AI uses in the EU. This framework must be created at European level to avoid fragmentation in the single market, ensure fair competition and protect European citizens and businesses from unrel iable AI. The use of algorithms is essential in insurance. Algorithms are at the heart of the actuarial science supporting the calculation of premiums, the determination of claims or the computation of reserves and AI can significantly improve operational efficiency and customer experience. The use of artificial intelligence in insurance is not new. For a very long time, insurers have been using algorithms for a wide range of purposes: from marketing or email classification to risk assessment. Each use rel ies on very different forms of AI: AI for automatic character recognition; internal automation processes; consumer relationship and assist ing system such as chatbots. Insurance is a large industry with a variety of activities, AI fits at various parts of the insurance value chain. Innovation represents an opportunity for insurers and policyholders to strengthen their relationship thanks to faster, more efficient, and better suited services. In accordance with EU values and rules, insurers can use AI to go further in services personalization to the benefit of policyholders. As early adopters, insurers are keen to ensure a high level of protection in the use of AI. They are already ensuring that European fundamental rights are respected while applying the n umerous rules governing insurance (insurance regulation s, GDPR, etc.) and industry initiatives on ethics and on bias management, the explicability of AI results, transparency of algorithms, etc. Therefore , the French Insurance Federation (FFA) supports the implementation of a framework for an ethical use of AI: human -centric, unbiased, transparent, explainable, and secure. French insurers welcome the Commission's proposal for a risk -based regulatory framew ork with differentiated rules according to the risks presented by AI uses that makes it possible to encourage technological innovation, while guaranteeing European values. However, FFA believes that the introduction of a Date: 06 08 2021 EU Transparency Register No. -37 2 regulation establishing harmonized rules on AI requires a very clear and precise definition of several notions: the definition of an AI system, the notion of negative impact and the difference from the concept of fundamental rights infringement as well as a precise definition of the actors of AI. Indeed, on this last point, insurers insist on the need to clearly define the role of each actor in the AI value chain in order to better identify responsibilities of each one of them. French insurers are grateful for the opportunity to share thei r views and contribute to the European Commission consultation. On the classification of high -risk AI systems Access to and enjoyment of essential private services and public services and benefits FFA agrees with the objective of ensuring access to an d use of public and private services. A relevant classification of high -risk AIs is essential to ensure that the activities of private companies providing essential services are not inappropriately impaired. Insurers call for a commitment from the EC to ensure that the list of high -risk AIs always shows a correct balance between consumer protection and a framework conductive to innovation. The criteria for updating the list of high -risk AI areas The FFA welcomes the possibility to update the list of high -risk AI areas according to different crite ria in order to keep the text evolving. However, the FFA stresses the need to clearly define the notion of impact on fundamental rights and to distinguish it from other impacts (e.g. economic) which may constitute a negative impact without affecting fundam ental rights. As a reminder, and as underlined in the report of the High -Level Expert Group on AI, various aspects related to innovation such as AI are already strongly regulated in Europe (e.g. GDPR, non -discrimination rules). Moreover, according to core principles of insurance such as non -discrimination and mutuali zation , we believe policyholders ar e already protected adequately from fundamental rights violations such as exclusion . On the obligations of AI operators Transparency FFA believes that companies should use the concept of explainable AI which presents the decision - making process of an AI to users in a transparent way. According to the EU High Level Experts Group and the OECD, explicability is one of the principles that u nderpin a responsible approach to trustworthy AI. When talking about the explicability of an AI system, all components are concerned: the input data (the training data), the model and the output data (the results, the predictions). The criteria for transpa rency introduced in Art. 52 of the proposal should perhaps be defined more precisely. The notion of explicability should be given priority in the regulation. (cf. Transparency and information of users , Art Human focused Human control must be adapte d, i.e. carried out by technical profiles (data scientists) but also business profiles or a new specializ ed professionals (e.g. an expert). Human control must be done in a proportionate way ( e.g. by sampling) so as not to lose the benefits of automation, o btained through AI. Moreover, in a logic of continuous improvement of quality, AI use in real situation must be supervised (model monitoring). Therefore, FFA welcomes rules on technical documentation (Art. and the need of human control (Art.. The nee d for human control has already been noted by many French bodies (e.g. the French High Authority for Health ). Quality datasets 3 Regarding the requirement to use quality datasets, the proposal foresees the use of error -free datasets, which is disproporti onate and unworkable in practice. The requirement for an error -free dataset is even contrary to the notion of AI, which conceptually incorporates this ability to replicate human analysis. The notion of reliability and long -term control of a dataset should be privileged. Recording keeping Article 12 of the proposal provides for an obligation to automatically record events during the operation of high -risk AIs. Although relevant, this obligation needs to be clarified (terms, duration, etc.) and to consider practical impossibilities: the difficulty of versioning data, the storage of heavy data which can be too voluminous. Access to the source code of AI systems & Information For high -risk AI, Art. 64 of the proposal provides the possibility for supervisory authorities to require access to the source code of AI systems for compliance monitoring. This requirement should be reviewed for cyber security reasons, a traceability or explicability requirement seems more proportionate and relevant. Beyond access to the source code, in a logic of transparency, it seems important that the user is informed that he interacts with an AI as a chatbot. Of course, they must also have the choice and the possibility to be redirected to a human if they wish. Other comments The articulation with other texts and existing supervisory bodies Insofar as data are very important for the development of AI, it is necessary to articulate the proposal for a regulation on AI with European texts relating to data alread y in force or under discussion (GDPR, Data Governance Act, Data Act, etc.) as well as with the competence of supervisory bodies in order to ensure the complementarity of legislative environment. A proper interplay between all legal tools is crucial to ensu re that there are no inconsistencies or overlapping obligations (access ing to, processing and sharing of data, obligations regarding its reliability , sanctions, etc.). Indeed, FFA wonders about the impact that the creation of a new supervisory body would lead to, especially regarding the risk of significant legal uncertainty for all users due to associate political prerogatives and potentially conflicting conclusions with those of other European bodies. The European Community labelling FFA is in favor of the labelling of AI systems and its harmonization at EU level but express es concerns about the articulation of these new certification requirements with the stand ardization tools already in place (ISO standards, EC marking, Machinery Directive, etc.) in order to not confuse users and for small player to remain competitive. FFA which underlines that the implementation of labels AI raises the issue of developing a co stly and hard to fit label for small actors. Such system would then only benefit to well -established companies or large companies active in other market sectors. Moreover, with the increasing development of AI systems, a labelling of each AI system would represent a huge labelling task. A solution could be for the EU to encourage labelling by providing a clear, simple and inexpensive model of label. As an example, the French National Institute of Intellectual Property (INPI) has launched a labelling and rec ognition mechanism easy to process and at extremely limited costs. Furthermore, the European Union could propose a system of labelling of companies processes which allow an AI to be labelled if it responds to the labelled process. The conformity assessme nt FFA welcomes self -assessment compliance and third -party compliance schemes for certain types of AI. AI is used in many fields for general use cases ( e.g. office automation) or very specialized ones ( e.g. automotive expertise through AI). The use of AI systems differs according to the type of 4 activity. Each sector ultimately has its own field of activity, which puts it in a position where it has the tools and knowledge to assess the compliance of the AI system it is about to use for its activities with h orizontal legislation. Indeed, among the various advantages of a self -assessment system is its adaptation to the entity's activity. However, to ensure a uniform assessment at European level, self - assessment should be based on common basic criteria. The liability insurance for conformity assessment bodies Insurers call for clarification of the meaning and implications of the term ""appropriate"" in order to avoid the consequences of scenario where liability turns out not to be ""appropriate"". In conclusion, FFA believes that AI solutions offer opportunities that must be seized in order to provide useful, safe and innovative products and services to policyholders. The development of AI must be done in compliance with European values. Insurers ar e convinced that a certain number of existing rules already frames the use of AI but that the Artificial Intelligence Act can give more clarification, provided that it does not slow down innovation and the adoption of AI by European companies as well as no t add an y disproportionate layer of regulation introducing irrelevant obligations . It is, thus, necessary to consider the extensive legislative framework already regulating to the insurance sector (GDPR, IDD, legislation specific to different types of insu rance contracts, etc.). Finally, EU must pay attention to maintain a proportionality in the regulation in order to make Europe a pole of excellence and trust in AI (e.g. risk of becoming less efficient than outside the EU). Contacts J r me BALMES Director for Digital and Innovation 26 Boulevard Haussmann 75311 Paris Cedex 09 France Phone: +33 1 42 47 93 30 St phane de MAUPEOU Head of European Office Rue du Champ de Mars 23 1050 Brussels Belgium Phone: +33 1 42 46 92 24",en,"In an ever -changing environment, faced with the emergence of new political, economic, social, techno logical and environmental risks, the French insurance industry constantly innovates to be more competitive, support the economy and extend the boundaries of insur ability.",risk
Impact AI (France),F2665589,06 August 2021,Non-governmental organisation (NGO),Micro (1 to 9 employees),France,"Impact AI Position Statement on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) 05 AUGUST 2021 1 Created in 2018, the French think tank Impact AI aims to explore ethical and social issues linked to artificial intelligence, as well as to build a better world through the support of sustainable initiatives. With more than 60 listed members, large enterpr ises, SMEs, consulting firms, start -ups, universities and NGOs, Impact AI makes its vocation to work with the entire digital ecosystem. The think tank associates economic players, institutions, research organisms and societal stakeholders to its actions, in order to create a responsible and inclusive approach of AI. More about us: -ai.fr 2 Dear Sir/Madam, Impact AI welcome s the European initiative to establish first ever legal framework on Artificial Intelligence (AI) aiming to promote Europe s innovation capacity in AI while supporting the development and uptake of ethical and trustworthy AI across the European Union (EU). It is a great achievement and a worldwide leading initiative that opens the path to an appropriate use of the leading -edge AI technologies in the world. While AI has become a clear and undeniable force for business disruption and transformation, it also entails certain risks, such as potentially exposing people, including children, to significant mistakes that may undermine fundamental rights and safety, gender -based or other kinds of discrimination, opaque decision -making, or intrusion in our private lives . Faced with the rapid technological development of AI and a global policy context where more and more countries are investing heavily in AI, the European Commission (EC) has acted and proposed a project for regulating AI usage. At Impact AI we believe that designing and implementing frameworks to manage AI usage in an ethical, robust, controlled and secured manner is key for supporting adoption and holistic transformation of the organization. We have created a task force and communicated on the market on the best practices to deploy trustworthy AI across organizations. We believe that this kind of legal regulation is necessary to guarantee the fundamental rights of EU citizens and residents. However, the Proposal of the EC is rather risk -oriented neglecting the wonderful benefits and opportunities the AI can bring to European citizens, to the society as well as to both the European economy and the economies of our trading partners. As a group of companies representing several industries and of dif ferent sizes, we wanted to take the opportunity to share some comments on the core principles and themes that we consider important when promoting the uptake of trustworthy AI in Europe. We thank you in advance for your consideration. Best regard 3 a. Conformity assessment ................................ ................................ ................................ b. European Data Protection Supervisor (EDPS) ................................ ................................ c. European AI Board (EAIB) ................................ ................................ ................................ d. The role of industry experts and national institutions ................................ Industrial impact ................................ ................................ ................................ a. Flexibility of the AI Act ................................ ................................ ................................ b. Impact on business activities, innovation, and competitiveness ................................ c. Source code protection and algorithm sharing ................................ ................................ Methodologies ................................ ................................ ................................ a. Lack of implementation guidelines ................................ ................................ b. No precise technical solutions to achieve compliance ................................ Conclusion ................................ ................................ ................................ ................................ 4 Governance a. Conformity assessment More details are required on the way the conformity assessment should be carried out as this is a time consuming and expensive procedure. The EC should further specify the nature of conformity assessment in different scenarios : o Should conformity assessments be carried out on a voluntary basis or by a third party ? Clarification is necessary regarding the introduction of certifications : o What can be certified? o Who can certi fy? o What type of certifications can be used? b. European Data Protection Supervisor (EDPS ) European Data Protection Supervisor (EDPS ) is designated as the competent authority and the market surveillance authority for the supervision of the Union institutions, agencies, and bodies when they fall within the scope of this Proposal. However, t he role and tasks of the EDPS are not sufficiently detailed and should be further clarified in the Proposal, specifically when it comes to its role as market surveillance authority. c. European AI Board (EAIB) The composition of the AI Board and the role of company experts (if possible) should be specified more precisely . The Proposal foresees to give a predominant role to the EC in the EAIB . Not only would the latter be part of the EAIB, but it would also chair it and have a right of veto for the adoption of the EAIB rules of procedure. This contrasts with the need for an AI European body independent from any political influence. Therefore, we believe that the future AI Regulation should give more autonomy to the EAIB and ensure it can act on its own initiative , in order to allow it to truly ensure the consistent application of the regulation across the single market . This entity should have the vocation to help AI system development and deployment, rather than stifling innovation. o How exactly the AI Board will be organized and how it will operate? o How the experts will be nominated? o How much those independent experts will be involved in the development of the EU policy on AI ? o What will the mandate of this en tity be ? d. The role of industry experts and national institutions Industry experts should take an important part in the establishment of this framework, as they are the ones who will be applying the AI Act in real life. This would render the framework more practical and operational. 5 o How the EC intends to incorporate AI practitioners opinions and contributions to the regulation of AI? o Will there be an institution or a board where representatives from different industries and academia could directly share their position on AI regulation in the EU ? The role of national institutions should be specified in a more detailed way. Industrial impact a. Flexibility of the AI Act Benefits The AI Act focuses on the deployment of AI systems based on their risk -level regardless of the industry they are implemented in. This flexibility of the legal framework is very important to accommodate future technological developments and dynamically adap t as new concerning situations or solutions will emerge . Risks / I nquir ies The EU AI Act is rather risk-oriented and does not account for cases where AI is actually beneficial for the society and the economy. We believe that this perspective is not satisfactory as AI risks and benefits are of the same nature1. Hence, a thorough analysis of AI benefits / risk ratio, as well as individual / societal impact2 of AI should be conducted . This way, the risk level of AI systems would be appreciated more ap propriate ly. Some very important industries , such as health, pharmaceutics, autonomous cars, are not developed in the regulation while they may be subject to high risk AI usage with direct impact on human health, safety, and security. o Why those topics are not included in the AI Act? o Is this a very specific matter that will be covered by other (maybe upcoming) regulations? AI technologies can help support Europe in achieving its green deal objectives. However, AI itself has a significant environmental footprint, especially in terms of energy consumption. o EC may also consider provid ing guidance on key performance indicators to identify and measure the positive and negative environmental impact of AI . o The Environment being one of the most important assets, should it be treated as a separate entity with special protection requirements ? Further clarifications are required regar ding real -time and post remote biometric identification systems: 1 For example, AI can be used to increase the security of the society , but AI can at the same time undermine that security . 2 AI systems bringing grate opportunities for the society as a whole may be rejected because of their potential impact on an individual level . 6 o What will be the procedure for getting a prior authorization for real -time remote biometric identification system in publicly accessible spaces? o What will be the procedure for getting a prior authorization during or after the use of real -time remote biometric identification in a duly justified situation of emergency ? o To be categorized as post system the remote biometric identification should occur only after a significant delay. Woul d it be possible to add c larifications on what should be understood as a significant delay as well as the require ments on the material involved? More details should be provided regarding the transitional / adjustment period granted to companies to get compliant after each yearly revision of high -risk AI system characteristics by the EC. Some of the terms mentioned in the Proposal, such as essential private services , high quality data , explainable AI systems should be clarified. o For example, is the justification of the AI system s behaviour by an expert enough to consider it as explainable? Should the context be taken into account? Should the explanation take place in real time? o This ambiguity of certain requirements will certai nly produce a proliferation of norms . However, the promulgation of those standards will take some time. Hence some common guidelines should be established to help AI system provider s while those norms are being established. b. Impact on business activities, innovation, and competitiveness Benefits The establishment of effective and harmonized rules could improve business activities by fostering the development, use and uptake of trustworthy AI in the EU. The proportionality of the EU framework i mposes regulatory burdens only when an AI system is likely to pose significant risks to fundamental rights , health, and safety. Hence, the deployment of most real -life use cases will not be restrict ed at all as they do not present a risk for individuals. The commonly established norms and standards will not only promote trustworthy AI that is consistent with Union values and interests , but also will create a best practice repository and benchmarks that companies will have to line up with. This standardiza tion of code of conduct and jobs will increase transparency and traceability , facilitate the control of business activities and AI system comparisons. Risks / Inquir ies The EU encourages national competent authorities to set up regulatory sandboxes. The absence of common guidelines for all EU Member states may increase the gap in terms of innovation opportunities , algorithm testing and improvement, and competitiveness. We 7 believe this may have a negative impact on the effectiveness the AI Act in achieving the goals established by the EC. More details should be provided on the way AI regulatory sandboxes will be used to develop AI systems in the public interest . o Which entity will be charged to provide this controlled environment ? o What type of controlled env ironment is it (infrastructure, central repository )? o How long these sandboxes will be available for? o What will be the framework and the rules (security, policies, encryption ) to support the sandboxes? o These controlled environments are available before the placement on the market or putting into service of AI systems pursuant to a specific plan . Who will be in charge of assessing and priori tizing those plans? o Will it be possible for companies to use thos e regulatory sandboxes to test and improve their high -risk AI systems (for example with fake data) without immediately engaging to any conformity assessment requirements? Those requirements will be satisfied only for AI systems that will be selected after sandbox test. This approach may encourage industries to innovate, and to create new products. o What would be the solutions/tools/guidelines provided and/or expected on the sandboxes to support the assessment and confirmation of the respect of trustworthy and ethical AI principles? The failure to formulate effective regulations will slow down investment and dampen the development of the European market compared to leaders in AI such as the USA or China. European entities that are directly affected by this regulation will have more restrictions to follow and requirement to satisfy than non -European entities leaving them in a less favorable position regarding algorithm testing and improvement. Some already heavily regulated sectors are targeted by the A I Act. How will the EC ensure that those restrictions will not be cumulative in order to avoid over -regulation or contradictory statements which may have a negative impact on company s competitiveness? c. Source code protection and algorithm sharing Benefits The EU database could improve the confidence in AI systems by increasing transparency , traceability , oversight and strengthen ex post supervision by competent authorities. In health, the European health data space will facilitate non -discrimina tory access to health data and the training of AI algorithms on those datasets, in a privacy -preserving, secure, timely, transparent and trustworthy manner, and with an appropriate institutional governance. Risks and challenges An exactly and sharply sta ted definition of source code should be provided: o How can the source code be qualified? 8 o Is data included in the source code? o Are libraries, machine codes or other elements included in the source code? The treatment and export of classified models3 should be considered in detail in the regulation. More information is required on the possibility to apply reverse engineering : o How and when should it be applied? o Should there be an application and usage distinction by industry? o Will it be possible to el aborate the legal basis for reverse engineering, such as defin ition of the NDA scope? o What guarantees could be provided to avoid losing secrets and patents due to reverse engineering when conducted with malicious intent? More details should be provided on the how source codes will be rendered available for the EC: o Will they get an access to the company s infrastructure or the code will be handed on a specific device? Code analysis is another aspect that should be further developed: o Will the code itself b e analyzed (line by line approach) or only the results produced by that code (more pragmatic approach)? Intellectual Property Leaks: Caring about the overall security of the source code is vital to the health of any organization, regardless of its type, its size, and any other characteristics. Source code can be likened to the secret sauce of a company. The code represents the intellectual property at a fundamental level it is the instructions that make software products work. It is also part of the competitive edge and is a highly strategic aspect of any company s innovation and place in the industry. The consequences of source code exposure include everything from allowing competitors to have an advantage to loss of innovative edge, financial c osts, as well as creating security issues for the firm and customers. o How will the EC treat source codes after their registration in the EU database? o How and what type of privacy and Intellectual Property protection guarantees will be provided by the EC ? Methodologies a. Lack of implementation guidelines The EC should establish more specific and common rules for impartial and unbiased AI system classification. Companies affected by the AI Act should dispose of precise guidelines to assess the impact of their AI systems on individuals and the society, as well as the probability of occurrence of that impact. 3 Classified models refer to models that are trained using classified data such as personal sensible data, confi dential data, or data on financial transactions. 9 Principals to c ompare different AI systems should also be introduced. Whether they are end -users, simply data subjects or other persons concerned by the AI system, the absence of any reference in the text to the individual affected by the AI system appears as a blind sp ot in the Proposal. Indeed, the rights and remedies available to individuals subject to AI systems should be explicitly address ed in the Proposal . The lack of stringent implementation guidelines may also result in different national strategies of the AI A ct application. As with the GDPR, some countries rigorously followed the rules established, while others adopted a softer approach. b. No precise technical solutions to achieve compliance Some safety measures should be provided to verify the conduct of conformity assessment. o Who will oversee conformity assessments (national institutions, new entity)? o A more explicit definition of fairness and ethics is required. What would be the referenc e to guide fair and ethical use principles that may qualify AI usage as prohibited? As an example, would the exact location (demographic information) of an individual be considered as a non -compliance if used in a credit score? o How algorithm performance and data quality will be evaluated? For example, a precise definition for bias should be provided. Conclusion Even though Impact AI welcome s the Proposal of the Commission and consider that such a regulation is necessary to guarantee the fu ndamental rights of EU citizens and residents, we believe that the Proposal needs to be adapted on several issues, to ensure its applicability and efficiency. Given the complexity of the Proposal as well as the issues it aims to tackle, a lot of work remai ns to be done until a well -functioning legal framework, efficiently supplementing the GDPR in protecting basic human rights while fostering innovation , can be established . However , some actions should be taken to reduce or eliminate the administrative and legal burden, that may be one of the main obstacles on the way to apply the AI Act. In Impact AI we look forward to continuing our thinking and advocacy related to the EU AI Act going forward with more representatives from academia and affected communi ties. 10 Impact AI 39, Quai du Pr sident Roosevelt 92130 Issy -les Moulineaux -ai.fr contact@impact -ai.fr",en,"AI technologies can help support Europe in achieving its green deal objectives. However, AI itself has a significant environmental footprint, especially in terms of energy consumption. o EC may also consider provid ing guidance on key performance indicators to identify and measure the positive and negative environmental impact of AI .",risk
European Evangelical Alliance (Belgium),F2665580,06 August 2021,Non-governmental organisation (NGO),Small (10 to 49 employees),Belgium,"The EEA, originally created in 19 52, is a pan -European movement representing more than 50 national and international evangelical Protestant organizations in Europe, and 23 million citizens in the EU. Rue Belliard 205 bt 14, B -1040 Brussels, Belgium | CH89 0900 0000 9177 0409 8 Summary The European Evangelical Alliance welcomes the draft AI law the European Commission presented. It is a good start for further negotiations but more needs to be done to protect humanity, both individually and collectively. Harm is more than just a specific violation of individual fundamental rights. The negative impact of Artificial Intelligence on moral agency, relationships, cognitive skills, dignity, life s opportunities and the environment should be taken into account as well. Therefore , we suggest the adoption of a specific Declaration of Digital Human Rights. This document should include a ban of all biometric identification and categorisation systems. To increase trust in AI, there should be a comprehensive assessment for all AI, not just AI believed to be high -risk today. Further, the discussion about the future of AI should not be limited to business and developers. Civil Society should be included in shaping the future of our societies. Authors The European Evangelical Alliance is a pan-European movement serving the estimated 23 Million Evangelical Christians across the continent. In collaboration with an interdisciplinary group of experts in the field, we have studied the Draft EU AI Law. Our discussions form the basis o f our commen ts included below. Being Human As Christians, we believe that all human beings are created in the image of God and are of innate dignity and worth . We are all uniquely gifted and uniquely imperfect. It is this combination of talents and limitations that ma kes our lives and our societies incredibly colourful and worthwhile. At the same time, preserving these unique human attributes does not always sit comfortably with the adoption of some technologies , Artificial Intelligence included. That s why humans must be masters of AI rather than slaves to it. Risk Risk assessment of AI should be carried out in a wider scope of harms to humanity rather than specific consumer orientated safety issues and human rights . The focus on defined risks (either unacceptable or high) does not adequately represent the range of economic, physical, psychological and/or societal harm than can be caused by AI. AI can be used for great benefit, not least achieving some of the UN Strategic Development Goal s, but it can also be used in such a way (intentionally and unintentionally) to cause injustice, Page 2 of 5 unfairness, and unequitable and/or unpalatable situations to an individual or a group of people. It can also result in marginalisation, and an incre ase in vulnerability, which is morally reprehensible. The focus on defined risks rather than a spectrum of potentially harmful outcomes could result in AI uses which are still inadvertently harmful to people, and that could discriminat e on grounds of religion or belief, amongst other fundamental human rights. The categories by which AI is deemed unacceptable or high -risk, leave other types of potentially harmful AI uncovered, or considered as somehow lower risk, and therefore not afford ed the protections of the AI governance safeguards proposed by the Draft EU AI Regulations. Our view is that a more diverse assessment of AI is required on the basis of an outcomes - based approach , as opposed to a mere risk -based approach without sufficient guiding principles. This is necessary to ensure that AI practices are prohibited and fully protected with necessary and proportionate safeguards based on their perceived harm , including environmental harm . There fore, it is our view that there should be a comprehensive assessment for all AI applications, not just AI believed to be unacceptable or high-risk today this way any legislation would survive the test of time and technological advances and might be future proof . We would like to see the following applications of AI added to the high -risk category: Research and Development of certain AI applications : these are not explicitly excluded from the regulation, allowing entities to create AI which may be detrimental and harmful to humanity but that, if popularised and profitable, could become mainstream and normalised . Any AI which extrapolates and uses in-depth insights into a pers on that the person may not themselves be aware of , e.g. AI used in A ugmented or Virtual Reality to infer certain human emotions or reactions from information which is extrapolated from the ir use such as skin reaction, eye movement, facial responses, stamina, stimuli, etc. Knowledge of these responses could be used to manipulate, to excessively nudge, o r inappropriately interfere with a person s freedom of thought, critical thinking processes, undermining their judgment. Augmented AI and transhumanism The current draft does not include any provision in relation to augmented humans and computer brain interfaces which advance or enhance human capacity and capability. There needs to be greater discussion as to the bioethics and AI ethics of such applications and clarity in law of the boundaries of what is considered safe and morally acceptable, particula rly given that this kind of adaptation will have an (in some cases) irreversible impact on a person(s) and impacts on how society views and treats disability, amongst other physical and body traits. Any AI which is either a standalone product or as a compo nent part of a device or machine which has a detrimental impact on the environment. We know that machine learning is data hungry, and that data processing requires a high level of energy consumption. Whilst some AI can be used to help reduce carbon footp rint and to lower energy consumption, other AI can put a heavy burden on the environment. An environmental impact is high -risk and must also be considered in the impact assessment of AI. Page 3 of 5 We would like to see the following applications fall within the unac ceptable category of risk: All biometric identification and ca tegorisation systems The technology is not sufficiently accurate or precise for the application domains in which such systems are and can be used. These systems can be developed for one purpose, but their use can be adapted and/or repurposed for in almost any public situation, readily allowing for purpose and mission creep ( and misuse) without sufficient check s and balance s. This is particularly concerning when such systems are used by powerful entities, such as governments, large technology companies, large data companies, and the military. Whether such systems are live in r eal time and/or post facto, they would still have the same source issues. In addition, such systems are an unacceptable intrusion into the privacy of an individual that is a fundamental aspect of personhood. Harm The main concerns of the proposed regulation of AI are to ensure consistency with the EU Charter of Fundamental Rights and the existing secondary Union legislation on data protection, consumer protection, non -discrimination and gender eq uality. This is too narrow a scope in regard to the impact of AI on humanity since it does not pick up on harms that fall outside of these concerns such as the following : Moral agen cy There does not appear to be any detail concerning an individual s moral accountability for the actions or inactions of AI. We believe that it harms humanity to delegate moral agency to an art efact. Relationships There does not appear to be any accou ntability for the impact of AI on relationships, and the ability of AI to devalue, de -humanise, and undermine human -to- human relationships. Furthermore, no account is made of the potential risk of dependency (both emotional and physical) on humanoid, embod ied or personified AI robotics, causing concerns over human -to-machine relationships. Cognitive skills There appears to be little or no protection to safeguard against loss of skills and in particular the slow erosion and phasing out of human cognitive s kills like critical thinking. Dignity Even though the right to human dignity lies at the very heart of fundamental rights, that respect for human dignity does not appear to be followed through into the draft AI law where AI can be used for human augmentation, for nurturing or caring for the elderly, sick, and inform, or after death in respect of representations of a deceased person. We believe that there is also dignity in work and the proposed legislation does not adequately cover the need to bal ance efficiency that AI software and robotics can provide with preserving human work. Limiting life s opportunities Whilst it is acknowledged that both private and public sector organisations need to manage day to day operational risks associated with people s actions and behaviours (such as criminal or fraudulent acts, or poor management of finances and credit repayment behaviours), there appears to be little protection or safeguarding against unfair and/or biased outcomes which limit life opportunities based on a person being profiled on the sum of their acts and/or behaviours, or more particularly the sum of the acts and behaviours of people with similar features and attributes to them. This does not just impact on education, but on employability, cred itworthiness, law enforcement, migration and administration of justice. No distinction is made between automated and/or autonomous decision Page 4 of 5 making, or assistive and/or semi -autonomous decision making, particularly where personal data is not used and there fore GDPR does not apply. Digital exclusion There are risks associated with having all pervasive AI adoption without viable non -digital alternatives or options to opt out. The categories of risk as they stand do not include the risk to people being exclu ded in society and/or exclusionary practices which means certain people or people groups get left behind. We must protect the vulnerable and marginalised, recognising that individual categorisation and optimisation (not just social credit scoring) may caus e wider injustice to a people group. Environmental impact As highlighted above, Al can reduce negative impact to the environment, but can also contribute to it. It has a second order impact on the ability of humans to flourish, in particular if this effects food and water supplie s, due to global warming. As fundamental rights are first and foremost individual rights, this lens could easily forego the impact of AI on humanity, human be ings in the aggregate, and on society as a whole. Society is more than a mere banding together of individuals and individual rights. Any protections in the draft EU AI Law should have due regard to this nuance whose frequency is likely to be higher where p rofiling, optimisation and categorisation of people groups occur. Hence, group harms are likely to occur more regularly than individual harms, yet (as it stands) there is no real effective means of those impacted and/or influenced by AI within a group who have suffered collective harm, to appreciate that harm or be aware of others who have been impacted and /or influenced in the same way. It is important to stress here that harm will not always be tangible and/or visible, but harm will have nonetheless o ccurred. It may take some time for that harm to transpire particularly where it impacts groups of people who are unknown one to another. Harm can be psychological, physical and economic, however quantifying harm done to a society spread out over time does not fall easily into these determinants which have a much more individualistic feel to them . Declaration of Digital Human Rights Given the potential impact of Artificial Intelligence on both individual citizens and our societies, the discussion about the future of AI should not be limited to business and developers. Civil Society should be included in shaping the future of our societies. Citizens and experts together could draft a Declaration of Digital Human Rights . This could include the right not to be manipulated, not to be exploited based on any attribute which might make them vulnerable, or otherwise take unfair advantage of a vulnerability (known to the person or not), not to suffer exclusion, detrimental or unfavourable treatment as a result of an i nterference or prediction of personal or personality traits, without consent or a lawful, equitable and justified reason to do so, the ability to seek human review and redress etc. Such a Declaration could also incorporate the other harms to humanity cited above, thus widening the protection given to individuals and societies, effectively providing a right not to be harmed in these ways. Page 5 of 5 Data Data (whether actual or inferred) is intrinsic to our personhood, and its use in AI has the power to impact a person positively and negatively. Data cannot be debiased. Whether the bias is introduced through the data, the modelling, the algorithmic intelligent system, and/or the people, the outcome is what has the impact. Data is not just drawing insight and inferenc e and meaning, it is delving into the depths of a person, their history, behaviours, transactions etc. AI techniques which result in categorisation and optimisation is averaging the uniqueness of human identity . A variety of recent research findings have h ighlighted the unrealistic expectations that many had for AI tools used in the battle against the Covid pandemic. A recent article in MIT Technology Review1 reported that , many hundreds of predictive tools were developed . Yet, it concluded that None of them made a real difference, and some were potentially harmful. It is suggested that the fault lay with the data and the way it was used. These findings should give legislators pause for thought in the drive to encourage more data sharing. At the heart of all concerns over the harms to humanity is the use of data that in an EU GDPR context can be regarded as private. There is a disconnect between recognising the harmful impact that the use of this data in certain AI systems has on humanity and the business and political drive towards economic and technological superiority. In effect, AI has become a new arms race with societies becoming increasingly damaged as a result. This raises the question of what sort of society do we ultimately want to creat e? Is it one where human flourishing is defined by economic prosperity and technological advance or is it one that values the uniqueness of human beings and seeks to preserve these attributes, thus taking control of technology that potentially undermines t hese values and attributes. Human flourishing does not equate directly with unfettered technological progress and innovation, especially in the area of AI and machine learning . 1 Will Douglas Heaven, Hundreds of AI tools have been built to catch covid. None of them helped, MIT Technology Review, July 30,",en,"This is necessary to ensure that AI practices are prohibited and fully protected with necessary and proportionate safeguards based on their perceived harm , including environmental harm . We know that machine learning is data hungry, and that data processing requires a high level of energy consumption. Whilst some AI can be used to help reduce carbon footp rint and to lower energy consumption, other AI can put a heavy burden on the environment. An environmental impact is high -risk and must also be considered in the impact assessment of AI. Environmental impact As highlighted above, Al can reduce negative impact to the environment, but can also contribute to it.",risk
Women in AI Austria (Austria),F2665578,06 August 2021,Non-governmental organisation (NGO),Small (10 to 49 employees),Austria,"Comments on the proposed Artificial Intelligence Act by Women in AI Austria Women in AI Austria Transparency Register ID: 815698241750 -24 Reschgasse 2/5 AT-1220 Vienna carina@womeninai.co 1 of 9 COMMENTS On the Regulation laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) Dear Madam or Sir, We, Women in Artificial Intelligence Austria (Frauen in K nstlicher Intelligenz sterreich , Women in AI Austria""), are gladly taking the opportunity to comment in the public consultation phase on the draft of the Artificial Intelligence Act ( AI Act ): Preliminary remarks We welcome the approach taken by European Commission to better protect consumers and their fundamental rights, ensure safe development of new technologies and foster innovation, growth and competitiveness within the single market. In order to improve transparency and strengthen the rights of users and small businesse s alike, an appropriate legal framework is required. We may, thus, draw your attention to the following provisions: Comments on the proposed AI Act Art. 3 p oint (: emotion recognition system means an AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data; Emotion recognition is based on disproven scientific theor ies and should therefore not be validated by virtue of being subjected to regulation. We strongly recommend to change this definition to reflect that these systems are at best an attempt to recognise emotion and affect and, if at all valid, require very specific training data sets to perform in very limited contexts due to the abundant variety in the culturally -determined expression of emotion. At the same time, the proliferation in the use of these systems is worrying precisely because of their shaky scientific grounding and we do believe that [people] should enjoy heightened protection if their data is pro cessed by such a system. Furthermore, emotion recognition systems may be developed on data that is not necessarily biometric, for instance based on speech patterns or engagement with content. In our opinion, the definition should therefore be adapted to re fer to all types of personal data, citing biometric data as an example. Finally, the definition of emotion recognition systems provided here would not necessarily include systems employed to detect psychological states, such as depression. The risks to th e fundamental rights of persons are equally high, if not greater, if an AI system seeks to define their mental state, which may be done in accordance with a much more solid corpus of scientific theory. We strongly believe that this Regulation should addres s these risks and encourage the Commission to refine the proposed definition with this background in mind. Comments on the proposed Artificial Intelligence Act by Women in AI Austria Women in AI Austria Transparency Register ID: 815698241750 -24 Reschgasse 2/5 AT-1220 Vienna carina@womeninai.co 2 of 9 Art. 3 point (3 biometric categorisation system means an AI system for the purpose of assigning natural persons to specific categories, such as sex, age, hair colour, eye colour, tattoos, ethnic origin or sexual or political orientation, on the basis of their biometric data ; The proposed definition supposes some relation between biometric data and ethnic origin and sexual or p olitical orientation. We strongly refute the claim that a person's ethnicity, sexual or political orientation may be detected using biometric data, since these are socially constructed categories and are not connected to measurable attributes of a person's physical presence in this world. Art. 3 p oint (: publicly accessible space means any physical place accessible to the public, regardless of whether certain conditions for access may apply In the course of the extraordinary circumstances of the past nine months, we have all experienced a shift from physical to virtual spaces. An AI system may be employed irrespective of the type of space it engages with because it draws on input data, which can be produced both in physical and in virtual spaces. Given the increasing use of digital space in different contexts professional, educational, leisurely and the indifference of AI systems towards different types of spaces, we would recommend to remove the reference to physical place , instead defining publi cly accessible space as any place accessible to the public, regardless of whether certain conditions for access may apply . Art. 3 p oint (: serious incident means any incident that directly or indirectly leads, might have led or might lead to any of the following: (a) the death of a person or serious damage to a person s health, to property or the environment, (b) a serious and irreversible disruption of the management and operation of critical infrastructure. We strongly support the inclusion of damage to environment in the definition of serious incident , in particular since there seem to be very promising applications for AI systems in environmental protection. However, as the definition in Art. 3 para. (44a ) stands, the irreversible loss of access or opportunity caused by a misclassification of a person would never have to be reported, despite breaching fundamental rights. The wording of the requirement in Art. 62 para. ( may be understood as a requirement to report malfunctions constituting a breach of fundamental rights as well as serious incidents: therefore, an explicit reference to fundamental rights in this definition would provide more clarity . Art 3 (new definition for AI subjects including also co mpanies ) The AI Act sets forth new obligation s for operators of AI systems in order to minimi se the risk of harm but one definition conspicuously absent is that of the person(s) who are to be protected . We urge the Commission to include a definition of AI subject in the AI Act, as has been done in the GDPR for data subject. We further believe that the term ""AI sub ject"" should be broad enough to include legal persons such as companies as this would be cons istent with the broad definition of user and both natural and legal persons can be affected by AI systems . In our view, the AI Act is a better place to introduce such a concept, rather than in other legislation focusing on liability, because it focuses Comments on the proposed Artificial Intelligence Act by Women in AI Austria Women in AI Austria Transparency Register ID: 815698241750 -24 Reschgasse 2/5 AT-1220 Vienna carina@womeninai.co 3 of 9 on minimising harm and will be foundational for subsequent legislation. The benefit of including not only natural, but legal persons in the definition of ""AI subject"" is that harm arising from situations which have not been covered by the AI Act (e.g. harmful business practices which are enabled by AI systems) could draw on an existing definition to further develop legal debate and theories. Art. 5 para. ( (a) the placing on the market, putting into service or use of an AI system that deploys subliminal techniques beyond a person s consciousness in order to materially distort a person s behaviour in a manner that causes or is likely to cause that person or another person physical or psychological harm; (b) the placing on the market, p utting into service or use of an AI system that exploits any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability, in order to materially distort the behaviour of a person pertaining to that group in a manne r that causes or is likely to cause that person or another person physical or psychological harm; We strongly support measures aiming to protect the fundamental rights and dignity of persons in the European Union. In the proposed provisions, AI systems ar e prohibited that cause physical or psychological harm. This provision is insufficient to protect persons in the European Union from other serious harms, such as exploitation. For this reason, we call for the reformulation of these provisions to achieve a better alignment with the European Charter of Fundamental Rights in the following manner: (a) the placing on the market, putting into service or use of an AI system that deploys subliminal techniques beyond a person s consciousness in order to materially distort a person s behaviour in a manner that undermines or is likely to undermine the fundamental rights of that person ; (b) the placing on the market, putting into service or use of an AI system that exploits any of the vulnerabilities of a specific gr oup of persons due to their age, gender, physical or mental disability, in order to materially distort the behaviour of a person pertaining to that group in a manner that undermines or is likely to undermine the fundamental rights of that person . Comments on the proposed Artificial Intelligence Act by Women in AI Austria Women in AI Austria Transparency Register ID: 815698241750 -24 Reschgasse 2/5 AT-1220 Vienna carina@womeninai.co 4 of 9 d): the use of real -time remote biometric ident ification systems in publicly accessible spaces for the purpose of law enforcement, unless and in as far as such use is strictly necessary for one of the following objectives: (i) the targeted search for specific potential victims of crime, including miss ing children; (ii) the prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons or of a terrorist attack; (iii) the detection, localisation, identification or prosecution of a perpetrator or suspect of a criminal offence referred to in Article 2( of Council Framework Decision 2002/584/JHA62 and punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least three years, as determined by the law of that Member State. The prohibition of 'real -time' remote biometric identification systems is, in our opinion, a valuable contribution towards securing freedom from arbitrary surveillance for all Europeans. However, the exem ptions provided allow for much leeway for the use of the se technologies . Trust is required i n order for AI technologies to flourish, but trust will only develop if solid protections provided by law for all Europeans provide good reasons for this trust . In the current proposal, all 'real -time' remote biometric identification systems currently in use for law enforcement purposes would not be regulated, un less updates lead to significant changes in the AI system. These updates, however, are necessary: countless studies and experiments have demonstrated significant biases in these technologies towards non -white and non -male persons . We believe this is unacceptable and regulatory proposals should seek to remedy this issue, not to incentivise law enforcement agencies to purchase systems now and avoid updates as soon as this Regulation comes into force. Furthermore, the exceptions for the use of 'real -time' biometric identification technologies allow for much leeway in the application of this exemption. There are no requirements for the presentation of objective evidence to the judicial authority regarding the existence and scope of the threats which the use of biometric identification systems aims to counter: the only provision mandating the provision of ""objective evidence or clear indications"" to judicial authorities regards the necessity and proportionality of the use of 'real -time' remote biometric identification systems in terms of achieving the objectives in Art. 5 para. ( point (d) . Also, there are no requirements for law enforcement authorities to disclose the use of 'real -time' biometric identificatio n systems, even after the threat has passed, or to be transparent about the effectiveness of these systems. As it stands, this provision will lead to the acquisition of AI systems capable of 'real -time' remote biometric identification systems by law enforc ement authorities with the aim of using them, when necessary; in turn, less research and funds will be invested in AI technologies which may aid law enforcement in less invasive ways, which do not undermine the fundamental rights of persons in the European Union at a large scale. Lastly, the formulation of this point allows other public authorities (such as migration and border control agencies ) as well as private operators the use of 'real -time' biometric i dentification systems , since these entities do not fall within the scope of 'law enforcement' as defined in Art. 3 point (. We believe this is a dangerous loophole which may underm ine a number of principles enshrined in Comments on the proposed Artificial Intelligence Act by Women in AI Austria Women in AI Austria Transparency Register ID: 815698241750 -24 Reschgasse 2/5 AT-1220 Vienna carina@womeninai.co 5 of 9 the Charter of Fundamental Rights, especially regarding the dignity and freedoms of persons. Law enforcement and other public authorities operate on the basis of a clear mandate to protect the public interest; private companies are under no such obligation and should not be able to exert control over persons living in the European Union to a greater extent than public authorities. The restrictions accorded to the use of these technologies should not only apply to law enforcement, but to all actors alike. Art. 7 Amendments to Annex III Art. 7 empowers the Commission to add new applications to Annex III which fall into one of the previously established high -risk areas and have equal or higher risk than those applications already listed by way of a delegated act. We support the flexibility this provision offers and encourage a regular evaluation of the market for AI systems and the effects of AI systems in certain use cases. However, we believe Art. 7 may not be sufficient to adapt the AI Act to new and emerging AI use cases and offer long -term protection of fundamental rights due to its limited scope . Since only the applications may be updated, but not the use case areas, we believe the definition of high -risk AI systems may not be future -proof and thus not offer lon g-term safety for people affected by AI systems in a negative way. We therefore support expanding this Article to allow for the amendment of new high -risk areas by the Commission via delegated acts . Art. 49 CE marking of conformity In the current version of the AI Act, providers of high -risk AI systems specified in Annex III may conduct self -assessments to ascertain conformity ( Art. 43/2 - in case of biometric identification systems, only those applying harmonised standards or common specifications as detailed in Art. 43/1 ). In effect, this means that providers of high -risk AI systems as defined in Annex III must apply the CE marking called for in Art. 49, yet for users of these s ystems, there is no possibility to verify whether the provider has acted in due diligence. In our view, there is a high risk that the CE marking may be applied inappropriately and the penalties may not be a strong enough deterrent from non - compliance. We wou ld prefer an obligation for providers of high -risk AI systems which have conducted a self -assessment to nonetheless register with a notified body, which accepts the required documentation and issues a certif icate of conformity. This will allow users to ver ify at minimum that the documentation has been presented to a notified body and is accessible to the market surveillance authority in case of any issues. Art. 52 Transparency obligations for certain AI systems In its paragraphs, Article 52 enumer ates three use cases of AI systems which shall be subjected to disclosure obligations. The first use concerns automated systems interacting with natural persons (bots for short), the second concerns emotion recognition or biometric categorisation systems a nd the third prescribes disclosure of the synthetic nature of AI -generated content (or deep fakes). We believe that these provisions may be insufficient in mitigating potential harm caused by the use of these systems. Regarding the interaction with bots in Art. 52( , affected parties should have the right to interact with the user in a different format , e.g. in the case of a chatbot, affected parties could instead contact the user via a service hotline or an e -mail address. The lack of such alternatives is more severe in Art. 52( , as it explicitly legitimises the use of emotion recognition systems (whi ch are not scientifically sound) . Since these AI systems are not based on valid science, their output cannot be considered reliable, creating greater harm for affected parties who for whichever reason Comments on the proposed Artificial Intelligence Act by Women in AI Austria Women in AI Austria Transparency Register ID: 815698241750 -24 Reschgasse 2/5 AT-1220 Vienna carina@womeninai.co 6 of 9 are dependent on its outcomes. Lastly, Art. 52( may serve to increase the trust of wider audiences in content presented to them. However, it is an insufficient response to the problem of deep -fake non -consensual pornography. Such images, video and audio can be easily generated, most often targeting w omen, and easily shared online, harming the reputation, mental well -being or even physical safety of the victims. This is a problem that calls for a multi -layered solution and we encourage the Commission to engage with stakeholders to find an appropriate f ramework. Art. 69 Codes of conduct We welcome the provision in Art. 69 regarding codes of conduct for all operators of AI systems based on the requirements for high -risk AI systems . This will enable start -ups to garner the trust of those companies or public authorities which would like to use AI systems, but hesitate to work with smaller, potentially more innovative but less well -known firms. To make these codes of conduct a reliable instrument, the commitments need to be enforced , as breaches of the codes of conduct which are not penalised can lead to a loss of trust in the codes of conduct themselves. The codes of conduct regarding topics not covered in the AI Act could also have a n important role in furthering the environmental and social sustainability of AI systems although these could potentially also be covered by standards. Considering the strong gender imbalance in AI-related fields, codes of conduct aiming at diversity of design and development teams are in our view essential to achieve less biased and more balanced AI systems. However, it is important that these codes of conduct are meaningful and have an actual impact on the design , development and deployment process. To avoid that these codes of conduct foster d iversity in name but not in fact , we believe it is important to in clude a variety of stakeholders , in particular civil liberties and fundamental rights organisations. Annex III: High -risk AI systems referred to in Art. 6( Annex III contains a list of areas where AI applications should be considered high -risk and therefore subjected to more stringent control. We suggest that the list of high -risk applications should be expanded to include: Emotion recognition systems these systems are based on invalid scientific theories , are highly invasive and could cause harm to self -expression. Greater regulatory oversight for these systems (if they are at all to be deployed) is necessary and should be anchored in the AI Act. Algorithm s deployed in the field of insurance these algorithms can determine who has access to quality insurance and at what cost, entailing a serious risk for discrimination in access to basic services . Algorithms used in health administration such algorithms have already been found to be discriminatory and again pose a serious risk to fundamental rights (Ziad Obermeyer et al. Dissecting racial bias in an algorithm used to manage the health of populations. In: Science 6464 (, pp. 447 ) Any algorithms that interact with children and youth should be considered high risk , whether or not they form part of a product . As the project Twisted Toys demonstrates, children and youth are exposed to manipulative design and algorithmic systems. The AI Act should seek to prevent any harm to children and youth by way of algorithmic manipu lation and actively monitor the development of the market of AI systems targeted to younger Comments on the proposed Artificial Intelligence Act by Women in AI Austria Women in AI Austria Transparency Register ID: 815698241750 -24 Reschgasse 2/5 AT-1220 Vienna carina@womeninai.co 7 of 9 people . Likewise, the impact of algorithms on persons of a higher vulnerability due to age, physical or mental capacity should be further researched, possibly leading to the addition of such a field of application to Annex III. As mentioned further above, w e would furthermore encourage the Commission to regularly evaluate the list of high -risk AI systems and add areas of application if there is a foreseeably high risk not only, as foreseen in Art. 7, adding AI applications to the areas defined in Annex III. AI systems are emerging technologies and it could prove too restrictive if high -risk areas of application are defined prio r to widespread adoption of the technology and cannot be expanded at a later point in time. Annex IV: Technical documentation The requirements for technical documentation which have been proposed are a welco me step. We strongly support the points included, in particular the requirement in Annex IV para. ( point (d) to include a datasheet describing the training methodologies and data sets used, in particular the provenance of the data used. In our opinion, this provision will incentivise the development of high - quality data sets which do not rely on data obtained without consent or contrary to decla red licenses (such as Creative Commons licenses). One of the aspects currently missing is the possibi lity of adverse effects due to interactions between different AI systems . Given the increased development and application of AI systems, a scenario in which multip le systems form a system complex is likely for use cases as different as home automation and medical treatments. Users should be informed if the provider has tested whether the AI system is interoperable with other types of systems, and if so, which systems were tested. Exemptions for law enforcement and related activities We are very concerned with the number of exemptions provided for AI systems used for law enforcement purposes. In a union of constitutional states such as the European Union, it is paramount to ensure balance between the rights and legitimate interests of citizens and the rights and legitimate interests of states. Citizens must have adequate assurances that products or services employed by law enforcement or related agencies (e.g. migration, asylum and border control) are safe, robust and reliable and refle ct the values of European society. Yet the exemptions provided in this proposal would at best relax the standards to which AI systems used by these agencies are held; at worst, these provisions could lead to a time -limited free -for-all, incentivising law e nforcement and related agencies to obtain AI systems before the Regulation comes into effect, and avoid enhanced compliance costs by not updating the systems previously obtained in a significant manne r. We are particularly concern ed about the provisions foreseen in Art. 83 and Art. 43 para. ( and ( . Considering the relaxation of requirements for AI systems used for law enforcement purposes, the reduction of fines in case these requirements are not complied with (Art. 71 para. ( and Art. 72 paras. (2 - for Union agencies, bodies an d institutions) are not appropriate. Public authorities have the duty to protect public interest. Should they fail to adhere to legislation protecting the public interest, they must be held re sponsible for breaking the public's trust in accordance also with the fundamental right to good administration. Beyond the concerns expressed above, we are deeply troubled by the exemptions from compliance with the Regulation provided through Art. 2 par a. 3, which exempts systems used for military Comments on the proposed Artificial Intelligence Act by Women in AI Austria Women in AI Austria Transparency Register ID: 815698241750 -24 Reschgasse 2/5 AT-1220 Vienna carina@womeninai.co 8 of 9 purposes, and Art. 2 para. 4, which exempts international organisations or public authorities in third countries applying AI in the EU based on agreements for law enforcement or judicial cooperation . As the Reg ulation stands, the lack of balance between the rights and legitimate interests of citizens and of states is increased by removing military applications from fundamental rights and safety requirements for other applications. Product safety p remise The proposed Regulation treats AI as a product or as a component of specific types of products, and introduces a specific form of legislation designed to mitigate risks arising from cer tain uses for this product. While this is certainly a very important aspect of AI technology and users of AI products as well as people who are affected by AI products must have adequate protection from harm, solutions to this aspect of the problem may not be sufficient to handle another aspect of AI technology namely its funct ion as infrastructure. We believe that an effective way of observing infrastructure is to observe whether, and if so to which exte nt, people adapt to these structures. As cars have entered our streets, we have begun to change our streets, outfitting them with traffic lights and pedestrian crossings and speedometers, and shifting expectations of t he basic knowledge and skills to be acquired in the course of one's life (e.g. learning to cross the street safely and how to drive a car). AI technologies are dev eloped to deal with an amount of information that humans are not able to process at the necessary speed. People have begun to adapt to this technology, whether by relying on search engines to offer the most relevant information or by actively changing thei r behaviour online in order to elicit a particular response from a recommendation algorithm. Cars have had a lasting impact on our society, not only in terms of travel speeds or pollution, but also in the way we build cities. The archetypical suburb is a form of living that is only practical if its residents have cars: suburbs consist only of hous es, without grocers or providers of other convenience goods available in easy reach without a car, that is . Similarly, buildings in city centres may be partially repurposed to offer indoor parking, changing the way pedestrians experience the city. If AI proves to be a technolog ical revolution and there are strong indicators that this might be the case, considering it is the core technology of some of the most widely used digital products in the world , it is necessary to think about the changes this te chnology may bring and develop strategies for how we as a society can act in the face of these changes. As it has been proposed, the AI Act offers a type of vehicle or building safety norms, but no city planning strategy. We encourage the European Commission to engage with a variety of different stakeholders in order to develop a vision and a concrete plan for the information infrastructure of the future , of which AI might form a part . Through the product safety perspective, questions of exploitation have unfortunately also been sidestepped. Product safety addresses concerns related to one aspect of the economic lifecycle of production, distribution, consumption and waste: namel y what happens when the product is consumed. Environmental damage during production is an issue that the proposed Regulation cannot cope with. Equally, questions of appropriated labour both in the data collection process and in the contribution of the us er in improving the s ystem cannot be solved through product safety legislation. Questions of competition in an age of AI systems such as algorithmic pricing Comments on the proposed Artificial Intelligence Act by Women in AI Austria Women in AI Austria Transparency Register ID: 815698241750 -24 Reschgasse 2/5 AT-1220 Vienna carina@womeninai.co 9 of 9 systems have also remained unanswered by this proposal. Most importantly, the AI Act as it has been drafted does not give affected persons the right to opt out of being affected by an AI system or even the right and means to complain about harmful or defective AI systems. These issues as well as others are sal ient and call for well -balanced, long -term strategies . We hope the European Commission will return to deliberate on these questions, together with all interested stakeholders, and develop solutions that will empower humans and ensure healthy and safe envir onments, be they ecological, social or economic . *** We commend the work of the European Commission in developing a framework for AI systems . The future of AI development, research and applications will be shaped by this framework and by those frameworks we hope the Commission will next turn its attention to . We hope our feedback contributes to the further refinement of the proposed provisions and will gladly answer any questions about our views on this topic. On behalf of Women in AI Austria, Carina Zehetmaier President of Women in AI Austria",en,"We strongly support the inclusion of damage to environment in the definition of serious incident , in particular since there seem to be very promising applications for AI systems in environmental protection. The codes of conduct regarding topics not covered in the AI Act could also have a n important role in furthering the environmental and social sustainability of AI systems although these could potentially also be covered by standards. Cars have had a lasting impact on our society, not only in terms of travel speeds or pollution, but also in the way we build cities. Environmental damage during production is an issue that the proposed Regulation cannot cope with. We hope the European Commission will return to deliberate on these questions, together with all interested stakeholders, and develop solutions that will empower humans and ensure healthy and safe envir onments, be they ecological, social or economic .",risk
Digital Therapeutics Alliance (United States),F2665561,06 August 2021,Business association,Micro (1 to 9 employees),United States,"Digital Therapeutics Alliance 1 06 August 2021 Digital Therapeutics Alliance Consultation Response: Artificial Intelligence Act The Digital Therapeutics Alliance (DTA) is a global non -profit trade association of industry leaders and stakeholders that works to enable expanded access to high quality, evidence -based digital therapeutics (DTx) for patients, clinicians, and payors to im prove clinical and health economic outcomes. DTA s 50+ member companies including DTx manufacturers, pharmaceutical , technology, and provider organizations represent 17 countries across Europe, Asia -Pacific, North America , and South America . DTA welcomes the opportunity to provide feedback to the European Commission s proposed regulation on the Artificial Intelligence Act . This is an important step towards expanding Europe s vision to become a global hub for safe, effective, and trustworthy Artif icial Intelligence (AI) technologies and systems . In line with DTA s primary focus on providing policymakers, payors, clinicians, and patients with the necessary tools to recognize, evaluate, and utilize digital therapeutics (DTx) many of which incorpor ate AI into treatment delivery processes we are encouraged by the Commission s leadership i n this quickly evolving ecosystem. Therefore, t o avoid these re gulations from becoming an obstacle to innovation in the development and application of AI technol ogies and systems in Europe , we encourage the Commission to consider the following: AI definition and risk assessment o Currently, the definition in the proposed Act of an AI system is too general and broad to appropriately reflect the available spectrum of AI techniques , approaches , and systems. o The definition of AI should clearer and more specific to differentiat e and reflect the spectrum of possible AI applications on the market, in addition to t he differences represented in the development, assessment, and delivery of underlying algorithms and interventions . o The propo sed Act introduces new oversight for high -risk AI systems which w ould require a case -by-case assessment from AI providers. Among the identified high risks are harm to health and safety that could result from human use , the risk of negative impact on fundamental rights , and the potential for discrimination. While it is very necessary to address these risks, th e propos ed Act creates additional challenges for develop ers of AI - driven software, especially if different risk -level s are applied compared to the EU Medical Device Regulations (Regulation (EU) 2017/745 and Regulation (EU) 2017/ . Relationship to EU regulations o Given the existing regulatory framework s related to the potential risks of developing, assessing, and implementing AI technologies and systems, it is necessary to clearly distinguish how this propos ed Act corresponds to and potentially overlaps with regulations such as the Medical Devices Regulation (MDR), the In Vitro Diagnostics Regulation (IVDR), the General Data Protection Regulation (GDPR), the Data Governance Act (DGA) , and the European Health Data Space (EHDS) . Digital Therapeutics Alliance 2 o Further clarity on post -market surveillance obligations, the types of data and experiences collected by and as a result of AI system use , and how data w ould need to be stored and protected, is also necessary to ensure that the proposed Act does not create duplication or overregulation w ith MDR/IVDR post -market requirements. Burdens on AI developers o It is critical that this proposed Act balance the need to enabl e small , medium, and large businesses to thrive in Europe , while simultaneously ensuring that end users have access to high -quality , safe, and effective AI technologies and systems . o One component of enabling the ongoing sustainability of product developers and manufacturers is to clearly identify AI technologies that are high -risk and the specific requirements they are subject to . It is important to differentiate these products from lower -risk AI systems in order to minimiz e unnecessary burdens on other organizations . Financial implications o The proposed Act does not sufficiently address the financial and time requirements that small and medium enterprises would face if this proposal moves forward as currently drafted. According to the European Commission, a quality management system could cost businesses between 193,000 and 330, 000 upfront , an additional 71,400 in yearly maintenance costs , and potentially result in a 40 percent decline in profits.1 Additionally, according to the Center for Data Innovation, this propos ed Act will cost the European economy 31 billion over the next five years and reduce AI investments by almost 20 percent. A European small or medium enterprise (SME ) that deploys a high -risk AI system will incur compliance costs of up to 400,000 , which could cause profits to decline by 40 percent.2 The prop osed Act also excludes other unquantifiable costs that may be impose d, such as deterring investment into European AI startups, slowing down the digitization of the economy, and encouraging a brain drain of European entrepreneurs to countries where they can build AI companies with fewer bureaucratic hurdles than they face at home. 3 The short and long -term burdens that this proposed Act will place on AI developers would have serious impacts on small and medium enterprises based in Europe, in additi on to other entities looking to enter the European market. The Commission should therefore further evolve this regulation to ensure it is both understandable and feasible for SMEs and start -ups. Thank you for the opportunity to provide insight on this proposed Act. We appreciate the Commission s efforts to ensure the safety , efficacy, and trustworthiness of AI technologies . We anticipate that the evolution of this proposal will further solidify a strong ecosystem within Europe that support s innovation , plus the safe and effective uptake of AI technologies and systems . 1 -strategy.ec.europa.eu/en/library/study -supporting -impact -assessment -ai-regulation 2 -aia-costs.pdf 3 Ibid. Digital Therapeutics Alliance 3 Appendix: Background on Digital Therapeutics (DTx) Digital therapeutics represent a quickly evolving category of medicine that frequently incorporates AI components into the generation and delivery of therapeutic interventions directly to patients . DTx products use scientifically developed, clinically evaluated software to treat, manage, and prevent diseases and diso rders. As such, DTx products are subject to rigorous patient -centered core principles (below ),4 an industry code of ethics,5 and product development best practices.6 Digital therapeutics address a wide array of health conditions ,7 with products developed for ADHD, anxiety, asthma, cancer side effect management, diabetes, depression, insomnia, migraine, movement disorders, and opioid and substance use disorder s to name a few. DTx products are used independently, alongside medicati ons, or in tandem with clinician -delivered therapy. They differ from pure lifestyle, wellness, adherence, diagnostic, and telehealth products. DTx products use numerous mechanisms of action to deliver high -quality medical interventions, such as : Provid ing personalized disease treatment, management, and prevention programs Offer ing therapies to address comorbidities, side effects, or affiliated conditions Provid ing treatments that produce direct neurologic changes Deliver ing cognitive behavioral therapy (CB T) and other evidence -based treatments Enhanc ing, support ing, and optimiz ing current in -person and medication treatments Deliver ing responsive physical exercises and behavioral interventions Building on the ease of DTx product scalability and access throu gh patient -owned devices, digital therapeutics enable healthcare decision makers to deliver treatments to populations that have otherwise been unable to secure care either due to geographic limitations, cultural and language boundaries, well -documented disparities, or health condition severity. Patients may now receive personalized therapeutic interventi ons based on their specific needs and abilities, in an engaging way, independent of their work or education schedule, with familiar languages and cultural references, in the privacy and safety of their own environment, and with access to actionable insight s that convey their movement toward clinical improvement. 4 -content/uploads/2019/11/DTA_DTx -Definition -and-Core -Principles.pdf 5 -content/uploads/2019/11/DTA_DTx -Industry -Code -of-Ethics_11.pdf 6 -content/uploads/2019/11/DTA_DTx -Product -Best -Practices_11.pdf 7 -content/uploads/2020/03/DTx -Disea se-State -Targets_03.pdf",en,o One component of enabling the ongoing sustainability of product developers and manufacturers is to clearly identify AI technologies that are high -risk and the specific requirements they are subject to .,risk
"Compliant and Accountable Systems group, University of Cambridge (United Kingdom)",F2665558,06 August 2021,Academic/research Institution,Micro (1 to 9 employees),United Kingdom,"Forthcoming in Computer Law & Security Review - 1 - Artificial Intelligence as a Service: Legal Responsibilities, Liabilities, and Policy Challenges Jennifer Cobbe and Jatinder Singh * Abstract: Artificial Intelligence as a Service ('AIaaS') will play a growing role in society's technical infrastructure, enabling, facilitating, and underpinning functionality in many applications. AIaaS providers therefore hold significant power at this infrastructural level. We assess providers position in EU law, focusing on assignment of contro llership for AIaaS processing chains in data protection law and the availability to providers of protection from liability for customers illegal use of AIaaS. We argue that in data protection law, according to current practice, providers are often joint c ontrollers with customers for aspects of the AIaaS processing chain. We further argue that providers lack protection from liability for customers illegal activity. More fundamentally, we conclude that the role of providers in customer s application functi onality as well as the significant power asymmetries between providers and customers challenges traditional understandings of roles and responsibilities in these complex, networked, dynamic processing environments. Finally, we set out some relevant iss ues for future regulation of AIaaS. In all, AIaaS requires attention from academics, policymakers, and regulators alike. Keywords: cloud computing, artificial intelligence, data protection, intermediary liability, internet consolidation, internet regulat ion * Compliant and Accountable Systems Group, Department of Computer Science and Technology, University of Cambridge ( firstname.lastname@cst.cam.ac.uk ). We acknowledge the financial support of the University of Cambridge (through the Cambridge Trust & Technology Initia tive), the UK Engineering and Physical Sciences Research Council (EPSRC) (through grants EP/P024394/1 and EP/R033501/, and Microsoft, through the Microsoft Cloud Computing Research Centre. Thanks to Reuben Binns, Jon Crowcroft, and the anonymous reviewer s for their helpful feedback on earlier drafts of this paper. Forthcoming in Computer Law & Security Review - 2 - Introduction Cloud computing now underpins many websites, mobile apps, and other internet -connected services (collectively referred to as applications , encapsulating a range of software, including websites, mobile applications, online services, and so on). Cloud prov iders make available, as a service, various assortments of technical infrastructure that support applications. Cloud services have seen significant uptake, as application developers seek to benefit from reduced barriers to entry and lower operating costs b rought about by providers economies of scale. Many cloud services are now available the common term Anything -as-a-Service1 reflects that most any technical infrastructure component is available as a service. In recent years, various cloud providers h ave begun to offer Artificial Intelligence as a Service ( AIaaS ). AI here refers to machine learning models trained on data that can be presented with new data to gain insights into that new data by making predictions and classifications2. Though the phrase Artificial Intelligence is overused sometimes describing even simple algorithmic systems we adopt Artificial Intelligence as a Service in accordance with dominant industry use of the term, referring to pre -trained models provided to customers on a commercial basis. Customers send inputs to the se rvice and receive back the AI analyses result, giving on -demand access to various AI -backed capabilities such as object recognition, face detection, speech transcription, and so on. Results returned from an AI service will directly influence the capabilit ies of the application itself that is to say, unlike traditional cloud services that support application deployment (such as hosting and storage), AI services are intrinsic to some of the application s functionality. The nature of many AI technologies requiring large quantities of training data, specialised hardware for building and training models, data centres to provide real -time analysis, and 1 Yucong Duan , Guohua Fu , Nianjun Zhou , Xiaobing Sun , Nanjangud C. Narendra , and Bo Hu , Everything as a Service (XaaS) on the Cloud: Origins, Current and Future Trends [2015] 2015 IEEE 8th Int ernational Conference on Cloud Computing , June, 621 2 The term s AIaaS and also Machine Learning as a Service (MLaaS) are sometimes also used to refer to services that allow customers to build and train their own bespoke models (see . Forthcoming in Computer Law & Security Review - 3 - technical expertise in machine learning raises costs of entry higher than many organisations can meet. B y offering AI as a service , large cloud providers with substantial resources can derive revenue, while smaller companies who may lack the skills, data, and resources to develop their own systems can take advantage of state -of-the-art technologies tha t may otherwise be beyond their reach. Indeed, Amazon markets their AIaaS with the line Build an AI -driven application No machine learning experience required As a result, AIaaS allows customers to embed AI capabilities in a much wider range of appl ications than might otherwise be possible. Indeed, AIaaS potentially enables functionality in a range of applications in both physical and virtual spaces; in websites, mobile apps, and so on, and in homes, vehicles, offices, shops, factories, commercial premises, public areas, and anywhere else with internet -connected services, systems, and devices. The AIaaS market is set for rapid growth4. As AIaaS greatly lowers the barriers to entry to state -of-the-art capabilities, much public and private use of AI will likely come to rely on providers services, rather than on bespoke systems developed in -house. As such, AIaaS raises several legal, regulatory, and policy questions. Some relate to the difficulties of assigning legal responsibilities and liabilities i n these complex processing environments. Others relate to the general implications of these powerful technologies working to both determine and drive the functionality of customer applications. Consolidation of AIaaS around a small number of companies that already dominate other digital and internet - connected services further entrenches those companies at an infrastructural level and confers greater power upon them. That providers also use customer data to improve their models raises concerns about privacy and the ability of providers to leverage their dominant position in this sector to develop systems that give them in advantage in others. The cross - border supply chains relied upon for developing systems can allow providers to evade responsibilities in dat a protection law. The potential for AIaaS to underpin widespread AI - augmented surveillance and analytics also risks transforming public and private spaces and 3 Amazon W eb Services, Explore AWS AI services < -learning/ai -services > accessed 13 November 4 Markets and Markets , Artificial Intelligence as a Service: Market Research Report ( < -Reports/artificial -intelligence -ai-as-a-service -market - html > accessed 13 November Forthcoming in Computer Law & Security Review - 4 - altering power dynamics and balances of rights and interests. Moreover, the potential for misuse and abuse of AI services, readily and cheaply available for deployment at scale, is a serious problem that requires attention. As a result of these concerns, AI services should receive close scrutiny from regulators and policymakers. In this paper, we exp lore some of the legal responsibilities of AIaaS providers and highlight some of these legal, regulatory, and policy challenges that AI services present. We first describe AIaaS in more detail distinguishing it from traditional cloud offerings, identifyi ng common AI services offered by the major providers, and setting out key actors and stages in the AIaaS data processing chain. Next, we assess the current legal position of AIaaS providers in terms of their data protection responsibilities and potential l iability for unlawful use of AI services, focusing our analysis in particular on key provisions of two relevant EU frameworks: the General Data Protection Regulation5 ( GDPR ) and the E -Commerce Directive6. In doing so, we assess the assignment of respon sibilities in the AIaaS processing chain and highlight several potential legally difficult issues for providers, including around data controllership and their protection from liability for illegal activity undertaken by customers using their services. We discuss whether existing legal frameworks can contend with the emergence of complex, networked, and dynamic processing arrangements and relationships such as in AIaaS, with significant asymmetries in power and technical capacity between customers and provi ders. We then turn to broader issues relating to AIaaS, identifying directions for future law and regulation in four key areas: the amplification of problems that could develop with AI systems through the scale that can be reached with AIaaS; the training of these AI systems; the potential for an expansion of AI -augmented surveillance in virtual and physical spaces; and the risks of misuse and abuse of AI systems offered as a service. 5 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of suc h data, and repealing Directive 95/46/EC (General Data Protection Regulation) [2016] OJ L119 /1 ( GDPR ) . 6 Directive 2000/31/EC of the European Parliament and of the Council of 8 June 2000 on certain legal aspects of information society services, in partic ular electronic commerce, in the Internal Market [2000] OJ L 178 / ( E-Commerce Directive ) . Forthcoming in Computer Law & Security Review - 5 - Artificial Intelligence as a Service AIaaS is a subset of cloud computing, whereby AI capabilities are offered commercially alongside other cloud services. To ground our discussion in the real -world practices of providers, we now provide a brief overview of the general nature of cloud and AIaaS, and the chain of data flow and processing in AI services. Cloud services Cloud computing involves the delivery of computing resources and capabilities over a network, usually the internet7. Cloud offerings are generally provided as services , in that they are typically rented or otherwise provisioned on demand and billed based on usage. Cloud services ( cloud ) work to underpin applications , which include websites, mobile apps, and other software and services. As Fig 1 illustrates, tenants the customers of cloud service providers leverage cloud to support their applications. Figure Simple representation of a common cloud service scenario. A tenant operates an application , which provides functionality for their users.8 The tenant , the cloud service provider s customer, uses the provider s services to support their application. Traditionall y, cloud was described in terms of the following service models: Infrastructure as a Service ( IaaS ), which involved lower -level computing resources9 (for example, virtual servers); Software as a Service ( SaaS ), which involved complete managed applications (for example, a pre -built webmail service that customers can brand as their own), and Platform 7 Christopher Millard, Cloud Computing Law (2nd edn, Oxford University Press . 8 Note that depending on the particular application, the users of the tenant s services could be external to the organisation (i.e. third parties), or part of the tenant organisation itself. We expand on this later. 9 Note that the internet s infrastructure level is different, and includes the technical architecture and protocols for the internetworking of and transmission across multiple networks operating around the world . User Tenant (customer) ServiceProviderApplication ServiceForthcoming in Computer Law & Security Review - 6 - as a Service ( PaaS ), which provides various components that support application development and deploym ent (for example, database systems). However, there are many cloud offerings and these distinctions may blur. As such, the term Anything as a Service ( XaaS )10 is increasingly used, reflecting the fact that most any computing and software infrastructure c an be accessible as a service. The specifics of a tenant s cloud usage depend on the application, the tenant s requirements, and the services the provider offers. In practice, multiple cloud services, potentially from different providers, may support a particular application. Access is often turn -key tenants can leverage cloud services on demand, as required, at a few clicks, with services provided as -is , typically without negotiating or interacting with the provider11. Cloud services are ubiquitous, underpinning most applications12, because they provide low - cost alternatives to running and managing supporting infrastructure, systems, and services in-house. For application developers, costs and barriers to entry are reduced by the on - demand, typically pay-per-use nature of cloud services, avoiding the overheads and expertise required for procuring and managing hardware and software. Cloud providers leverage economies of scale by sharing their infrastructure and expertise and reusing technical component s between tenants. As technical systems become ever more pervasive, with sensors and actuators embedded in physical surroundings, cloud increasingly supports applications in both virtual and physical spaces not only in websites and apps, but in homes, of fices, shops, commercial premises, public areas, and, ultimately, anywhere else where internet -connected services, systems, and devices operate. AI as a Service AI is an overloaded term. In this context, AI is used in relation to machine learning13 ( ML ). ML works to uncover patterns in data to build and refine representative models of that 10 Duan et al (n . 11 Services may have options for customers to configure some aspects of the service. However, the mechanism and scope for any configuration are pre -defined by the provider for the particular service. 12 Jennifer Cobbe, Chris Norval, and Jatinder Singh , What lies beneath: transparency in online service supply chains ( 5 Journal of Cyber Policy 13 Despite the focus on ML, we use AI and AIaaS to be consistent with the terminology used by providers in this space. Forthcoming in Computer Law & Security Review - 7 - data14. These models can be used to make classifications, predictions, decisions, and so on with new data. To illustrate15, a model might be developed to recognise certain objects from images. This model will be trained on a dataset containing many different images to learn (statistically recognise) aspects that characteris e particular objects. Once trained, the model can be presented with new images, and work to classify objects in those images. Models and their outputs are probabilistic : as ML involves deriving a statistical model representing the training data provided, there will be some degree of error or uncertainty regarding their representation and outputs. The performance and operation of an ML model is determined by how it is engineered relevant factors include the specifics of the data used for training, testing , and analysis; how the data is selected, cleaned, and processed; the machine learning methods, configurations, and parameters used to build the statistical model representing the data; any post -processing (corrections and adjustments of model outputs); an d so on. ML is therefore often described as differing from traditional software engineering, where the functionality and outcomes are explicitly programmed for. AIaaS in practice AIaaS is a subset of cloud services that can encompass: (i) providing techn ical environments and resources to facilitate customers in undertaking their own ML (sometimes called Machine Learning as a Service ); or (ii) providing access to pre -built models that customers can essentially plug into their applications. There are a range of possible AIaaS arrangements; for instance, some services may represent a hybrid of the above, such as those involving prebuilt or partially trained models that can be customised by the customer through additional ML operations. This paper focuses specifically on (ii), the most prominent form of AIaaS , though aspects of our discussion may also be relevant for other AIaaS variations. As such, we use AIaaS to refer to commercial offerings that allow access to 14 Junfei Qiu, Qihui Wu, Guoru Ding, Yu hua Xu, and Shuo Feng , A Survey of Machine Learning for Big Data Processing [2016] Eurasip Journal on Advances in Signal Processing 15 This illustration details a supervised machine learning system. That is, where the system learns from input data tha t is labelled with the desired output. Most AI services involve supervised systems. Some services such as anomaly detection involve unsupervised machine learning. That is, where data is not labelled, but the system itself is tasked with finding pattern s and correlations in the data. Forthcoming in Computer Law & Security Review - 8 - generalised pre -built ML models as a ser vice. As Fig 2 illustrates, this process entails customers16 sending input data to providers; the ML model is then applied to those inputs; and the provider returns the results (analyses, classifications, predictions, decisions, and so on) to the customer. Figure A simplified AIaaS s cenario. An image is sent via the customer s application to the provider s image recognition service. The service analyses the image (applying the model) and returns the detected objects. The most prominent AIaaS providers tend to be bigger technology com panies with the financial and technical resources required to develop and offer complex machine learning systems. These include Amazon (through Amazon Web Services17, or AWS ), Microsoft (through Microsoft Azure18), Google (through Google Cloud19), IBM (Watson)20, though some smaller providers also exist (for example, BigML21). We focus on the three largest and most prominent providers Amazon, Microsoft, and Google as they dominate the market and collectively offer a range of services indicative of t hose of other providers. AI services exist in a wider ecosystem which includes, for instance, data brokers supplying both customers and providers, and training data labelling services (such as Mechanical Turk22). 16 In this paper, we use the term customer rather than tenant as is traditionally used for cloud, as customer more naturally fits the API -based (request/response) access to models of such services. In practice, the customer i s likely to use other cloud services that are more tenant -like, hence the terminology persisting. 17 . 18 . 19 . 20 -learning . 21 om. 22 Amazon Mechanical Turk AWS introduces a new way to label data for Machine Learning with MTurk (13 December < g.mturk.com/aws -introduces -a-new -way-to-label -data -for-machine -learning - with -mturk -2f9c19866a98 > accessed 13 November Forthcoming in Computer Law & Security Review - 9 - Various AI services are available; these typically offer generic capabilities useful in many application contexts. Broadly speaking, commercial AIaaS providers offer four categories of service,23 though others also exist: Language for example, text sentiment analysis; translation; and knowledge base creation. Speech for example, speech transcription; speech synthesis; and voice recognition. Vision for example (for both still images and video), image analysis and classification; object recognition; and facial detection, analysis, or recogniti on24. Analytics for example, web usage; behavioural analysis; recommendations and personalisation; content moderation; and anomaly detection. AI services are usually closely integrated into a provider s other cloud services, thereby offering a wide ran ge of services that customers may employ to support their application. If a customer uses AWS hosting for their application, for example, they can readily leverage Amazon s AI services as part of that hosting package, thereby extending application function ality (though customers could also use services from other providers to support that application). This ease of integrating AIaaS with other cloud services allows customers to take advantage of cloud infrastructure to deploy AI -augmented applications at a scale that might otherwise be unreachable. As with many cloud services, AIaaS is available turn -key on demand, with standard form contracts specified by the provider, at relatively low cost, often with only a few steps required by the customer to con figure and use the service. S ome providers offer AI services on a consultancy basis, working closely with the customer to tailor services to their needs 23 See Amazon: -learning/ai -servic es; Microsoft: -gb/services/cognitive -services ; and Google: /ai. 24 While all three of Amazon, Microsoft, and Google offer other vision services, only Amazon and Microsoft offer facial recognition; this is not currently part of Google s portfolio, although they do offer face detection and analysis. Forthcoming in Computer Law & Security Review - 10 - (sometimes involving pre -built models that are heavily or entirely customised for the customer), though this is usually for certain higher -value customers. Motivations for considering AIaaS AIaaS warrants particular attention for several reasons. First, AI services are intrinsically tied to realising particular functionality in customer s applications. Traditional cloud services generally support application deployment relating to operation al aspects such as availability, storage, connectivity, scalability, and security. Even services more closely integrated with applications, as in PaaS, such as database services or around identity (sign - on) management, still orient towards supporting appli cation delivery. Indeed, even for SaaS, where providers define the application itself, they still ultimately support delivery and deployment. AI services, however, play a more direct role in enabling, facilitating, and underpinning core functionality of customer -defined applications that is to say, an AI service provides classifications, analyses, detections, predictions, and other capabilities on which particular functionality of the customer s application is predicated and relies25. As the performance o f the AI service s model is determined by the specifics of the provider s engineering processes, so too is the particular functionality in the customer s application that uses the model. Second, AI systems can exhibit errors, biases, inequalities, and oth er problems, which, through AIaaS, could be reproduced at scale. Further, by making state -of-the-art AI capabilities widely accessible at scale, often with little provider oversight, AIaaS risks enabling a range of undesirable, problematic, or possibly ill egal applications. This raises 25 Note that AIaaS contr asts with Software as a Service (SaaS), whereby the service provider defines, offers, and manages the complete application on behalf of the customer (though typically provides some possibility for customisation). AIaaS to some extent resembles a form of Platform as a Service (PaaS), more traditional forms of which provide general infrastructure that enables some supporting function (for example, for managing deployment and operation specifics, storage and data management, sign -on services, and so on). AIaaS , however, provides generic AI capabilities that enable, facilitate, underpin, and directly determine particular application functionality . Forthcoming in Computer Law & Security Review - 11 - questions around the roles, responsibilities, and potential liabilities of both AIaaS providers and their customers. Third, AIaaS is likely to grow in prominence. In -house machine learning can be prohibitive, given the need for data, expertise, and computational power. By enabling developers to plug -in state -of-the-art ML capabilities to their applications, at low cost and without requiring great expense, AIaaS increases the likelihood that ML will underpin a larger range of applications. In future, many organisations wishing to use AI may rely on AI services for their desired functionality. Finally, and again due in part to the data, expertise, and computational power needed to develop sophisticated systems, it will like ly be companies already dominant elsewhere in the digital economy who can practically offer a range of AI services. Indeed, as noted previously, AIaaS has already coalesced around Amazon, Microsoft, and Google, each of which is also market -leading in multi ple other online services sectors. In other work , we have explored consolidation at this sub -application infrastructure level26. While this did not study AIaaS, specifically, we found consolidation in infrastructure services more generally (particularly ar ound Amazon and Google). This echoes the consolidation of user -facing web applications around much the same companies27 (affording them significant societal and market power28). Dominance in those sectors allows these companies to take leading positions as AIaaS providers, particularly where they repurpose systems developed to support their activities elsewhere and offer access commercially. As with existing cloud services, other organisations are likely to in time become dependent on AI services offered by these providers. 26 Cobbe et al (n . 27 Internet Society , Consolidation in the Internet Economy ( Technical Report . 28 Zeynep Tufekci, As the Pirates Become CEOs: The Closing of the Open Internet ( 145 D dalus, the Journal of the American Academy of Arts & Sciences 1, 74; Martin Moore and Damian Tambini (eds) , Digital Dominance: The Power of Google, Amazon, Facebook, and Apple (Oxford University Press ; Lina M Khan 'Sources of Tech Platform Power' ( 2 Georgetown Law Technology Review 2; Jennifer Cobbe and Jatinder Singh Regulating recommending: Motivations, Considerations, and Principles ( 10 European Jour nal of Law and Technology Forthcoming in Computer Law & Security Review - 12 - Moreover, not only does AIaaS consolidation offer providers a potentially fruitful and monopolistic revenue source, but providers will also be at the heart of any societal transformation brought about by the wide and affordable availabil ity of AI technologies. This has significant implications for the power of providers and their role in future society. The leading AIaaS providers are three of the five Big Tech companies29, and represent three of the four most valuable publicly traded c ompanies in the world by market capitalisation30. In recent years, the five Big Tech companies have emerged as an oligopoly that concentrates power through their financial resources and their dominance of online services31. They have aggressively pursued s trategies intended to dominate markets and centre themselves in online services, and their dominance of AIaaS allows them to further entrench t hemselves at an infrastructural level in both virtual and physical spaces. The AIaaS processing chain We now describe the chain of processing in AIaaS. As with cloud more generally (Fig , the AIaaS processing chain typically involves at least two entities those offering the service ( providers ), and application vendors availing of the service ( custome rs , or tenants in common cloud parlance). This may also involve third -parties32, typically where customers use AIaaS to either (i) add functionality (by providing a particular capability) to applications that are in turn used by these third -parties, or (ii) analyse or process activities and behaviours of third parties (as they use the customer s application or in some other form of surveillance or behavioural analysis). Third -parties in AIaaS are broadly either active users of services (where third -party end -users directly interact with aspects of a customer s application that rely on AIaaS for functionality) or passive subjects of data processing (where the customer collects data from or about third -parties for processing that involves AIaaS, but whe re those third parties are not themselves doing or directly interacting with something that relies on the AI service). Though third -parties will be discussed where 29 Sometimes abbreviated as GAFAM , for Google (Generally understood in this context as synonymous with its parent holding company, Alphabet), Amazon, Facebook, Apple, and Microsoft. 30 As of mid -2020 . 31 Nick Smyrnaios The GAFA M effect: Strategies and logics of the internet oligopoly ( 188 Communication & Languages 32 Note that in this paper we do not use third -party as defined in GDPR art 4( to mean sub -processors . Forthcoming in Computer Law & Security Review - 13 - relevant, they are largely outside our analysis, as they are usually not directly commission ing or provisioning the AI service from the provider. They may, for example, use a customer s application that includes speech transcription which utilises an AI service obtained by the customer, but that third -party is not involved in commissioning the sp ecific transcription services of a particular provider. Customers access AI services remotely by initiating a request to the provider accompanied by the input data, which is then analysed by the provider and the results of that analysis returned to the c ustomer (Fig . Models are accessed through an Application Programming Interface ( API ). APIs are commonly used in software to obtain functionality through calls or requests (with specified input parameters) to the service, software, database, or libra ry in question and receiving responses (tailored to the input parameters). In AIaaS, API calls are typically executed over a network between customers applications and the provider s AIaaS servers. Requests may pass from or through the customer s equipmen t in this process (as customers often use other cloud services to support deployment, they may not necessarily own that equipment), or the customer s (application) executing on an active third -party s equipment (device or cloud) may transfer requests direc tly from the application to the provider s server and then receive responses. In all cases, the customer s application determines that interaction. Depending on the nature of the service, often this process takes milliseconds to complete. Figure Representation of the stages of the AIaaS processing chain . First, the customer collects input data from one or more sources (such as third -parties or data brokers). This stage may involve some processing data collation, pre - processing, analysis, etc. by the customer. The customer then transfers input data to the provider (request), who analyses that data. The provider returns outputs of that analysis to the customer (response). The return stage may be followed by further processing by the customer or the transmission of data onwards to others, but that activity before the collection stage and after the return stage is not part of the AIaaS chain itself. Forthcoming in Computer Law & Security Review - 14 - AIaaS therefore involves a chain of data flow between customers and providers (Fig . This AIaaS processing chain consists of several stages, detailed below. Our depiction is an abstract representation of the stages of processing rather than representing specific actions or operations performed on data; there may be several activities, organisational or technical, at each stage. We identify four stages of the AIaaS processing chain: ( Colle ction of input data and any processing of that data by customers. This could involve, for example, data input by third -parties to the customer s application; collecting usage or behavioural data; surveilling physical spaces using cameras, microphones, or s ensors; acquisitions from data brokers, and so on. Collection does not necessarily involve storage, aggregation, or temporal delay; it could include, for instance, live sensor readings or video feeds. Again, collection does not necessarily mean that the da ta touches any equipment owned or managed by the customer. ( Transfer of input data by customers to providers through a networked API request. This may in practice involve transferring input data directly from a customer application s third -party end -users t o the provider at the customer s instigation and direction but without the data passing through any customer managed server, or it may come from the customer themselves. ( Analysis of that data by the provider using a machine learning system, applying a mode l (or models). These models are trained using datasets collated by the AIaaS provider and are usually routinely and iteratively updated by the provider (to, for instance, improve accuracy, address identified problems, and so on). Again, our focus is on mod el(s) generally available to customers, not those tailored to specific customer concerns. ( Return of the results of analyses by providers to customers over a network as API responses. As results are generally returned to the requesting entity, it is possib le for output data to be transferred directly from the provider to the third -party end -users of the customer s application for display or further processing without passing through the customer s managed equipment. The processing chain effectively forms a loop in which data flows from customers, to providers, and back to customers. Subsequent to the return stage, and therefore after the end of the chain itself, customers may perform their own analyses or other processing of Forthcoming in Computer Law & Security Review - 15 - the service s outputs. Output da ta (or results of its subsequent processing) is used by customers applications to determine subsequent functionality; outputs may be displayed on-screen on a website or app, may trigger other application functionality including subsequent AIaaS calls, may be stored in a database, may inform device functioning, may help the customer better understand usage of their application, may be transferred onwards to another entity, and so on. Providers may also process input data for their own purposes in a manner removed from and additional to the AIaaS processing chain itself. This may be to monitor and improve their service s performance, to update their models, or to monitor customers use for billing purposes or to enforce the platform s terms of service. We di vide this additional processing by providers broadly into two categories33. The first is ancillary processing necessary for and directly connected to providing a commercial AI service (for instance, for billing, security, technical maintenance, or legal and regulatory compliance). The second is supplementary processing for purposes other than those necessary for the commercial supply of the AI service itself. This includes things like improving models or systems34, undertaking market research, or enforc ing aspects of the provider s terms of service unconnected to the ancillary processing described above35. Access to customers data for model building and development helps improve models by giving a broader range of example deployments, allowing models to better account for a wider range of (changing) circumstances and use cases. Importantly, there is a key distinction here between AIaaS and other cloud services: while cloud providers use information that they gather about customers' usage of their other services (i.e. metadata) to inform product improvement and development, in AIaaS it is often the customer input data itself , obtained directly through 33 A similar (though not identical) distinction can be found in Microsoft Azure s service agreement between Processing for Microsoft s Legitimate Business Operations (largely falling under ancillary processing ) and Processing to Provide Customer the Online Services (including what we call supplementary processing ) (Microsoft, Online Services Data Protection Addendum (July , 6 < - us/licensing/product -licensing/products > accessed 19 March . 34 Note that there are hybrid services whereby a pre -built model is offered to customers which they can then customise and further refine by training the model on input data that they supply. As we focus on pre -trained models, which represent the bulk of AI services, these hybrid services are outside the scope of our discussion. 35 Unlike monitoring for legal and regulatory compliance, enforcing other aspects of providers terms of service is not strictly speaking necessary for provid ing the service itself. Providers have significant discretion to decide what they permit or prohibit and could, if they so wished and in line with their policies, allow anyone to use their services for any legal purpose or impose requirements or restrictio ns as a condition of using the service. Forthcoming in Computer Law & Security Review - 16 - the AIaaS processing chain, that providers use for supplementary processing to improve the models that d rive their core product. That is to say, supplementary processing of customer data for model improvement, a key feature of AIaaS, does not have an analogy in most other cloud services. This supplementary processing drives improvements both in the provider s models used across their customer base and, consequently, in the functionality of individual customers applications. Precise details about AIaaS providers use of customer data are limited, but publicly available information suggests that all the major AIaaS providers engage in supplementary processing using customer data in some form. For example, Google asks customers to permit Google to use customers speech recognition inputs to refine its models36. Amazon uses customer data from their Rekognition v ision service to improve its models on an opt - out basis37. At present, Microsoft states explicitly that they do not use customer data from certain Azure AI services (such as vision) for supplementary processing38, and that they may use customer data from other services (such as their video analysis service)It is unclear whether and to what extent Microsoft uses customer data from other AI services for supplementary processing, though documentation implies that customer data is used in this way in relation to certain services but not for others40. Microsoft retain provision in their service agreement permitting them to use customer data for ongoing improvement (such as improving efficacy) of its online services, including Azure41. Providers may financially incentivise customers to permit access to customer data (Google, for instance, discounts 36 Google Cloud , 'Cloud Speech -to-Text: Data logging' < -to-text/docs/data - logging > accessed 13 November 2020 ; see also Google Cloud, Cloud Speech -to-Text: Terms for opt -in for data logging < -to-text/docs/data -logging -terms > accessed 13 November 2020 . Note that Google does not use customer data to improve all its AI services for example, customer data is not used to improve its Cloud Vision models ( Google Cloud, 'Cloud Vision API: Data Usage FAQ' < -usage?hl=en > accessed 13 November 2020 ). 37 Amazon Web Services, 'Amazon Rekognition FAQs' < > accessed 13 November 38 Microsoft Azure, Computer Vision < -gb/services/cognitive - services/computer -vision/#faqs > accessed 29 March 39 Microsoft Azure, Analyze videos in near r eal time < -gb/azure/cognitive - services/computer -vision/vision -api-how -to-topics/howtoa nalyzevideo_vision > accessed 29 March 40 Microsoft Azure, Azure Cognitive Services < -gb/support/legal/cognitive - services -compliance -and-privacy > accessed 29 March 41 Microsoft, Online Services Data Protection Addendum (July , 6 < - us/licensing/p roduct -licensing/products > accessed 13 November 2020 ; see also: Microsoft 'Data management at Microsoft' < -gb/trust -center/privacy/data -management > accessed 13 November Forthcoming in Computer Law & Security Review - 17 - speech recognition services for customers who opt -in to data logging 42, which permits Google s use of customer data to refine their s ervices). Though supplementary processing is beneficial to the provider (and indirectly beneficial to customers through improved models), it is essentially at the providers discretion as it is not necessary to provide the service itself. Legal respon sibilities and liabilities We now highlight some legal issues arising from AIaaS in relation to European data protection and intermediary liability law, focusing primarily on the roles, responsibilities, and potential liabilities of providers. In particul ar, we assess how providers use of customer input data to train their models in AIaaS affects the assignment of roles in data protection law in light of recent CJEU jurisprudence, whether supplementary processing is likely to have a valid legal basis in G DPR, and whether the nature of AIaaS itself affects providers protection from liability for illegal activity involving their services. As we repeatedly highlight, many of the potential legal pitfalls for AIaaS providers relate to their own activities (such as around supplementary processing) and the fact that, where services are offered generically on a turn -key basis, they have little knowledge of what customers are doing with their services. More fundamentally, we also question whether current legal fram eworks are suitable for the complex, networked, and dynamic relationships found in AIaaS. Data protection GDPR provides a framework governing the processing of personal data43, defined broadly to include any information relating to an identified or ident ifiable natural person. Under GDPR, any natural or legal person (from here: entity ) involved in processing personal data will be either a data controller44 or a data processor45. Processing means any operation performed 42 Google Cloud, 'Cloud Text -to-Speech: Pricing' < -to-text/pricing > accessed 13 November 43 GDPR art 4(. 44 GDPR art 4(. 45 GDPR art 4(. Forthcoming in Computer Law & Security Review - 18 - on personal data (including, for e xample, collection, storage, transmission, and analysis)The AIaaS processing chain consists of various operations performed at each stage (see Table . All processing of personal data covered by GDPR47 must be based in one of the lawful grounds specif ied therein48. Collection stage Transfer stage Analysis stage Return stage Collecting; recording; organisation; structuring; storage Disclosure by transmission Adaptation or alteration; structuring; organisation; alignment or combination; consultation Disclosure by transmission Table Some processing operations49 performed at each stage of the AIaaS processing chain. This is neither an exhaustive list nor are all operations necessarily performed in each instance o f AIaaS for example, the collection stage may not always involve storage. Data will be personal data where two questions are answered affirmatively: ( does the data relate to a natural person? If so, then ( is that person identifiable from that da ta? In many cases, data processed in the AIaaS chain will clearly relate to a natural person. Sometimes the individual will clearly be identifiable obvious examples include in voice recognition, facial recognition, or other biometric identification servi ces; where faces, images, or voice recordings of individuals are otherwise being analysed; or where direct identifiers (such as names, email addresses, unique ID numbers, and so on) are defined inputs being processed. In other cases, the question of identi fiability is less straightforward. According to GDPR, an identifiable person is one who can be identified, directly or 46 GDPR art 4(. 47 GDPR does not apply to some kinds of personal data processing (GDPR art . The processing of personal data by law enforcement agencies, for example, is regulated by the Law Enforcement Directive ( Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with reg ard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Co uncil Framework Decision 2008/977/JHA ( OJ L 119 /. 48 GDPR art 6 , art 49 As set out in GDPR art 4(. Forthcoming in Computer Law & Security Review - 19 - indirectly50. According to the CJEU, to be personal data, information need not by itself allow an individual to be identified51, nor mu st all information needed to identify an individual be held by one person52. Indeed, GDPR itself contemplates circumstances where controllers process personal data but are unable to themselves identify the data subject (for instance, if the purposes of pro cessing do not require them to identify data subjects then they are not obliged to acquire additional identifying information for the sole purposes of complying with GDPR53). Just because a controller does not know who the data subject is, or cannot itself identify them, that does not mean that they are not identifiable. Taking the AIaaS example of speech transcription (voice -to-text) services; clearly the voice belongs to an individual, even if the provider does not know their identity. The fundamental question in relation to identifiability is whether the person to whom the data relates can be distinguished from others54, either through that data alone or in combination with other data. Determining whether this is possible, according to GDPR s recitals, means taking into account all means reasonably likely to be used by the controller or by someone else to identify that person either directly or indirectly55. In making such a determination, factors such as the costs and time needed for identification should be taken into account, considering the available technology and potential technological developments56. The CJEU has held that identification would not be reasonably likely if it is prohibited by law or is practically impossible (requ iring disproportionate time, cost, and effort), such that the risk of identification is in reality insignificant57. Providers may in some (but not all) cases be reasonably likely to be able to distinguish individuals from others. However, customers will in many or most cases be in such a position, as they often explicitly and directly interact with third -parties. Moreover, many AI services inherently involve 50 GDPR art 4(. 51 Case C -582/14 Breyer v Bundesrepublik Deutschland [2016] ( Breyer ) para 41 . While this case related to the Data Protection Directive, which was superseded by GDPR, the definition of personal data in the Directive was the same as in GDPR . 52 Breyer para 43 . 53 See, for example, GDPR art 54 See Frederik J Zuiderveen Borgesius , 'Singling out people without knowing t heir names - Behavioural targeting, pseudonymous data, and the new Data Protection Regulation' [2016] Computer Law & Security Review 32, 256 -55 GDPR recital 26 . 56 GDPR recital 26 . 57 Breyer para 46 . Forthcoming in Computer Law & Security Review - 20 - personal data; again considering speech transcription, though the identity of the speaker may not be clear, the fact that a voice is unique distinguishes it from others. Many AI services will therefore involve processing personal data, including for audience analytics, voice recognition or speech transcription, facial detection, analysis, or recognitio n, as well as those that process sound, images, and text and so on. Customers will usually have some idea of whether they process personal data in AIaaS or not, particularly when their application directly interacts with or monitors third parties. For prov iders, however, the ability to determine which data is non -personal data, which is ordinary personal data, and which is special category data58 is more limited. For some services, such as biometric identification (which inherently involves special catego ry data59), it may be obvious that personal or special category data is involved. Similarly, where AI services are offered to customers on a consultancy basis, the provider may have detailed knowledge of how the customer will use the service and what kinds of data are likely to be processed. In many cases, though, particularly where services are provisioned as turn -key , AIaaS providers will have little to no direct knowledge of what the input data transferred from customers relates to (particularly in mor e generic services, such as object recognition). This situation arises from the nature of AI services themselves as providers offer these services generically, potentially to millions of customers each with their own use cases and deployments and with fe w (if any) checks by the provider on what each customer will use the service for, providers will be unaware of what kind of data they will process (except, again, for services where personal or special category data is inherent, such as biometric identific ation). We therefore argue that, in practice, to avoid inadvertently processing personal data unlawfully, AIaaS customers and providers should apply data protection law s standards to all processing in the AIaaS chain (whether it in fact involves personal data or not). This is particularly so given that data protection law s application is irrespective of whether processing also involves the same operations performed on non -personal data without distinguishing between the two60. Without safeguards, there i s therefore a serious risk of 58 GDPR art 59 GDPR art 9(. 60 Case C -131/12 Google Spain v AEPD and Mario Costeja Gonz lez [2014] ( Google Spain ) para 28 . Forthcoming in Computer Law & Security Review - 21 - providers inadvertently processing personal data without meeting data protection law s standards. Moreover, fines imposed on controllers and processors for infringing GDPR can take into account situations where they have acted negligently61 (which, we argue, would include processing personal data unlawfully on the mistaken assumption that it was not personal data). While all of GDPR s requirements will apply, several issues are particularly relevant for AIaaS. Due to the compl ex interactions of third -parties62, customers, and providers, the assignment of the roles of data controller and processor in the AIaaS processing chain is not simple and may conflict with how providers themselves envision their responsibilities. Moreover, the complex, networked, and dynamic nature of AIaaS makes it difficult for providers to manage data protection responsibilities and obligations, such as around legal bases for processing. As such, we now provide a high -level analysis of data protection is sues arising from certain common practices of providers to indicate the general legal situation for AIaaS as typically offered as a model -based service provided generically to a variety of customers on a turn -key basis. As our focus is on the legal posit ion of providers, we assess their real -world practices against GDPR and CJEU case law, rather than detailing specific use cases. However, in practice particularly where AI services operate on a consultancy basis, or where models are built or customised s olely for specific customers we note that there may be arrangements or permutations that do not align with the below. Controllers and processors The bulk of GDPR s compliance obligations fall on controllers, though processors are also under specific ob ligations. As such, the obligations that providers and customers will have in relation to the AIaaS processing chain will depend on whether they are acting as a controller or as a processor at each stage. At minimum, for processing for which they are a con troller, 61 GDPR art 62 In our analysis, we do not mean third -party as defined in GDPR (GDPR art 4(: a natural or legal person, public authority, agency or body ot her than the data subject, controller, processor and persons who, under the direct authority of the controller or processor, are authorised to process personal data ). We use it as discussed in the Artificial Intelligence as a Service section of this pap er. Forthcoming in Computer Law & Security Review - 22 - they must implement the data protection principles63 of lawfulness, fairness, and transparency; purpose limitation; data minimisation; accuracy; storage limitation; integrity and confidentiality; and accountability. They must ensure that there is a valid basis in law for any processing of personal data64. They must also take technical and organisational measures to ensure that their processing complies with GDPR more generally and to be able to demonstrate compliance65, to implement data protection by design and by default66, and to ensure a level of security for the processing appropriate to the risk it presents to the rights and freedoms of natural persons67. As of August 2020, AWS, Microsoft Azure, and Google Cloud each specify in their service agreements that they will act as a processor for their services, including AI services, and that customers will typically be the controller68 (note though that Microsoft claim to be an independent i.e. separate data controller for a small num ber of services69). However, while contractual relationships can guide courts and regulators, the roles of controller and processor based on the factual position and cannot be assigned definitively in contract70 where an entity in practice determines the purposes and means of processing (i.e. why personal data is being processed and how it is being processed71), they will be a controller72. Processors merely process personal data for and at the direction of controllers73. More than one entity can be a controller for the same processing; in that situation, they will either act 63 GDPR art 64 GDPR arts 5, 6, and 9 . 65 GDPR art 66 GDPR art 67 GDPR art 68 Amazon, AWS GDPR DATA PROCESSING ADDENDUM , section 1 < - gdpr/AWS_GDPR_DPA.pdf > accessed 13 November 2020 ; Microsoft, Online Services Data Protection Addendum (July , 7 < -us/licensing/product -licensing/products > accessed 13 November 2020 (see also Microsoft Azure, 'Azure Cognitive Services' < - gb/support/legal/cognitive -services -compliance -and-privacy > accessed 13 November 2020 ); Google Cloud, Data Processing and Security Terms (Cu stomers) , section 1 < - processing -terms > accessed 13 November 69 Microsoft Azure, Azure Cognitive Services < -gb/support/legal/cognitive - services -compliance -and-privacy > 29 March 70 Article 29 Data Protection Working Party , Opinion 1/2010 on the concepts of controller and processor ( 00264/10/EN WP1 69, 10-71 Article 29 WP (n , 72 GDPR art 4(, art 28( . 73 GDPR art 4(, art Forthcoming in Computer Law & Security Review - 23 - separately as controllers, or, where they jointly determine the purposes and means, will act together as joint controllers. In either case, all cont rollers are subject to GDPR. The leading CJEU cases on assigning controllership (separately or jointly)74 are Google Spain75, Wirtschaftsakademie76, Jehovah s Witnesses77, and FashionID78. Several key points distil from these cases: The concept of cont roller should be understood broadly, to ensure complete and effective protection for data subjects79. An entity will take part in determining the purposes and means of processing and will therefore be a controller where they exert influence over the processing for their own purposes80, such as to help achieve their goals81 or for their own commercial benefit or economic interests82 (though exercising control over the personal data in question is not required83). This can include, inter alia , defining parameters, according to the objectives of that entity, that have an influence on the processing in question84; contributing to determining the purposes of another controller s processing85; and making processing by other controllers possible86. Where multiple entities are involved, having access to personal data is not a prerequisite for any one of those entities being a controller for the processing87. Joint controllership does not exempt any one controller from compliance obligations88, nor doe s using another controller s platform for the processing mean that a controller can escape those obligations89. 74 While these cases related to the Data Protection Directive, which was superseded by GDPR, the definition of data controller in the Directive is the same as in GDPR. As such, they can be read across to GDPR. 75 Google Spain . 76 Case C -210/16 Unabh ngiges Landeszentrum f r Datenschutz Schleswig -Holstein v. Wirtschaftsakademie Schleswig -Holstein GmbH [2018] ( Wirtschaftsakademie ). 77 Case C -25/17 Tietosuojavaltuutettu v. Jehovan Todistajat [2018] ( Jehovah s Witnesses ). 78 Case C -40/17 FashionID GmbH & Co. KG v Verbraucherzentrale NRW e.V . [2019] ( FashionID ). 79 Google Spain para 80 Jehovah s Witnesses para 68 ; FashionID para 81 Jehovah s Witnesses para 71 . 82 FashionID para 80 . 83 Google Spain para 34 . 84 Wirtschaftsakademie para s 36-85 Wirtschaftsakademie para 31 86 FashionID para s 75, 78 . 87 Wirtschaftsakademie para 38; Jehovah s Witnesses para 69; FashionID para 82 . 88 Google Spain para 40 . 89 Wirtschaftsakademie para 40 . Forthcoming in Computer Law & Security Review - 24 - Joint controllership does not, however, necessarily imply joint responsibility for all stages of processing90. Processing may involve several o perations performed on personal data at different stages91. Different actors may be involved at these different stages of processing to different degrees92. Those different actors may therefore be (joint) controllers for operations performed at certain sta ges of processing, but cannot be controllers for operations performed at stages that precede or are subsequent to stages for which they determine the purposes and means93. Assigning controllership Straightforwardly, as with most other cloud services94, customers will always be controllers for the entire AIaaS processing chain, as throughout they determine the purposes and means of processing (a position reflected in AWS, Microsoft Azure, and Google Cloud service agreements). For example, wher e customers offer to active third -parties functionality that relies on AIaaS, they essentially delegate the processing that enables, facilitates, and underpins that functionality to the provider. In that case, they not only enable the provider s processing of third parties data, but also determine the processing s means (analysis by that AIaaS provider) and purposes (realising the functionality offered by the customer to third -parties). Where customers input data obtained from passive third -parties to an a udience analytics service (for example), they similarly determine the processing s means (analysis by that AIaaS provider) and purposes (providing insight into third -parties behaviour). In both cases, customers are clearly controllers for that processing. This remains the case however the data is transferred to the provider, including where input data does not pass through the customer s (owned or managed) equipment as it is transferred between third -parties and providers for example, where it is transmi tted directly from their application s end -users (i.e. third -parties) to a provider s AI service. In that scenario, while they do not process the data on hardware or infrastructure that they own, manage, or 90 Jehovah s Witnesses para 66; Wirtschaftsakademie para 43 ; FashionID paras 70 -91 FashionID para 72 . 92 Jehovah s Witnesses para 66; Wirtschaftsakademie para 43; FashionID para 93 FashionID para s 70-94 Millard (n . Forthcoming in Computer Law & Security Review - 25 - direct, customers still determine the purposes an d means of processing in the AIaaS chain. Their application determines how the data is processed in line with their operational decisions and business practices, making and receiving responses from the API calls necessary for the AIaaS processing chain to run. Due to the differences between providers activities in AIaaS and other cloud services, in particular around supplementary processing, determining their role is more complicated. Whether providers determine the purposes of processing will, following the CJEU jurisprudence discussed above, depend on whether they influence or undertake any processing for their own purposes other than directly connected to providing the AIaaS service itself95. In our view, if an AIaaS provider is merely analysing input data and returning outputs to customers and engaging in ancillary processing necessary for providing the service for instance, for billing, security, or legal and regulatory compliance then it will act as a processor96 (see Fig . In that case, the cu stomer determines the purposes. Figure Role assignments in the AIaaS chain where providers do not influence the processing of input or output data for their own purposes. Here, customers are controllers for all stages and likely also for any subsequent processing that they perform. AIaaS providers act as processors for their customers in the transfer , analysis , and return stages. 95 See Jehovah's Witnesses paras 65 -68; Wirtschaftsakademie para 31; FashionID paras 68 -96 Microsoft s service agreement says that they will comply with the obliga tions of an independent data controller under GDPR ' [emphasis added] in relation to some ancillary processing , though notably unlike in some other areas of their service agreement they do not claim to actually be a controller for that processing (Microsoft, Online Services Data Protection Addendum (July p.7 < - us/licensing/product -licensing/produ cts> accessed 13 November . Forthcoming in Computer Law & Security Review - 26 - If, however, the provider also does anything else with the customer input or output data for their own purposes (for instance, improving or updating their models or systems97) then their position would be different. Though we acknowledge that publicly available information on the precise details of providers' supplementary processing is limited (see , the key legal question for us hinges on whether providers process personal data for their own purposes other than as necessary for providing the requested service to the customer . Supplementary processing is not part of the AIaaS processing chain itself. It forms a separate processing chain, a separate set of operations performed by the provider, with separate purpos es determined by the provider. Providers would, in that case, influence the purposes for processing operations in the AIaaS chain for their own commercial benefit98 at the transfer and (where model outputs are used for supplementary processing) analysis stages, as those purpose would then include facilitating that supplementary processing. However, providers would not influence the collection stage, as the customer undertakes processing at that stage prior to the provider s involvement99. Nor would they inf luence the return stage, as output data is transmitted to the customer at a point in the chain logically separate from and subsequent to the provider s supplementary processing (though chronologically that processing may occur on copies of the output data after the return stage is complete). In relation to the means of processing, the situation is again complicated. Means doesn t just relate to narrow technical details of hardware or software (what the European Data Protection Board ( EDPB ) calls the non-essential means ), but to broader questions of which data will be processed, in what ways, for how long, and so on (the essential means , closely linked with the purpose)Where providers engage in supplementary processing, we argue, they play a rol e in determining the means of processing for the transfer and 97 Note that this remains the case even where customer data is aggregated or where metadata (i.e. data about customer data) is used to improve services, as aggregation of data or derivation of metadata would itself involve operations performed on the personal data provided by the customer and would thus constitute processing . 98 See, by analogy, FashionID para 80, where the fact that the processing served FashionID s own commercial advantage was a key consideration in finding that it was a controller. 99 See Jehovah s Witnesses para 66, Wirtschaftsakademie para 43 , and FashionID paras 70 -74, where the Court reasoned that different entities can have a different degree of influence over different stages of processing. 100 European Data Protection Board, Guidelines 07/2020 on the concepts of controller and processor in the GDPR (, 13 -15; Article 29 WP (n , Forthcoming in Computer Law & Security Review - 27 - analysis stages of the AIaaS chain. This is because processing at those stages effectively then supports two purposes: first, providing the AI service to the customer; second, facilitating the p rovider s supplementary processing. In effect, though the AIaaS chain and any supplementary processing are separate, operations performed in the transfer and analysis stages of the AIaaS chain facilitate that supplementary processing and thus fulfil that s econd purpose, determined by the provider. They therefore form the means by which that purpose is achieved. Providers therefore determine not just why processing at those stages occurs (in part, to facilitate their own supplementary processing) but how tha t purpose is pursued, which data will be processed, in what ways it will be, and so on (through their API, their systems, and their models). Indeed, as the CJEU held in Jehovah s Witnesses , a natural or legal person who exerts influence over the processin g of personal data, for his own purposes [ ] participates, as a result, in the determination of the purposes and means of that processing 101 (emphasis added). The effect, according to the CJEU, is that the entity in question in this context, the provider acts as a controller102. Where they engage in supplementary processing, AIaaS providers would therefore, we argue, be controllers for p rocessing in the transfer and analysis stages of the chain103. However, following the Court holding in FashionID that entities cannot be controllers for operations in the chain of processing that precede or are subsequent to those for which they determine t he purposes or means104, they are not controllers for processing after the analysis stage, as transmitting output data to and its subsequent use by customers does not serve the purpose of facilitating providers supplementary processing. Instead, they are processors for this return stage. They do not play either role for any processing at or prior to the collection stage105. That providers who engage in supplementary processing are controllers for certain stages of the AIaaS processing chain is a potentially significant 101 Jehovah s Witnesses para 68; FashionID para 68 . 102 Jehovah s Witnesses para 68; Fashion ID para 68 . 103 See, by analogy, Jehovah s Witnesses paras 68 -69 and FashionID paras 70 -The transfer stage involves processing consisting of the operation of disclosure by transmission; the analysis stage involving processing consisting of the set of operations of u se, consultation, and adaptation. 104 FashionID paras 70 - natural or legal person cannot be considered to be a controller [ ] in the context of operations that precede or are subsequent in the overall chain of processing for which that person does not determine either the purposes or the means . 105 The collection stage involving processing consisting of the set of operations of collection, recording, organisation, and structuring. Forthcoming in Computer Law & Security Review - 28 - departure from the legal position found in traditional cloud services, which typically do not involve such supplementary processing, and arises directly from those supplementary processing activities of providers themselves. As discussed previo usly, the main AIaaS providers (AWS, Microsoft Azure, and Google Cloud) all engage in this supplementary processing for various AI services. In many cases, therefore, according to current real -world practice, the legal assignment of responsibilities in AIa aS stands contrary to claims in providers service agreements that they are processors. The relationship between controllers Where customers and providers are both controllers for various stages of the AIaaS processing chain (that is to say, where provid ers engage in supplementary processing of customer data), the next question is whether they are joint controllers or separate controllers for the same processing. According to GDPR, multiple controllers will be joint controllers where they jointly determin e the purposes and means of processing106 i.e. pursuing the same purposes using the same means. However, as the Article 29 Working Party acknowledged, many kind of pluralistic control may arise in reality107. In complex, networked, multiparty environmen ts such as AIaaS two controllers for the same processing may each pursue their own purposes which may or may not align to a greater or lesser degree. According to both the Working Party108 and the EDPB109, jointly should mean together with or not alone , depending on the arrangements between controllers. Following from the CJEU s jurisprudence in Wirtschaftsakademie , Jehovah s Witnesses , and FashionID , the EDPB says that joint controllership can arise where controllers make a common decision about purposes and means (i.e. where they decide together110) or from converging decisions about the same purposes and means111 (for instance, where the processing is mutually beneficial, such as where it serves both parties commercial or 106 GDPR art 26( . 107 Article 29 WP (n 7 , 18 . 108 Article 29 WP (n 7 , 18 . 109 European Data Protection Board (n 10 , 110 See, for example, Jehovah s Witnesses paras 70 -111 European Data Protection Board (n 10 , Forthcoming in Computer Law & Security Review - 29 - economic interest112). In the latter case, a key consideration is whether the processing would not be possible without both parties participation113. Similarly, the European Data Protection Supervisor ( EDPS ) suggests that wher e controllers do not converge on the same general objective , they will potentially act as separate controllers114 (the corollary being that where they do converge on such a same general objective, they are likely to be joint controllers). As noted previ ously, the three main AIaaS providers service agreements each specify that customer data may be used for supplementary processing (in some cases, this may be on an opt-in or out -out basis; in others, it may be covered by non -negotiable terms). Before usin g an AI service, customers necessarily agree to the terms of the relevant service agreements. Unless customers have opted -out of supplementary processing (or chosen not to opt -in, where that is possible), they may have agreed that the provider can their in put data for that processing. If so, they will have agreed (implicitly or explicitly) to use the AI service on the basis of serving the additional purpose of facilitating the provider s supplementary processing. In that case, where supplementary processing takes place, the purposes of the customer s and provider s processing will converge: providing the AI service to the customer and facilitating the provider s supplementary processing. Though they have not made a common decision (that is to say, they have not decided on a shared purpose), their converging decisions that the processing chain should serve two purposes to benefit each of them mean that they will have jointly determined the processing s purposes and means115. Providers and customers would th erefore in those circumstances be joint controllers for the transfer and (potentially116) analysis stages of the AIaaS processing chain. 112 See, for example, FashionID para 80; Wirtschaftsakademie paras 34 -39; European Data Protection Board (n , 113 European Data Protection Board (n 10 , 114 European Data Protection Supervisor EDPS Guidelines on the concepts of controller, processor and joint controllership under Regulation (EU) 2018/1725 (, 24; see also Article 29 WP (n , 17 -115 See, by analogy, FashionID para 80 -81, where Fashion ID s (implicit) acceptance of Facebook s terms for embedding a social plugin to benefit FashionID on Fashion ID s website including that personal d ata would be generated by the plugin and would be transmitted to Facebook for processing for Facebook s own purposes was taken to mean that they jointly determined the purposes of processing . See also, by analogy, Wirtschaftsakademie paras 34 -39, where W irtschaftsakademie s implicit acceptance that administering a fan page for their own benefit would also permit Facebook to process data for its own purposes was taken to mean that they similarly jointly determined the purposes of processing. 116 Depending on whether the supplementary processing involves the results or some other aspects of the analysis stage of the AIaaS chain. Forthcoming in Computer Law & Security Review - 30 - If, however, providers use customer data for supplementary processing even after the customer has opted -out or not opted -in (where that choice has been offered), the provider will be a separate controller for those stages. Questions of that processing s lawfulness aside, the customer s purposes would then not converge with the provider s, and they would not therefore jointl y determine the purposes and means. Figure Role assignment in the AIaaS chain where providers do influence the processing of customer data for their own purposes (i.e. to facilitate supplementary processing, here using input data and model outputs). Customers are controllers for the transfer and analysis stages, and are also controllers (either solel y or jointly with others) for the collection stage and likely also for any subsequent processing that they perform. Providers are joint controllers with customers for the transfer and analysis stages and processors for the return stage. Whether providers engaging in supplementary processing are joint controllers with customers matters, as joint controllers share responsibility for various aspects of processing and must enter into formal arrangements to divide those responsibilities, including around fulfilling data subject rights117. They must make the essence of this arrangement available to data subjects118. Yet, at the time of writing, none of the service agreements of Amazon, Microsoft, or Google provide for such a situation. Instead, as previously dis cussed, they each envisage themselves as processors for their (controller) customers (or, in the case of Microsoft, for a minority of services, as an independent i.e. separate controller). However, even following FashionID , it remains unclear exactly what the consequences are 117 GDPR art 118 GDPR art 26( . Forthcoming in Computer Law & Security Review - 31 - for joint controllers who do not properly arrange their relationship119. GDPR itself does not specify any particular enforcement powers relating to joint controllership, though the corrective powers afforded to supervisory authori ties including warnings, orders to comply, and bans on processing120 and significant fines121 would be available. Controllership for supplementary processing While our analysis indicates that providers who engage in supplementary processing are joint controllers with customers for the transfer and analysis stages of the AIaaS processing chain, we argue that providers are sole controllers for the supplementary p rocessing itself. Again, it is important to distinguish here between (a) processing of customer data undertaken as part of the AIaaS chain (i.e. to provide the AI service to the customer and to facilitate supplementary processing) and (b) processing of cu stomer data undertaken separately by the provider as part of their supplementary processing. The latter set of operations will often involve data logically separate from (and often chronologically after) the AIaaS processing chain. As such, supplementary p rocessing forms its own separate processing chain, serving only the provider s purposes (of course, in a more abstract sense customers may also benefit from general improvements to the providers service). Though engaging in this supplementary processing m akes the provider a joint controller for the transfer and analysis stages of the AIaaS chain, the customer cannot be a controller for the separate supplementary processing chain as they influence neither its purposes nor its means122. As such, only the prov ider can be a data controller for that supplementary processing. 119 Ren Mahieu, Joris van Hoboken, and Hadi Asghari , Responsibility for Data Protection in a Networked World: On the Queston of the Controller, Effective and Complete Protection and its Application to Data Access Rights in Europe ( 10 Journal of Intellectual Property, Information Technology and E -Commerce Law 120 GDPR art 121 GDPR art 86( . 122 See, by analogy, Fashion ID para s 76 and 85 , where the Court found that it would be impossible for Fashion ID to determine purposes and means for Facebook s subsequent and separate processing undertaken for Facebook s own purposes. Forthcoming in Computer Law & Security Review - 32 - Legal bases for processing Controllers must have a valid legal basis for processing personal data123. As controllers for the entire AIaaS chain, customers will require a legal basis for proc essing throughout. In cases where providers are controllers for the transfer and analysis stages (i.e. where they engage in supplementary processing), they will also require a legal basis for processing in those stages, as well as for their supplementary p rocessing. GDPR sets out several possible legal bases124. For ordinary personal data, six bases for processing are available. Those likely to be relevant for commercial AIaaS include where the data subject has given their consent to processing for specifi c purposes125; where processing is necessary for the performance of a contract to which the data subject is party126; and where processing is necessary for the purposes of the legitimate interests of the controller (unless those interests are overridden by t he interests or fundamental rights and freedoms of data subjects)Processing special category data is prohibited unless one of ten specified exemptions to that prohibition applies. Generally only two are likely to be available for AIaaS where the data subject has given e xplicit consent to processing for specified purposes128, and where processing is necessary for reasons of substantial public interest and is done on the basis of an EU or domestic law that meets certain requirements129. We discuss first t hese potential legal bases for processing in the AIaaS chain, and then for providers supplementary processing. We focus here on special category data for two reasons. First, the available bases for processing special category data are more restrictive a nd potentially more difficult to fulfil. Second, the difficulties in distinguishing non -personal, ordinary personal, and special category data, and the additional requirements for special category data and potentially 123 GDPR art s 5, 6, and 124 GDPR arts 5, 6, and 9 . 125 GDPR art 6((a) . 126 GDPR art 6((b) . 127 GDPR art 6((f) . 128 GDPR art 9((a) . 129 GDPR art 9((g); the requirements are that the law is proportionate to the aim purs ued, respects the essence of the right to data protection, and provides for suitable and specific measures to safeguard the fundamental rights and interests of the data subject. Forthcoming in Computer Law & Security Review - 33 - serious penalties arising from unlaw ful processing130, mean the safest approach for providers may be to treat all input data as special category data. Though this is potentially quite burdensome for providers, the need for this precautionary approach arises directly from the nature of turn -key AI services and the practices of providers themselves as we argue above, their supplementary processing positions them as controllers for certain stages of the AIaaS chain. They do not, however, require their own legal basis for processing (and therefo re do not need to take such a precautionary approach) where they do not engage in supplementary processing. In that case, following our preceding analysis, they are processors acting under the instruction of their controller customers. Providers should therefore balance any benefits they realise through supplementary processing against the legal risks they may incur as a result. Where special category data is in fact processed in the AIaaS chain, customers and providers as controllers will usually need to have the explicit consent of data subjects for the purposes for which the AIaaS processing chain is executing131. This is because the restrictive nature of the substantial public interest exemption means that it will generally be inapplicable to AIaaS. Fo r that exemption to apply, the processing must be necessary (there must be no alternative or more privacy -preserving means of achieving the same outcome132), and it must be based on an appropriate EU or domestic law. In the UK, for instance, this law is the Data Protection Act 2018, which sets out 22 conditions in which substantial public interest might apply133. These conditions are narrow and generally require controllers to meet certain criteria (and many, such as administration of justice and parliamen tary purposes 134, political parties 135, and elected representatives responding to requests 136, are unlikely to be relevant). Thirteen of those conditions require controllers to justify not obtaining explicit 130 GDPR art 58, art 131 This is not to say that explicit consent is the only ground on which providers can rely for any of their processing; for ordinary personal data, they may be able to use consent, contract, and legitimate interests. We do, though, argue that, due to the d ifficulties in distinguishing special category data, this would involve considerable risk for providers. 132 European Data Protection Supervisor , Assessing the necessity of measures that limit the fundamental right to the protection of personal data: A Tool kit ( <h - 04-11_necessity_toolkit_en_0.pdf > accessed 13 November 133 Data Protection Act 2018 ( DPA 2018 ), sch 1, pt 134 DPA 2018 , sch 1, pt 2, para 7 . 135 DPA 2018 , sch 1, pt 2, para 136 DPA 2018 , sch 1, pt 2, para Forthcoming in Computer Law & Security Review - 34 - consent, and 11 require controllers to demonstrate the substantial public interest137. The Information Commissioner s Office (the UK s supervi sory authority) emphases that processing must be in the public interest not simply a private or commercial interest and that controllers should be able to make specific arguments about the concrete wider benefits of [their] processing The substantial public interest exemption will therefore not apply to most commercial use of AIaaS. Explicit consent thus appears to be the only exemption available in most circumstances. This raises the question of when and by whom explicit consent should be obtained from data subjects. In FashionID , the CJEU held that it was a joint controller s duty to obtain consent from data subjects for the set of processing operations for which it is actually a controller, but not for prior or subsequent operations139. In that case, the operations for which FashionID was a joint controller took place at the beginning of the processing chain. The Court determined that consent must therefore be obtained by FashionID prior to performing those operations140. According to the Court, because it was the data subject visiting FashionID s website that triggered the rest of the processing chain, it was incumbent upon FashionID itself rather than on controllers involved later in the chain to obtain consent so as to ensure timely protection of the data subject s personal data141. The analogous stage of the AIaaS processing chain is the collection stage, for which the customer is the sole controller. As such, it would be incumbent on the customer to obtain explicit 137 See table at Information Commissioner's Office, 'Guide to the General Data Protection Regulation: What are the substantial public interest conditions?' < r-organisations/guide -to-data - protection /guide -to-the-general -data -protection -regulation -gdpr/special -category -data/what -are-the- substantial -public -interest -conditions > accessed 13 November 2020 138 Information Commissioner's Office, 'Guide to the General Data Protection Regulation: What are the substantial public interest conditions?' < -organisations/guide -to-data -protection/guide - to-the-general -data -protection -regulation -gdpr/special -category -data/what -are-the-substantial -public - interest -conditions > accessed 13 November 2020 ; Some private companies that wish to deploy live facial recognition systems have argued that they can rely on the 'substantial public interest' ground for preventing and detecting crime. We find this argument unconvincing. While preventing or detecting cri me may be in the public interest, the deployment of facial recognition does not, in our view, meet the necessity test (unless we are to believe that there has until now been an epidemic of crime that only augmenting the already extensive use of CCTV with f acial recognition systems deployed by a private company can possibly prevent). Nor do we agree that deploying facial recognition systems in a shopping centre, for example, is in the substantial interest of the public, rather than principally in the interes ts of the private owner of the shopping centre and their commercial tenants. 139 FashionID paras 100 -140 FashionID para 102 . 141 FashionID para 102 . Forthcoming in Computer Law & Security Review - 35 - consent from the data subject for processing special category data. As the customer is also a controller for the entirety of the AIaaS processing chain, they should obtain explicit consent from data subjects for all processing operations in that chain142. GDPR says that whe re different purposes are pursued by the same processing such as where the AIaaS processing chain serves the additional purpose of facilitating supplementary processing consent should be given for each of those purposes143. The customer would in that ca se need to obtain separate explicit consent from data subjects for the processing in the AIaaS chain for each of the purposes of providing the service to customers and of facilitating providers supplementary processing. This presents some complications for AIaaS. Aside from whether any explicit consent meets GDPR s high standards144, actually obtaining explicit consent from data subjects may not be straightforward. Customers will likely be able to seek that consent where data subjects are active third -parties (i.e. where data subjects use aspects of a customer s application which rely on AIaaS for functionality). They will also likely be in such a position where they directly interact with passive third -parties in some way (where, for instance, data subjec ts use aspects of a customer s application which do not rely on AIaaS for functionality and the customer is using AIaaS to analyse their behaviour). But it is not clear how customers could obtain explicit consent where they do not directly interact with pa ssive third parties (where customers are, for instance, directly or indirectly surveilling a physical space). Providers who are joint controllers for the transfer and analysis stages of the AIaaS chain having little to no interaction with third -parties may choose to rely on the express assurances of customers that explicit consent has been obtained from data subjects for their processing. They would, though, risk processing special category data unlawfully where the customer has in fact failed to obtain valid explicit consent. Providers may be able to mitigate this risk somewhat by requiring some form of evidence from the customer that the data subject has actually explicitly consented. However, though Google Cloud does require customers to 142 This situation would be significantly complicated where active third -parties are also controllers for the AIaaS processing. Precisely when and by whom explicit consent should be obtained (where necessary) is likely in that case to depend largely on the specific circumstances; as such, we do not explore this scenario here. 143 GDPR recital 32; European Data Prote ction Board Guidelines 05/2020 on consent under Regulation 2016/679 (version 1 , 144 GDPR art 4(, art 7, recital 32; Article 29 Data Protection Working Party , Guidelines on consent under Regulation 2016/679 ( 17/EN WP259 rev.01, 18; see also European Data Protection Board (n 14 , Forthcoming in Computer Law & Security Review - 36 - obtain and ma intain any consents necessary to permit processing145, none of the service agreements of the three main providers require that any consents obtained by customers be evidenced or indeed any assurances relating to legal bases for processing (perhaps because providers consider themselves processors, not controllers). For providers supplementary processing146, things are yet more difficult. As discussed, due to the nature of AI services, providers will likely have less of an idea than their customers whether they are processing special category data (except where implicit, such as for biometric services). If they are, they may in theory (depending on the circumstances and subject to the qualifications relating to this ground for processing discussed previousl y) rely on the substantial public interest exemption for monitoring for criminal use of their AI service, or (perhaps) for reducing bias in their models147, but not for other forms of supplementary processing148. However, the generic, turn -key nature of most AI services and the need to avoid unlawful processing again presents a challenge. As providers will in many cases not know whether the customer data they retain for supplementary processing originates with end -users of customers applications, and will si milarly have no reliable way of determining whether it is special category data (if indeed it is personal data at all), we argue that providers should in practice rely on the explicit consent basis for that processing. However, as we note previously, cus tomers obligation to obtain explicit consent extends only to processing for which they are actually a controller149 i.e. to processing in the AIaaS chain itself and so would not cover the provider s supplementary processing. Providers would therefore n eed to have customers obtain evidenced, valid explicit consent on their behalf from the data subjects for each purpose pursued by their supplementary processing, and make that evidence available to the provider so that consent can be demonstrated by the pr ovider if necessary. However, as providers do not provide customers with detailed 145 Google Cloud, Terms of Service section 2 < > accessed 13 November 146 Bearing in mind that if they are not engaging in that processing then they will be data processors and the question of them obtaining explicit consent for any of their processing becomes moot. 147 The UK s Data Protection Act 2018, for instance, includes e quality of opportunity or treatment as a condition for the substantial public interest exemption ( DPA 2018 , sch 1, pt 2, para . 148 Although if only ordinary personal data is being processed, then providers may for some purposes be able to rely on the ground relating to the legitimate interests of the data controller (GDPR art 6((f)). 149 FashionID paras 100 -Forthcoming in Computer Law & Security Review - 37 - information about their supplementary processing, customers are not positioned to obtain fully informed, specific consent from data subjects, as GDPR requires150. Moreover, fo r consent to be freely given another GDPR requirement151 data subjects must have the real option of refusing consent to the provider s supplementary processing without suffering any detriment or loss of service152. Yet, in many cases, providers do not gi ve customers the opportunity to opt in or out of supplementary processing, let alone to do so on each service request (as would be necessary to account for the wishes of each data subject). Further, though the main AIaaS providers all engage in some suppl ementary processing, and though they may ask customers for permission to use their input data to improve models (through an opt -in or opt -out), it is not clear that those providers have implemented the necessary measures to obtain the valid explicit consen t of data subjects . Again, AWS, Microsoft Azure, and Google Cloud service agreements contain no provisions relating to evidencing consent. While this is not necessarily a problem for ordinary personal data (where non -consent grounds may be available153), we therefore suggest that supplementary processing that actually does involve special category data is in many cases currently being done unlawfully. Indeed, t hough not all data will be special category data, much supplementary processing will involve special category data in some way . As a result, as sole controllers for their supplementary processing, AIaaS providers are potentially engaged in widespread, systematic, and sustained violations of GDPR in particular, of the first data protectio n principle154, of the prohibition on processing special category data without a valid legal basis155, and, potentially, of the obligations to inform data subjects about their processing and data subjects rights relating to it156. They have, as a result, lef t themselves 150 GDPR art 4(, a rt 7, recital 32 . 151 GDPR art 4(, art 7, recital 32 . 152 GDPR art 7(, recitals 42 -43; though refusal of explicit consent to the processing necessary to provide the core functionality of the AI service (i.e. operations performed in the AIaaS processing ch ain for the purpose of providing the service) could quite reasonably result in withdrawing the service from the data subject. 153 Potentially legitimate interests and contract, depending on the circumstances (GDPR art . Note, though, that AIaaS providers w ould still need to provide information to data subjects about their supplementary processing and data subjects rights relating to it, including, where the provider s legitimate interests are relied on, inform data subjects of their right to object to that processing (GDPR art . 154 GDPR art 5((a) . 155 GDPR art 156 GDPR art Forthcoming in Computer Law & Security Review - 38 - open to enforcement action by supervisory authorities (including, but not limited to, warnings, fines of up to 4% of global revenue157, or, significantly for a major cloud provider, bans on processing158), not to mention judicial remedies159 and claims in tort for compensation for any material or non -material damage resulting from the processing160. Intermediary liability AI services are often offered turn -key , with few checks by providers as to the identity or intentions of customers or the ac tual purposes to which they are put in practice. There is therefore potential for misuse to support illegal activity. This could involve a wide range of possible activity for example, financial crime, fraud, harassment, intellectual property infringement s, or liability arising in tort for harm caused by use of services. While customers are likely directly liable for illegal use of AI services, there is also the question of whether providers may face some liability for enabling, facilitating, underpinning or, indeed, undertaking that illegality. The EU s E -Commerce Directive offers providers of information society services ( any service normally provided for remuneration, at a distance, by electronic means and at the individual request of a recipient o f services some protection from liability for illegal user activity. Under the Directive, protection is potentially available to service providers for any of three activities: (i) acting as a mere conduit ( transmitting information between individuals at their direction for example, ISPs and messaging services )162, (ii) caching (in this context, a technical activity associated with acting as a conduit)163, and, on a more qualified basis, (iii) hosting 164 (storing information provided by recipients o f the service). Whether AIaaS 157 GDPR art 83( . 158 GDPR art 159 GDPR art 160 GDPR art 161 Directive 98/34/EC of the European Parliament and of the Council of 22 June 1998 laying down a procedure for the provision of information in the field of technical standards and regulations [1998] OJ L 204 /37 ( Technical Standards and Regulations Directiv e ) art 1 (as amended by Directive 98/48/EC of the European Parliament and of the Council of 20 July 1998 amending Directive 98/34/EC laying down a procedure for the provision of information in the field of technical standards and regulations [1998] OJ L 2 17/; see also E - Commerce Directive recital 18 . 162 E-Commerce Directive art 163 E-Commerce Directive art 164 E-Commerce Directive art Forthcoming in Computer Law & Security Review - 39 - providers are eligible for protection will depend on (a) whether AIaaS is an information society service, and, if so, (b) whether providers activity falls into one of these three categories. If they are eligible for protectio n, whether they actually have that protection will depend on meeting any specified conditions. As with other cloud services, AIaaS is an information society service. AIaaS is normally provided for remuneration (providers typically bill customers for thei r usage). It is provided at a distance by electronic means (over a network, typically the internet). It is provided at the individual request of recipients (defined in the Directive to include any natural or legal person who uses an information society ser vice165; either customers or active third -parties). AIaaS is thus a service normally provided by remuneration, at a distance, by electronic means, at the individual request of recipients. AIaaS providers are therefore service providers within the scope of the Directive166. AIaaS providers do not act as mere conduits. To do so, they would need to transmit information provided by recipients (customers or third -parties) to others without selecting or modifying the transmitted information167. As AIaaS providers analyse input data using their models and then return novel outputs to recipients, they do not simply transmit that data without modifying it. Nor, consequently, are they caching in connection with acting as a conduit. They are therefo re not eligible for protections available for acting as a mere conduit or for caching. More traditional cloud services are often covered by the hosting protection168. To qualify, storing information provided by recipients is a necessary but not sufficient aspect of the service provider s activity. In storing that information, the service provider must, according to the CJEU, act neutrally in relation to the information, and their activity must be passive and merely technical169. AIaaS providers are not ther efore engaged in hosting as defined; 165 E-Commerce Directive art 2(d). 166 E-Commerce Directive art 2(b): any natural or legal person providing an infor mation society service . 167 E-Commerce Directive art 168 Jasper P Sluijs, Pierre Larouche, and Wolf Sauter ( 'Cloud Computing in the EU Policy Sphere' ( 3 Journal of Intellectual Property, Information Technology and e -Commerce Law 169 Case C -324/09 L Or al SA and Others v eBay International AG and Others [2011] ( L Oreal v eBay ) para 113; Cases C -236/08 Google France SARL and Google Inc. v Louis Vuitton , C-237/08 Google France SARL v Viaticum Forthcoming in Computer Law & Security Review - 40 - they do not store information provided by recipients as part of providing the AI service in any meaningful sense (though some data may be stored as part of the provider s supplementary processing, this is not part of th e service itself). Nor is their activity passive or merely technical they actively analyse customer input data using their own complex algorithmic systems to probabilistically generate new information which they then return to customers; in doing so they often also directly benefit both financially and, where supplementary processing is involved, by improving the models that they offer. We therefore argue that AIaaS providers have no available protection from liability for illegal activity carried out using their AI services. However, even if this is incorrect and AIaaS providers do somehow act as hosts, we argue that they still operate beyond the Directive s protections. The CJEU has held that, to avail of protection, service providers must act as an intermediary service provider170. The Directive s recitals state that liability protection is limited to activities that are of a mere technical, automatic and passive nature, which implies neither knowledge of nor control over the information which is transmitted or stored According to the CJEU, this means that service providers are not intermediary service providers and therefore not eligible for protection where they (a) take an active role in relation to information that would (b) give them e ither knowledge of or control over that information172 (note that control here does not relate to being a controller in data protection law) . It is not our position that AIaaS providers have actual knowledge of input data transmitted to them by customers. However, providers use systems they have designed, developed, and deployed to undertake the analysis of customer data, inputted in line with the provider s specifications, to produce new information (in line with the system building and model training und ertaken by the providers engineers), and to return that new information generated by the providers to customers as part of the commercial service offered by those providers. Providers do thus exercise control over that data. SA and Luteciel SARL , and C -238/08 Google France SARL v Centre national de recherche en relations humaines (CNRRH) SARL and Others [2010] ( Google France and Google ) para 170 E-Commerce Directive s 4; Google France and Google para 171 E-Commerce Directive recital 42 . 172 E-Commerce Di rective recital 42; Google France and Google para 114; L Oreal v eBay para Forthcoming in Computer Law & Security Review - 41 - AIaaS providers are therefore not protected from liability for illegal activity of customers using their AI services, whether they are considered to somehow host information or not. AIaaS as a distinct activity developed after the E -Commerce Directive s adoption in 2000 and thus sit s outside its liability shields. This is not to say that providers are necessarily liable for customers illegality, but the fact that AIaaS providers analyse customer inputs and then return novel outputs to customers means that it is information produced by those providers on which the illegal activity is based and depends. As AIaaS enables, facilitates, and underpins application functionality, providers analyses and outputs are integral rather than incidental to that activity. Without the Directive s pro tection, providers are thus in a position of significant uncertainty as to whether, when, and to what extent they could be liable for illegal activity by customers using their services. Challenges for ex isting law The issues identified above are potentially problematic for AIaaS providers and customers alike. But they also highlight that these legal concepts and arrangements originated before today s more complex, networked, and dynamic processing architectures and relationships emerged. The polit ical economy of data processing also differs, with new information economy business models, increasingly dominant platforms, and heavily asymmetric power dynamics. This calls into question whether existing frameworks can adequately address these newer deve lopments. With the Data Protection Directive, the concepts of controller and processor entered EU law at a time when processing was understood in a more linear way173. In principle, one entity would control the processing architecture174 and may delegate some or all aspects to another, who would be essentially subordinate. In GDPR, this dominant -subordinate understanding of the controller -processor relationship remained essentially unchanged: controllers determine how and why processing takes place and are largely responsible for compliance; processors act under their instruction and on their behalf175. However, as 173 Omer Tene , Privacy Law s Midlife Crisis: A Critical Assessment of the Second Wave of Global Privacy Laws ( 74 Ohio State Law Journal 1217 . 174 Mahieu et al (n 1 . 175 Although GDPR did assign more compliance obligations to processors than in the Data Protection Directive. Forthcoming in Computer Law & Security Review - 42 - others have noted176, the increasingly complex, networked, and dynamic nature of contemporary processing environments challenges this understandin g of roles and responsibilities and thus data protection law s ability to effectively protect data subjects. In its cases on controllership, the CJEU has attempted to shape data protection to these newer realities177. Leaving it to the Court to develop the law as disputes come before it, however, has inevitably resulted in a piecemeal response. The effect has been failure to comprehensively account for how the law can adapt. Following our controllership analysis, in some circumstances there could be three potential controllers for some of the AIaaS chain the provider, the customer, and a third -party. This could arise where customers provide services to other entities who are themselves controllers (for example, where the customer offers an applicati on to a third -party that is itself processing data for its own commercial activities). What roles and relationships they take three separate controllers; three joint controllers; two joint controllers and one separate controller; two separate sets of two joint controllers; a processor and two separate controllers; a processor and two joint controllers; or two processors and a sole controller is heavily circumstantial. Neatly dividing responsibilities may be impracticable. This arrangement could conceiva bly be even more complicated, since, in contemporary environments, third -parties applications may themselves connect with those of yet others178. This illustrates the challenges of assigning responsibilities under current law in these complex, networked, d ynamic environments. Where many third -party controllers transiently engage with the customer s application, and thus also with the provider s service, it is not clear how roles and responsibilities could reliably be assigned to ensure a high level of prote ction. Nor is it clear how data subjects could understand these data processing relationships or effectively exercise their rights against each controller, or how supervisory authorities could obtain meaningful insight into these processing activities to 176 Seda G rses and Joris van Hoboken , Privacy after the Agile Turn in Jules Polonetsky, Omer Tene, and Evan Selinger ( eds) Cambridge Handbook of Consumer Privacy (Cambridge University Press ; Mahieu et al (n ; Lilian Edwards, Mich le Finck, Michael Veale, Nicolo Zingales , 'Data sub jects as data controllers: a Fashion(able) concept?' [2019] Internet Policy Review ; Christopher Millard, Christopher Kuner, Fred H Cate, Orla Lynskey, Nora Ni Loideain, and Dan Jerker B Svantesson 'At this rate, everyone will be a [joint] controller of personal data!' ( 9 International Data Privacy Law 177 Jehovah s Witnesses , Wirtschaftsakademie , FashionID . 178 See, for example, Jatinder Singh, Christopher Millard, Chris Reed, Jennifer Cobbe, and Jon Crowcroft , Accountability in the I oT: Systems, Law, and Ways Forward ( 51 IEEE Computer Forthcoming in Computer Law & Security Review - 43 - oversee them. While key concepts and principles of data protection law developed at a time when data processing involved relatively stable relationships between data subjects, controllers, and processors (for instance, the then -emerging automated processing of employee records), they do not easily translate to the relatively unstable nature of many contemporary networked arrangements. Moreover, the dominant -subordinate understanding of controller -processor relationships, still depicted in GDPR, does not ref lect the actual power dynamics in many contemporary environments. Though customers are sometimes afforded some choice, turn -key AI services are offered generically as determined by the provider s standard form contracts179. These contracts may affect the le gal position of customers regarding joint controllership and other responsibilities, yet customers can usually do little if anything to influence them180. Large companies with technical expertise and market power will of course use AI services, perhaps on a consultancy basis involving close cooperation with the provider, though still typically through the provider s specified contractual arrangements (effectively positioning providers to influence activities of large firms in many other sectors). But it is a lso likely that many customers will be small organisations with little technical knowledge or expertise, much less the power that larger companies may possess. In that event, are we really to believe that, as the law would have it, the customer is the domi nant party in that relationship, with control over the data processing architecture, and Amazon, Google, or Microsoft are merely its subordinate? In reality, there may be sizable technical, financial, informational, and power asymmetries between the two, a nd providers will exercise significant influence over how and in what ways data is processed in AI services and for what kinds of purposes they can be used. GDPR is silent on what level of influence over processing actually constitutes determining its mea ns, and the CJEU is yet to consider that question in any depth. As we indicated previously, the Article 29 Working Party has indicated that some aspects (such as relating to specifics of hardware and software) can be delegated to processors, whereas essen tial elements relating to questions of the core of lawfulness of processing what is processed, 179 Millard (n , chs 3 and 4 . 180 Millard (n . Forthcoming in Computer Law & Security Review - 44 - for how long, and so on are inherently reserved to controllers181. The European Data Protection Supervisor182 and the European Data Protection Board both take the same view183. Yet, in AIaaS, providers merely processors under a traditional understanding, unless they undertake supplementary processing greatly influence the analysis of input data and thus the service's outputs through development of their models (design, training, testing, and so on)Moreover, the model -driven nature of AI services means that how they operate, and thus how they drive customers application functionality, is inherently probabilistic and depends on the ML processes underta ken by the provider in building the service. This is unlike more traditional cloud services, which typically support application deployment rather than underpinning particular functionality, and where the specifics of what a service supports are (or can be ) better defined. As such, providers engineering of their models plays a key role in realising customers application functionality. At the same time, customers are largely prevented from understanding how this engineering occurs or how it might impact th eir functionality; given the statistical nature of ML models, the complexity of AI systems and engineering processes, intellectual property restrictions, and providers general lack of transparency regarding the specifics underpinning their services. Ther e is also the question of who actually determines the purposes of processing in environments such as AIaaS. Consolidation around three providers means that each can influence the shape of the AI services market as a whole, and thus the range of purposes fo r which AIaaS may be used. Smaller providers do exist, but the fact that the major AIaaS providers are also the dominant cloud providers means that customers may be locked in to a provider s ecosystem. In other cases, the customer s choice of provider ma y depend on which provider permits their intended purpose, with the effect that providers policy decisions can in turn influence the means of processing (some providers, for example, 181 Article 29 WP (n 7 , 14-182 European Data Protection Supervisor ( n 10 , 183 European Data Protection Board, Guidelines 07/2020 on the conc epts of controller and processor in the GDPR (, 13 -15 184 The UK s Information Commissioner s Office, for instance, suggests that where providers decide questions relating to model design and development they may be considered to be data controllers (Information Commissioner's Office, 'Guide to Data Protection: What are the accountability and governance implications of AI?' <https: //ico.org.uk/for -organisations/guide -to-data -protection/key -data -protection -themes/guidance -on- ai-and-data -protection/what -are-the-accountability -and-governance -implications -of- ai/#howshouldweunderstand > accesse d 13 November Forthcoming in Computer Law & Security Review - 45 - choose not to offer facial recognition services185). Additionally, barrie rs to entry for new AI technologies technical expertise, large quantities of data, and significant financial resources and the fact that AIaaS providers typically make available the state -of-the-art at low cost, will likely mean that AIaaS may be the o nly way that many companies can realistically integrate AI capabilities into their applications and workflows186. In effect, three dominant companies would make decisions that influence how and why many other companies in many other sectors wishing to use AI could do so. None of these factors may on their own mean that providers definitively determine the purposes of any particular processing, but they do together provide something akin to a terroir187 for processing. That is to say, these factors, determin ed by providers, together influence the general environment in which AIaaS operates and the bounds of what purposes are possible, with potentially consequential effects on customers applications. The law s traditionally narrower understanding of what it m eans to influence the purposes and means of processing cannot contend with these more structural aspects, which, given the political economy of contemporary data processing, are inevitably bound up in power dynamics and other factors beyond customers cont rol. Data protection may be able to confront these challenges. As AIaaS enables, facilitates, and directly underpins customers application functionality (rather than just supporting deployment), a reasonable position for the law to take may be that, wher e AI services are offered turn -key188, providers always take part in determining the purposes and means of processing in the AIaaS chain, and are therefore always controllers, regardless of any supplementary processing. This would result from providers (i) developing the model and 185 BBC News, 'IBM abandons 'biased' facial recognition tech' (9 June 2020 ) < -52978191 > accessed 13 November 2020 ; Abner Li 'Google Cloud won't sell facial recognition tech yet, 'working through' policy questions' (13 December < -not-selling -face -recognition/ > accessed 13 November 186 Seyyed Ahmad Javadi, Richard Cloete, Jennifer Cobbe, Michelle Seng Ah Lee and Jatinder Singh , Monitoring Misuse for Accountable Artificial Intelligence as a Service ( Proceedings of the 2020 AAAI/ACM Conference on AI, Et hics, and Society (AIES ', ACM, New York, NY, USA, 2020 < > accessed 13 November 2020 ; Seyyed Ahmad Javadi, Chris Norval, Richard Cloete, and Jat inder Singh, Monitoring AI Services for Misuse ( Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society (AIES , ACM, Virtual Event, USA, 2021 . 187 . 188 Services offered on a consultancy basis typically allow customers much greater latitude in defining their relationship with the provider and the service offered. Forthcoming in Computer Law & Security Review - 46 - supporting systems and thus determining its (probabilistic) analyses and outputs and the purposes for which it is intended, (ii) deciding which services to offer to customers (particularly where customers are locked in to the pro vider s cloud platform), and (iii) putting restrictions on use beyond prohibiting illegal activity. Perhaps, though, trying to squeeze complex, networked, dynamic processing chains and relationships like those in AIaaS into comparatively simplistic data pr otection frameworks with neatly delineated and assigned roles and responsibilities does not work at all. Data protection law attempts to apply the same understanding of roles and responsibilities across all data processing arrangements, from simple ones wi th a single controller and no processors through to complex, networked, multi -party scenarios. But this idealised understanding of data processing relationships largely ignores the transient, dynamic nature of many contemporary arrangements as well as thei r power asymmetries and the role of providers in society s technical infrastructure. As a result, data protection law increasingly does not bear close connection with reality. To confront these developments, we should perhaps consider whether specific data processing frameworks a lex specialis would be more appropriate for certain sectors or for certain activities. Legal reform would also provide an opportunity to address the gap in liability protection identified above. Some protection for providers i s plainly desirable, lest it become essentially impossible to offer AI services without significant legal risk. But providers role in enabling, facilitating, and underpinning customer s application functionality arguably calls for greater consideration of whether the kinds of protection afforded by the E -Commerce Directive would be suitable for AIaaS. As with data protection law, that Directive originated at a time when service providers largely took a more passive role than in AIaaS. Perhaps, given AIaaS providers more active involvement in application functionality, they should face some liability where they have not proactively taken steps to mitigate against the possibility of illegal activity. These steps could involve, for instance, ending the practi ce of offering AI services turn -key and instead operating an expanded customer onboarding process, with know your customer -esque background checks, customer consulting and application vetting, requiring specifications of envisaged use in line with well -defined and context - specific terms of service provisions, and then verifying compliance and adherence to service Forthcoming in Computer Law & Security Review - 47 - agreements on an ongoing basis189. It would of course be impossible to prevent all illegality, and unreasonable to expect providers to do so, but if as seems likely AIaaS will be how many people and organisations use AI technologies in future, offering some protection to providers in exchange for taking mitigating steps such as these could help reduce its potential for illegal use by customers. Considerations for legal reform While the starting point for legal reform is to address the problems with data protection law and the gap in liability protection for providers, discussed above, legislators, regulators, and policymakers should also consider issues arising from AIaaS and the activities, responsibilities, and role of providers beyond data protection and intermediary liability. The emergence of AIaaS as an internet -enabled commercially supplied smart infrastructure service, positioni ng providers at the core of new functionality in both physical and virtual spaces across society, is relevant to wider policy discussions around internet and platform law and regulation as well as the regulation of AI. As we now discuss, AIaaS potentially both exacerbates various problems with AI and brings new ones, in a way that is underappreciated and requires attention from legislators, regulators, and policymakers alike. In discussing issues around AI, it is important to understand that these technolo gies are not simply impartial or passive tools190, nor is offering them commercially as a service a neutral 189 Javadi et al (n 1 ; these would be similar to some of the checks undertaken in, for instance, the banking and financial services industry 190 Engin Bozdag , Bias in algorithmic filtering and personalisation [2013] Ethics in Information Technology 15, 209-227 < -013-9321 -6> accessed 13 November 2020 ; Tarleton Gillespie , The Relevance of Algo rithms In Tarleton Gillespie, Pablo J Boczkowski, and Kirsten A Foot (eds) , Media Technologies: Essays on Communication, Materiality, and Society (MIT Press ; Robin Hill , 'What An Algorithm Is' ( 29 Philosophy and Technology 35 < -014-0184 -5> accessed 13 November 2020 ; David Beer , 'The Social Power of Algorithms' ( 20 Information, Communication & Society 1 < > accessed 13 November 2020; Natascha Just and Michael Latzer , Governance by algorithms: reality con struction by algorithmic selection on the Internet ( 39 Media, Culture & Society 2, 238 -258 < journalCode=mcsa > accessed 13 November Forthcoming in Computer Law & Security Review - 48 - act. They are contextual and contingent in nature, embedded within and a product of the wider socio -technical context of their development, deploymen t, and use191. Computer code establishes boundaries, norms, and capabilities, acting as architecture that permits or constrains behaviour192, empowering some and potentially disempowering others. Systems design encodes the goals, priorities, and normative a ssumptions of their designers and engineers, managers, and organisations. Through deployment and use they translate those goals, priorities, and normative assumptions into reality, producing societal effects and downstream consequences193. The normative nat ure of AI systems and their potentially profound and long -lasting impact on society when embedded into physical and virtual infrastructure requires careful attention. That technologies such as AI produce societal effects confers significant power on thos e who control them. As AI services enable, facilitate, and underpin functionality in customer applications, AIaaS providers can engage in private ordering with potential regulatory effects194 on the behaviour of people and organisations using their servic es (either as customers or as third -parties). For the most part, providers determine the responses (if any) to the issues discussed below; they decide who gets access to what kind of services, on what terms, and for which purposes. As AIaaS plays an increa singly important societal infrastructural role, the lack of independent, public, accountable regulation and oversight will only become more problematic. Yet, f acial recognition aside, many regulatory and policy discussions have largely ignored the market, infrastructural, and societal effects of AIaaS. The EU s proposed Artificial Intelligence Act195, for instance, covers AI services196 (among other things). This seeks to prohibit several activities (including law enforcement facial recognition and social sco ring by public authorities)197, while also addressing other particularly high risk activities through specific requirements intended to reduce that 191 Rob Kitchin , Thinking critically about and researching algorithms ( 20 Information, Communication & Society 192 Lawrence Lessig , Code, and Other Laws of Cyberspace ( 193 Kitchin (n . 194 Sylvie De lacroix , Beware of Algorithmic Regulation ( SSRN < > accessed 13 November 195 Proposal for a Regulation of the Europ ean Laying Down Harmonised Rules On Artificial Intelligence (Artificial Intelligence Act) And Amending Certain Union Legislative Acts ( Proposed Artificial Intelligence Act ). 196 Proposed Artificial Intelligence Act arts 1 -197 Proposed Artificial Intelligence Act art Forthcoming in Computer Law & Security Review - 49 - risk198. Yet the Act as proposed is largely silent on the market and infrastructural effects of a future where companies and organisations of all kinds primarily use AI on a services basis, with most underlying computing handled by a few dominant providers. In the absence of effective regulation of AIaaS, these companies will set exercise ever -growing power over the technical infrastructure that supports digital society. We now discuss four issues arising from AIaaS in particular: (i) the amplification of general ethical problems with AI through the scale that can be reached with AIaaS ; (ii) the data on which AIaaS models are trained ; (iii) the potential growth in AI -augmented surveillance facilitated by AIaaS ; and (iv) the potential for misuse and abuse of AI services. This is not a comprehensive overview of all potential concerns around AIaaS; rather , in accordance with the rest of this paper, we focus on some general issues relating to the activities , roles , and power of providers in AIaaS that are worthy of greater attention from legislators, regulators, and policymakers. We recognise , though, that AI services operate transnationally, challenging governance, regulation, and enforcement mechanisms. Note that t he processing chains and socio -technical processes around AIaaS are typically opaque, with providers making little information available. We d o not , however, make specific arguments for transparency of providers systems and processes . Though such transparency may be useful (we note that the proposed AI Act includes relevant transparency requirements199 and that transparency is the direction of travel of legal and regulatory frameworks ), transparency will not itself address the problems arising from AIaaS200. Greater transparency of supplementary processing, for instance, might offer more information abou t providers' use of input data . But it would not address the legal problems arising from supplementary processing we identify above: providers not properly arrang ing their legal relationship with customers as joint controllers, for example, or lacking mech anism s to obtain data subjects consent to supplementary processing of special category data. Similarly, increased transparency in providers processes would not address 198 Proposed Artificial Intelligence Act Title III. 199 Proposed Artificial Intelligence Act arts 11 -14, Title IV. 200 For a fuller exploration of the limitations of the transparency and accountability framing, see Frank Pasquale, The Second Wave of Algorithmic Accountability (201 Law and Political Economy Project < -second -wave -of-algorithmic -accountability > accessed 2 June Forthcoming in Computer Law & Security Review - 50 - the imbalances of power that produce the mismatch between the domina nt-subordinate mod el envisaged by data protection law and the reality of provider -customer relationships . Our focus here is instead on the need for interventions that address the problems we identify below , rather than on making the processes and systems that contribute to these problems more transparent. Ethical concerns and AIaaS Ethical concerns regarding AI have been much discussed in recent years. However, much of this discussion has implicitly assumed that companies and organisations will generally develop AI in -house. Less discussed is that AI services available cheaply at low marginal cost; with few checks o n the customer s identity, background, or intentions; requiring little technical expertise; and easily integrated into other cloud services will allow customers AI-augmented applications to realise functionality at a scale that may otherwise be impossib le. As a result, AIaaS potentially amplifies widely discussed problems relating to AI. For instance, the potential for biases against different population groups has been widely identified201. Bias is a significant problem in ML and can develop in various ways for example, training data could reflect structural societal marginalisation of people with particular characteristics (such as race, gender, disability, sexual orientation, and so on), or the biases and prejudices of system designers can be encoded in a model s representations and outputs202. Without mitigating steps, the model will inherit those biases and reproduce them in its operation. This could produce unlawfully discriminatory effects, potentially opening customers up to serious legal conseque nces. The consolidation in the AIaaS market, the scale at which those providers operate, and the scale that can be reached by customers applications potentially amplifies these problems. That each provider s AI services are offered to many customers who are themselves 201 For an overview of academic w ork on bias and fairness in ML, see Sahil Verma and Julia Rubin , 'Fairness definitions explained' [2018] FairWare 'Proceedings of the International Workshop on Software Fairness . 202 Harini Suresh and John V Guttag, A Framework for Understanding Uninten ded Consequences of Machine Learning [2019] arXiv preprint, arXiv:10002 < > accessed 29 March Forthcoming in Computer Law & Security Review - 51 - engaged in a wide range of pursuits means that if systems are biased then without corrective intervention those issues are likely to propagate across many different applications operating in many different areas, with potentially wide -ranging (and possibly discriminatory) effects. For example, some systems failed to recognise Black women203; such biases in a provider s system would apply across their customer base and, from there, across customers applications. While there is on -going wo rk on dealing with ML biases, challenges will remain for AIaaS. A key problem with AIaaS is that many issues will manifest only in particular contexts (i.e. depending on the particulars of the customer s application and its environment of use), the scope f or which providers in offering generic and widely - applicable models, typically without knowledge of customers specific deployments are unlikely to have considered. This is known as the portability trap , where a model used in one context can cause ha rm when deployed in another204. Moreover, there are a range of ML fairness and bias measures and mitigations, some of which can conflict; providers will select and implement these according to their own concerns, priorities, and interests. The nature of the ir choices, omissions, and the values they embed could manifest differently in different application contexts. Again, due to the nature of AI services, providers are unlikely or unable to consider the range of potential consequences of their decisions in a dvance. Various sets of principles for ethical AI have been proposed attempting to address these kinds of issues205. While these may usefully augment baseline legal and regulatory standards206, they cannot replace legal and regulatory intervention. A fundam ental problem is that ethics principles and other non -legal frameworks for AI are voluntary, leaving enforcement to market forces. Though it may benefit providers commercially to be seen to address these issues to some extent to avoid negative publicity, commercial pressures alone are likely not sufficient to ensure that AIaaS providers take adequate steps to minimise the 203 Joy Buolamwini and Timnit Gebru, 'Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification' [2018] Proceedings of Machine Learning Research , 204 Andrew D Selbst, Danah Boyd, Sorelle Friedler, Suresh Venkatasubramanian, and Janet Vertesi, 'Fairness and Abstraction in Sociotechnical Systems' [2019] 2019 ACM Conference on Fairness, Accountability, and Transparency (FAT* < /papers.cfm?abstract_id=3265913 > accessed 29 March 205 For a review of some proposed sets of AI ethics principles, see Thilo Hagedorff, 'The Ethics of AI Ethics: An Evaluation of Guidelines' [2020] Minds & Machines 30, 99 -206 Elettra Bietti, From E thics Washing to Ethics Bashing: A View on Tech Ethics from Within Moral Philosophy [2020] FAT* 'Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency . Forthcoming in Computer Law & Security Review - 52 - risk of problems developing across their customer base. Though Google s decision to not offer facial recognition services, for example, is welcome, and we encourage other providers to do the same207, fundamentally we argue that it shouldn t be up to these private companies to decide on critical public issues like this in the first place. In the absence of enforceable legal and regulatory frameworks to address the policy issues arising from AIaaS, providers are themselves empowered to essentially operate as their own regulators, generally promoting market considerations and commercial priorities above others. Providers decisions take effect at scale, across their customer base, potentially affecting many applications and their users in a wide range of areas. However, the fact that AIaaS operates at scale as an infrastructure service does offer potential points of legal and regula tory intervention. Given AI services will likely be widely used in future, then regulating at this infrastructural level could potentially be an effective way to address some of the potential problems with the growing use of AI. Model training data Two key categories of policy problems relating to AI services arise from the data used in training their models: ( issues arising from training data obtained from customers, and ( issues arising from training data obtained elsewhere. We discuss the conside rations these raise in turn. Training data from customers As we discuss above, providers supplementary processing can include using customer data to improve the AI service s model. Often, this customer data will actually be third -party data i.e. obtained from third -parties (such as application end -users or sub jects of surveillance) 207 IBM has already ceased offering facial recognition services ( BBC News (n ). Amazon and Microsoft have chosen not to offer facial recognition to law enforcement in the US ( Isobel Asher Hamilton, 'Outrage over police brutality has finally convinced Amaz on, Microsoft, and IBM to rule out selling facial recognition tech to law enforcement. Here's what's going on' Business Insider (13 June 2020 < -microsoft -ibm-halt-selling -facial -recognition -to-police -2020 -6> accessed 13 November 2020 ). Forthcoming in Computer Law & Security Review - 53 - and inputted to AIaaS by customers. Providers use of this data to train models presents several concerns. First, there are privacy concerns about providers using third -parties data to train their models without the knowledge of t hose third -parties. AIaaS typically operates behind the scenes of applications, giving end -users little information on the data flows and processing chains that underpin functionality. Third -parties may therefore not know that the application they are in terfacing with itself relies on AI services provided by a separate organisation, let alone that their data may be used to train the underlying models. This is particularly concerning given that, in some cases, providers employees or contractors may access customer data for labelling or to assess whether the model is functioning correctly. Amazon s Rekognition service, for instance, allows Amazon s trusted employees 208 to access facial and image recognition data supplied by customers. Where this customer d ata is actually third -party data, this represents a potentially serious privacy violation. Moreover, following our previous analysis (see , it is unclear how providers can lawfully use this data for supplementary processing given the potential to pr ocess special category data without first obtaining data subjects explicit consent. Second, there is a risk that, through model inversion attacks 209, it may sometimes be possible to analyse a system s inputs and outputs to extract information about the model s training data (with varying degrees of accuracy). In some cases, this extracted information may contain personal data210, and could therefore potentially reveal details about individuals. While this is a problem for machine learning models in variou s contexts, the potential privacy harms caused by such attacks in this context are exacerbated where third - party data supplied by multiple customers (which may as a result represent many data subjects) is present in the training data. 208 Amazon Web Services (n . 209 Michael Veale, Reuben Binns, and Lilian Edwards ( Algorithms that remember: model inversion attacks and data protection law ( 376 Philosophical Transactions of the Royal Society A 210 Veale et al (n 21 . Forthcoming in Computer Law & Security Review - 54 - Finally, access to c ustomers data and information about their real -world use of models gives providers an advantage over others in developing more sophisticated systems. More data allows for bigger and more representative training datasets. More use cases and information abo ut customer deployments allows providers to assess their systems operation in practice, potentially facilitating better testing and refinement of models. Supplementary processing thus both helps reduce the resources that providers need to devote to system s research and development, and allows them to integrate research and development into a process for which they are paid by customers. AIaaS providers can therefore (in theory) develop better, more accurate, and more generalisable systems at lower net cost than may otherwise be incurred. Moreover, consolidation of AI services around Amazon, Google, and Microsoft gives three already -dominant companies an advantage given they have access to large quantities of customer data and deployments. They can leverage that advantage to develop and improve models for use both in their AI services and in their activities in other sectors. This may fuel expansion in AI services as providers pursue evermore data and deployments for developing increasingly sophisticated systems. This could also contribute to further platform expansion and monopolisation, as those companies leverage their technical superiority over established actors in other sectors to expand their activities into other areas of economy. Beyond the direct ben efits of improving their systems, this helps build institutional knowledge and capacity that can also be beneficial for these companies. Given the questionable legality of processing special category data, the more general privacy concerns around the use of third -party data, and the contribution to platform power and monopolisation, prohibiting AIaaS providers from using customer data to train or improve models or systems should be seriously considered. Although this may drive AIaaS providers to obtain mo re training data from other sources, we note that currently they hardly refrain from doing so (as we discuss next), and that other regulatory mechanisms exist or could be brought into existence to address this issue (for example, GDPR s restrictions on rep urposing211, which, if properly enforced, could help). 211 GDPR art Forthcoming in Computer Law & Security Review - 55 - Training data from elsewhere As well as customer data, AIaaS providers typically rely on extensive data and technical supply chains that underpin their services by providing aggregated, cleaned, and l abelled training data. Some sources of this data are uncontroversial. Some are questionable but mostly harmless a commonly encountered example is Google s reCAPTCHA, whereby users verify that they are human by identifying everyday objects in images. In d oing so, users label training data for Google s image recognition algorithms212; in effect, Google leverages their dominance of captcha systems to get end -users to do valuable work for free which then feeds into developing systems from which Google commercially benefits. Others, however, are considerably more problematic. Some providers of AI services have scraped websites for training data, for example213; Clearview a smaller but controversial provider scraped images of users from Facebook, Twitt er, YouTube, Instagram, LinkedIn, and other major social platforms for their training datasets214. Clearview s facial recognition service is used by law enforcement agencies around the world and by commercial actors215. Cross -border AIaaS data flows also f acilitate concerning supply chain practices. Firstly, providers may obtain training data from countries with lax or no data protection or privacy laws and use that data to develop systems for use in jurisdictions with those protections216. Secondly, provide rs often contract with low -paid, precarious workers in the Global South to label and clean data that feeds into the development of services used in Europe217. 212 James O'Malley , 'Captcha if you can: how you've been training AI for years without realising it' TechRadar (12 January < -if-you-can-how -youve -been -training -ai-for- years -without -realising -it> acc essed 13 November 213 Rachel Connolly, 'Scraping Faces' London Review of Books (28 January < -faces > accessed 13 November 214 Kashmir Hill, 'The Secretive Company That Might End Privacy as We Know It' The New York Times (18 January < chnology/clearview -privacy -facial -recognition.html > accessed 13 November 2020 ; Kate Cox, Facebook, YouTube order Clearview to stop scraping them for faces to match ArsTechnica (7 February < -policy/2020/02/facebook -youtube - order -clearview -to-stop -scraping -them -for-faces -to-match > acce ssed 13 November 215 Ryan Mac, Caroline Haskins, and Logan McDonald, 'Clearview s Facial Recognition App Has Been Used By The Justice Department, ICE, Macy s, Walmart, And The NBA' Buzzfeed News (27 February < -ai-fbi-ice-global -law-enforcement > accessed 13 November 216 Kristina Irion, and Josephine Williams , Prospective Policy Study on Artificial Intelligence and EU Trade Policy ( Amsterdam: The Institute for information Law . 217 Noopur Raval , 'Automating Informality: On AI and Labour in the Global South' ( Global Information Society Watch ; Irion and Williams (n ; Madhumita Murgia, 'AI's new workforce: the data -labelling industry Forthcoming in Computer Law & Security Review - 56 - Outsourcing in AI supply chains is similar to outsourcing in supply chains for physical products to evade workers rights, environmental protections, and minimum wage laws. In effect, the cross -border nature of technical supply chains potentially allows AI aaS providers to outsource key work underpinning AI services to other jurisdictions escaping privacy, data protection, employment, and possibly other laws and then offer those services to European customers. Any future regulatory initiatives relating t o AIaaS should consider whether AI systems offered as a service in the EU should be trained on data obtained, labelled, cleaned, and processed in accordance with European data protection, employment, and other relevant laws. Surveillance An overarching p olicy issue with AIaaS is how these services can enable AI -augmented surveillance. Internet -enabled surveillance (both public and private) has for some years crept into greater areas of contemporary life. AIaaS potentially facilitates a kind of AI - augmente d Surveillance as a Service , particularly for those who would otherwise lack the technical capabilities or resources to develop systems of their own. Exacerbating these concerns is that physical spaces are increasingly monitored with cameras, microphones , and a range of sensors , collecting data from and about people in those spaces (potentially without those people s knowledge or their awareness of how it will be used) . For example, AI services could help retailers track customers through their stores and analyse their behaviour; facial recognition or other biometric services could allow public spaces to be surveilled by public or private actors and otherwise anonymous individuals identified and monitored; speech and voice recognition services could simi larly be used to monitor otherwise private conversations and identify individuals by voice. Rolling out AI -augmented surveillance systems cheaply and at scale, enabled by AIaaS, could potentially transform spaces (both physical and virtual) and relations hips and power dynamics between watchers and watched. AI capabilities enable those undertaking surveillance to move from passively watching people (which can itself affect their choices, spreads globally' Financial Times (24 July < -aa40-11e9 -984c - fac8325aaa04 > acce ssed 13 November Forthcoming in Computer Law & Security Review - 57 - behaviour, and agency and produce differentials of power218), towards potentially identifying and analysing them to more actively, dynamically, and reflexively influence their behaviour219 or subject them to further intervention or detention. Moreover, the prevalence and intractability of biases and errors in ML systems threa tens to reinforce societal divisions and hierarchies along gender, racial, and ethnic lines. We thus echo assertions that introducing AI into video surveillance and other digital information -gathering infrastructure alters power dynamics in favour of those controlling previously dumb systems, requiring reconsideration of how to retain an appropriate balance of societal interests and fundamental rights and potentially limiting the expansion of digital infrastructure that might otherwise seem appropriate220. Misuse and abuse The scale that customers can achieve with AIaaS affords significant potential for misuse, abuse, or undesirable use of AI services that produces serious effects. Providers therefore typically specify in their service agreements that cus tomers cannot engage in or promote illegal or unlawful activity (the main providers generally refer at least to criminal activity, fraud, infringement of intellectual property rights, and defamation)Providers typically also impose other limitations on the purposes for which their services can be used. Amazon prohibits the use of AWS for harmful content such as worms, viruses, trojan horses, and so on; offensive content such as material that is obscene, abusive, or depicts non -consensual sexual activity; security violations; and network abuse such as Denial of Service attacks222. Microsoft prohibits the use of its services, inter alia , to violate rights of others; to spam or distribute malware; or in applications that could result in death, serious bodily injury, or 218 Through what Foucault described as the disciplinary power of panopticism (Michel Foucault , Discipline and Punish (trans Alan Sheridan, Penguin ). 219 Bringing this kind of disciplinary power away from panopticism and closer to t he kind of control through universal modulation as described by Deleuze ( Gilles Dele uze 'Postscript on the Societies of Control' ( 59 October , 3-. 220 Michael Veale , A Critical Take on the Policy Recommendations of the EU High -Level Expert Group on Artificial Intelligence ( UCL Working Paper Series, No. 8/2019 < > accessed 13 Nove mber 2020 , 5-221 Amazon Web Services, AWS Acceptable Use Policy < /> accessed 13 November 2020 ; Microsoft, Online Services Terms < -us/licensing/product - licensing/products#OST > accessed 13 November 2020; Google Cloud, Google Cloud Acceptable Use Policy < oud.google.com/terms/aup > accessed 13 November 222 Amazon Web Services (n 22 . Forthcoming in Computer Law & Security Review - 58 - severe physical or environmental damage223. Google prohibits the use of its services, inter alia, to infringe rights of others; to distribute viruses, worms, trojan horses, and other malicious software; or for spam224. While the lack of liability protection for AIaaS should incentivise providers to proactively identify and prevent illegal use of their services , the primary incentives for providers to identify legal but undesirable use appear to be commercial pressures and risks of reputational dama ge (which have already driven several providers to prohibit law enforcement use of their facial recognition services225). However, methods for identifying illegal or prohibited use of AIaaS in practice appear to be little considered. Other work has propos ed several ways that AIaaS providers could attempt to identify possible misuse or abuse of their systems226. As previously discussed ( , one potentially fruitful policy change would be to end the practice of offering AI services turn -key, instead consul ting with and vetting customers and their applications, requiring specifications of envisaged use in line with well -defined and context -specific terms of service provisions , and verifying compliance and adherence to service agreements on an ongoing basis227. Interventions to limit the scale at which customers can use AI services would also potentially be beneficial; it is not a given that customers should use powerful models at scale without safeguards. Some relatively straightforward mechanisms for tackling the problems of scale include rate limiting (restricting the number and frequency of each customer s API requests and potentially of the customer s end -users) or limiting the extensiveness and detail of the request or response (for example, the number of objects that can be detailed in an image). These interventions should be within providers current capabilities Amazon and Microsoft already employ similar methods for certain services, such as those that process faces228. More complex monitoring by providers of customers service usage to identify illegal activity or terms of service violations is theoretically possible, but not always practicable. However, 223 Microsoft, Online Services Term s (n 22 . 224 Google Cloud (n 22 . 225 Hamilton (n . 226 Javadi et al (n 1 . 227 Javadi et al (n 1 . 228 Javadi et al (n 1 . Forthcoming in Computer Law & Security Review - 59 - various methods of assessing AIaaS usage are possible229 potential ly either identifying suspicious usage patterns (from metadata ) or examining customers inputs and outputs (for example, a large number of people detected in an image recognition service would indicate that the service may be used for crowd surveillance). While these methods would not conclusively allow for all illegal activity or terms of service violations to be discovered, they could assist providers in identifying cases for further investigation. However, any such monitoring must be balanced against the potentially serious privacy issues that this would raise. As noted previously, customer data is often actually third -party data systematic monitoring of this data by providers for indicators of misuse or abuse could potentially reveal end -user behaviour s and activities. This kind of intervention could therefore be justified only by serious, overriding public policy interests and only where necessary and proportionate. Conclusions and further research AI services will be increasingly relied upon by a wide variety of applications. Three companies Amazon, Microsoft, and Google are particularly dominant in the AIaaS market, although others do have smaller market shares. Crucially, AIaaS moves cloud providers beyond merely offering supporting infrastru cture for applications (as in traditional cloud services) to enabling, facilitating, and underpinning customers application functionality. This shift in the role of AIaaS providers challenges legal understandings of the roles and responsibilities of actor s in these complex, networked, and dynamic environments. Moreover, the nature of many AI services themselves offered generically to potentially millions of customers on a turn -key basis presents several legal complications for providers, as we have hig hlighted. AIaaS also raises various underdiscussed issues relevant to any future legal or regulatory frameworks for AI services. Consolidation of AI services around a few companies that are already dominant in other areas of the economy further entrenches their position and 229 Javadi et al (n 1 . Forthcoming in Computer Law & Security Review - 60 - confers upon them greater power to make decisions about the digital infrastructure that increasingly underpins society. The scale that can be achieved with AIaaS potentially exacerbates problems with bias in ML systems and the broader e thical questions around AI. The use of customer data to improve systems raises concerns about the privacy of third - parties, and the cross -border data and technical supply chains on which AI services rely potentially allow providers to avoid data protection law and workers protections. The prospect of AI services augmenting surveillance that transforms power relations and balances of rights and interests in public and private spaces requires careful attention to ensure that people are protected. Finally, th ere is potential for AI services, available cheaply to deploy at scale, to be misused or to be repurposed for undesirable or problematic uses. There are potential ways forward. The gap in liability protection for AIaaS and the fact that AI services are consolidated around a few providers opens space for regulatory interventions at this infrastructural level to address potential harms arising from the inappropriate or illegal misuse of AI. T hrough AI services, providers are more involved in customers app lication functionality and should arguably hold more responsibility in exchange for protection from liability. AIaaS may allow more general ethical issues with AI to scale more easily and become more pronounced. Given the systemic risks this could cause, p roviders should take greater care around how, where, and why their services are used in customer applications. Moreover, AIaaS should feature in the ongoing AI -related discussions of legislators, regulators and policymakers. While AI services are at presen t typically offered by the major providers on a turn -key basis, introducing more comprehensive customer onboarding processes and ongoing monitoring could mitigate against potential misuse (though these must be balanced against the potentially serious priva cy problems it could raise). Regulatory interventions to address problems arising from AIaaS supply chains could potentially also help. In all, Artificial Intelligence as a Service challenges established understandings of roles and responsibilities of clo ud providers in data protection law and of their responsibility for activities of their customers. The growing use of AI services raises data protection concerns, as well as arguably more significant issues around the societal power of dominant platforms through their control of digital infrastructure services and the potential for misuse and Forthcoming in Computer Law & Security Review - 61 - abuse of these powerful systems. As policymakers worldwide grapple with how to deal with AI more generally, these services which may come to be the primary means by which many organisations and applications implement AI should be front and centre in policy discussions and regulatory proposals.",en,"217 Noopur Raval , 'Automating Informality: On AI and Labour in the Global South' ( Global Information Society Watch ; Irion and Williams (n ; Madhumita Murgia, 'AI's new workforce: the data -labelling industry Forthcoming in Computer Law & Security Review - 56 - Outsourcing in AI supply chains is similar to outsourcing in supply chains for physical products to evade workers rights, environmental protections, and minimum wage laws. Forthcoming in Computer Law & Security Review - 58 - severe physical or environmental damage223.",risk
European Patients' Forum (EPF) (Belgium),F2665528,06 August 2021,Non-governmental organisation (NGO),Small (10 to 49 employees),Belgium,"1 Public consultation on the White Paper on Artificial Intelligence 14 June 2020 The European Patients Forum (EPF) is an umbrella organisation of patients organisations across Europe and across disease -areas. EPF represents the interests of over 150 million patients with chronic conditions across the EU who expect and rely on European cooperation to improve healthcar e delivery and quality for all. In concert with its 75 members, EPF ensures the patient perspective in European key health debates , including digital health and health data . To achieve this goal , over the past few years EPF has been particularly active in these fields through both policy work1 and several projects. 2 This statement outlines EPF s response to the European Commission s White Paper on Artificial Intelligence consultation , submitted through the EU Consultation portal. The response and this statement have been developed in a consultative process together with our members and the EPF Digital Health Working Group. In this accompanying statement we will further explore some key elements , challenges and core issues related to the White Paper and to artificial intelligence (AI) in the field of health , that we deem crucial for the patient community. Intro duction AI together with big data has the potential to transform several care delivery methods, and can provide great benefits at several levels of the healthcare value chain: improving population health, healthcare operations and healthcare -related innovation.3 The 2020 EIT Health -McKinsey report Transforming healthcare with AI the impact on the workforce and organisations highlights six areas where AI has a direct impact on the patient: self -care, prevention and wellness, triage and early diagnosis, diagnostics, clinical decision support, and care delivery in the context of chronic care management. AI can allow medical professionals to spend time on other activities , such as interacting with patients in a more meaningful way. AI-supported tools can also result in reduced costs, and support patients in taking control of their health. Furthermore, the COVID -19 c risis has shown how artificial intelligence can be a n added value to the management of epidemics , and play an important role in diagnosis and modelling the spread of new cases.4 1 EPF policy and advocacy work related to digital health and data includes our position paper on eHealth (, GDPR guide for patients and patients organisations (, Data and Artificial Intelligence EU Policy Briefing for Patient Organisations ( , brief summary of our recent EPF survey on Electronic Healthcare Records ( , EPF Response and accompanying state ment - Public consultation on the European strategy on data ( 2 EPF recent projects related to digital health and data include: Digital Health Europe , EHDEN The European Health Data and Evidence Network , and Data Saves Lives . 3 EIT Health McKinsey & Company Transforming healthcare with AI The impact on the workforce and organisations (, /McKinsey/Industries/Healthcare%20Systems%20and%20Services/Our%20Insights/Tr ansforming%20healthcare%20with%20AI/Transforming -healthcare -with -AI.ashx 4 McCall B. COVID -19 and artificial intelligence: protecting health -care workers and curbing the spread. La ncet Digital Health 2020 Apr; 2(: e166 e167., 2 However, as with any new technology, there may also be unrealistic expectations. Artificial intelligence has risks , limitations and concerns including ethical , technical, and legal issues, which are often closely connected. AI depends on the availability of very la rge amounts of good -quality data . AI also risks making wrong decisions wrong decisions or lead to overdiagnosis , and its reliability and safety are particularly critical in healthcare, where errors can have serious consequences. Furthermore, lack of skills and health literacy, limited human autonomy and potential issues with access to AI solutions can also limit the potential of artificial intelligence in health. The EU can set positive global standards when it comes to technological development in the AI field , but it must do so while ensuring inclusi vity, trust, empowerment, and respect of everyone s fundamental rights . As stated in our Manif esto for the 2019 European elections , the EU should ensure that Europe s future digital health tools and systems start from patients priorities , and are co - developed with patients. 5 TOWARDS A EUROPEAN FRAMEWORK FOR TRUSTWORTHY, E THICAL AND SAFE ARTIFICIAL INTELLIGENCE IN HEALTH EPF welcomes the European Commission s White Paper on Artificial Intelligence and its approach based on excellence, trust, human rights, and fundamental values . The EU now has the chance to develop a strong AI framework that benefits people, businesses and governments , matching innovation with safety and trust. The EU can achieve this goal by involving patient organisations as key stakeholders in shaping policy to ensure trustworthy, ethical , safe, and inclusive artificial intelligence in health care. Addressing the key challenges of AI in health The application of AI in healthcare raises a series of concerns , in terms of ethics, safety and fundamental rights for citizens and patients.6 The White Paper consultation addresses several crucial issues related to AI: it may endanger safety or lack accuracy ; it may breach fundamental rights7 and lead to discriminatory outcomes; it may take actions for which the rationale cannot be explained; it may make it more difficult for persons having suffered harm to obtain compensation. In our view, they must all be addressed with clarity and transparency t o ensure safe and trustworthy AI in healthcare in Europe . Ethicists have also identified a risk on limiting human autonomy in terms of a patient s right to free, fully -informed choice o f, for example , treatment, if an AI system made a certain decision based on what it thinks is best for t he patient. 8 Clearly, an important limitation and ethical implication of AI is that it does not possess all the human qualities that have a bearing on healthcare which is fundamentally about human relationships. A specific concern in this regard is tha t artificial intelligence might be so good at picking up anomalies, for example in medical imaging such as X-rays and MRI 5 EPF, Europe for Patients Manifesto, 6 EPF, Data and Artificial Intelligence EU Policy Briefing for Patient Organisations 7 Including human dignity, privacy, data protection, freedom of expression, workers' rights etc. 8 EPF, Data and Artificial Intelligence EU Policy Briefing for Patient Or ganisations 3 scans, that it will end up increasing overdiagnosis and overtreatment. 9 Overdiagnosis by AI can increase the number of unnecessary medical interventions and as any medical intervention carries potential risks increase the risk of harm ing patients.10 To limit these risks, AI s initial assessment should be complemented by regular human checks and monitoring needs to ensure unbiased and controlled processes . Human oversight of the system and the decisions flowing from it should therefore remain central in healthcare , with AI as an important supporting tool. Furthermore, AI, if used to replace real human contact ,11 may actually increase social isolation cause the patient to get confused. 12 Transparency and increased explainability13 on how AI algorithms work and, when possible, on which data sets are used to test, train, and validate algorithms, are also fundamental to increase trust in artificial intelligence in healthcare. EPF calls for particular attention in ensur ing that AI in healthcare enhances society , and is an enabler of and not a threat to patients rights and wellbeing , guaranteeing that the value of real human contact is not minimi sed or entirely replaced by technological alternatives. Better data for safer and bette r AI In addition to the ethical and human rights -related risks and challenges, harnessing the potential of AI in healthcare also raises important technical questions and concerns. The main one is the dependency of AI on large amounts of good quality, unbiased, standardised, and interoperable data. If the available data are not enough, not of good quality, inconsistent, or biased, this can strongly limit the potential of AI to be useful , accurate and safe and can lead to AI errors or overdiagno sis. Biases in data also introduce ethical issues in terms of the potential for AI -enabled decisions themselves to be biased or discriminatory. Biases in data collection can affect the type of patterns AI will identify. This is an issue since, for example , specific population groups are often under - represented clinical trials and large data sets used to train AI. Bias in the data will have an effect on the algorithm that is developed, replicating the bias found in society.14 Patients with multiple or rare diseases may also be affected by this.15 Other issues need to be considered in using data for AI, such as fundamental rights, privacy and protection of personal data . There are risks for unwanted identification of individuals, for example based on their un ique brain architecture visible on MRI scans or by using their genomic data. Improved and harmonised techniques of pseudonymisation 9 Symptom checker apps present an interesting case, as their recommendations might be overly cautious, potentially increasing demand for unnecessary tests and treatments. , Nuffield Council on Bioethics, Artificial intelligence (AI) in healt hcare and research (, -content/uploads/Artificial -Intelligence -AI-in- healthcare -and-rese arch.pdf 10 -cancer -diagnosis -dangers -mammography -google -paper -accuracy 11 E.g. some systems called social AI , such as virtual reality avatars, interact with humans by simulating human social characteristics. 12 Confusi on between humans and machines could have multiple consequences such as attachment, influence, or reduction of the value of being human. The development of human -like robots should therefore undergo careful ethical assessment. High -Level Expert Group on Artificial Intelligence, Ethics Guidelines for Trustworthy AI (, p.33, -single -market/en/news/ethics -guidelines -trustworthy -ai, 13 Guidelines of the High -Level Expert Group and the Communication on Building Trust in Human -Centric Artificial Intelligence, -alliance -consultation/guidelines#Top 14 Nuffield Council on Bioethics, Artificial intelligence (AI) in healthcare and research ( 15 Treviranus J., Sidewalk Toronto and Why Smarter is Not Better (, -toronto -and-why -smarter -is-not-better -b233058d01c8 4 and anonymisation play a central role to ensure data safety for AI , avoid data misuse and increase users trust.16 Lack of interoperability and standardisation of data sets, such as electronic health record systems, is a major challenge. Sometimes traditional analytical methods outperform machine learning, or the addition of AI does not improve results.17 As with any scientific endeavour, correct use of AI hinge s on whether the correct scientific question is being asked, and whether one has the right high -quality data to answer that question. As machine learning is based on patterns in big data, the system is only as good as the data that is fed to it . Availabil ity of well -annotated and appropriately -pseudonymised clinical data is also fundamental for AI research and innovation projects . Data access and data sharing, crucial to the success of these projects, can be a significant bottleneck, incurring long delays in their execution as well as substantial legal and administrative costs. Difficulties in executing data access and data sharing agreements are further compounded when public and private sector priorities clash. An y action s to enhance secure , but rapid acc ess to valuable research and innovation datasets would surely enhance the quality of AI research and innovation at European level . Finally, t he importance of data for AI clearly connects the future frameworks and initiatives on artificial intelligence and healthcare to the upcoming EU work on the European Health Data Space , which EPF also commented on via its dedicated public consultation .18 The EU strategy on AI should also be aligned with and take into account relevant initiatives and projects on data, such as the EMA strategy on big data.19 Furthermore, the EU should explore coordination on data standards at international level, increasing shared knowledge on quality and interoperability beyond EU borders. Developing an ecosystem of excellence for AI in healthcare The White Paper includes six key actions deemed fundamental in build ing an ecosystem of excellence that can support the development and uptake of AI across the EU economy : working with Member States ; focussing the efforts of the research and innovation community ; skills development ; focus on SMEs ; partnership with the private sector and p romoting the adoption of AI by the public sector . EPF agrees that the six actions are all key in build ing excellence in AI at European level, but we also emphasise their strong interdependency and need to be adapt ed to address the uniqueness of AI in healthcare. These actions will have to be tailored to take into consideration the specific challenges of healthcare and to ensure inclusion of the patient perspective in research, development of policy frameworks , and im plementation of innovative solutions. 16 EPF, EPF Response and accompanying statement - Public consultation on the European strategy on data, 2020, -patient.eu/globalassets/library/data -strategy -consultation -response ---epf-statement_finalversion.pdf 17 Austin PC, Tu JV, Lee DS, Logistic regression had superior performance compared with regression trees for predicting in - hospital mortality in patients hospitalized with heart failure, J Clin Epidemiol. 2010 Oct;63(:1145 -doi: 1016/j.jclinepi.Epub 2010 Mar 21 . 18 EPF, EPF Response and accompanying statement - Public consultation on the European strategy on data, 2020 19 HMA -EMA Joint Big Data Taskforce Phase II report: Evolving Data -Driven Regulation and key recommendations, ( -ema -joint -big-data -taskforce -phase -ii-report -evolving -data - driven -regulation_en.pdf ; -recommendations -hma -ema -joint - big-data -task-force_ en.pdf 5 The efficiency of the proposed actions will be compromised without collaboration across sectors, and in the absence of meaningful citizen and patient involvement at all levels . To reinforce this point, EPF calls the E uropean Commission to ensure the involvement of citizens, patients and other relevant stakeholders healthcare professionals , in particular as the seventh key action to achieve a European ecosystem of excellence for AI in healthcare . Transparent, effect ive, and sustainable AI research and innovation Creating an ecosystem of excellence for research on AI in Europe is key , and all the actions mentioned in the White Paper support the establishment of a world -class research hub, connect research excellence and set up a public -private partnership for industrial research are surely important. When it comes to healthcare -specific public -private partnerships for industrial research in the AI field, they should be driven by public interest and their results should contribute to public health and wellbeing. If it is true that innovative solutions often require collaboration between multiple stakeholders, there must be clear priority -setting criteria based on potenti al impact on unmet health needs, and any entanglements between these priorities and other interests must be avoided. Innovative products and services developed with EU funding must be, at the end of the day, accessible and affordable to those who can benef it from them, be it individual patients or health systems . Innovation20 in AI, as in other s, should be valued for its potential to improve the quality of care and of life, over and above mere potential for putting a product on the market. EPF would also l ike to emphasise the importance of sustainability in the context of research and innovation . Investment in this field should also include funding to ensure the sustainability of high - value assets developed by past and present initiatives , on which future innovation can be built. For health research and innovation projects, ensuring the sustainability of clinical assets would recognise the valuable contributions patients make to these projects, sacrificing their time and, in some cases, exposing themselves to risks. A sustainable ecosystem of excellence for research on AI should therefore aim to guarantee access to data a nd assets in the post -project period. AI should also be used to develop solutions for health inequalities , including addressing social determinants of health, but also increasing equitable access to high -quality healthcare for all, in line with the fundame ntal shared values of European health systems. Finally , to strengthen AI research and innovation there is a need for accurate risk assessments that identify the probability and magnitude of potential harms. Consequently, efforts should also be directed tow ards ensuring that, where required, AI research and innovation projects undergo ethics review, by panels that possess the necessary AI and data science expertise. Improve European coordination on AI Improved c oordination at European level will be key to advance together on AI while limiting inequalities and harmonising innovation and accessibility to AI -related benefits for patients. The EU should also promote the uptake of AI by business es and the public sector but possibly adopting different approaches, such as specific regulation and standards for the business sector , and enhanced support and coordination for the public sector (e.g. capacity building and of course ad -hoc regulation 20 EPF calls for a wide definition of innovation that includes people -focused, social, organisational and systems innovation. Research into the design of health and social care and how care is delivered, can add significant value in providing evidenc e for targeting resources efficiently, thus contributing to the sustainability of health systems. 6 and standards). EPF welcomes the priority given to the healthcare sector in the White Paper when it comes to the adoption of AI by the publ ic sector. Given the importance of data for artificial intelligence, a ddressing AI will be fundamental when discussing the European Health Data Space (EHDS) . As mentioned in our response to the Data Strategy consultation21 and considering the peculiarity o f the health sector and its specific challenges and risks, a sector -specific approach on healthcare and data is needed. However, the development of the EHDS must include the necessary mechanisms, such as governance structures and appropriate capacity - build ing, to ensure the meaningful involvement of patients from the very beginning , in order to shape a framework that benefits from the unique experience and knowledge of patients and provides benefits for society. Ethical AI governance systems are another a rea where policy alignment and inter -state coordination are essential , and e nhanced cooperation at European level should also look at ensuring equitable access and avoid exacerbation of health inequalities within and across Member States. Boost skills and digital health literacy as a precondition to exploit AI at European level Health literacy is a key component of patient empowerment22 and a major priority for patients .23 Enhancing digital health literacy and data literacy levels is crucial to increase patients knowledge and trust on AI in health and enable them to better understand and exercise their rights while realising the societal benefits of AI innovation in healthcare . Education and health literacy for the public , and for patients specifically, can also increase civil society s capacity to be engaged in developing policy and practice on AI, especially in healthcare . EPF welcomes the importance given to skills and literacy in the White Paper and stresses the need for active patient involvement in shaping future skills, educational and training policies for AI and health. We would like to emphasise that health literacy is not only about the skills of individuals, but a relational concept that requires healthcare professionals, organisations and systems to become more easily understandable and navigable to all individuals, whatever their health literacy levels . Health literacy including digital and data literacy is therefore an important st rategy for health equity and to avoid exacerbation of the digital divide.24 Considering the rapid speed of innovation in the AI sector, EPF calls for particular attention to developing skills, health literacy and dedicated education and training for citiz ens, patients and healthcare professionals, through dedicated resources and initiatives both at European and National level. Also, considering that AI is clearly a constantly evolving sector, it will be crucial to address skills, literacy, education, and t raining with a dynamic and equally evolving approach. In doing so, healthcare professionals will be able to be up to date with innovation in the field, use AI safely and efficiently, but 21 EPF, EPF Response and accompanying statement - Public consultation on the European strategy on data, 2020 22 EMPATHiE Project, -patient.eu/whatwedo/Projects/completed -projects/EMPATHiE/ 23 EPF, Charter on Patient Empowerment (, - patient.eu/whatwedo/campaign/PatientsprescribE/charter -on-patient -empowerment/ ; EPF, Campaign on Patient Empowerment: Roadmap for Act ion (, -patient.eu/whatwedo/campaign/PatientsprescribE/roadmap - for-action/ ; EPF, Europe for Patients Manifesto (, 24 Roediger A et al. ( Nothing about me without me: why an EU health literacy strategy embracing the role of citizens and patients is needed , Archives of Public Health vol.77, no: -019-0342 -4; EPF, Consensus paper: Making health literacy a priority for in EU policy, -patient.eu/globalassets/policy/healthliteracy/health -literacy -consensus - paper_2016.pdf 7 also adequately interact with patients and citizens. Informed patient s and citizens, might therefore feel more confident and ready to harness the potential of artificial intelligence and better engage with the underlying digital ecosystem (e.g. data sharing). Definin g risk for AI application s in healthcare The White Paper defines high -risk applications based on two -levels risk -based approach, identifying two key cumulative criteria: sector of employment of AI and use of AI applications. While healthcare is clearly acknowledged as one of the high -risk sectors, the Paper also highlights that softer application of AI in healthcare, for example AI -driven appointment scheduling system in a hospital, would not be necessarily flagged as high -risk . In general terms, the adoption of new and specific rules addressing specific risks related to the application of AI in healthcare should surely be considered. However, especially for AI applications in the healthcare sector, whether the introd uction of new compulsory requirements should be limited to high -risk applications depends on how the future EU rules on AI will detail the risk -based approach and therefore the list of high -risk applications uses. Indeed, although the White Paper already c learly considers healthcare to be a high -risk sector of application (first principle ), AI in healthcare has a series of particular risks that require a thoroughly developed sector -based approach to clearly define what should be considered a high -risk us e (second principle ). EPF calls for particular attention on the definition of high -risk AI in healthcare and a dedicated discussion on this topic inclusive of the views of patients . Since the EU approach to define high -risk will be linked to more restrict ive or relaxed assessment procedures, it will be necessary to carefully evaluate what could be considered to be low-risk AI application in healthcare . This should be done not only taking into consideration obvious risks , such as inaccurate diagnosis or pro gnosis and incorrect treatme nts,25 but also indirect or secondary negative impacts on the life of the patients such as unwanted identification of individuals ,26 even minimal delays of care, 27 reduced freedom of choice, social isolation or distress. When addressing the balance between potential benefits and potential harms, the balance should always be on the benefits side. Establish assessment mechanisms for safe and ethical AI in healthcare High -Risk AI applications in healthcare should be subject to strict assessments procedures , harmonised at EU level, to fully ensure patient safety and address ethical questions . In EPF s view, the best solution would be a combination of ex -ante compliance and ex -post enforcement mechanisms, with the important specification that ex -ante compliance should be assessed by an independent body . These mechanisms, which should include an ethics review , should be transparent, and in the case of healthcare applicati ons, should include end users patients as contributors to the assessment procedure, where possible. 25 As mentioned in our briefing paper on big d ata and artificial intelligence, AI might be so good at picking up anomalies, for example in medical imaging such as x -rays and MRI scans, that it will end up increasing overdiagnosis and overtreatment. Symptom checker apps also present an interesting case as their recommendations might be overly cautious, potentially increasing demand for unnecessary tests and treatments. 26 There are for examples risks for unwanted identification of individuals, for example based on their unique brain architecture visible on MRI scans or by using their genomic data. 27 For example , due to malfunctioning in AI -led systems of hospital manag ement 8 It is crucial to ensure that the assessment mechanisms ensure safety and effectiveness of AI systems as they are used and enable prompt action if problems arise during their lifetime. This is particularly important for AI, considering that machine learning systems adjust themselves as they learn . As concern s the proposal of a voluntary labelling system for non -high -risk applications , such an approach could indeed be useful to facilitate the identification of trustworthy applications for both patients and professionals. As previously mentioned, however, the k ey question here is how to define high -risk and no n high -risk applications. We would like to see meaningful and inclusive involvement of end users in the development of labelling systems, to ensure that labelling systems are easily interpretable and tailor ed to the specific needs of the end user group. A fit -for-purpose regulatory framework to increase trust on AI AI-specific risks, safety and liability, when it comes to healthcare in particular , should be addressed specifically in EU legislation, avoid ing duplication with existing regulatory frameworks (from data protection to medical devices) while ensuring adequate protection for individuals. EPF calls for inclusion of patients views from the very beginning in the process of adaptation and update of the current legislation or, where necessary , the development of new legislation . In addition, given the speed of technological advancement in this field, any new AI -related legislative framework should incorporate requirements for systems to be re -evaluated, and for the new legislation to be adapted , in a rapid and nimble way in response to technological evolution thereby future -proofing the legislation. Further more , we believe that any new legislation should be founded on solid ethical principles and recommendations, for example those recently outlined by AI4People and in the Asilomar AI Principles, developed in consultation with all relevant stakeholders including the individuals that the principles are designed to protect. Respect of these AI -specific, foundational ethical principles should be ensured via expert ethical review of AI applications, particularly in the field of health. Ethical risks and potential harms of AI should be carefully consid ered to provide more legal certainty, particularly for healthcare products and for products that make use of personal data. Clarity and transparency on liability and responsibility , therefore on who should respond to potential harm caused by AI application s, will be also crucial to increase trust in AI . People should always have effective and transparent mechanisms for redress. In general terms, on top of ensuring ample protection and safety, the exercise of rights defined by EU legislation should be made simple and not overly burdensome. 1 Public consultation on European Health Data Space EPF accompanying paper 26 July 2021 The European Patients Forum (EPF) is an umbrella organisation of patients organisations across Europe and across disease -areas. EPF represents the interests of over 150 million patients with chronic conditions across the EU who expect and rely on European cooperation to improve healthcar e delivery and quality for all. In concert with its 77 members, EPF ensures the patient perspective in European key health debates , including digital health and health data . To achieve this goal , over the past years EPF has been particularly active in these fields through both its policy work1 and several projects2. This statement is an addition to the E PF s response to the European Health Data Space (EHDS) Public Consultation , submitted through the EU Consultation portal. The response and this statement have been developed in a consultative process with our members and our EPF Digital Health Working Group. In this accompanying statement , we further elaborate on some of the key elements included in our response and summarise our views on the EHDS. NOTE the responses included in the Consultation and in this accompanying paper are based on the current understanding of the European Health Data Space proposal development and on the interpretation of the questions included in the questionnaire. On this point , several of the questions have been identified quite broad and unclear, at least in some of their elements , in particular in terms of prioritisation and identification of what should constitute a preco ndition for the EHDS design and implementation . For instance, it is essential to note that many of the proposed options , tools, platforms, and policies mentioned in the questionnaire as ways to facilitate health data sharing , can be considered viable choices only if patients are first ensured proper access and control over their health data with a transparent and trustworthy framework . Furthermore, EPF s response s might not entirely reflect individual organisations views or pr ecisely capture national or disease -specific challenges and suggestions. They should be therefore considered in parallel with the inputs shared by our members, both patients national coalitions and European disease -specific organisations Intro duction Health is an area where Europe can undoubtedly benefit from the data revolution . Proper use of health data can improve health systems sustainability, increase the quality , safety and patient - centred ness of healthcare, decrease costs and transform care int o a more participatory process .3 Health data can support the work of regulatory bodies, facilitating the assessment of medical products and demonstration of their safety and efficacy. Furthermore, the COVID -19 pandemic has demonstrated how accurate and qu ickly accessible data is also fundamental in the management of cross -border public health emergencies. Nevertheless , the road to fully exploit the potential benefits 1 EPF policy and advocacy work related to digital health and data includes our position paper on eHealth (, GDPR guide for patients and patients organisations (, Data and Artificial Intelligence EU Policy Briefing for Patient Organisations ( and a brief summary of our recent EPF survey on Electronic Healthcare Re cords ( . Furthermore, EPF responded to the EC Consultations on the Data Strategy and AI White Paper (, European Health Data Space roadmap and Data Governance Act (. 2 EPF recent projects related to digital health and data include: Digital Health Europe , EHDEN The Eur opean Health Data and Evidence Network , and Data Saves Lives . 3 Europe for patients Manifesto, 2 of data in health is only partially built , still extremely fragmented and not yet develope d with the patients views at the centre. Given this context, t he EHDS can be considered as a welcome exercise to better harmonise and clarify the health data panorama in Europe , while also having a potential positive impact on digital health in more general terms (e.g., digital health services, Artificial Intelligence, etc.) . If shaped and implemented in the right way, the EHDS can become a crucial pillar of the European Health Union , and ultimate ly improve citizens and patients lives. At the same time, its broad scope makes prioritisation and planning effort s a necessity in order to ensure that all the elements of the EHDS will be enshrined on a series of principles based on citizens and patien ts needs , to be considered essential preconditions . Indeed, the EHDS must overall : be shaped to ensure barrier -free access and control of health data in an easy and transparent way, with the highest possible level of data protection and based on consent ; ensure patient safety at all levels ; deliver harmonisation while keeping in mind the differences between different health systems; take into consideration and tackle current and potential inequalities and gaps in health literacy and access to digital ; tackle the ethical and practical challenges linked to current and future digital health transformation; foster a digital transformation of healthcare that delivers added value for patients and responds to their true needs and concer ns; concretely and meaningfully involve patients in its shaping, governance and implementation. Without building on these elements, the EHDS might no t be able to deliver on its promises independently of the choice on specific options, tools, platforms, or guidelines . On the contrary, it might further exacerbate existing inequalities within Europe, potentially increase mistrust and, ultimately, not be accepted by th e very individuals that should be at the centre of this initiative. For th e European Health Data Space to work, it will have to be more than a large -scale flagship European project. It must reach patients and citizens, be shaped with them, be accepted by them, respond to their needs, and ultimately ensure that health data and the digital transformation of health and care will help delivering better care and increase quality of life. 3 In this section of the EPF EHDS consultation accompanying paper , we will focus on the most important elements of our responses to the four pillars of the questionnaire, covering: access to and exchange of health data for healthcare ; access an d use of personal health data for research and innovation, policy -making and regulatory decision ; digital health services and products ; Artificial Intelligence (AI) in healthcare . IMPROVING ACCESS AND CONTROL OF HEALTH DATA WHILE ENSURING THE HIGHEST POS SIBLE LEVEL OF DATA PROTECTION THE KEY TO A PATIENT -CENTRED EUROPEAN HEALTH DATA SPACE In EPF s view, a European framework on the access and exchange of personal data should have the ultimate goal in improving healthcare delivery for all Europeans, both within and across borders, while ensuring the highest level possible of interoperability, safety, data protection while avoiding the potential misuse of data. To achieve better and more trustworthy use of personal data in the field of healthcare, patients must be in control of their data . They should be able to freely access it, decide who to share it with, and on what conditions. As identified by the EPF community and confirmed by the Inception Impact Assessment (IIA), exercising barrier -free access and control over their own health data is often difficult for patients. For example, electronic health records (EHRs) are not yet a reality across the whole EU, and many patients cannot easily access, understand and use the information they contain, or transfer them between healthcare provider s, including when they move across borders. Achieving a higher level of barrier -free4 access and control should therefore be considered as the key priority of the EHDS, and subject to prioriti sation when developing such European framework. Access should a lso be linked to measures ensuring that failures in providing access and control to patients health data, or the eventual unwanted use and sharing of patients data, would be linked to sanctions or fines . These measures should be seen as a way to increase patients trust in health data , safeguarding their essential rights and they should be based on a clear framework, easy for patients to exercise. Transparency is also key , in particular with regards to how handling and processing of the data will be organ ised and through which platforms/providers (e.g., if not located in Europe). Furthermore, it is essential to avoid patients' data being leaked or misused as it can have a dramatic impact on the life of individuals. Instances such as the mental health data leak in Finland ( or more re cent leaks occurred in France , Unit ed Kingdom , and Ireland ( must not happen under the European Health Data Space. Once the precondition of secure and protected access and control to health data is enshrined and prioritise d, it will be important to set a clear framework granting the needed options to patients to 4 Barrier -free access for patient s to control and administer their own healthcare data is essential, especially patients with sensory or cognitive impairment. For instance, the healthcare data for visually impaired patients should be accessible via acoustics and screen reader. 4 exercise their essential rights related to health data while ensuring the highest possible level of protection . These should include decisions on how to access such data in the easiest way possible and for the broadest spectrum of the population (e.g., considering different levels of health literacy or access to digital means) , how to share it and with whom , for which purposes , how to ensure that such decisions are respected5, and , eventually , how to withdraw access. Patients should also be able to feed information and corrections to their health data . Out of date , incomplete or incorrect information has the potential to lead to mistakes and errors, both in care, but also for planning care , policy, and research .6 This is essential to improve quality of data . Fundamentally, the EU framework should firstly support and enable barrier -free access for patients to their healthcare data , granting control in the most secure environment possible. The EU should investigate carefull y driving minimum standards to ensure that such possibility is granted across Europe , while taking into consideration the existing difference between health systems and ensuring that no one is left behind. The EHDS as a unique chance to shed light on healt h data complexity One of the central questions of the EHDS consultation (Question refer s to whether additional rules on conditions for access to health data for research, innovation, policy -making and regulatory decisions are needed at EU level . As this is an example of a very broad question with equally broader multiple choices offered within th e questionnaire , further clarification on EPF s position and responses is needed. While in the questionnaire response we have identified how additional rules in all cases would be EPF s preferred option , our position does not intend to call for new, over ly burdensome or duplicating efforts not taking into consideration the already available rules and initiatives (e.g., GDPR, guidelines by the European Data Protection Supervisor , European Data Protection Board and national data protection authorities, EU -funded projects). In our view, the EHDS has the chance to set up a framework that sheds light, clarity, and transparency on the complex panorama of health data sharing, addressing the peculiarity of health data, ensuring security and privacy but without creating additional unnecessary hurdles to use data in the public interest . The EHDS should help streamline and navigate health data, in particular for patients, clinicians, and researchers. This could be done through guidance, clarification of rules, better tackling known gaps and in silos approaches, and developing dedicated code of conducts . Of course, particular attention should be dedicated to areas where the EHDS will b ring particular innovation in procedures, access and data sharing. On the specific issue concerns data sharing outside of the EU . On this point, rules should be shaped to avoid jeopardising research happening beyond our borders, if health data is shared un der clear and transparent circumstances, with a specific focus for data protection. Particular attention must be dedicated to ensuring secure access to health data, in particular if not anonymised or pseudonymised. Independently of the rules/guidelines ad opted in shaping the European Health Data Space, the primary focus should always be on ensuring safe, clear, protected and transparent patients access and control to their health data. 5 As indicated in our response to Q3 and Q4 of the EHDS 2021 Public Consultation questionnaire 6 This was identified as a key ask in our recent EPF survey on EHRs 5 Defining frameworks and mechanisms within the EHDS Several questions of the EHDS consultation concern the definition of possible mechanisms of collaboration between Member States and the EU -level, including the development of standards and technical requirements or their application. In EPF s view, the choices related to developing standards and technical requirements should be taken in strong collaboration between national digital health bodies and possibly coordinated through a dedicated EU structure/body in charge of overseeing the process and ensuring a harmonised appr oach as well as the involvement of patients in the governance . Furthermore, better coordination and harmonisation of national approaches on health data exchanges across the EU to build a less fragmented, more accessible and trustworthy framework should be the ultimate goal of the European Health Data Space . This is important, in particul ar in light of the current implementation challenges of the GDPR. Such approach should also build on common principles such as the FAIR pillars:7 data should be findable , accessible , interoperable , and reusable. Building on this, in EPF s view, a coordin ated authorisation scheme managed by national bodies, taking into consideration the specificity of the healthcare sector and the specific risks linked to health data, could be the best option to ensure safe exchanges of data. At the same time, while avoidi ng too much complexity, it could be interesting to explore the option of a labelling system for inter operability as part of the mandatory prior approval, which may be useful for identifying good practices, increasing trust, transparency, and understandabil ity of the process. In terms of defining possible mechanisms, a key question revolves around the choice of the most appropriate option to facilitate access to health data for research, innovation, policy -making and regulatory decision (Q uestion . In EPF s view, once again , independently of the body selected to handle access to health data, it will be fundamental to ensure full independence and accountability. It should be built on transparent processes and with the inclusion of patients repre sentatives in its governance/decision -making structures. Public body and mandatory -based options should be the preferred ways to reduce fragmentation and increase clarity . Private not -for-profit entities , as presented in the Consultation, have been defined as the least preferred option , mainly due to additional questions concerning the nature of such entities, their affiliation and governance. The Consultation also discusses the issue of voluntary data sharing and the data altruism term. Concern ing data altruism , as identified in our Data Governance Act8 response and considering the importance granted to it within the DGA and in the EHDS, it is necessary to ensure a harmonised and clear definition of the term to ensure that patients are fully aware of its meaning and impact. The development of protocols or procedures for the practical exercise of such voluntary transfer of data should also be considered and patients should be able to check information on who has had access to their data, on what basis a nd for what purpose . Furthermore, while many patients are willing to make their healthcare data available to foster new therapies and treatments on a voluntary 7 -fair.org/fair -principles/ 8 EPF, Response to Data Governance Act Consultation (2021 ), -patient.eu/news/latest -epf- news/2021/shaping -a-patient -centred -european -health -data -environment/ 6 basis, those who are not able nor willing to share their data should still be granted full access to high - quality car e. 9 Facilitating access to health data for research , innovation, policy -making and regulatory decision Access to data must be subject to the consent of patients, especially where third parties are using data for innovation or commercial purposes . Many patients will agree to their data being used for research, policy, and public services, in particular where they believe there is public benefit in doing so. They are in general less inclined to share for the purposes of vaguely defined innovation . There a re numerous examples of patients opting out of their data being used, especially because of concerns of these external organisations being involved and because of poor communication around projects or how data will be used , etc. Furthermore, a ccess to dat a held by private stakeholders should be facilitated for research, innovation, policy -making and regulatory decisions in accordance with existing legal frameworks and based on the initial consent by data subjects. The consent frameworks should be shaped ke eping into consideration the potential unwanted impact o f data use for research, innovation, policy -making and regulatory decision, for instance taking into consideration broad or dynamic consent. The Consultation also mentions the possibility for the establishment of an EU body governing access to health data for resear ch, innovation, policy making and regulatory decisions (Question . Such a body could, for instance, bring together national bodies dealing with secondary use of health data, setting interoperability standards, act as technical intermediary, facilitate c ross-border health data sharing. In EPF s view , this could potentially help harmonis e the currently fragmented health data panorama in the European Union. At the same time, such an EU body should be built on enhanced cooperation between national bodies and ensure the inclusion of patient representatives in its governance structure to ensure that patients needs are fully taken into consideration. It is also noteworthy to mention that, when adopt ing an EU pathway, it will be necessary to shape it in a consi derate way to avoid more regulatory obstacles and increase the burden of administration, potentially impeding rather than facilitating progress. Potential benefits and expected impacts of the EHDS Ensuring efficient, safe, and affordable care for patients should be considered as a key goal of the European Health Data Space framework to improve access to health data. Concerning innovation , it will be particularly important that the data used to drive advancements in treatmen ts, medicines, devices, and services will lead to innovation answering the patients unmet needs. The EHDS Consultation foresee s six ( potential main benefits from the EHDS: Availability of new treatments and medicines ; Increased safety of health care and of medicinal products or medical devices ; Faster innovation in health ; Better informed decision -making (including risks and errors) ; 9 As concerns the data altruism term, as identified in our Data Governance Act response9 and considering the importance granted to it within the DGA and in the EHDS, it is necessary to ensure a harmonised and clear definition of the term to ensure that patients are fully aware of its meaning and impact. 7 Reduced administrative burden in accessing health data ; Technological progres s. While all these benefits can have a considerable potential high impact, their short to medium term actual impact might be rather moderate if we consider a more realistic forecast for the deployment of the EHDS. As it concerns administrative burden s, it is noteworthy to mention that additional rules, complexity, and processes introduced by the EHDS could potentially have a negative/limited impact , especially if not carefully deployed and implemented at the national level with all stakeholders fully on board, the right platforms and development of skills and literacy. In addition, the availability of new treatments, medicines , etc., is affected by a range of factors well beyond the scope of the EHDS. While the delivery of efficient, safe, and affordab le care for patients is a laudable goal for the EHDS, simply increasing the access to (and sharing of) data is only the first step towards this goa l. In terms of potential additional impacts, EPF recognises that the increased availability of data can help policy makers and regulators to make better and more effective evidence -based decisions while facilitating research and innovation based on outcomes that really matter to people . However, this must go hand in hand with providing patients with clear assuran ce on how the data is used and that it is used in line with the purposes for which the personal data were initially collected . Patients should also be made aware of the possible consequences of the intended further processing of data subjects . Adequate safeguards must be ensured (encryption, anonymisation and pseudonymisation). Patients should be also granted opt -out possibility if they believe that their data is used bey ond the agreed use. Finally, t he creation of a future EHDS may also help identify and ultimately tackle differences and inequalities between Member States (and potentially between sectors) in terms of health data digitisation, access and sharing mechanisms . Said differences and inequalities will have to be carefully considered in the deployment of the EHDS to avoid increasing disparity across Europe in the digitalisation of health and care systems. A FRAMEWORK FOR DIGITAL HEALTH SERVICES AND PRODUCTS WITHIN THE EHDS Broader deployment and use of digital health products and services can surely benefit patients at different levels . Better communication with healthcare professionals, improving self -management and monitoring of their own condition, easier access to their health records and sharing of their health data within and across -borders, improved access to healthcare for patients in remote areas are only few examples of the main positive impacts of digital health. However, the deployment and use of digital health products and services must take into a series of current challenges into consideration, including cultural and link to the potential reticence to use digital health. Digitalisation levels, both in terms of infrastructures, literacy and access to digital means, are highly unequal across the European Union and even within Member States territories . The EHDS framework should therefore keep into consideration this divide to avoid further exacerbating already existing inequalities again, within and across Member States. This should be done by targeted work and support to specific Member States, areas and population categories to limit as much as possible the gap in accessing digital health. 8 We need to keep patient choice and control in primary consideration , as some cannot access these services and even those who can, may not wish to use these products. While digitalisation is extremely important, it should be seen as supplementary/complementary to existing models of healthcare and servic es. Access and sharing health data nationally and across borders through digital health services and devices Accessing and sharing health data through digital health services and device s must go hand in hand with ensuring and safeguard ing proper consent co ming from the patients . They must be in full control of what kind of data they want to share/transmit. Indeed, patients are generally willing to provide access to their data provided that proper and clear consent is granted and that they have control o ver how and what kind of the data is accessed and for what purpose. In EPF s view, actions to improve how patients control their data, for instance , granting enhanced possibilities to transmit it from their m -health/tele -health tools into both EHRs and an EU health data exchange infrastructure, are important elements for the development of the EHDS framework. Once the consent is clearly granted, and the actual use of data is respectful of such consent, data can be considered as a fundamental tool to improve collaboration between HCPs and patients for the delivery of better care. Furthermore, the relationship between healthcare professionals and patients over health data through digital health services and de vices should be integrated in the European Health Data Space as a collaborative interaction to ensure information to patients about the opportunities offered by digital health ; exploitation of existing opportunities provided by digital health to improve ca re and self - management ; facilitating control of their data and digital health use. Minimise risks related to tele -health and improve the relationship between patients and healthcare professionals While the correct application of tele -health solutions can improve the relationship between patients and healthcare professionals, and access to care, there are some essential elements to be taken into consideration: Tele -health should, in normal conditions, not be seen as a replacement for tradition al care but rather as an additional tool; Increased trust issues from the patients ' point of view; The correct use of tele -health needs adequate skills and access to digital health solutions, both for healthcare professionals and patients; Additional stre ss for both patients and doctors, from difficulties in accessing and using digital solutions to the de-personalisation of care, and adopting additional tools in already overcrowded schedules; Potential risks of misdiagnosis, errors and miscommunication exa cerbated by the use of tele - health solutions; Tele -health also requires proper access to digital tools. The digital divide currently existing within and across EU -countries should be therefore taken into consideration. Patients with hearing, vision or phys ical impairment, dementia and other conditions are potentially prevented from using technologies related to tele -health. 9 Given the broad scope of the European Health Data Space, there is a chance to tackle such issues and promote better harmonisation , to drive a higher level of coordinated protection and clarity for both patients and healthcare professionals . A more coordinated approach could also facilitate patients to travel across the EU without facing too many diverse frameworks that would increase unce rtainty and potentially hamper patients willingness to engage with telehealth solutions. This could also facilitate healthcare professionals to travel across borders, facilitate more coherent training and education on how to use and communicate about tele health, and ultimately increase safety for patients. At the same time, stronger harmonisation must take into consideration how the use of tele -health is directly connected and linked to healthcare professionals and to their clinical practice, which operate in very diverse healthcare systems with significant variations. T o tackle this while supporting a progressively less diverse European panorama, guidance, certifications and recommendations be developed at EU level, thereby enabling and supporting integrat ion of telehealth in diverse Member State health systems. Fostering uptake of digital health products and services In EPF s view, ensuring clear authorisation schemes and the certified interoperability of digital health products and services is essential to foster the uptake of digital health products and services . It is important to consider how mandatory prior approval by national authorities can increase patients trust in d igital health products and services. Furthermore, assessment of interoperability levels will be essential to drive a true European cross -border adoption of digital health solutions that can help patients travel within the EU. Concerning labelling , especial ly if voluntary, while it should not be directly preferred to mandatory and prior assessment, it could be already considered as an improvement compared to the current situation. Labelling schemes when co -developed with patients and clinicians can help increase the accessibility and understanding of digital health solutions, providing a straightforward means for patients and clinicians to identify solutions that are trustworthy and meet their requirements. Furthermore, creating a more harmonised European approach and guidelines towards reimbursement and assessment of digital health should be seen as an essential building block of the European Health Data Space framework. Such European approach should ensure that all patients in Europe can have the same le vel of access to digital health services and products, while of course keeping into consideration the differences between European health systems . Without such a harmonised approach there is a risk of moving towards a multi -speed system that would ultimate ly exacerbate the already existing differences in the digitalisation of health and care systems, with a negative impact on patients lives and hampering European coordination. EPF also supports the proposal for a transparent, easy to access and clear repository of digital health products and services assessed according to EU guidelines to aid national bodies, both to facilitate reimbursement decisions and to increase transparency towards patients. National authorit ies should also make lists of reimbursable digital health products and services available as an additional transparency measure. Efficient and coordinate d use of EU funds to drive digitalisation EU funds dedicated to support the adoption and scale -up of digital health services should be conditional to interoperability within and across borders with EHRs and national healthcare 10 services. Ensuring access and control of patients over their health data, but also patients' involvement in the research and innovation process, should be also considered as an essential condition to access EU funds for digitalisation in healthcare. Furthermore, achieving adequate acceptance of the EHDS at a patients level will be connected to addressing well -known and underly ing issues such as the access to digital means and health literacy. These issues must be tackled through all relevant EU funding programmes, such as the EU4Health Programme to Digital Europe and Horizon Europe, building on pre -existing pilots and ensuring efficient and impactful use of funding responding to the actual needs of patients and health systems . ARTIFICIAL INTELLIGENCE AND THE EUROPEAN HEALTH DATA SPACE DEPLOYING AI IN THE BEST INTEREST OF THE PATIENTS AI, together with big data has the potential to transform care delivery methods and can provide great benefits at several levels of the healthcare value chain . However, as with any new technology, there may also be unrealistic expectations . Artificial intelligence has risks, limitations and concerns including ethical, technical, and legal issues, which are often closely connected . AI depends on the availability of very large amounts of good/quality data . If the available data are not enough, not good quality, inconsistent, or biased, this limits the potential of AI to be useful. AI also has the potential to make wrong decisions; reliability and safety are particularly critical in healthcare, where errors can have serious consequences. The EHDS can surely play an important role in making sure that European AI solutions will be built on unbiased and good quality data . The EHDS framework can facilitate AI manufacturers' access to data in a secure and compli ant framework in line with GDPR rules while minimis ing potential risks in terms of data protection. The EHDS should also ensure that AI is built on good quality and unbiased data: through technical support, the EHDS can ensure that data will be by default suitable for AI purposes. Furthermore, the development of AI and machine learning also creates significant ethical risks , including in relation to the anonymisation and pseudonymisation of data, which poses risks to the privacy of individuals (e.g. thro ugh reverse engineering of data to identify individuals). A strong governance approach, that includes patient representation, should be embedded in the EHDS, ensuring that ethical risks are quickly identified and managed. Finally, the EHDS should indeed al so serve as a supporting framework to promote a harmonised approach to assess AI products and services for medicine s agencies, notified bodies or other competent bodies. Finally, the EHDS should carefully consider the type of data use and AI, between data used for public good versus commercial benefit. Collaboration within the EHDS for businesses and companies should be therefore guided by criteria of value and legitimacy (e.g. through participation in EU funded research, or return of results/data insights). For an overview of EPF s broader view on AI in healthcare , it is possible to consult our response to the European Commission White Paper on AI .10 10 EPF, Response and Accompanying Paper - Public consultation on the White Paper on Artificial Intelligence , (, -patient.eu/globalassets/documents/ -ai-white -paper_consultation - response_epf_statement -final.pdf 11 AI and the EHDS How to shape the new relationship s between patients and healthcare p rofessionals AI is already creating a new type of relationship between patients and healthcare professionals . AI can be seen as a way to both facilitate healthcare professionals in delivering better care to patients while, at the same time, provide patient s with additional tools to have a more informed dialogue with their doctors through enhanced control and monitoring of their medical condition. However, this potential two -way positive new relationship comes with a series of questions related to human over sight on AI decisions, limiting human autonomy and potentially even issues in terms of increased social isolation and loss of the essential human component in healthcare. In EPF view, the adoption of AI within healthcare should be seen as a support element , and not a replacement, to the traditional way of delivering care. Professionals must have oversight of decisions, as they should be informed by AI, not directly made by AI . This should be supported by ad equate skills development guaranteed to healthcare professionals to make them able to understand, securely and efficiently exploit the potential of AI to provide more efficient care to their patients. On the other hand, digital health literacy for patients also plays a crucial role to enhance their trust a nd understanding of the role of AI in their care and to better engage with it in collaboration, where possible, with healthcare professionals. AI and the EHDS Addressing key ethical risks Ethicists have identified a risk of limiting human autonomy if AI were to make a calculation on risk or restrict a patient s right to free, fully informed choice of treatment . An example would be if an AI system made certain decisions based on what it thinks is the best for the patient. Maintaining human oversight of AI -based decisions and the decisions flowing from it is thus particularly important in healthcare. When discussing AI in healthcare, it will be fundamental to keep in mind the essential relation between the AI systems, healthcare professional s and patients. As previously mentioned, AI must be seen as a support tool to improve care delivered by healthcare professionals (from diagnosis to treatment), but not as a replacement . Furthermore, AI, if used to replace real human contact, may increase social isolation and additional stress. This approach should clearly apply beyond clinical practice, when AI is used to inform broader delivery of services, public health interventions, and policy making in the field of healthcare. Biases in data also introduce ethical issues in terms of the potential for AI -enabled decisions themselves to be biased or discriminatory . Biases in data collection can affect the type of patterns AI will identify. This is an issue since, for example, women and ethnic mi norities are often underrepresented in clinical trials and large data sets used to train AI. Bias in the data will affect the algorithm that is developed, replicating the bias found in society. Patients with multiple or rare diseases may also be affected b y this. This issue should be tackled by making sure that AI is based on good quality and unbiased data. Transparency is another key issue when it comes to Artificial Intelligence: as previously stated, explainable and ethical AI solutions should be preferr ed over black box methodologies, with rules for transparency and data governance. Clear rules, strategies, risk management and certification mechanisms will also have an impact on user confidence in AI -based products and services. 12 EPF calls for particula r attention in ensuring that AI in healthcare enhances society, and is an enabler of and not a threat to patients rights and wellbeing, guaranteeing that the value of real human contact is not minimised or entirely replaced by technological alternativ es. Finally , a crucial point related to AI in healthcare is linked to the crucial role of information for patients: patients have the right to be fully informed about the functionality, consequences, and possible consequences of AI incorporation in e.g., health information, diagnosis and treatment procedures, health monitoring, transactions, and interaction. As matter of prudence, responsible parties ( e.g., health professionals, authorities, industry) should follow the existing principles for informed con sent and decision making.",en,"EPF would also l ike to emphasise the importance of sustainability in the context of research and innovation . Investment in this field should also include funding to ensure the sustainability of high - value assets developed by past and present initiatives , on which future innovation can be built. For health research and innovation projects, ensuring the sustainability of clinical assets would recognise the valuable contributions patients make to these projects, sacrificing their time and, in some cases, exposing themselves to risks. Research into the design of health and social care and how care is delivered, can add significant value in providing evidenc e for targeting resources efficiently, thus contributing to the sustainability of health systems. Proper use of health data can improve health systems sustainability, increase the quality , safety and patient - centred ness of healthcare, decrease costs and transform care int o a more participatory process .3 Health data can support the work of regulatory bodies, facilitating the assessment of medical products and demonstration of their safety and efficacy.",risk
Equifax (Spain),F2665520,06 August 2021,Company/business,Large (250 or more),Spain,"Proposal for a Regulation laying down harmonised rules on AI: Com/2021/206 Equifax response to the request for feedback 6 August 2021 Areas where we agree with the proposals We agree with three fundamental aspects of the Commission s proposals: Systems to evaluate creditworthiness and establish credit scores that use more advanced machine learning can outperform traditional methods, with many bene ts for borrowers, lenders and the economy. These bene ts include more access to credit for consumers and SMEs, reductions in the cost of credit and a reduction in the rate of overindebtedness/non-performing loans. For example, Equifax has successfully submitted an application to the Bank of Spain s sandbox for a fully explainable credit risk model that uses machine learning. Using AI in such systems without high standards of model governance and explainability would increase risks for borrowers, lenders and the economy. These risks include non-compliance with GDPR, loss of control over loan origination models and an increase in the rate of overindebtedness. It would also lead to some consumers who are denied credit getting inaccurate explanations for lending decisions. So the regulatory framework for loan origination should promote innovation and use of AI systems, while maintaining existing high standards for model governance and explainability. Our main concerns and recommendations We have serious concerns that some aspects of the proposals will have a signi cant negative impact on the price and availability of credit by greatly increasing the costs for the users and providers of all systems that evaluate creditworthiness and by reducing the pace of innovation in the EU. In particular: The de nition of AI includes fully explainable techniques plus systems that are not highly complex and that do not continuously adapt. These systems are low risk and in many cases, such as credit scores, have been used safely for decades. Including them in the de nition greatly expands the scope and burden of the proposed regulations yet delivers little or no additional consumer protection. The evidence that horizontal AI regulation designating systems that evaluate creditworthiness as high risk will improve access to nancial services, electricity and housing is far too weak. It is much more likely that the proposed regulation will have the opposite e ect - increasing costs and holding back innovation that would otherwise improve the accuracy of creditworthiness assessments and access to responsible, a ordable credit. Furthermore, these systems and are already subject to separate regulation by banking and nancial services regulators. To ensure the Proposal is proportionate and risk-based, we strongly recommend the Commission better target the de nition of AI, and remove from the list of high risk systems those that evaluate the creditworthiness of natural persons or establish their credit score. The de nition of AI should be more speci c and better targeted so the Proposals achieve their objective in a risk-based and proportionate way The de nition of AI, as well as the list of techniques covered in Annex I, is too vague and too wide. The de nition includes statistical approaches , which could encompass the most straightforward techniques, such as calculating an average from a series of numbers. It also includes expert systems , which could be the most basic decision tree based on business experience. The Commission s impact assessment (section states that the main problem driver is the speci c characteristics of AI systems which make them qualitatively di erent from previous technological advancements . These characteristics are then described in detail in Annex IV. However the proposed de nition of AI is so wide that it emcompasses techniques that do not have those characteristics and which cannot be described as quantitatively di erent from previous advancements. For example, logistic regression is covered by the proposed de nition of AI. Logistic regression is an AI technique but it is not new or a technique that is quantitatively di erent from previous advancements - it 2has been very widely used in credit scoring since the 1950s. In the table below, we show that logistic regression does not have the characteristics of what the impact assessment de nes as a problematic AI system that requires further regulation: Characteristic of a problematic AI system (taken from annex IV of the impact assessment) Characteristic of systems that use logistic regression Does a logistic regression system have this characteristic of a problematic AI model? Complexity : AI systems often have many di erent components and process very large amounts of data. This amount of parameters are not in practice understandable for humans, including for their designers and developers. The systems and their parameters are intrinsically fully interpretable and explainable, ex ante. No. Transparency/opacity in: how the AI system functions as a whole (functional transparency); how the algorithm was realized in code (structural transparency) and how the program actually runs in a particular case, including the hardware and input data (run transparency). A logistic regression system is fully transparent in all three aspects; it is not a black box . No. Continuous adaptation: the process by which an AI system can improve its own performance by learning from experience. Unpredictability: where the outcome of an AI system cannot be fully determined Logistic regression systems are fully completed when they enter the market; they do not adapt or self-learn unless they are continuously retrained with new data. Logistic regression systems are also fully predictable because they are stable and fully transparent. No. Autonomous behaviour: functional ability of a system to perform a task with minimum or no direct human control or supervision Logistic regression can be used in autonomous models, such as credit scoring. However, the steps are programmed and can be fully understood by humans. Yes, partly. Data: the dependence of AI systems on data and their ability to infer correlations from data input can in certain situations a ect the values on which the EU is founded, create real health risk, disproportionately adverse or discriminatory results, reinforce systemic biases and possibly even create new ones. This characteristic could apply to any method of taking decisions that use data, even those taken by a human with no algorithm involved such as a jury reviewing evidence in trial. The risk is surely lower in a logistic regression system that is far less complex than some AI techniques, is fully transparent and does not continually adapt. No, not to a signi cant or growing extent. 3The lack of clarity and the breadth of the de nition of AI are a major concern because they greatly increase the scope of the proposed regulation and the regulatory burden it will place on organisations that use low risk systems.Organisations that use low risk statistical approaches, and have done so for decades, will fall into the scope of the AI Act; disincentivizing innovation and increasing their costs even though the low risk to citizens has not changed. We strongly recommend that: The de nition be better targeted on high risk techniques so that the proposed AI Act achieves the Commission s goal of creating a proportionate regulatory system centred on a well-de ned risk-based regulatory approach . Expert systems, statistical approaches, Bayesian estimation, search and optimization methods should be removed from the de nition. The de nition should exclude logistic regression and other techniques that are as explainable and as predictable as logistic regression. AI systems intended to be used to evaluate the creditworthiness of natural persons or establish their credit score should be removed from the list of high risk AI systems The evidence for including systems that evaluate creditworthiness and credit scores in the list of high risk systems is too weak. It also fails to recognise that any risks are, and can be, mitigated more e ciently and e ectively through vertical, sector-speci c regulation. Designating systems for evaluating creditworthiness and credit scores as high risk will increase the price of credit for consumers and SMEs. It will also disincentivise innovation that would otherwise reduce the rate of overindebtedness and improve access to responsible credit. Recitals 32 and 37 states that systems are high risk if they pose a high risk of harm to the health and safety or the fundamental rights of persons and that AI systems used to evaluate the credit score or creditworthiness of natural persons should be classi ed as high-risk AI systems, since they determine those persons access to nancial resources or essential services such as housing, electricity, and telecommunication services . What evidence there is to support this is set out in Annex IV of the impact assessment. Focussing rst on the use of AI systems to evaluate creditworthiness and establish credit scores in nancial services, we do not agree that they pose a su ciently high risk, or su ciently determine access to nancial resources, to justify inclusion in the list of high risk systems. Our reasons are: Risks are already e ectively mitigated by existing regulation. Financial and data protection regulation at EU and Member State level mitigates risk, giving borrowers the right to seek explanations, manual review and redress. This includes existing sector-speci c regulation (notably the Consumer Credit Directive and Mortgage Credit Directive, and the EBA guidelines on loan origination and monitoring that speci cally address models for creditworthiness assessments that are technology-enabled). There is also horizontal legislation, in particular GDPR. Creditors are regulated entities and Member States have the option to regulate credit reference agencies too should they wish to. A 2021 survey of European credit providers found half were regulated at Member State level and half operated in Member States that have not judged regulation of credit reference agencies necessary or proportionate. Based on the proposed de nition of AI, it is wrong to say the use of AI systems to evaluate creditworthiness is growing. The impact assessment (table 7, annex IV) cites growing use by credit bureaux and in the nancial sector as an essential reason to designate systems that evaluate creditworthiness as high risk. Similarly, the EDPB and EDPS partly justify the need for an AI Act on the basis that AI will lead to more predictions being made and algorithmic decision making replacing human made decisions. This is not the case in creditworthiness assessments. The number of predictions about creditworthiness that are done rises and falls with the number of applications consumers make for credit. If algorithms have replaced human activity in those 4assessments then that happened decades ago. Displacement of human activity is not happening now, or in the future. Any emerging risks that do arise from the use of AI in creditworthiness assessments and credit scoring in nancial services can be mitigated through new nancial services regulation. This includes new EBA guidelines, changes to nancial legislation (such as the measures in the the new proposed Consumer Credit Directive that prohibit some datasets from being used in creditworthiness assessments) and through GDPR (enforcement, guidelines and approved codes of conduct). New nancial regulation can impose high standards for model governance and explainability in systems that use AI techniques, which Equifax supported in our response to a previous Commission consultation on the AI Act. The evidence that inaccurate or biased creditworthiness systems or credit scores are a signi cant barrier to accessing credit is very weak . The Commission s own research on nancial exclusion found that only in Romania, Bulgaria, Slovenia, Slovakia and Portugal do more than 1% of people live in households for which an application for credit or a loan has been turned down and only 2% of people across the EU think that there is no point in applying for a credit facility because it would be refused . Note, that is not to say creditworthiness assessments for those very small populations are unfair or inaccurate. At no point does that report suggest inaccurate creditworthiness models are a barrier to accessing credit rather it nds that those who want to access credit and cannot do so tend to be on too low an income to a ord repayments. In other words, creditworthiness assessments provide e ective protection from overindebtedness. The use case is incorrectly targeted. Even if the argument were accepted that horizontal regulation is needed to protect consumers' access to nancial services, it is wrong to focus on systems that evaluate creditworthiness isolation since they do not determine people s access to nancial services. A creditworthiness assessment is one factor in a lender s decision to originate a loan or not. Lenders also consider a ordability assessments, ID veri cation, counter fraud checks and the lender s overall risk management and business model. A lender could use an AI system in any step of that lending process; meaning a consumer may pass a creditworthiness assessment but be denied credit by an AI system the lender uses to choose who to lend to for commercial reasons or to ag potential frauds. If the Commission s aim is to protect access to nancial services then it does not make sense to apply a higher bar to one AI system a lender may use in loan origination decisions and not the others. The evidence that AI systems for creditworthiness assessments and credit scoring signi cantly a ect access to essential services such as housing, electricity, and telecommunication services is even weaker than for nancial services. There is no evidence in the impact assessment or the proposals that designating systems for evaluating creditworthiness assessment or credit scores as high risk will improve access to housing. Nor are such systems cited as a barrier to accessing housing in more robust reports for the Commission or by groups ghting homelessness in the EU. If landlords are using credit scores in decisions about which tenants to accept, it is clear this is insigni cant in determining access to housing compared to the real systemic causes of exclusion cited in the Commission s rigorous study. The Commission concluded that the actual reasons for housing exclusion have nothing to do with creditworthiness or credit scores, rather the causes are problems such as: A lack of a ordable housing Poverty, unemployment, the low level of welfare bene ts, the lack of social protection, the changing nature of work Legal obstacles (e.g. lack of documents, regularisation procedures) Individual and family related causes (e.g. divorce and/or separation, family violence, drug and alcohol addiction problems, and health and mental health problems) 5So if the Commission s aim is to protect access to housing, designating credit scores and creditworthiness assessments as high risk AI systems is a disproportionate and misguided response because it will yield no signi cant bene t. As in lending, credit scores and creditworthiness assessments are however widely used by some providers of electricity and telecommunication services as part of their decision about whether to o er a contract to consumers on a credit basis (rather than pay as you go ). Again though, it is wrong to say that such credit scores and creditworthiness assessments determine access to those services they are at most just one element in the provider s decision about which clients to service and on what basis. Furthermore, the evidence that those systems for evaluating creditworthiness or credit scores are a signi cant problem that prevents people from accessing electricity and telecommunication services is very weak. The EU has carried out detailed work on the digital divide and how to overcome it. There is no evidence in that work that credit scores or creditworthiness assessments are a signi cant driver of exclusion from telecommunications. Similarly, a report for the European Parliament identi ed the three main causes of energy poverty as: energy prices, falling household incomes, and living in an energy ine cient home. Those causes have nothing at all to do with systems for evaluating creditworthiness or credit scores. It is also important to note that providers of energy and telecommunications have used these systems for evaluating creditworthiness for many years and the risks that arise from that activity have not changed. To protect consumers in these markets, there is sector speci c legislation and regulation at EU and Member State level, as well as protection from horizontal legislation such as GDPR. Any residual or emerging risk can be tackled through proportionate sector speci c regulation and enforcement of GDPR. Designating AI systems that evaluate creditworthiness as high risk will have no signi cant impact on access to electricity and telecommunications. It will however impose a disproportionate burden on rms, discourage innovation and likely increase costs for the people that use those services as well as all users and suppliers of credit information. We strongly recommend: The Commission remove AI systems used to evaluate the credit score or creditworthiness of natural persons from the list of high risk systems. As the Commission will have to power to update the list of high risk systems over time, it can better address its concerns by rst investing in independent, credible research to better understand the actual extent to which AI systems for creditworthiness assessments and credit scores impact on people s access to nance, housing, electricity and telecommunications respectively relative to other factors, and the extent to which further regulation would be proportionate and bene cial. Additional concerns and recommendations Giving the Commission the power to expand the list of high risk AI systems through delegated acts will create signi cant legal uncertainty in the market . The unpredictable evolution of the scope of the regulation could discourage developers from developing and implementing innovative AI solutions in the EU. The proposals should be changed to deliver a more stable and predictable regulatory framework. Transparency requirements must more clearly protect intellectual property. For example, were providers of AI systems forced to reveal too much detail on algorithms then users could copy those products. That would greatly limit investment and innovation in AI in the EU. As regards the duty of transparency, this should be done under the terms and conditions already regulated by the GPGR. Article f of GDPR states that ""the existence of automated decisions, including pro ling, referred to in Article 22, paragraphs 1 and 4, and, at least in such cases, meaningful information about the logic applied, as well as the signi cance and expected consequences of such processing for the data subject, must be reported"". This is a concise regulation, already in place in all Member States, and so transparency in AI must necessarily be linked to what is already stipulated in GDPR. 6Too many organisations will be responsible for enforcing and administering the AI regulations in the area of creditworthiness/loan origination. It is not clear how they will work together to apply the regulations in a harmonious way. Fragmentation, uncertainty and inconsistency in the application of the regulations will be a barrier to the creation of a single market in credit and digital nancial services. It would also have a negative impact on consumer protection, the cost of compliance for businesses, and on investment and innovation in AI. For example, where AI systems are provided or used by regulated credit institutions, the proposals seem to envisage that Member States data protection authorities will enforce the regulations alongside the authorities responsible for the supervision of the Union s nancial services legislation at Member State and EU-level. It is not clear how these divisions of responsibilities and interfaces will work in practice, how those authorities would consider relevant EBA guidelines, or how consistency will be achieved for consumers, providers and users across di erent Member States. Nor is it clear who would oversee a provider of AI systems to regulated nancial institutions where the provider itself is not a regulated nancial institution. For example, in Spain, credit reference agencies that provide models for creditworthiness assessments are not regulated by a nancial authority but the creditors that use those models are. We also note that the competencies of data protection control authorities are de ned in article 55 of the GDPR. Supervising AI models would seem to exceed what is regulated in that article. Some authorities responsible for enforcing the regulations will likely nd it di cult to acquire the capacity, experience and expertise they will need to ful ll their responsibilities e ectively and e ciently. For example, a data protection authority that became fully or partly responsible for enforcing regulations relating to the accuracy and robustness of a credit score model would have little or no experience and expertise in this area today. Furthermore, the broader the de nition of AI and the more products that are in scope, then the greater the regulators workloads will be. That will make it harder for regulators to ful ll their duties and reach judgements quickly enough to protect consumers and avoid becoming a bottleneck on innovation. The Act should be better targeted and its implementation delayed until authorities are able to implement it e ciently and e ectively. About Equifax Equifax is a global data, analytics, and technology company and a credit reference agency. Headquartered in the USA, Equifax operates or has investments in 24 countries in Europe, North America, Central and South America and the Asia Paci c region. It is a member of Standard & Poor s 500 Index and employs approximately 11,000 people worldwide. In Europe, Equifax operates in Spain, Portugal, Ireland and the UK. For more information, please contact: Stuart Holland, Head of Government Relations - Europe, stuart.holland@equifax.com 7",en,"Similarly, a report for the European Parliament identi ed the three main causes of energy poverty as: energy prices, falling household incomes, and living in an energy ine cient home. It is also important to note that providers of energy and telecommunications have used these systems for evaluating creditworthiness for many years and the risks that arise from that activity have not changed.",risk
Enel SpA (Italy),F2665516,06 August 2021,Company/business,Large (250 or more),Italy,"Dear Members of DG CNECT A.2, Enel SpA, a multinational company in the energy sector , highly appreciates the EC proposal for a regulation aimed to create the conditions for an ecosystem of trust for Artificial Intelligence products and services, on the EU market . At Enel, we use Artificial Intelligence (AI) to make the energy and power systems more efficient, more predictable and more sustainable, for instance making easier for our business customers to play a more active role in the liberalized energy market. In Enel s view, to establish a legislative framework which would harmonize rules for placing AI on the European market, within safe and ethical boundaries , the following set of provisions deem consideration and further action: Review of definitions, with focus on requirements f or high -risk AI systems (A rticle 1 to Article The AI Regulation considers high -risk systems, posing significant risks to the persons , AI systems intended to be used as safety components in the management and operation of critical infrastructure s (supply of gas, heating and electricity ); as well as essential private and public services (Annex . Enel would like to highlight that other legislative dossiers under definition are dealing with similar terms to essential private and public services and critical infrastructures such as: essential entities and operators of e ssential services (both proposed at the recast of the NIS Directive ), critical entities and essential services (proposed in the Directive for Resilience of critical entities) critical assets (included at ACER framework guideline for a network code on cybersecurity ), essential business processes (at the recommendations for a network code on cybersecurity); The relation of the listed terms with essential private and public services is not clear , and therefore nor obligations for companies in the energy sector . In addition, the lack of a n EU common restricted list of infrastructures considered critical , will add uncertainty and bring an inhomogeneous implementation of the Regulation on AI on the energy sector. To avoid possible future imbalances in the application of the AI Regulation (e.g. smart meters could be considered critical infrastructures in a certain number o f Member States but not in all), we call the Commission for a EU restricted approach that would enhance clarity, compliance and a level implementation of the Regulation . Moreover, we call the Commission for monitoring for a homogeneous implementation of the AI Regulations in all Member States in order to avoid any difference that could impact on critical services delivery and on related supply chains (that can include cross -border participants) . Hence, Enel believes necessary to better define a concrete methodology to assess what action on a safety component or on an essential service might be considered high -risk . Poor categories and definitions might deter private investments and become a competitive disadvantage to European companies. Within this view, the different classes of use -cases must be integrated . As an example of a use case related to critical infrastructure which might not be considered of high -risk, predictive maintenance of components might be remarkable. With the aim of considering the multiple specifities of the energy sector, Enel calls on t he Commission to open the work on a methodology for a likely future expansion of the list of high -risk AI systems, to public feedback by means of an open consultation, and take it into consideration before it will be adopted. Furthermore, when updating the list in Annex III by adding high -risk AI systems, Enel considers that the Commission shall take into account the measures for prevention of such risks, among the criteria for the assessment. Apart from the consideration of what AI systems might be classified as high -risk , Enel would like to point out that certain terms at the Regulation require further clarification or an adequate definition: high -quality datasets to test bias: given the crucial function of this definition for the future trust a nd development on AI, the distinction from what is high -quality from what is not, should be carefully defined . substantial modifications to high -risk AI systems during their life cycle, will require re - certification. To level the AI certifications market and ensure that certain modifications get back on certification schemes in the whole EU, what is a substantial modification should be appropriately defined under the AI Regulation. unicorn should be added to Article 3, given that the definition was sett led on 2003 and unicorn was created as definition 10 years later. The definition of biometric categorisation system , to be complete should include more categories such as: o physiological: fingerprints, height, weight, colour and size of the iris, retina, outline of the hand, palm of the hand, vascularisation, the shape of the ear, the physiognomy of the face o behavioural: the vocal imprint, graphic writing, signature, typing style on the keyboard, body movements The definition of a publicly accessible space and of what is considered public sec urity , need clarification. By way of illustration , the Regulation leaves unclear if, in case of an attack to an electric network through a n AI system, the incident will be categorized as a threat for public security. Cybersecurity requirements (Article 15 and Article Following the last considerations on public security and cybersecurity, Enel highly encourages the Commission to avoid the development of heterogeneous cyber security requirements, by choosing a unique actor (e.g. ENISA) that will identify and/or will define the adequate cybersecurity standards, to be used for: software development, that could positively affect the supply chain, using SSDLC (Secure Software Development Lifecycle) built on the security by design paradigm1, a risk-based delivery of AI service, for which Enel has already adopted similar guidelines; also for critical infrast ructure and data protection. In general, Enel strongly advices that security standards must exclude certifications, and shall be adopted only on a voluntary basis. In article 42, Presumption of conformity with certain requirements , it is explained that the fulfilment of the measures reported in art icle 15 are no longer necessary if the product is certified according to the certification defined by (i) Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency for Cybersecurity) and on (ii) information and communications technology cybersecurity certification and repealing Regulation (EU) No 526/2013 (Cybersecurity Act) . The adoption of certification schemes could become a constraint ( nowadays the fulfilment of art icle 15 and certification schemes are considered alternatives) , and if relevant actors opt for a certification scheme such as the Common Criteria (sponsored by Germany and France) or a similar one. In fact, the Common Crite ria is not only a long, onerous and costly product certification scheme, but it is also very rigid and therefore unsuitable to the rapid evolution of digitalisation. In other words, indistinctively, a small or huge functional modification of a product could require a reboot of the certification process, with the consequent impacts. Transparency obligations and codes of conduct (Article 52 and Article Enel deems fundamental that natural persons will be informed when they are interacting with an AI system . This need is particularly evident in the case of emotion recognition systems, a biometric categorization system, AI systems that generate or manipulate image s, audio or video content. Enel considers paramount to define clearly and list all the categories of AI systems that should be introduced in article 52 . To do so, Enel recommends to start with the definition of the impacts aimed to avoid , and later define the measures that should be put in place. Transparency and e xplai nability are key, while Europe s proactive AI regulation, could also consider new civil liability laws for products that contain AI, aligning liabilities between EU 1 In the Strategia nazionale per l intelligenza artificiale , published last September, the Italian Government promote s an approach by AI technology providers based not only on the principle of ethics by design but also on trustworthiness by governance, c apable of ensuring reliability to the entire IA product cycl e, both they are static systems (once trained, they are deterministic in their executive phase ), and even more in the case of systems with continuous learning and evolution. Member States to avoid incongruity among the Union . Besides, liabilities should be clarified for each stage of the development life cycle of AI systems: at the learning phase , there should be a very clear definition of liabilities, ensuring the data quality minimize the risk of biases. For High -Risk AI systems the learning should be always supervised and the cybersecurity assurance level should be the highest. during the running phase there would be new emerging biases, hence it is highly recommended to clarify liabilities for these checks . to enhance trust, the supply chains of high -risk AI systems should be transparent and contain a clear definition of responsibilities and liabilities. Support measures for innovation art 53 -55 On AI topics we need to foster experimentation and innovation to guarantee always state - of-the-art technologies. To correctly evaluate the goodness of an experimentation, a benchmark with traditional approaches should be provided, just to support reliabili ty and trust of the results. Both the innovative and traditional approach should use the same set of data, validated from a data owner, who is responsible of them and of metadata. Data owner will guarantee data quality and regulate the access to them for t he different users. As last remark, Enel would like to reiterate its support to the Commission proposal on an AI Regulation, representing a fundamental cornerstone of the EU digital strategy aim ed to promote human -centred norms and EU standards on the global stage , while ensuring not only the security and resilience of its digital supply chains, but also the delivery of global solutions. These goals will be achieved by alignment between the various approaches of the Regul ation adopted in different Member States , and national policies (e.g. the Italian Strategia nazionale per l intelligenza artificiale , and the Spanish Estrategia Nacional de Inteligencia Artificial ). Therefore clarification or a single identification me thodology for what is considered high -risk AI systems in the energy sector should be established, leading to a more predictable identification and closer alignment of Mem ber States.",en,"Dear Members of DG CNECT A.2, Enel SpA, a multinational company in the energy sector , highly appreciates the EC proposal for a regulation aimed to create the conditions for an ecosystem of trust for Artificial Intelligence products and services, on the EU market . At Enel, we use Artificial Intelligence (AI) to make the energy and power systems more efficient, more predictable and more sustainable, for instance making easier for our business customers to play a more active role in the liberalized energy market. Enel would like to highlight that other legislative dossiers under definition are dealing with similar terms to essential private and public services and critical infrastructures such as: essential entities and operators of e ssential services (both proposed at the recast of the NIS Directive ), critical entities and essential services (proposed in the Directive for Resilience of critical entities) critical assets (included at ACER framework guideline for a network code on cybersecurity ), essential business processes (at the recommendations for a network code on cybersecurity); The relation of the listed terms with essential private and public services is not clear , and therefore nor obligations for companies in the energy sector . In addition, the lack of a n EU common restricted list of infrastructures considered critical , will add uncertainty and bring an inhomogeneous implementation of the Regulation on AI on the energy sector. With the aim of considering the multiple specifities of the energy sector, Enel calls on t he Commission to open the work on a methodology for a likely future expansion of the list of high -risk AI systems, to public feedback by means of an open consultation, and take it into consideration before it will be adopted. Therefore clarification or a single identification me thodology for what is considered high -risk AI systems in the energy sector should be established, leading to a more predictable identification and closer alignment of Mem ber States.",risk
AI4Belgium (Belgium),F2665502,06 August 2021,Other,Large (250 or more),Belgium,"1 AI4Belgium Feedback Note on the European Commission s Proposal for an Artificial Intelligence Act Key feedback points Overall, the proposed regulatory framework is much appreciated and welcomed. The proposal s list-based approach risks being incomplete, and it requires periodic assessments. The scope of the proposal requires further refinement, and overlap with sectorial legislation that already covers AI needs to be more closely considered. Further clarifications are needed for certain terms that are used throughout the proposal. The requirement of ex ante third party conformity assessments could potentially be extended in certain cases. Some open questions remain around the value of the proposed CE label. Concerns about the regulation s enforcement mechanism should be addressed. There is a need for guidance as to how the requirements should be implemented. Additional efforts are needed to stimulate innovation. More attention should be given to the impact of AI on societal interests and the environment. Introduction AI4Belgium is the Belgian coalition of key actors in AI from academia, the private sector, the public sector and civil society. It enables individuals and organisations in Belgium to capture the opportunities of AI while facilitating the ongoing transition towards the technology s increased adoption in a responsible and trustworthy manner. AI4Belgium has the ambition to position Belgium and its regions on the European AI landscape, drawing on the many assets vested in the Belgian AI ecosystem, from high quality researchers, excellent entrepreneurs and companies to innovative public entities, all the while being mindful of the ethical, legal and social challenges that this technology brings forth. AI4Belgium welcomes the opportunity to provide feedback on the European Commission s proposed Artificial Intelligence Act in the form of a new EU regulation (the proposed AI Regulation or the proposal ), which was published on 21 April The Commission s proposal was overall well-received by the AI4Belgium community, and is considered to be a strong starting point to ensure that AI systems are deployed in a manner that respects fundamental rights within an environment that is keen on socially beneficial innovation. There is, however, still some scope for clarification and specification throughout the proposed AI Regulation. Without aiming to be exhaustive, this feedback note provides an overview of the main considerations formulated by the AI4Belgium community. This note was prepared in the context of a virtual workshop organised by AI4Belgium s Working Group on AI Ethics & Law on 18 May 2021 for the members of the AI4Belgium community, in 2 cooperation with the different regional AI hubs, including Kenniscentrum Data & Maatschappij, CRIDS/NADI and the AI Institute for the Common Good (FARI). During and after the workshop, feedback was gathered on the proposed AI Regulation s strengths and weaknesses from the perspective of the Belgian AI ecosystem. On this basis, a first draft note was prepared, which was subsequently circulated within the AI4Belgium community in order to give all members an opportunity to consult the document and provide further input. The end result comprises the consolidated feedback of the AI4Belgium members on the proposed AI Regulation. In what follows, the key points of feedback that were raised by the AI community are described, with suggestions on how the proposal could be further improved. Given the rich diversity of AI4Belgium s membership, there are certain aspects of the proposal on which members disagreed. In those instances, we have reflected this variety of opinions in this document, in order to provide a comprehensive overview. Overview of AI4Belgium s feedback Overall, the proposed regulatory framework is much appreciated and welcomed AI4Belgium commends the Commission s overall approach to the regulatory framework for AI that is being put forward in the proposal. The debate on an adequate ethical and legal framework for Artificial Intelligence has been ongoing for several years now, and the time was more than ripe to propose a number of binding rules to ensure that individuals and organizations can trust AI systems that are deployed across the EU through verifiable procedures rather than voluntary guidelines. These procedures to enable trust are not only important to ensure compliance with fundamental rights, but also to stimulate the adoption (and intra-EU trade) of AI, and to capture the benefits that this technology can generate. It can be hoped that the EU model will be adopted beyond the Union s borders, and provisions about the extraterritorial effect of the proposed regulation are hence also welcome. The emphasis on regulating the use of AI rather than the technology itself is an advantage of the proposal. At the same time, given that AI systems can be repurposed for various uses, the Commission s proposed design, development and deployment requirements (in terms of risk-management, data governance, technical documentation, transparency, human oversight, and accuracy, robustness and cybersecurity) are essential, and will need to be translated into practice through various methods, which can also be adapted to the specific sectors in which the systems are used. The list-based approach of the proposal risks being incomplete, and it requires periodic assessments AI4Belgium members particularly appreciated the risk-based approach of the proposed AI regulation, and the important signposting of risk levels by way of the chapters headings (namely a set of prohibited AI practices, a set of high-risk AI practices, a set of AI practices that require further transparency, and other AI applications). Some members, however, rightfully remarked that this list-based approach risks being incomplete. Others pointed towards the risk of the list to become overly-inclusive. It is acknowledged that it is difficult to ensure the comprehensiveness of such lists from the outset. It is therefore crucial that the said lists are updated periodically and in a speedy manner, without the need to revisit the entire regulatory framework. 3 While such procedure is explicitly foreseen in the context of high-risk AI systems (through delegated powers granted to the European Commission), this procedure is currently lacking as regards the updating of the list of prohibited AI systems. Some members therefore pointed out that it may be advisable to include such a procedure also for the list of prohibited systems, in order to enable its period review and revision, just as is the case for high-risk AI systems. Other members, however, disagreed, and noted that the line between prohibited systems and high-risk systems should remain in place, in particular by ensuring that the list of prohibited systems can only be revised through an amendment of the regulation itself. In any event, when the Commission applies its delegated powers to update these lists, it is essential that it ensures full transparency during the entire process and involves a broad set of stakeholders, so that representatives of civil society, industry, academia and the public sector can have their voices heard. Furthermore, the Commission should ensure that the delegated acts come with adequate transition time as development and testing of certain AI systems may take a long time. The Scope of the Regulation requires further refinement The approach in the proposed AI regulation consists of providing a broad definition of AI, by including both new and old approaches to this technology (e.g. not only machine learning but also symbolic logic). Many members considered that this approach is helpful to evaluate as many AI innovations as possible, since the focus should lay on the potentially problematic use rather than the underlying technology. If the same harmful conduct can be enabled through a classical type of AI system, this is equally problematic from a fundamental rights point of view. The broad definition of AI is supplemented with an Annex of concrete technologies that fall under its scope. One downside to this list concerns the fact that the quality of the scope of this regulation is now largely influenced by this annex (and by the clarity and comprehensiveness of the concepts listed in there), which relies on efforts from the Commission who can review this list over time. What is more, the list may ultimately lead to regulation shopping; innovations could be defined or framed by AI developers in a different way so as to fall inside or outside of the list, hence potentially leading to under-enforcement. The regulation s scope is, after all, mostly defined by whether or not a given application falls under any of the listed risk-categories (i.e., prohibited, high-risk, or requiring further transparency), and thereby excludes (AI) systems that do not fall under those problematic categories. Given the focus on the problematic use / consequences of technology rather than on the technology itself, many members propose keeping a broad definition of AI , which could, for instance, cover each practice or system that relies on automated processes, in particular where information is collected to make a choice among options. Hence, for each such practice or system it should be verified whether it falls under the risk categories covered by this regulation or not. Otherwise, the regulation risks having a bias towards problematic uses only by known/hyped AI systems, rather than by systems that potentially cause the same problematic impact but are not listed in Annex I. At the same time, however, some members also considered that, instead, the current definition is overly broad and also captures software which are not AI systems. They propose instead that the scope should be limited to AI systems which generate outputs that are not predetermined by 4 the natural person developing the system. They stress that, in fact, some AI tools often have no broader purpose beyond serving as building blocks for various user-designed applications, which in turn serve more specific user-generated intended purpose; such general-purpose tools are not in and of themselves AI systems, but rather serve as components or precursors of AI systems. The text of the Regulation should hence be more explicit regarding the allocation of responsibilities when it comes to general purpose tools, as in that case it is the user who often ultimately decides on the intended use of the AI system rather than the provider. Furthermore, the proposed definition also seems to focus on AI as a software exclusively. This can cause some concerns from the scientific community around the exclusion of AI as a hardware (e.g. in robotic devices). If the aim is to secure a broader definition of AI (which includes robotics), this should ideally be clarified in the text or through other guidance documents. Similarly, it would be beneficial to provide more clarity on whether and to which extent AI research is seen in scope of the AI Act, given that it is not directly related to putting an AI product in service or on the market. Finally, attention can be drawn to sectorial legislation that already legislates AI systems, in particular the medical device and in-vitro diagnostic regulation. The potential legislative and standardization overlap brings a risk of conflicts and could increase costs for AI providers, which in the area of AI in healthcare, for instance may ultimately be transposed onto the healthcare system or the patient. Great care should hence be taken to align the definitions and requirements of the proposed regulation and existing sectorial legislation. Further clarifications are needed as regards certain terms Putting on the Market/Into service: The proposal specifies that AI systems should comply with the regulation when placed on the market or put into service . This concept is already well-established and documented in existing legislation relating to physical products, but for digital products this concept is quite new. As digital systems often undergo multiple iterations and developments before they are considered fully implemented and ready to release, clarification is needed on when the AI system should be compliant. Some members pointed out that if, for example, compliance should already be ensured when a proof of concept is delivered to a client, this would significantly increase the costs of developing an AI business case while at that point there is no certainty the application will actually be purchased by the client. Citizen Scoring: Public services already assign scores to citizens in numerous contexts (from verifying eligibility to social benefits, to the risk of recidivism, and from the need for intervention to protect / place children to the chances of easily finding a job). Further clarification about the practices that fall under the prohibition on general citizen scoring would hence be welcomed, as the language that is currently used in the proposed regulation, and the examples provided by the European Commission in its presentations, are not always clear. Remote biometric identification system: The current definition is not entirely clear. Some may argue that remote is referring to cloud services, while others will consider this as referring to no-physical-contact identification systems (such as cameras). For reasons of legal certainty, it would be good to clarify this. Task allocation: Annex III bullet 4(b) considers AI systems as high-risk if these are used for task allocation in the context of employment, workers management and access to self-employment. Already today, a lot of software is used to assign tasks in production plants, call-centers, 5 distribution and warehousing plants, repair and maintenance scheduling, worklist orchestrators for homecare providers, or even to assign testing and coding tasks in software development pipelines. Further clarification is therefore needed on which type of system falls in scope of this category. Under their authority (user): Article 3( defines a user as any natural or legal person [ ] using an AI system under its authority, except [ ]. Given the importance thereof to understand the scope of the legal obligations for AI actors involved, the term under their authority should be clarified. Deployer: Some members suggested that it could be useful to give a definition of deployer . For instance, a deployer could refer to the entity that makes the AI system available for use in a specific operational context. Sometimes (e.g., if the system is custom-built for the deployer by a developer) the deployer might be the same as the provider. But this may not always be the case, for instance if general purpose AI systems are being used. Beyond these examples, several members pointed out - as overall comment on the Proposal - that its language is more generally too vague for a legislative document. As a result, a number of important terms are left undefined. Examples include terms such as state of the art , robustness , error in the context of error-free datasets , entire-life cycle of an AI system or impact on children . The risk of using language which is too vague is that the obligations that flow from it may be under- of over-inclusive. Furthermore, the explanatory memorandum (section indicates that the Commission s survey highlighted the need to define the notions of risk and harm . However, neither is currently defined in the proposal and, throughout the Act, the term risk is used in relation to many, often disparate meanings of harm. Some articles in the proposal relate to harm to health, safety and fundamental rights, others to breach of obligations to protect fundamental rights, or more vaguely other aspects of public interest protection , yet other articles refer to harm to critical infrastructure or harm to fundamental rights of data subjects. In addition, some articles point to the definition of product presenting a risk in the market surveillance regulation, but that definition refers to risks at national level and may hence not be applicable to risks to the individual where the AI system is customized/trained on the data of one person. The market surveillance definition also does not apply to AI systems sold as a service and lacks a clear inclusion of harm to fundamental rights. It would therefore be advisable to more clearly define these terms in the proposal itself, and to align the different articles that relate to risk. The requirement of ex ante third party conformity assessments could potentially be extended in certain cases Currently, the proposal stipulates that only AI systems intended to be used for real-time and post remote biometric identification of natural persons will be subject to an ex ante third party conformity assessment. Other AI systems classified as high-risk, like those used in the context of recruitment or predictive policing, will be subject to self-assessment by the provider. However, given the fact that such high-risk systems pose significant risks for individuals in terms of fundamental rights and societal impact, some members raised the need to consider whether the requirement of third-party conformity assessments should be extended to all high-risk AI systems. Such extension could be beneficial for all parties concerned, as it can enhance trust in 6 the fact that the AI systems meet the required standards. Moreover, this could also improve legal certainty by further reducing the risk of liability claims for unintended consequences of these high-risk AI systems. In such case, specific provisions about the status of auditors would be welcome. Other members, however, disagreed on this point. They consider that the application of third-party conformity assessments would go against the goal of taking a risk-based approach. They also noted that the use of third-party conformity assessment for only one high-risk AI system makes sense because that system, in different circumstances, is prohibited. The value of the proposed CE marking is questioned The CE-marking for AI applications in Europe can help to create trust and assurance in technology that is complex to understand for users. However, with the CE-marking, certain risks arise such as a potential false flag for unreliable products being marketed in Europe. To achieve the trustworthiness of such a CE-marking, there is a big need to have standards to support organizations to comply with the requirements. As it is, these standards do not yet exist, and it will take a huge effort to ensure that they are published in time before this regulation comes into effect. In this regard, when setting up these standards or best practices, it is also important to consider the views of various stakeholders, including civil society, so as to ensure that compliance with the requirements is not only claimed but can also be demonstrated and lead to earned trust. Moreover, the CE-marking has been developed for hardware and it is unclear how it would work for software, from the basics of how it can be affixed to the more complicated issue of how it would work in practice. This provision seems telling of the Commission s approach to AI coming from a product liability perspective, which can present serious limitations in certain instances, given the nature of AI. Indeed, many of the obligations set out in the Regulation seem most appropriate for the products covered in Annex II, such as the requirement to comply with harmonized standards or to undergo a new conformity assessment every time there is a substantial modification. These obligations may, however, be less suitable for AI services, since these services can often be deployed in an almost unlimited range of scenarios, and are already frequently updated. There is a need for guidance on how to implement the various requirements As noted above, many of the requirements contain vague language, and will hence need to be clarified through guidance from the European Commission in order to ensure their implementation. In this regard, lessons should be drawn from the implementation of the GDPR, where a lack of guidance in combination with high potential fines prompted organizations to massively seek consent from data subject, leading to overflooding citizens with messages and legal uncertainty on how to comply. To avoid the difficulties in interpretation that seem to be prevalent with the GDPR, it is important that guidance be provided at the European level (including through the European Artificial Intelligence Board). Providers, importers and users of AI operate in different countries and do business across the EU they hence need uniform guidance to implement these requirements, regardless of the member state in which they primarily operate. Attention should also be given to the way in which different types of AI work, such as platform and general-purpose systems. These systems may not be high-risk in the form initially placed on the 7 market, but may nonetheless be caught by the Regulation if used in a high-risk context later on. Unlike standalone AI systems, it would not always be feasible or desirable for providers to prescribe specific intended uses of these systems, as this may unduly restrict the customer s ability to deploy the system in an innovate way. Further, given that the user will often control how these systems are used including whether they are used in high-risk scenarios it may be appropriate to place certain compliance obligations on the user. At the moment, the way in which obligations are divided between providers and users can be confusing, which risks being an obstacle to innovation. While the proposal currently imposes the majority of its compliance obligations on the provider, on some occasions, and depending on the AI system at stake, the user may be better placed to ensure compliance with those obligations in practice. This can, for instance, be the case for some general purpose and platform AI systems, where the user may not only determine the intended use and context, but can often also provide the input data necessary to operate the system (which can have a substantial impact, for instance, on the system s accuracy or its potential bias). It should hence be considered whether additional obligations should be placed on the user from the outset, instead of only carving out circumstances in which a user may step into the shoes of the provider (Article 28(). In addition, it should be clarified whether (and if so, when) providers and organizational users of AI systems may contractually allocate their responsibilities under the proposal. In addition, guidance is needed as regards compliance with Annex IV. Although the need to explication and/or documentation related to the AI system is clear, the technical documentation request is far from complete in order to protect individuals. The terms used are often too vague, and the description seems to be based on stand-alone AI systems, which profit from a human data entry for training input and older fashioned training techniques. This part should hence ideally be reviewed in order to avoid misunderstandings in the context of a serial or parallel AI system architecture, or the use of newer training techniques such as, for instance, single shot/few shot/incremental or transfer learning. Additional points such as the context in which the system has been trained could be very valid to include as well, in order to avoid unintentional misuse in another environment. Finally, the Regulation should ideally also further clarify how providers of component parts should be treated (e.g., the provider of speech recognition technology in a broader AI system), since their position including their responsibilities are currently under-addressed. Concerns about the regulation s enforcement mechanism should be addressed The proposed regulation sets out various requirements that AI providers and users will need to meet, and establishes an enforcement structure to ensure compliance with those requirements. During the AI4Belgium feedback session, an overwhelming majority of members indicated their preference for a shared governance and enforcement approach, with a good balance between the national and the EU level. Currently, the regulation lays more focus on the national supervisory authorities who will be first in line to implement and enforce this regulation. However, lessons should in this regard be learned from the enforcement structure of the GDPR, where the guidance given by national DPA s was quite limited in the beginning. As an example, the first code of conduct was approved by the Belgian Data Protection Authority (DPA) 3 years after the GDPR came into effect. Moreover, enforcement of the GDPR is also the responsibility of national 8 authorities and due to unequal investments in the various member states authorities citizen protection is not at the same level in each EU country. Also in the context of this regulation, it should be considered that too much emphasis on the national level can lead to a risk of unequal implementation in different member states, at different speeds and potentially different interpretations. Belgium has, for instance, been lagging behind with regard to the implementation of the GDPR; this delay may affect innovation and a European level playing field, and the same risks to happen in the field of AI. Strong coordination at the European level will hence be crucial; also given the fact that many AI systems may be used transnationally and may be imported from third party countries. Moreover, given the importance of the risks attached to the use of AI as set out in this regulation, it will be essential that these authorities receive proper funding (and a sufficiently skilled workforce which may be difficult in this field) so that they can provide adequate guidance for organizations and ensure a high level of citizen protection. The issue of different implementation speeds will also affect the creation of codes of conduct that can be voluntarily applied to AI systems other than high-risk systems. If it is assumed that the creation of a code of conduct is roughly the same effort and cost for any sector or member state, this absolute cost will mean that there may be more codes of conducts for sectors and member states with a higher turnover. Smaller member states with smaller markets will thus likely have less means to create these codes of conduct. This is another reason why a common European approach would be preferential. In this regard, the obligation to appoint an authorized representative established in the European Union in case an importer cannot be identified (recital 56 and article is welcomed. Building on the experience with the GDPR, it is crucial to allow all organizations in charge of the implementation and enforcement of the regulation be it the national competent authorities (NCAs), market surveillance authorities or other bodies to be able to conduct all necessary steps towards the authorized representative, independently of where in the European Union it is established. Related to this point, it is essential that several NCAs can oversee the notified bodies and technical services performing the conformity assessment. In other words, not only the NCA of the country in which the notified bodies and technical services are established, but also NCA from other European member states should be able to do so, especially to ensure protection in case a specific NCA would be too under-resourced. Furthermore, while currently not foreseen in the proposed regulation, citizens should be provided with measures for redress and a right to file a complaint with national authorities, since this will not only help closing the protection gap of the proposal, but it can also help national authorities to assess and establish potential breaches of the regulation. In this way, public and private enforcement can be more complementary, and citizens will have a more active role in ensuring the protection of their rights. In the same line of thought, the link between the GDPR and this regulation should be highlighted. More than the fact that the Proposal does not, currently, foresee any mechanisms through which citizens can file a complaint, it seems to ignore the rights of citizens altogether. Despite the fact that the Recitals make numerous references to protecting health, safety and fundamental rights, the conceptual structure of the proposal is built on existing market surveillance schemes known from product safety legislation. The proposal seems to combine two concepts that are 9 fundamentally distinct: AI systems that are high-risk in the context of safety and health under the product safety legislative framework and stand-alone high-risk AI-systems that may have otherwise an impact on people s live or pose risks to fundamental rights. It is questionable whether the same requirements designed for product safety will indeed result in the protection of those rights (such as non-discrimination, privacy, fairness etc.), especially without specific rights allocated to individuals. Individuals should therefore be provided with substantive rights in the proposal, such as the right not to be subjected to prohibited AI systems, or the right not to be subjected to high-risk AI systems that do not comply with the proposal s requirements. In this regard, it should also be noted that the proposed AI Act prescribes human oversight, transparency and provision of information to users of high-risk AI systems. However, considering that Article 3( defines user as a professional user, the unintended side effect is that non-professional users of AI systems do not benefit from a similar level of protection in the form of transparency and information. Such protection is however essential to not just create trust with professional users of AI systems, but also with citizens and consumers that use such systems or are otherwise affected thereby. It is therefore important to extend the transparency requirements towards all users of AI systems, including non-professional ones. Finally, the creation of an EU database for high-risk AI systems deployed in Europe is a welcome development, as it provides for more transparency that can benefit both public and private enforcement of the regulation and of fundamental rights that can potentially be breached by the use of AI. Some members suggested, given the crucial task of the public sector and the importance to secure trust and transparency, to broaden this database to include all AI systems used by the public sector rather than only those that were explicitly listed as high-risk. At the same time, it should then be ensured that such extension does not unduly raise the administrative burden of civil servants. Additional efforts are needed to stimulate innovation The abovementioned issue of different member states speeds of implementation of the regulation risks being even more prevalent with regard to regulatory sandboxes, as the Commission expects the initiative to be taken by national competent authorities and member states. However, these sandboxes will be crucial to stimulate innovation in AI and ensure that European citizens can also benefit from the opportunities of this technology. In order to mitigate this issue, additional measures should hence be taken to incentivize the establishment of regulatory sandboxes at member state level. Indeed, while it is positive that the Commission urges Member States to set up regulatory sandboxes, this process should be further formalized. Regulatory sandboxes can only foster innovation if they are supported by adequate incentives. The relevant national authorities, like the national data protection authorities, need to build competences and experience in this area, and should be supported by the Commission on this. Adequate funding and skilled personnel are also essential. Sandboxes should be closely monitored, and information about regulatory sandboxing initiatives should be shared as openly as possible to provide learning opportunities and best practices for other member states, and transparency for all those who may be affected. Finally, the measures to help small scale AI providers and users may need to be extended. While such providers and users have priority access to regulatory sandboxes, participating in them will 10 require a huge time and resource investment from their side. Hence, they may need to be financially encouraged to participate. While a reduction of the cost of third-party conformity assessment for small scale providers and users is welcome, this should be further extended to specific funding for organizations who undergo a self-assessment procedure. In Belgium and elsewhere, we see that start-ups, scale-ups and SMEs develop many of the most innovative AI applications. Without adequate incentives, this regulation may put them at a competitive disadvantage compared to bigger companies who already have a lot of experience with compliance mechanisms. More attention should be given to the impact of AI on societal interests and the environment While the proposed AI regulation certainly advances citizens protection against AI s adverse impact by countering some of the risks posed thereby, many have raised that the attention to collective and societal interests is fairly limited (as opposed to risks to individual interests). For instance, the adverse effects that the use of AI can have on the rule of law or on the integrity of the democratic process, does not seem to be tackled through this regulation, nor is the environmental impact of AI systems. The focus is currently placed on individual human rights, yet AI might also affect groups of people, not only according the traditional criteria (race, political, religious or philosophical opinions) but also based on less traditional profiling criteria, as well as society at large. There is hence further scope left to provide attention to these aspects, and grant the public a greater role to counter these risks. This can be done, for instance, by allowing individuals or associations to file complaints with supervisory authorities, providing for collective redress mechanisms, and stimulate public interest litigation against uses of AI that may breach public values for instance through the cumulative effects of the use of AI systems on a wider scale. In addition, stakeholder consultations should be organized at a frequent basis in order to determine the extent to which the regulation needs periodic revision to effectively counter the risks of problematic AI practices. Some members noted that this attention to societal risks of AI systems should be reflected in the list of high-risk AI systems (e.g. through the inclusion of AI systems present on the web that recommend content or that moderate content and generate risks for freedom of expression and information and therefore for democracy, as reflected in the EP resolution on an ethical framework for AI (2020/2012(INL))). In the same vein, some suggested this should be reflected in the list of prohibited AI systems. Notably, in their joint comment on the proposed regulation, the EBDP/EDPS advocated that all remote biometric identification in public spaces should be prohibited as it generates extremely high risks for non-democratic intrusion into individuals private life. Similarly, emotion recognition systems might also pose an unduly high risk to societal interests that may not be justified in a democratic society. Evidently, countering AI s societal (and individual) impact necessitates not only protective measures on the supply side, but also broader awareness and education initiatives to ensure that citizens are well-equipped to understand the potential risks that accompany AI s opportunities. Such increased awareness will also contribute to the enforcement of the proposed regulation.",en,"For instance, the adverse effects that the use of AI can have on the rule of law or on the integrity of the democratic process, does not seem to be tackled through this regulation, nor is the environmental impact of AI systems.",risk
"Legal, Ethical & Accountable Digital Society (LEADS) Lab, University of Birmingham (United Kingdom)",F2665480,05 August 2021,Academic/research Institution,Micro (1 to 9 employees),United Kingdom,"HOW THE EU CAN ACHIEVE LEGALLY TRUSTWORTHY AI: A RESPONSE TO THE EUROPEAN COMMISSION S PROPOS AL FOR AN ARTIFICIAL INTELLIGENCE ACT by Nathalie Smuh a,a Emma Ahmed -Rengersb Adam Harkens ,c Wenlong Li ,d James MacLaren ,e Riccardo Pisellif and Karen Yeungg 5 August 2021 a Researcher and FWO Scholar, KU Leuven Faculty of Law, Department of International and European Law. Visiting Researcher, University of Birmingham School of Law, LEADS Lab. b Doctoral Researcher, University of Birmingham School of Law & School of Computer Science , LEADS Lab. c Postdoctoral Researcher, University of Birmingham School of Law, LEADS Lab. d Postdoctoral Researcher, University of Birmingham School of Law, LEADS Lab. e Doctoral Researcher, University of Birmingham School of Law, LEADS Lab. f Postdoctoral Researcher, U niversity of Birmingham School of Law, LEADS Lab. g Professor of Law, Ethics and Informatics, University of Birmingham School of Law & School of Computer Science, Head of the LEADS Lab. ii EXECUTIVE SUMMARY This document contains the response to the European Commission s Proposal for an Artificial Intelligence Act laying down harmonised rules for Artificial Intelligence (AI) from members of the Legal, Ethical & Accountable Digital Society (LEADS) Lab at the U niversity of Birmingham. The Proposal seeks to give expression to the concept of Lawful AI which was not addressed by the Ethics Guidelines for Trustworthy AI published in 2019 by the Commission s High - Level Expert Group on AI, as it confined its discu ssion to the concepts of Ethical and Robust AI. We develop the concept of Legally Trustworthy AI , arguing that it should be grounded in respect for three pillars on which contemporary liberal democratic societies are founded, namely: fundamental righ ts, the rule of law, and democracy. To promote Legally Trustworthy AI, the Proposal must ensure ( an appropriate allocation and distribution of responsibility for the wrongs and harms of AI; ( a legitimate and effective enforcement architecture, includ ing adequate mechanisms for transparency to secure the effective protection of fundamental rights and the rule of law , and to provide clear, stable guidance to legal subjects in a manner that coheres with other applicable laws; ( adequate rights of publi c participation , and information rights necessary to ensure meaningful accountability for the development, deployment and oversight of AI systems that is necessary in a democratic society. We welcome many aspects of the Proposal, including its commitment to dealing with the risks of AI through a set of obligations and a public enforcement mechanism ; the more refined nature of its risk -based classification of AI systems compared to the Commission s White Paper on AI of February 2020 ; the introduction of prohibited practices ; and the introduction of a European database and logging requirements for high -risk AI systems. We wholeheartedly support the Proposal s aim to protect fundamental rights. However, as currently drafted, the Proposa l does not provide adequate fundamental rights protection , nor does it provide sufficient protection to maintain the rule of law and democracy, and thus fails to secure Legal ly Trustworth y AI. Accordingly, the Proposal suffers from the following three prob lems: Firstly, it fails to reflect fundamental rights as claims with enhanced moral and legal status which subjects any rights interventions to a demanding regime of scrutiny that must satisfy tests of necessity and proportionality for limited and narro wly defined purposes. Additionally, the Proposal does not always accurately recognise the wrongs and harms associated with different kinds of AI systems and appropriately allocate responsibility for harms and wrongs. This problem is reflected in the lack of clarity concerning its scope. Hence, we recommend revision and refinement of the definition of AI, the position of academic AI researchers, the status of military AI, and the role of national security agencies. This problem is also reflected in the incomplete list of prohibited practices, and the failure to include mechanisms for its review and revision. The Proposal should include stronger protection against AI -enabled manipulation, social scoring and the use of b iometric identification systems . Moreover, it should prohibit the use of emotion recognition systems and the use of live remote biometric categorisation systems by law enforcement and by other public actors with coercive powers, or private actors acting on their behalf . Given their intrusive nature, it is not understandable why emotion recognition systems and biometric categorisation systems were not at the very least recognised as high -risk. The provisions for high -risk AI systems do not provide sufficient protection against the actual and threatened harms and wrongs generated by AI, including fundamental rights violations. They leave too much discretion to AI providers to decide on politically iii charged matters, which stems from the Proposal s poor understan ding of fundamental rights. Moreover, the eight -point list of high -risk systems must be amendable to ensure that it is future -proof and fit for purpose. Finally, the requirements for the data governance, transparency, and human oversight of high -risk syste ms would benefit from clarification and improvement. The proposal does not appear to ensure that fundamental rights interferences by AI are subjected to demanding tests of necessity and proportionality for narrowly defined purposes, nor to consistently al locate legal responsibility for the wrongs and harms of AI in an appropriate manner, thus failing to secure the first pillar of Legally Trustworthy AI. Secondly, the Proposal does not ensure an effective framework for the enforcement of legal rights and duties. It envisages an inappropriately large role for AI providers in the implementation of the Regulation, granting them too much discretion and putting undue faith in the effectiveness of conformity assessment and CE marking. In doing so, it risks cre ating incoherence between the proposed Regulation and other EU legal instruments, such as fundamental rights law, the GDPR, the Law Enforcement Directive and MiFID II. The Proposal also fails to recognise the status of individuals adversely affected by AI systems in its enforcement mechanism s, reflected in a complete lack of procedural rights for individuals, such as the rights to contest and to seek redress , and the lack of an adequate complaints mechanism. Finally, the Proposal s enforceme nt mechanism relies heavily on national competencies without providing assurances regarding the proper staffing and funding of the Member State authorities in charge of enforcing the proposed Regulation. As the Proposal s enforcement architecture has signi ficant weaknesses and its coherence with other EU legal instruments is currently wanting , the Proposal fails to adequately protect the rule of law, as the second pillar of Legally Trustworthy AI. Thirdly, the Proposal neglects to ensure meaningful transp arency, accountability, and rights of public participation, thereby failing to provide adequate protection for democracy as the third pillar of Legally Trustworthy AI . In particular : The public is not provided with consultation and participation rights reg arding future revisions of the list of high -risk AI systems, nor regarding the determination of what constitutes an acceptable residual risk in the context of high -risk AI systems. The Proposal does not provide individuals with substantive rights not to be subjected to prohibited or otherwise noncompliant AI systems, illustrating the Proposal s complete lack of attention to ordinary people. Nor are i ndividuals granted meaningful information rights to enable them to form informed opinions and contest the development and deployment of controversial AI systems. The Proposal does not provide for democratic input on the development of the technical standards crucial for the implementation of the proposed regulatory framework. Based on the three pillars of Legally Trustworthy AI and the abovementioned concerns, we make detailed recommendations for revisions to the Proposal, which are listed in the final Chapter of this document. iv WELCOME ASPECTS OF THE PROPOSED REGULATION ................................ ................................ THE THREE ESSENTIAL FUNCTIONS OF LEGALLY TRUSTWORTHY AI ................................ 1 ACHIEVING LEGALLY TRUSTWORTHY AI AS THE PROPOSAL S CORE MISSION ................................ 2 WHAT DOES LEGALLY TRUSTWORTHY AI REQUIRE ? ................................ ................................ a) Allocate and distribute responsibility for wrongs and harms appropriately, and in a manner that adequately protects fundamental rights ................................ ................................ ................................ ................................ b) Establish and maintain an effective framework to enforce legal rights and responsibilities, and secure the clarity and coherence of the law itself (rule of law) ................................ ................................ ................................ c) Ensure meaningful transparency, accountability and rights of public participation (democracy) HOW THE PROPOSAL FALLS SHORT OF THESE THREE FUNCTIONAL REQUIREMENTS 1 THE PROPOSAL DOES NOT ALLOCATE AND DISTRIBUTE RESPONSIBILITY FOR WRONGS AND HARMS APPROPRIATELY , AND IN A MANNER THAT ADEQUATELY PROTECTS FUNDAMENTAL RIGHTS 1 The Proposal operationalises fundamental rights protection in an excessively technocratic manner ................................ ................................ ................................ ................................ a) The distinct nature of fundamental rights is overlooked ................................ ................................ b) The current technocratic approach fails to give expression to the spirit and purpose of fundamental rights . 11 c) The risk -categorisation of AI systems remains unduly blunt and simplistic ................................ 2 The Proposal s scope is ambiguous and requires clarification ................................ a) The Proposal s current definition of AI lacks clarity and may lack policy congruence ................................ . 14 b) Clarification is needed on the position of academic research ................................ ................................ c) The potential gap in legal protection relating to military AI should be addressed ................................ d) Clarity is needed on the applicability of the Proposal to national security and intelligence agencies 3 The range of prohibited systems and the scope and content of the prohibitions need to be strengthened, and their scope rendered amendable ................................ ................................ a) The scope of prohibited AI practices should be open to future review and revision ................................ b) Stronger protection is needed against AI -enabled manipulation ................................ ................................ c) The provisions on AI -enabled social scoring need to be clarified and potentially extended to private actors ... ................................ ................................ ................................ ................................ ................................ 4 The adverse impact of biometric systems needs to be better addressed ................................ a) Different types of biometric systems under the Proposal: an overview ................................ b) The prohibition on biometrics is not a real prohibition ................................ ................................ c) The Proposal does not take a principled approach to the risks of various biometric systems d) The distinction between private and public uses of remote biometric systems requires justification 5 The requirements for high -risk AI systems need to be strengthened and should not be entirely left to self -assessment ................................ ................................ ................................ ................................ a) Outsourcing the acceptability of residual risks to high -risk AI providers is hardly acceptable b) It can be questioned why the listed high -risk AI systems are considered acceptable at all c) The adaptability of the Scope of Title III is too limited ................................ ................................ d) The list of high -risk AI systems for law enforcement should be broadened ................................ e) The requirements that h igh-risk AI systems must comply with need to be strengthened and clarified 2 THE PROPOSAL DOES NOT ENSURE AN EFFECTIVE FRAMEWORK FOR THE ENFORCEMENT OF LEGAL RIGHTS AND RESPONSIBILITIES (RULE OF LAW ) ................................ ................................ ................................ 1 The Proposal unduly relies on (self-) conformity assessments ................................ a) The Proposal leaves an unduly broad margin of discretion for AI providers and lacks efficient c ontrol mechanisms ................................ ................................ ................................ ................................ b) The conformity assessment regime should be strengthened with more ex ante independent control 2 There is currently insufficien t attention to the coherency and consistency of the scope and content of the rights, duties and obligations that the framework seeks to establish ................................ a) The consistency of the Proposal with EU (fundamental rights) law should be ensured ................................ . 40 b) The Proposal s relationship with the General Data Protection Regulation should be strengthened c) The Proposal s relationship with the Law Enforcement Directive should be clarified ................................ d) Concerns around the Proposal s implicit harmonisation with MiFID II should be addressed 3 The lack of individual rights of enforcement in the Proposal undermines fundamental rights protection ................................ ................................ ................................ ................................ a) The Proposal does not provide any rights of redress for individuals ................................ b) The Proposal does not provide a complaints mechanism ................................ ................................ v 4 The Proposal s enforcement mechanism is inadequate ................................ ................................ .46 a) The enforcement structure hinges too much on national competencies ................................ b) The enforcement powers conferred upon supervisory authorities should be clarified ................................ 3 THE PROPOSAL FAILS TO ENSURE MEANINGFUL TRANSPARENCY , ACCOUNTABILITY AND RIGHTS OF PUBLIC PARTICIPATION (DEMOCRACY ) ................................ ................................ ................................ 1 The Proposal does not provide consultation and participation rights ................................ a) The scope of the public consultation prior to the Proposal s drafting would have benefitted from more targeted questions regarding prohibited and high -risk applications ................................ ................................ b) Insufficient opportunities for consultation and participation enshrined in the Proposal itself 2 The Proposal lacks meaningful substantive rights for individuals ................................ a) The Proposal does not provide any substantive rights for individuals ................................ b) The Proposal does not provide meaningful information rights for individuals ................................ 3 The Proposal suffers from a democratic deficit in standard -setting and conformity assessment .54 OUR KEY RECOMMENDATIONS ................................ ................................ ................................ 1 RECOMMENDATIONS FOR TITLE I OF THE PROPOSAL ................................ ................................ 2 RECOMMENDATIONS FOR TITLE II OF THE PROPOSAL ................................ ................................ 3 RECOMMENDATIONS FOR TITLE III OF THE PROPOSAL ................................ ................................ 4 RECOMMENDATIONS FOR TITLE IV OF THE PROPOSAL ................................ ................................ 5 RECOMMENDATIONS FOR TITLES VII AND VIII OF THE PROPOSAL ................................ 6 OTHER FUNDAMENTAL RIGHTS RECOMMENDATIONS , INCLUDING REDRESS AND PARTICIPATION 1 INTRODUCTION This document sets out the response to the European Commission s Proposal for an Artificial Intelligence Act laying down harmonised rules for Artificial Intelligence (AI)1 from members of the Legal, Ethical & Accountable Digital Society (LEADS) Lab at the University of Birmingham . We are grateful for the opportunity to voice our views on this unique and ground -breaking piece of legislation, which we very warmly welcome. We wholeheartedly support the Commission s commitment to providing legally binding no rms and standards for the development and use of AI, as we believe that it is high time for a robust and coherent framework which prevents the gravest harms and wrongs of AI and allocates responsibility for the risks associated with this exciting yet power ful and intrusive technology. Throughout this document, we stress the importance of the legal nature of the Proposal, which sets it apart from all other initiatives on AI ethics or technical best practices. In this context, we expressly welcome the Proposa l s focus on protecting fundamental rights which are not only moral precepts, but also legal rights that include a concrete and well-established framework for interpretation, implementation and enforcement. Yet, despite the Proposal s significant achieve ment of providing legally binding rules for AI, our analysis shows that the Proposal does not yet live up to its promise. It suffers from a number of deficiencies which compromise the value and importance of its legal nature, and which need to be addressed in order to ensure that the Proposal can fulfil its noble aspirations . To explain these shortcomings, we sketch a tripartite analytical framework based on the concept of Legally Trustworthy AI . We also offer practical suggestions on how the se shortcomings might be addressed. Our evaluation of the Proposal does not adopt the perspective of EU constitutional law. Nor does it offer a doctrinal legal analysis which evaluates the Proposal against the internal requirements of EU law. Although w e reco gnise that not all amendments we suggest might be possible considering the current legal basis of the Proposal and the competences of the different bodies of the EU legal order , we urge the Commission to take seriously our recommendations if not in this Proposal, then in another legal instrument. Our analysis begins with the earlier work done towards establishing Trustworthy AI in Europe by the European Commission and its High -Level Expert Group on AI (HLEG) , which provides the background against which the Proposal must be understood . In their Ethics Guidelines for Trustworthy AI ,2 the HLEG provided the contours for Trustworthy AI defined as AI which is lawful, ethical and robust. This Proposal represents an important attempt to address th e requirement of Lawful AI which was excluded from the HLEG s remit . On the basis of this requirement of Lawful AI , we develop the concept of Legal Trustworthiness, which we argue rests on three pillars upon which contemporary lib eral democratic societies are rooted : ensuring respect for fundamental rights, the rule of law, and democracy. The three pillars of Legally Trustworthy AI in turn require that the regulation of AI: ( appropriately allocates responsibility for the harms a nd wrongs resulting from AI systems, especially where these pertain to fundamental rights; ( establishes and maintains a coherent legal framework 1 European Commission, Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts, COM/2021/206 final, EUR -Lex, April 21, 2021, -lex.europa.eu/legal - content/EN/TXT/?uri=CELEX%3A52021PC0206 . Hereafter refe rred to as the Proposal , the proposed Regulation, or the draf t Regulation. 2 High -Level Expert Group on AI, Ethics Guidelines for Trustworthy AI, 8 April 2019, - strategy.ec.europa.eu/en/library/ethics -guidelines -trustworthy -ai. 2 accompanied by effective and legitimate enforcement mechanisms to secure and uphold the rule of law; and ( places democratic deliberation at its centre , which includes the conferral of public participation and other information rights necessary for effective democracy. In what follows, we first outline the welcome aspects of the proposed Regulation (Chapter . Then, we explain why Legally Trustworthy AI should be the core mission of the Proposal, and how Legal Trustworthiness can be attained through respect for each of its three pillars (Chapter . We argue that the Proposal falls short of each of these three pillars (Chapter , setting out our concrete recommendations (Chapter . In particular, w e recommend significant changes concerning the Proposal s understanding of fundamental rights; its scope; its provisions for prohibited system s, biometric systems and high -risk systems; its enforcement mechanism s; the (lack of a) role allocated to the individuals subjected to or otherwise directly affected by AI systems; and the lack of meaningful public participation and information rights. WELCOME ASPECTS OF THE PROPOSED REGULATION Before we address the ways in which we believe the proposed Regulation could be improved, we emphasise that we wholeheartedly welcome the European Commission s initiative to introduce legally enforceable standards for the development and use of AI. With its ground - breaking Proposal, the Commission explicitly acknowledges both the adverse effects on fundamental rights and safety that AI can generate, and that voluntary self-regulation offers inadequate protection. By establishing a public enforcement structure, the Commission affirms that the risks associated with AI need to be dealt with as a ma tter of public interest, and in a manner that secures a level playing field of protection across the EU, regardless of an individual s Member State of residence. Despite being primarily based on a legal basis meant to harmonise the internal market, the Proposal seeks to affirm and operationalise a shared commitment to upholding fundamental rights. While various aspects of the Proposal merit significant improvement, the Proposal present s a concrete and systematic way forward to regulate AI, which in turn provides EU legislators and stakeholders with an opportunity to discuss and refine this approach before adoption. The Proposal s underlying ambition of prioritising respect for fundamental rights (including safety and health) is both important and warmly w elcomed (although , as further detailed below, we believe this foundational commitment must be accompanied by the protection of democracy and the rule of law , and requires a strengthening of the Proposal in order to be achieved ). With this initiative, the EU sends a strong signal that it takes the risks and dangers raised by AI seriously, and does not shy away from taking legally -enforceable measures including the imposition of prohibitions or red lines. We hope that this ambit ion will act as a catalyst for regulatory action in this field by other legislators across the world. Compared to the blueprint for a potential AI regulation set out in White Paper on Artificial Intelligence ( February , this Proposal shows significant improvement in several respects. First, the Proposal takes a more nuanced stance towards the risk-based classification of AI systems, swapping the binary risk -categori sation (high -risk/no -risk) for a more refined approach. Besi des distinguishing between ( AI practices that pose an unacceptable risk and are hence prohibited (Title II), ( AI practices which pose a high risk to the fundamental rights or to the safety of natural persons and are subject to specific requirements ( Title III), ( AI practices which require enhanced transparency (Title IV) and ( AI practices which pose no risk or a low level of risk yet may benefit from a voluntary code of conduct (Title IX), the Proposal also renders explicit the criteria which are considered relevant to determine the high - risk nature of an AI system (in Article . Nevertheless , as we explain below, t he protection 3 afforded by the specified risk criteria should be refined and enhanced , and it is questionable whether the listed AI practices are appropriately categorised . Secondly, we welcome the introduction of prohibited practices as per Title II of the proposed Regulation, which constitutes an important addition to the White Paper of February By providing a set of red lines , the Commission acknowledges that certain uses of AI are by definition incompatible with fundamental rights. While the scope of these prohibitions needs to be broadened, this acknowledgment is an important step towards securing fundamental rights protection in Europe. Thirdly, the Proposal introduces a European database, managed by the European Commission, in which essential information on all stand -alone high -risk AI systems deployed in the EU is to be registered. This database c ould increase urgently needed transparency around the use of potentially problematic AI systems and may be a useful resource for citizens and civil society organisations. Moreover, this database c ould be a helpful tool for national authorities, allowing them to strengthen public supervision and advancing the proposed Regulation s enforcement. However, we believe the matters which must be disclosed need significant expansion if the level of public transparency provided under the regime is to meet threshold requirements of healthy democratic societies. Fourthly, the Proposal introduces requirements t o ensure that the mitigation of risks arising from problematic AI systems does not remain a dead letter, creating the possibility for meaningful implementation and enforcement. The introduction of mandatory logging requirements for high -risk AI systems is, for example, a welcome development in this regard, as is the need to specify the purpose of the AI system in a suffic iently concrete manner to allow for context -specific testing mechanisms. Furthermore, if the user of the AI system subsequently uses it for a different purpose, the AI user (whose main obligation is to ensure human oversight over the AI system once deplo yed) becomes the provider of an AI system, hence incurring the specific obligations imposed on providers rather than users (as set out under Article . Finally, the Proposal appears to acknowledge that testing technical aspects of an AI system may lead t o different results in a lab environment and in the real -world, and recognises that these differences need to be taken into account when seeking to comply with the Regulation s various testing requirements. At the same time, the proposed Regulation remain s deficient in numerous ways, both in its substance and enforcement architecture . While proclaiming the protection of fundamental rights as a core aim, the Proposal does not appear to take fundamental rights sufficiently seriously. The proposed Regulation also fails to adequately reflect the EU s commitment to democracy and the rule of law , which together with respect of fundamental rights can be seen as constituting the three core pillars of the EU legal order.3 In what follows, we first outline the contours of the concept of Legally Trustworthy AI with its three pillars of fundamental rights, rule of law, and democracy. We use these pi llars as a measure for the quality of th e Proposal . We argue that the Proposal falls short of all three of 3 Article 2 Treaty on European Union sets out the values on which the EU is founded, these being common to the Member States in a society in which pluralism, non -discrimination, tolerance, justice, solidarity and equality between women and men prevail . On the importance of the triad of human rights, democracy and the rule of law in the context of AI and the EU, see also e.g., Paul Nemitz, Constitutional Democracy and Technology in the Age of Artificial Intelligence, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 376, no. 2133 (. See also HLEG, Ethics Guidelines , It is through Trustworthy AI that we, as European citizens, will seek to reap its benefits in a way that is aligned with our foundational values of respect for human rights, democracy and the rule of law. 4 the pillars of Trustworthy AI, after which we make specific recommendations on how the Proposal should be revised and improved . THE THREE ESSENTIAL FUNCTIONS OF LEGALLY TRUSTWORTHY AI Our analysis of the proposed Regulation is rooted in the concept of Trustworthy AI set out in two primary publications produced by the European Commission s High -Level Expert Group on AI (hereafter the HLEG ): the Ethics Guidelines for Trustworthy AI4 (hereafter the Ethics Guidelines ) and the Policy and Investment Recommendations for Trustworthy AI (hereafter the Policy Recommendations ) .5 In the following sections, we explain why Trustworthy AI requires Legal Trustworthiness, what Legally Trustworthy AI is, and how it can be attained. The rest of this document then sets out how and where the Proposal falls short of the requirements of Legal Trustworthiness, and therefore risks failing its core mission. 1 Achieving Legally Trustworthy AI as the Proposal s core mission The Proposal s aims and objectives must be understood in the context of earlier work on Trustworthy AI by the HLEG and by the European Commission wh ich endorsed the HLEG s work .6 The underlying purpose of the Proposal is to fill the gap that the Ethics Guidelines left open regarding Lawful or Legally Trustworthy AI. The Ethics Guidelines state that, [i]t is through Trustworthy AI that we, as European citizens, will seek to reap its benefits in a way that is aligned with our foundational values of respect for human rights, democracy and the rule of law. 7 The Ethics Guidelines also explain that: Trustworthy AI has three components, which should be met throughout the system's entire life cycle: (a) it should be lawful , complying with all applicable laws and regulations ; (b) it should be ethical , ensuring adherence to ethical principles and valu es and ; (c) it should be robust , both from a technical and social perspective since, even with good intentions, AI systems can cause unintentional harm. Each component in itself is necessary but not sufficient for the achievement of Trustworthy AI. Ide ally, all three components work in harmony and overlap in their operation. If, in practice, tensions arise between these components, society should endeavour to align them.8 The Ethics Guidelines have no binding legal force9 and deliberately refrain from considering whether additional legal measures would be needed to ensure Lawful AI. Instead, their operative provisions are confined to Ethical and socio -technically Robust AI, while Lawful AI is not discussed. Yet, the Ethics Guidelines explicitly recognise the critical importance of Lawful AI in supporting the other two components of Trustworthy AI (i.e. , the ethical and 4 HLEG, Ethics Guidelines. 5 High -Level Expert Group on AI, Policy and Investment Recommendations for Trustworthy AI, 26 June 2019, -strategy.ec.europa.eu/en/library/policy -and-investment -recommendations -trustworthy - artificial -intelligence. 6 European Commission, Building Trust in Human -Centric Artificial Intelligence, 8 April 2019, ocument.cfm?doc_id=7 HLEG, Ethics Guidelines, As the Guidelines specify at page 9, Trustworthy AI is explicitly grounded in a deep commitment to three fundamental principles: We believe in an approach to AI ethics that is based upon the funda mental rights enshrined in the EU Treaties, the Charter of Fundamental Rights of the EU (EU Charter) and international human rights law. Respect for fundamental rights, within a framework of democracy and rule of law, provides the most promising foundation s for identifying abstract ethical principles and values which can be operationalised in the context of AI. 8 HLEG, Ethics Guidelines, 9 The initial call for experts for the HLEG , stating that the HLEG would propose to the Commission AI Ethics Guidelines, is available at: -strategy.ec.europa.eu/en/news/call -high-level -expert -group - artificial -intelligence . 5 socio -technical dimensions). This is affirmed and endorsed in the Policy Recommendations stating tha t: On 8 April 2019, we published our Ethics Guidelines which set out three components for Trustworthy AI: ( lawful AI, ( ethical AI and ( robust AI. The Ethics Guidelines only deal with the two latter components, yet the first is equally crucial.10 We understand the primary aim of the proposed Regulation as address ing this third component of Trustworthy AI , by establishing a set of legally binding norms and institutional mechanisms necessary to ensure Lawful AI. While the HLEG did not explain the c oncept of Lawful AI, it is best illuminated by considering the key distinction between legal standards on the one hand, and voluntary ethical and technical standards on the other hand . The most powerful features of legal standards concern the fact that unlike ethical or technical standards they are mandatory and legally binding: promulgated, monitored and enforced in the context of a system of institutions, norms and a professional community which work together to ensure that laws are properly interpret ed, effectively complied with and duly enforced.11 Mature legal systems consist of institutions, procedures and officeholders which are legally empowered to gather information to monitor compliance with legal duties. Where a possible violation of legal requirements is identified, those with standing may initiate enforcement action before an independent tribunal seeking binding legal remedies, which include both orders to bring any legal violations to an end and, if applicable, the imposition of san ctions and orders for compensation. This institutional structure differentiates law from both ethics and technical best practices. The Lawful component of Trustworthy AI gives the concept of Trustworthiness teeth . However, the mere existence of some teeth does not by definition contribute to or ensure Legal Trustworthiness. As is evidenced by the Commission s efforts to address the requirement of Lawful AI with a whole new regulation, the concept of Lawful AI cannot mean that Trustworthy AI simply c omplies with whichever legal rules happen to exist. For legality to contribute to Trustworthiness, it is crucial that the legal framework itself adequately deals with the risks associated with AI. This desideratum goes far beyond simple legal compliance checks it requires the existence of a regulatory framework which addresses the foundational values of fundamental rights, the rule of law, and democracy. As the Policy Recommendations state: Ensuring Trustworthy AI necessitates an appropriate governance and regulatory framework. By appropriate, we mean a framework that promotes socially valuable AI development and deployment, ensures and respects fundamental rights, the rule of law, and democracy while safeguarding individuals and s ociety from unacceptable harm.12 Legally Trustworthy AI (as opposed to simple legally compliant AI) operates at two levels. The first level concerns the extent to which the regulatory framework around AI promotes the values of fundamental rights, democracy and the rule of law. The second level con cerns the way in which AI systems themselves affect those three values. For an AI system to be Legally Trustworthy, it must therefore ( be regulated by a governance framework which promotes fundamental rights, democracy and the rule of law, and ( not i tself be detrimental to fundamental rights, democracy and the rule of law. The second level is conditional on the first level: if the regulatory framework adequately protects fundamental rights, the rule of law and 10 HLEG, Policy and Investment Recommendations, 37 . 11 See generally Peter Cane, Responsibility in Law and Morality (Oxford: Hart Publishing, . 12 HLEG, Policy and Investment Recommendations, 6 democracy, it does not allow systems whic h negatively affect fundamental rights, democracy and the rule of law to exist without due precautions and protections. While the Proposal explicitly claims to protect fundamental rights in the context of AI , we argue that it does not currently provide adequate fundamental rights protection. Moreover, it does not explicitly address the other two pillars upon which liberal constitutional political communities rest and upon which the EU legal order is founded and explicitly reaffirmed in the Ethi cs Guidelines namely that of democracy and the rule of law. Before we outline that the Proposal therefore cannot guarantee Legally Trustworthy AI, we explain how Legally Trustworthy AI is attained. 2 What does Legally Trustworthy AI require? In what fo llows, we briefly outline three core functions which the legal system must provide to establish L awful or L egally Trustworthy AI. It builds on the tripartite framework for Trustworthy AI set out in the Ethics Guidelines , explained in the previous section. To ensure Legally Trustworthy AI within liberal democratic constitutional systems, the legal system must: (a) Allocate and distribute responsibilities for wrongs and harms appropriately, which includes the effective protect ion of fundamental rights ; (b) Establish and maintain an effective framework to enforce legal rights and responsibilities, and secure the clarity and coherence of the law itself these being essential elements of the rule of law; (c) Reflect a commitment to democracy by securing meaningful transparency, accountability and rights of public participation to its members. Accordingly, our analysis and recommendations are rooted in our belief that law must ensure these three core functions are properly established and maintained. Legally Trustwo rthy AI therefore requires both a regulatory framework for AI which embodies these three functions, and guarantee s that the AI systems permitted under the regulatory framework do not undermine these three functions. Each of the dimensions of Legal Trustwor thiness - which are closely entwined - are briefly outlined below: a) Allocate and distribute responsibility for wrongs and harms appropriately, and in a manner that adequately protects fundamental rights Firstly, Legal Trustworthiness requires the appropriate allocation of responsibility for harms and wrongs. A core function of modern legal systems is to provide a binding framework to enable peaceful social cooperation between strangers. The legal system achieves this inter alia by attributing legal responsibility to those whose activities produce other -regarding harms or wrongs, whether intentional or otherwise, resulting in the imposition of either (or both) civil or criminal liability as appropriate. In this way, the law seeks to reduce and prev ent harm to others, and to ensure appropriate redress where such adverse events occur. The law establishes and publicly promulgates legally binding rules which identify the scope and content of the rights and responsibilities of legal and other persons, th ereby providing guidance to legal subjects so that they can alter their behaviour accordingly so as not to fall foul of the law s demands. This legal guidance function plays an important role in protecting the legal rights, interests and expectations of al l members of the community against unlawful interference by others. To this end, fundamental rights can often be understood as providing the justification for concrete legal standards, instruments and mechanisms, thereby providing legal subjects with 7 clearer and enforceable forms of legal protection. While fundamental rights can be understood as moral rights, bestowed on individuals by virtue of their moral status as human beings , they are realised in concrete legal arrangements.13 For example, the G eneral Data Protection Regulation ( GDPR ) is known to concretise the fundamental right to personal data protection.14 Similarly, laws concerning the proper conduct of elections can be understood as rooted in a concern to protect the fundamental right to vot e and the right to free and fair elections. The responsibilities of those involved in designing, testing and putting into service AI systems is of particular importance, as those who are subjected to the adverse effects of those systems might not always be aware of this fact. This is because AI systems can operate in way s that are opaque or even invisible, in real time and at scale. Consider the example of Clearview AI , a US-based company with a database of three billion facial images scrapped from Facebook, YouTube, Venmo and millions of other we bsites,15 which has been used by a number of law enforcement agencies and other European actors. Today, almost every law enforcement agency in Europe uses AI-enabled biometric identification technology.16 Examples are, of course , not limited to the law enforcement sphere. In fact, the list of high -risk AI systems in the Proposal s Annex III provides a good overview of problematic AI -enabled practices, from workplace surveillance to the detection of the emotional states of persons in the context o f a migration management.17 Legally Trustworthy AI therefore requires a regulatory framework which prevents the gravest harms and wrongs generated by AI systems, and appropriately allocates responsibility for them if they do occur, particularly when they violate fundamental rights. b) Establish and maintain an effective framework to enforce legal rights and responsibilities, and secure the clarity and coherence of the law itself (rule of law) Secondly, Legal Trustworthiness must ensure adherence to the essential elements of the rule of law including , inter alia , effective enforcement, judicial protection, and the coherence of the law. 13 See HLEG, Ethics Guidelines, Understood as legally enforceable rights, fundamental rights therefore fall under the first component of Trustworthy AI (lawful AI), which safeguards compliance with the law. Understood as the rights of everyone, rooted in the inherent moral status of human beings, they also underpin the second component of Trustworthy AI (ethical AI), dealing with ethical norms that are not necessarily legally binding yet crucial to e nsure trustworthiness. Accordingly, although the Ethics Guidelines recognise that there is often overlap between the requirements of ethics and of law, the two are not coterminous. 14 On the need for the concretisation of human rights in the context of AI , see e.g. , Nathalie A. Smuha, Beyond a Human Rights -Based Approach to AI Governance: Promise, Pitfalls, Plea, Philosophy & Technology 24 (, -020-00403 -w. 15 Kashmir Hill, The Secretive Company That Might End Privacy as We Know It, The New York Times , January 18, 2020, -privacy -facial - recognition.html. 16 There is a risk that, without knowledge or consent of the persons affected, these databases may, over the longer term, may be used to track ever more aspects of people s lives, through what Evgeny Morozov calls the 'invisible barbed wire. See Evgeny Morozov, The Real Privacy Problem MIT Technology Review , October 22, 2013, -real-privacy -problem/. In Bill Davidow s words, such opaque databases may serve as a key component in the construction of an algorithmic prison where the images c ollected may be used to profile persons of interest, to use for facial recognition and characterisation, or even to predict a person s propensity for committing a crime (Bill Davidow, Welcome to Algorithmic Prison, The Atlantic , February 20, 2014, -to-algorithmic -prison/283985/). 17 See respectively Point 4(b) and Point 7(a) of the Proposal s Annex III, to give but a few examples. 8 If wrongs or harms arise due to the failure of another person(s) to discharg e their legal duties, the legal system must provide an effective and legitimate framework to ensure that those duties are duly enforced and that legal rights are effectively protected. This requires effective public enforcement mechanisms , including the provision of an independent judiciary. It may also require an independent public enforcement office endowed with legal powers to gather information and investigate potential violations, and to initiat e enforcement action against those who appear to have failed to abide by their legal duties and obligations . Effective judicial protection is a crucial element within the broader framework of the rule of law in the European Union. Victims of fundamental rights violations must be legally empowered to seek effective remedies, including compensation for harm against those deemed legally responsible. Another element of the rule of law is the need for legal clarity and coherence. The various duties and obligations must be appropriately structured so that they can provide clear guidance to legal subjects concerning unlawful behaviour and its consequences. In addition, the laws themselves , when taken as a whole , must display consistency and internal cohere nce. The law cannot usefully serve its critical guidance role to members of the community unless its laws, taken together, are intelligible, constituting an internally coherent and consistent body of rules and principles that ground and enable peaceful coo rdination and co -existence among its members .18 A regulatory framework for Legally Trustworthy AI therefore requires an effective enforcement architecture, which establishes and protects procedural rights and is internally coherent. c) Ensure meaningful t ransparency, accountability and rights of public participation (democracy) Thirdly, Legal Trustworthiness requires that the regulatory framework is rooted in democratic deliberation and continuously promotes opportunities for public participation and trans parency . As discussed earlier, one of the main differences between legal and ethical standards is the extent to which institutions exist to promulgate, interpret, and enforce those standards. Moreover, in well -functioning democratic communities, significa nt legislative measures are preceded by open and active public debate and deliberation to establish the community s views about those measures. In democracies, the legal system endows affected stakeholders and the community at large with meaningful rights to participation in determining the legal norms which affect their collective life (both ex ante and ex post ). In short, d emocracy requires that the regulatory framework around AI derives its legitimacy from consultation with citizens and grants the m a prominent role in the design and enforcement of the Regulation. Moreover, democracy requires that the AI systems which are al lowed under the Proposal do not undermine the ideals of transparency and accountability, which are both required for meaningful public participation and democratic accountability . For example, individuals and the community at large are entitled to transpar ency so that they may be informed of, adequately understand, and contest the way in which AI systems make or are directly implicated in making decisions which significantly affect them. 18 See generally Lon L. Fuller, The Morality of Law (New Haven: Yale University Press, , 67 -68; Ronald Dworkin, Law s Empire (Cambridge: Harvard University Press, . 9 To summarise , Legal Trustworthiness requires ( appropriately allocat ed responsibility for the harms and wrongs associated with AI, especially concerning potential fundamental rights implications ( an effective enforcement framework and respect for the rule of law and ( a commitment to democracy through meaningful right s of transparency and public participation. HOW THE PROPOSAL FALLS SHORT OF THESE THREE FUNCTION AL REQUIREMENTS Below w e demonstrate the ways in which the Proposal falls short of the three functions of Legal Trustworthiness just outlined, and thus fails to secure Legally Trustworthy AI in its current form . Firstly, we argue that the Proposal does not allocate and distribute responsibility for wrongs and harms appropriately and in a manner that adequa tely protects fundamental rights. We consider the Proposal s understanding of fundamental rights, its scope, prohib itions, and the provis ions for biometric systems and high-risk AI ( . Secondly, we argue that the Proposal does not ensure an effective framework for the enforcement of legal rights and responsibilities. We discuss conformity assessments, the Proposal s coherence, the absence of procedural rights for individuals, and the Pr oposal s enforcement architecture and mechanism s ( . Thirdly, we argue that the Proposal fails to ensure meaningful transparency, accountability, and rights of public participation. We argue for public participation and information rights, highlighting substantial democratic deficit in the Proposal s standard - setting framework ( . Drawing on those considerations, the last chapter of this document offer s concrete recommendations which we urge the Commission to consider (. 1 The Proposal does not allocate and distribute responsibility for wrongs and harms appropriately, and in a manner that adequately protects fundamental rights The following sections address the ways in which the Proposal does not adequately allocate responsibility for the wrongs and harms associated with AI the first pillar of Legally Trustworthy AI. The way in which the Proposal operationalises fundamental ri ghts protection appears to rest on a flawed understanding of the nature of fundamental rights. We highlight inadequacies in the way in which the Proposal understands and operationalises fundamental rights protection (; the scope of the protection off ered by the Proposal (; the content and scope of the prohibited AI practices (; the scope and strength of protection offered against the risks arising from the use of AI -enabled biometric systems (; and the fundamental rights protection p rovided against the risks posed by high -risk systems (. 1 The Proposal operationalises fundamental rights protection in an excessively techn ocratic manner Fundamental rights protection is a core aim of the Proposal (as mentioned in its explanatory texts and recitals), and one of the core elements of Legal ly Trustworthy AI. Yet the following sections argue that the Proposal does not provide sufficient fundamental rights protection. First, while the Proposal s text is infused with fundamen tal rights -language, it seems to take an overly techn ocratic approach to the protection of fundamental rights (a). Second, in doing so, it also overlooks the distinct nature of fundamental rights, which cannot be equated to safety standards (b). Third, the Proposal s risk categorisation of AI systems remains insufficiently stratified to safeguard individuals against the harms that they can pose (c). a) The distinct nature of fundamental rights is overlooked The Proposal fails to properly understand the distinctive nature of fundamental rights, which requires a particular form of protection well -established in EU fundamental rights law. 10 To ensure due respect for the fundamental rights of all persons in virtue of their humanity, fundamental rights are acc orded special weight in the architecture of legal rights protection in which rights are not merely interests of individuals to be balanced against the interests of others, including collective interests.19 Treating fundamental rights as an afterthought in any balancing process whereby economic and innovation concerns take priority means failing to recognise that fundamental rights enshrined in the Charter generate legitimate expectations for EU citizens regarding their relationship between themsel ves, EU institutions, Member States, and private organisations. This crucially includes legally enforceable safeguards against an increased scope for unchecked abuse of power and power asymmetries arising from the introduction of new technologies (between empowered users of said technologies, whether public or private, and relatively disempowered individual s). Moreover, failing to recognise the distinct nature of fundamental rights risks overlooking the Commission s obligations to respect, promote20 and to avoid adversely affecting the rights contained within the Charter,21 and can ultimately undermine , rather than protect , the Union values of human dignity, freedom, equality, and solidarity. Due to their enhanced moral strength and importance, fundamental rights are accorded strong presumptive legal protection, and the justification for any fundamental rights interference therefore consists of three essential elements: (a) First, except for rights which admit of no qualification, such as the right t o freedom from torture and slavery, fundamental rights can only be interfered with in a narrow and designated range of legitimate purposes, where those interferences are prescribed by law and necessary and proportionate in a democratic society. In modern l egal systems, the structure of rights protection establishes a clear framework for addressing normative conflict between rights and other important collective interests, and between rights inter se . (b) Second, the duty of demonstrating this demanding burden o f justification lies on those seeking to interfere with fundamental rights. (c) Third, the determination of whether this burden of justification has been discharged is a matter for adjudication by a legitimately constituted independent body (i.e., a court or body with similar powers of authoritative decision -making, such as a tribunal). As presently drafted, the Regulation appears to treat fundamental rights as equivalent to mere interests. Each fundamental right engaged by this Proposal and the ac tivities it enables is limited and made subject to a balancing process by the Proposal itself at least to some degree by virtue of (a) the inherent tension in this Proposal between the goals of promoting economic activity and innovation, with the prote ction of fundamental rights; and (b) the legal bases upon which it has been founded , namely, a rough amalgamation of Article 114 TFEU (which enables the Commission to propose harmonisation measures relating to the internal market) and Article 16 TFEU (whic h enables the introduction of harmoni sation measures designed to protect the right of personal data of individuals). This creates an uneasy marriage that does not provide protection for the full gambit of potential interferences, intrusions and violations of 19 Consider in this regard also Article 52 of the EU Charter, which provides that Any limitation on the exercise of the rights and freedoms recognised by this Charter must be provided for by law and respect the essence of those rights and freedoms. Subject to the principle of proportionality, limitations may be made only if they are necessary and genuinely meet objectives o f general interest recognised by the Union or the need to protect the rights and freedoms of others (European Union, Charter of Fundamental Rights of the European Union, Official Journal of the European Union C83 53, (). 20 European Union , EU Char ter, Article 51(. 21 European Union, EU Charter , Article 11 fundamental rights made possible by the development, deployment and use of AI systems in the EU. Worryingly, the mechanisms through which fundamental rights are accorded protection by the proposed Regulation seem to fall seriously short of the three ele ments just outlined. This is particular ly demonstrated by the centrality in its regulatory architecture accorded to a risk - based approach , which is regarded as a largely technical and administrative matter to be attended to by AI providers by putting in place suitable risk management systems. Accordingly, the Proposal s risk -based approach does not appear compatible with the regime s claim to offer a high level of protection to fundamental rights due largely to a lack of systematic, meaningful scrutiny and oversight, particularly given the existing weak enforcement and remedial framework currently propose d (see section 2 ). It is taken for granted that a system of self -assessment by AI providers guarantees sufficient protection against the dangers which these technologies generate for the fundamental rights of those living in the EU. For a legal instrumen t to be considered adequately rights protective particularly where it makes significantly strong claims of the kind made in this Proposal we would expect that it would at least recognise the distinctive nature of fundamental rights. This should be clea rly demonstrated in the binding content of the Proposal itself. The Proposal needs to be aligned with existing European fundamental rights legislation and practice, which establishes substantive and procedural requirements for potential interferences with fundamental rights, including the principles of proportionality, necessity, and independent oversight. Problems relating to fundamental rights manifest themselves in several problematic ways which are addressed throughout this document, including : the creation of a narrow and overly -prescriptive regime of rules for the identification of high -risk systems, which is likely to produce a weak and formalist regime of rights protections; the creation of a system of asymmetric protection that does not o ffer equal protection for the fundamental rights of individuals against interferences by private organisations and public authorities; a lack of an independent, external body to evaluate whether rights interferences are necessary in a democratic society for pre -specified legitimate purposes and therefore proportionate; the provision of excessively broad discretionary power to developers and providers to determine whether their AI systems are fundamental rights compliant; a failure to offer affec ted subjects a suite of individual rights and remedies by which to obtain redress for any (actual or potential) fundamental rights interferences, as appropriate; weak transparency and accountability requirements which are not explicitly tied to fundamenta l rights protection. In sum, the Proposal does not yet do justice to the distinct nature of fundamental rights and their need for an effective and comprehensive system of protection, as required by the first element of Legal Trustworthiness. b) The current technocratic approach fails to give expression to the spirit and purpose of fundamental rights Instead of taking the approach well -established in fundamental rights law, the proposed Regulation translates the protection of fundamental rights to a set of re quirements, primarily 12 of technical nature, which should be complied with. These a re insufficient to ensure that fundamental rights are effectively respected in the context of AI systems, and should hence not be seen as equivalent to the protection of fundamental rights . The Proposal s requirements erroneously reduce the careful balanc ing exercise between fundamental rights to a techn ocratic process, thus rendering the need for such balancing invisible. Currently, the Proposal claims to present : a balanced and proportionate horizontal regulatory approach to AI that is limited to the minimum necessary requirements to address the risks and problems linked to AI, without unduly constraining or hindering technological development or otherwise disproportionately increasing the cost of placing AI solutions on the market.22 Yet when consider ed through the lens of fundamental rights, this approach falls well short of effective rights protection offering a weaker form of market -focused regulation as opposed to specific and clear protections against fundamental rights interference which might be generated by particular AI systems . We argue that these burdens should be reversed . Innovation is a legitimate and perhaps necessary aim, given the remit, goals and obligations of the Commission and the legal basis upon which this Proposal has been de veloped. However, as argued in the previous section, internal market considerations cannot take priority over fundamental rights . Doing so significant ly threatens the fundamental rights of individuals arising from the development, deployment and use of AI systems , particularly when those systems are used to inform and even to automate the exercise of decision -making power by public authorities and private companies . A serious weakness of the regime is its failure to grapple with the highly controversial nature of AI applications, treating AI systems as analogous to other consumer products. However, while the deployment of many existing AI applications already implicates a wide range of fundamental rights, there is no systematic regulatory oversight to en able independent evaluation of whether any resulting rights interferences can be justified for the achievement of permissible purposes which are necessary and proportionate in a democratic society. The Proposal must avoid treating AI systems solely as tech nical consumer products which may yield claimed economic and social benefits . Instead, it needs to acknowledge that they are socio -technical systems which mediate social structures (and in doing so, must grapple with administrative and institutional requir ements and cultures), and which can produce significant consequences for individuals, groups, and society more generally. c) The risk -categorisation of AI systems remains unduly blunt and simplistic Not only does the Proposal fail to take seriously the distinct nature and strength of fundamental rights, the risk -based approach taken by the Proposal also remains unduly blunt and simplistic. Compared to the binary approach put forward in the White Paper on AI , the Proposal s more nuanc ed degree of risk strata amongst AI systems is an important improvement. A distinction is currently made between ( prohibited practices (with certain exceptions) in Title II, ( high - risk AI systems in Title III, ( systems requiring increased transpar ency measures due to a risk of deception in Title IV (which could also fall under the high -risk systems category) and ( all other systems. However, for several reasons, this approach may still not be sufficiently stratified in practice to adequately prot ect fundamental rights. First, the Proposal does not prohibit the deployment of AI systems which violate fundamental rights other than systems which engage in the prohibited practices. Instead, AI systems 22 European Commission, The Proposal, Explanatory Memorandum, 3 . 13 (including high -risk systems) may be deployed even if they interfere with fundamental rights, if those deploying them adhere to the requirements set out under Title III and have in place a self-certified quality risk management system which has not been independently evaluated or reviewed. Second, only AI systems which have specifically been identified by the European Commission as high -risk are subjected to mandatory ex ante requirements, such as ensuring high quality data, testing for bias, or securing human oversight. In other words, based on the Proposal, either a system is considered as high -risk, or it poses no risk at all. Systems which are not specifically listed as high risk might in practice pose a risk to fundamental rights, but are not subjected to these requirements, nor to an impact assessment or other mechanisms which require providers to reflect on potential risks. And while Title IV of the proposed Regulation does introduce a category of AI systems which require increased transparency measures, it only pertains to three types of systems and is limited to an obligation to inform people that they are subjected to an AI system, rather than including some of the requirements of Title III. The protection afforded by the Proposal hence enti rely depends on whether the closed high -risk list is sufficiently comprehensive and up to date. This demand may be hard to meet, especially given the technology's evolving nature. While the Commission s aspiration to clearly delineate high -risk AI appl ications is understandable from the point of view of providing legal certainty to AI providers, it fails to capture the more nuanced reality of the way AI systems are used and have an impact on fundamental rights. Moreover, the Proposal s reliance on a list-based approach also opens the door to restrictive interpretations of the AI applications that fall under its scope. AI providers who wish to circumvent the mandatory requirements imposed on high -risk AI systems may find creative ways to argue that th eir system does not fall under one of the narrowly defined high-risk categories, thereby pushing effectively high -risk systems towards the no-risk category. In other words, the lack of effective fundamental rights protection is due partly to the adopti on of a highly prescriptive , list-based approach to the identification of high -risk systems. Although the Proposal appears to rest on a belief that this prescriptive list-based approach will enable legal certainty, it is likely to produce the opposite effect. Even with the possibility for the Commission to update the high -risk list by means of a delegated act, the danger remains that the list which already merits improvement will date rapidly and fail to provide a future -proof layer o f protection against the substantial adverse impact s of the technology. To close this protection gap, an extension of the current list -based approach with an approach based on broader risk -criteria, as can also be found under the General Data Protection Regulation, should hence be considered. To summarise, the Proposal does not provide adequate fundamental rights protection, as it fails to recognise the distinct nature of those rights and the well -established doctrines on fundamental rights interferences. Its requirements for dealing with AI sy stems which implicate fundamental rights do not acknowledge the specific and demanding scrutiny (including the tests of necessity and proportion required by human rights law ), and the risk categories specified by the Proposal may be too simplistic to provi de adequate protection. As a result, responsibility for interferences with fundamental rights currently generated by AI applications (and which may arise from future AI applications) is not appropriately distributed under the Proposal, leading to a deficit in Legal Trustworthiness. 2 The Proposal s scope is ambiguous and requires clarification The previous sections demonstrate why the Proposal s approach to fundamental rights protection is currently inadequate , resulting in a failure to appropriately allocat e responsibility 14 for the actual and threatened wrongs and harms of AI. The next sections outline shortcomings associated with the Proposal s scope , which also affect the way in which responsibility for the harms of AI is distributed , and thus the first element of Legal Trustworthiness . We address (a) the Proposal s lack of clarity in its definition of AI, (b) lack of clarity concerning the position of academic researchers, (c) the gap in legal protection agains t military AI, and (d) the fact that security and intelligence agencies are not mentioned . a) The Proposal s current definition of AI lacks clarity and may lack policy congruence The primary objective of the Proposal is to regulate AI systems . How AI is defined is therefore essential for the determination of who bears legal duties under the Proposal and the scope of its protection . We argue that the definition of AI could lead to confusion and uncertainty , and although we do not propose a concrete solution in this document , we believe it merits significant attention. Article 3 ( of the proposed Regulation states that artificial intelligence system (AI system) means software that is developed with one or more of the techniques and approaches listed in Annex I and can, for a given set of human -defined objectives, generate outputs such as content, predictions, recommend ations, or decisions influencing the environments they interact with. The techniques and approaches listed in Annex I include (a) Machine learning approaches, including supervised, unsupervised and reinforcement learning, using a wide variety of methods including deep learning; (b) Logic - and knowledge -based approaches, including knowledge representation, inductive (logic) programming, knowledge bases, inference and deductive engines, (symbolic) reasoning and expert systems; (c) Statistical approaches, Bayesian estimation, search and optimization methods. The description of software which provides outputs for human -defined objectives is incredibly broad, as it encompasses virtually all algorithms.23 Annex I is meant to specify which particular techniqu es fall under artificial intelligence techniques. However, the techniques mentioned seem to include virtually all computational techniques, as machine learning, inductive, deductive, and statistical approaches are all included. As the title of Annex I re fers to Artificial Intelligence Techniques and Approaches, it could be assumed that the definition of, for example, logic -based approaches is limited to logic -based approaches to artificial intelligence . However, as artificial intelligence is defined as any algorithm which uses the techniques listed in Annex I, this specification has become circular and is therefore not a specification at all. For computer scientists working with any computational techniques which draw on logic or statistical insights , but which are not conventionally seen as falling within the domain of artificial intelligence, it is hard to determine whether this Regulation applies to them. This applies especially to instances of simple automation , in which logic -based reasoning and probabilistic approaches are used in algorithms which simply execute pre -programmed rules and do not engage in optimisation and learning (think, for example, of virtual dice which display the number six with a certain pr obability). In safety -critical and fundamental rights - critical contexts, it is of little relevance whether a system relies on simple automation rather than on optimisation and learning although the specific risks and dangers associated with the respectiv e categories are distinct, and risk assessment procedures for the different categories of systems might need to be designed with the distinct properties of the category in mind. 23 If we take the definition of an algorithm to be A finite series of well -defined, computer -implementable instructions to solve a specific set of computable problems, from The Definitive Glossary of Higher Mathematical Jargon: Algorithm, Math Vault, accessed 21 June 2021, - glossary/#algo. 15 However, it remains the case that the providers of simple automation systems will not consider their systems as being covered by the list in Annex III, as it remains a list of high- risk AI systems . The fact that AI is not clearly defined may therefore lead to legal uncertainty for providers of simple systems which use logic -based or statistical approaches which do not rely on optimisation and learning, and would therefore not immediately be thought of as artificial intelligence . There is an open question as to whether this legal uncertainty could be solved by changing the name and scope of the proposed Regulation, which could be done in two ways. The first option consists of broadening the scope of the Proposal, and changing the name from Artificial Intelligence Act to Algorithms Act or Software Act. This is a reasonable approach if the goal of the proposed Regulation is to guarantee the safe ty of products in the Union internal market and the respect for fundamental rights and Union law. From the point of view of risks to safety or fundamental rights, it is irrelevant whether a system which is used by law enforcement authorities to detect dee p fakes (Annex III, 6(c)) is based on deep learning or on cryptography -based authentication, for example. While one system could be appropriately described as AI, and the other cannot, in both cases, deepfake detection for law enforcement remains a high -risk domain of software engineering with potentially profound impacts on fundamental rights. If the risk categories are defined by reference to the social domain in which a system is used, rather than the specific computational technique used, it would ma ke sense to extend the scope of the application to non -AI software applications in those specific domains. This would also contribute to legal certainty, as it would completely circumvent the thorny question of whether a particular software system can be s aid to fall under the contested notion of AI, and instead focus on the domain of application, which can be specified in Annex III. The second option for solving the issue of the legal uncertainty created by the definition of AI is changing the name and scope of the proposed Regulation in a limiting manner. Instead of broadening the scope to include all algorithms used for the purposes specified in Annex III, the Commission could limit the scope to only include systems which rely on machine learning methods. This would have advantages regarding legal certainty, as machine learning systems are much easier to define than AI systems in general. However, this approach would have the crucial disadvantage of excluding from its scope many computational tech niques in safety - critical and fundamental rights -critical domains, and can therefore not be recommended from the point of view of fundamental rights protection. The current approach taken by the proposed Regulation may be attempting to find a middle groun d, in that it gives the impression that it only covers a precisely defined subset of computational techniques, while in reality covering a poorly defined range of computational techniques in a more precisely defined range of social domains. This could crea te legal uncertainty for software providers who provide systems which operate in the domains listed in Annex III but do not necessarily think of themselves as AI providers. As the determination of who is an AI provider and therefore subject to the Prop osal is crucial for the allocation of potential legal responsibilities to this person, the uncertainty around the definition of AI merits attention. b) Clarification is needed on the position of academic research The second issue relating to the scope of the Proposal and the allocation of responsibility is the position of academic researchers working within universities ( academic research ) . It is currently unclear to what extent the proposed Regulation applies to academic research. The overall tone of the Proposal, which focuses on the promotion of a well -functioning internal 16 market 24 of AI in the Union, as well as the legal basis of the Proposal, seem to suggest that academic research falls outside of the scope of the proposed Regulation, to the extent that it is done in a non -commercial capacity (thereby outside the context of the market ) and subject to its own scientific ethics standard s. However, the text of the Proposal is not clear on the position of academic researchers. Article 2((a) states that the Regulation applies to providers placing on the market or putting into service AI systems in the Union, irrespective of whether thos e providers are established within the Union or in a third country. A provider is defined in Article 3(25 as a natural or legal person, public authority, agency or other body that develops an AI system or that has an AI system developed with a view t o placing it on the market or putting it into service under its own name or trademark, whether for payment or free of charge. Article 3( defines putting into service as the supply of an AI system for first use directly to the user or for own use on the Union market for its intended purpose. While an academic researcher may not be a provider who places an AI system on the market in the course of a commercial activity (Article 3(), they might be accurately described as a natural person who d evelops an AI system with a view to supplying it for first use directly to the user or for own use free of charge. The text of the Proposal seems to suggest that in such a case, the proposed Regulation would apply to academic researchers. However, recital 16 seems to contradict this interpretation. It makes an exception to the prohibition on AI systems which deploy subliminal techniques: research for legitimate purposes in this area may still occur if such research does not amount to use of the AI system in human -machine relations that exposes natural persons to harm and such research is carried out in accordance with recognised ethical standards for scientific research. On the one hand, recital 16 seems to suggest that research general ly does fall within the scope of the proposed Regulation, as it outlines specific circumstances in which the prohibition in Article 5((a) does not apply (namely if no natural persons are exposed to harm and ethical principles are followed). On the other hand, this exception is not repeated in Article 5 or any of the other articles. The applicability of the Proposal to academic research must therefore be clarified. In this clarification, it would be useful to make a clear distinction between research which takes place in academic institutions, which have well -established ethics review systems and accountability measures in case of breach of these procedures, and R&D departments in private corporations which often lack such institutionalised et hics procedures. The recognised ethical standards for scientific research mentioned in recital 16 can differ wildly between academia and corporate R&D. A situation in which the level of fundamental rights protection differs between these contexts would b e unacceptable. Finally, attention should be drawn to the fact that there are currently no mandatory rules for so-called in -the-wild' testing or experimentation with AI systems by public or private actors. If research falls outside of the scope of the Reg ulation, this creates a potential loophole which makes it possible for providers and users of systems which would otherwise be subject to the requirements f or high-risk or prohibited systems to claim that they are merely doing in -the- wild' experiments, rather than actually developing or deploying a high -risk or prohibited AI system. Yet, in -the-wild testing of some AI applications such as the trials, testing and training of autonomous vehicles on public road s, for example is safety -critical and often occurs 24 See European Commi ssion, The Proposal, 25 The Proposal says 3( but it comes after 3( and before 3(, so it is reasonable to assume that the document meant to say 3(. 17 without the consent of the affected public . Accordingly, it is vital that such activities are subject to appropriate legal regulation. This is, however, currently lacking and appears to be excluded from the scope of the Proposal. c) The potential gap in legal protection relating to military AI should be addressed The third issue concerning the scope of the Proposal is the potential gap in legal protection against fundamental rights interference s through military AI. Recital 12 explains that AI systems exclusively developed or used for military purposes should be excluded from the scope of this proposed Regulation where that use falls under the exclusive remit of the Common Foreign and Security Policy regulated under Title V of the Treaty on European Union (TEU). Additionally, Article 2( states that [t]his Regulation shall not apply to AI systems developed or used exclusively for military purposes. The condition where that use falls under the exclusive remit of the Common Foreign and Security Policy regulated under Title V of the TEU is not repeated in Article 2(. It is therefore unclear whether this condition actually applies, or whether all AI systems developed or used exclusively for military purposes are excluded from the scope of the proposed Regulation, regardless of whether they fall within the exclusive remit of the Common Foreign and Security Policy. This difference is crucial, considering the existence of the European Defence F und (EDF), which invests heavily in military AI.26 The legal basis of the EDF is not Title V TEU (mentioned in recital 12 of the Proposal), but Article 173(, Article 182(, Article 183 and the second paragraph of Article 188, of the Treaty on the Funct ioning of the European Union. 27 Article 173 TFEU falls under Title XVII, Industry, and Articles 182, 183 and 188 fall under Title XIX, Research and Technological Development and Space. These legal bases differ significantly from Title V TEU General Provisions on the Union s External Action and Specific Provisions on the Common Foreign and Security Policy. This choice of legal basis for the EDF is also reflected in the fact that the European Commission plays a dominant role in the allocation of funds under the EDF, while the European Defence Agency (established under the Common Foreign and Security Policy (CFSP), Title V TEU) merely has an observer role, assisting the Commission.28 AI projects developed in the context of the EDF therefore do not fall within the exclusive remit of the CFSP and would therefore not be excluded from the scope of the proposed Regulation if one follows the text in recital However, the fact that the condition of the exclusive remit of the CFSP is not repeated in Article 2 ( of the Proposal suggests that the AI systems developed in the context of the EDF would in fact be excluded from the scope of the Regulation. This discrepancy between recital 12 and Article 2( must be clarified, as the text in recital 12 seems to cre ate an apparently arbitrary distinction between military AI systems developed in 26 See Christoph Marischka, Artificial Intelligence in European Defence: Autonomous Armament? The Left in the European Parliament , January 14, 2021, -intelligence -in- european -defence -autonomous -armament/. 27 European Parliament, European Parliament legislative resolution of 18 April 2019 on the proposal for a regulation of the European Parliament and of the Council establishing the European Defence Fund (COM(0476 C8-0268/2018 2018/0254(COD)), 18 April 2019, -8-2019 -0430_EN.html. 28 European Parliament, Legislative resolution of 18 April 2019 , Article The Commission shall be assisted by a committee within the meaning of Regulation (EU) No 182/2011 . The European Defence Agency shall be invited as an observer to provide its views and expertise. ( ) 18 the context of the CFSP and military AI systems which were developed in the context of the EDF. Moreover, Title V TEU concerns the Common Foreign and Security Policy, which d oes not cover the full range of Member State foreign and security policy activities, only those foreign and security policy activities which relate to the common Union defence policy. Recital 12 therefore seems to suggest that military AI systems which wer e developed in the domestic context of Member State militaries, and which are used according to individual Member State policies, rather than common Union policies, would in fact be subject to the proposed Regulation. This, again, is something that the phr asing of Article 2( seems to contradict. Again, the condition of the exclusive remit of the CFSP in recital 12 creates confusion as to the scope of Proposal and must therefore be clarified. In addition to the legal uncertainty created by recital 12, ther e are substantive concerns about the exclusion of military AI from the scope of the proposed Regulation. The HLEG provided four examples of critical concerns raised by AI: ( Identifying and tracking individuals with AI; ( Covert AI systems; ( AI enabled citizen scoring; ( Lethal autonomous weapons systems. 29 While the Regulation deals with the first three concerns at least to an extent (in Article 5((c); Article 5((d); and Article , Article 2( makes it so that lethal autonomous weapons systems are excluded from the scope of the Proposal. This is concerning as the development of autonomous lethal weapons raises fundamental ethical concerns, such as the fact that it could lead to an uncontrollable arms r ace on a historically unprecedented level, and create military contexts in which human control is almost entirely relinquished and the risks of malfunction are not addressed. 30 The exclusion of military AI from the scope of the Proposal is not only worr ying given the risks associated with autonomous lethal weapons, it is also problematic considering that the European Union is itself actively involved in the development of military AI systems through the European Defence Fund. This involvement includes th e development of the so -called Eurodrone which has the capacity to be armed.31 The Union itself could therefore be considered as actively contributing to the risks associated with autonomous lethal weapons. The Regulation of the European Defence Fund est ablishes an ethics review for funding applications.32 However, this ethics review only consists of the Commission reviewing an ethics self -assessment performed by the applicants, which is woefully insufficient considering the high -stakes nature of military AI. Moreover, the Regulation of the EDF only applies to those military AI projects which seek funding from the EDF. Military AI systems developed with other sources of funding are therefore not even subject to the minimal requirement of ethics self -assess ment under Union law. The current legal landscape means that one of the most critical concerns raised by AI identified by the HLEG is not addressed by the proposed Regulation, and is insufficiently addressed by the Regulation of the European Defence Fund. This is an unacceptable gap in legal protection, which is even more egregious considering the active Union involvement in the creation of high risks through the funding of projects like the Eurodrone. 29 HLEG, Ethics Guidelines, 30 HLEG, Ethics Guidelines, 33 -31 Marisc hka, Artificial Intelligence in European Defence , 32 See Regulation (EU) 2021/697 of the European Parliament and of the Council of 29 April 2021 establishing the European Defence Fund and repealing Regulation (EU) 2018/1092, OJ L 170, 2021, 149 177, Article 7(. 19 d) Clarity is needed on the applicability of the Proposal to national security and intelligence agencies The final concern about the scope of the Proposal and how it leads to a particular distribution of legal responsibility concerns national security and intelligence agencies. The scope of the proposed Regulation explicitly excludes AI systems exclusively developed or used for military purposes (Article 2(), and it explicitly includes AI systems used by law enforcement (Annex III(), administrative agencies dealing with migra tion (Annex III(), and judicial authorities (Annex III(). However, while both the excluded and the included domains can be informed by the work of intelligence agencies, the Proposal and the Annexes do not explicitly mention national security or intell igence agencies. Article 3( defines law enforcement authority as any public authority competent for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including the safeguarding again st and the prevention of threats to public security. This definition could include intelligence agencies, which could be seen as a public authority competent for the safeguarding against threats to public security. This, however, depends on how safeguard ing is interpreted, as intelligence agencies often do not have executive powers and mainly function as a source of information for other authorities such as defence ministries, public prosecutors, immigration agencies, etc. (although this may depend on th e Member State legal system). Does the collection of information amount to safeguarding ? Moreover, as intelligence agencies often lack executive powers, they are commonly thought of as separate from law enforcement agencies, with a jurisdictional firewa ll 33 separating them. If intelligence agencies fall under law enforcement authorities as defined in Article 3(, they should be explicitly mentioned. If intelligence agencies do not fall under law enforcement, it is unclear whether AI systems developed and used for national intelligence purposes fall within the scope of the proposed Regulation, as national intelligence activities could be classified as related to military action, law enforcement, border control , and the administration of justice depending on the specific context. Moreover, national security is a fundamental rights -critical domain even if it does not immediately inform the high -risk domains listed in Annex III. Suppose an intelligence agency deploys a natural language processing (NLP) model which automatically screens social media posts for potential extremist content. This model flags extremist content posted by an individual. This individual is then put on a watchlist, which is shared with la w enforcement agencies. The individual is now put under extra surveillance by the intelligence agency and might be apprehended by the police for having posted particular content. This NLP system would not fall under the individual risk assessment, polygrap hs, deep fake detection, evaluation of the reliability of evidence, predicting the occurrence of a crime, profiling, or crime analytics listed in Annex III under law enforcement. Yet, such a system affects the fundamental rights of the person subjected t o it. Moreover, if intelligence agencies are not covered by Article 3(, this might also prevent the application of the prohibition on real -time remote biometric identification (Article 5() to the activities of intelligence agencies. This would cause a n unacceptable gap in fundamental rights protection, as one of the goals of Article 5( is to 33 Jonathan M. Fredmant, Intelligence Agenc ies, Law Enforcement, and the Prosecution Team, Yale Law & Policy Review 16, no. 2 ( , 20 prevent a feeling of constant surveillance which dissuades the exercise of the freedom of assembly and other fundamental rights (recital . To sum up, the scope of the Proposal significantly affects the way in which responsibility for any harms and wrongs caused by AI systems is allocated and distributed. Several a spects of the scope therefore merit clarification or improvement: (a) the definition of AI, (b) the position of academic research by university researchers, (c) military AI , and (d) national security and intelligence agencies. 3 The range of prohibited systems and the scope and content of the prohibitions need to be strengthened , and their scope rendered amendable So far, w e have argued that the Proposal reflects an inadequate understanding of fundamental rights and requires substantial clarifications regarding its scope. In the following sections, we address the list of prohibite d practices, which is an important element of the Proposal s protection of rights and its allocation of responsibilities. We welcome the inclusion of a set of prohibited practices in the proposed AI Regulation . It provides much -needed legal protection against a set of AI applications and practices which are so rights -intrusive that they cannot be justified and therefore should be legally prohibited. This approach is in line with the HLEG s Policy Recommendations which supported the introduct ion of precautionary measures when scientific evidence about an environmental, human health hazard or other serious societal threat (such as threats to the democratic process), and the stakes are high. 34 By offering concrete legal protection to individ uals and communities against the wrongs and harms which arise from such practices, the prohibitions have the potential to significantly strengthen the legal protection of the fundamental rights they implicate . However , we argue that the way in which these prohibitions are currently drafted is deficient. In particular , for various reasons set out below, we argue that the scope of the list of prohibited practices should be revised to (a) strengthen the prohibition on manipulative practices , and (b) clarify the provision s on AI -enabled social scoring . Moreover, the Proposal remains too tolerant of rights -intrusive biometric applications which also require stronger prohibitions. The subject of biometrics is however dealt with later (under section , since these systems fall partly under prohibited practices and partly under high -risk applications. a) The s cope of prohibited AI practices should be open to future review and revision The Proposal currently lists prohibited AI practices in Article In co ntrast to the list of stand - alone high -risk systems in Annex III, the list of prohibited systems in Article 5 of the Proposal cannot be amended by the European Commission. The current list of prohibited practices seems heavily inspired by recent controversies, especially the prohibitions on social scoring and remote biometric identification by law enforcement authorities. The problematic nature o f certain uses of AI can sometimes only be grasped when those uses are actually put in practice (e.g., much of the controversy around social scoring systems emerged in the context of recent, concrete developments in China). However, the fact that some prac tices have received recent media attention does not mean that they are the only AI practices which are deeply problematic. Future uses of AI systems can be hard to predict, and it seems premature to permanently fix the list of prohibited AI practices. 34 HLEG, Policy and Investment Recommendations , 38, section 26(. 21 Against this background, we recommend considering an option for the European Commission to add prohibited practices to Article 5 following review and consultation , in particular with the European Parliament . This review mechanism , which should ensure due res pect for legal certainty, could be triggered by reference to a set of criteria (set out in Title II) which would allow the Commission to revise and update the scope of prohibited practices . A requirement for wide public consultation prior to the addition of prohibited practices should be considered, to ensure that public debate on the matter is duly reflected in the Regulation. These additions would also strengthen the current weaknesses of o verall fundamental rights protection in the Proposal explained earlier, and contribute to the appropriate allocation of responsibilities in the context of AI. Indeed, if it is impossible to add AI practices to the list of prohibited practices over time , it might not be possible to allocate appropriate responsibility for the wrongs and harm ensuing from potentially da maging future AI practices. Recourse to the lengthy procedure of the adoption of a new regulation by the European Parliament and the Council (w hich could take several years) risks leaving individuals unprotected from future AI practices which could pose an unacceptable level of risk to fundamental rights . b) Stronger protection is needed against AI -enabled m anipulation The prohibition on subliminal manipulation is also under -protective , as it only applies to the exploitation of a limited set of vulnerabilities , leaving the door open to many non -subliminal manipulative AI practices . Moreover, it does not impose any obligations for uses of subliminal techniques which could be considered tolerable if undertaken with the full and informed consent of the affected target in a carefully monitored and rigorously supervised manner . First, we question the choice regarding the practices outlined in Art icle 5((a) and (b) to concentrate on only physical and psychological harms. Other significant, but excluded harms include financial and economic harms, cultural harms, harms of recognition and autonomy harms, but also collective and societal harms.35 There is no reason to hold that the harms following from the use of manipulative AI should be limited only to the first two types, or that they are necessarily more serious. Manipulative technologies which interfere with a person s fundamental rights and lead to significant harm of all sorts ought to be prohibited. We therefore recommend that references to physical and psychological harm are removed to be replaced simply by harm and fundamental rights interference . Second, we are concerned by the concentration of attention in the practices outlined in Article 5((a) and (b) on harms as they might be experienced by an individual person. Some of the harms and wrongs outlined above may extend beyond individuals to grou ps of people and society as a whole. Certain applications of AI are best judged as impacting society; for instance, algorithmically driven processes used on social media sites might have negatively impacted democratic engagement .36 In cases like these, w e can observe harm which is difficult to discern and demonstrate on an individual level, but still has far reaching adverse societal implications. Accordingly, in addition to the alterations suggested in the previous paragraph, the Commission could conside r extending the prohibition s provision to include harm to groups and to collective interests and values, with values referring to the Union values indicated in Article 2 of the Treaty of The European Union (TEU). 35 See also Victoria Canning and Steve Tombs, From Social Harm to Zemiology (London: Routledge, , Chapter 36 See, for instance Siva Vaidhyanathan, Anti-Social Media: How Facebook Disconnects Us and Undermines Democracy (Oxford: OUP, ; Nathaniel Persily and Joshua Tucker (Eds.), Social Media and Democracy: The State o f the Field, Prospects for Reform (Cambridge: CUP, , various chapters. 22 Third, Article 5(b) of the Proposal p rohibits the use of AI systems that exploits any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability, in order to materially distort the behaviour of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm. This Article highlights the vulnerabilities of persons on account of age, physical or mental disability. However, the logic behind the choice of only these protected ch aracteristics and the exclusion of characteristics such as race, sex, religion and ethnicity which are all protected characteristics under EU equality law, is deeply puzzling and apparently unjustified. Age and mental disability could be read as indica ting a question of mental capacity, but the inclusion of physical disability confounds this. Rather than limit the characteristics this way, we recommend that the clause should be expanded to include all grounds listed in Article 21 of the EU Charter on Fu ndamental Rights. Fourth , there are manipulative AI practices which do not rely on subliminal techniques. Article 5((a) only prohibits subliminal manipulation, or AI systems which deplo[y] subliminal techniques beyond a person s consciousness in order to materially distort a person s behaviour in a manner that causes or is likely to cause that person or another person physical or psychological harm. Subliminal techniques are methods of presenting information below the subjective threshold of awarenes s. 37 While it is debated exactly how much these methods can act as a stimulus for action, there is general agreement that, to some degree, they do. This definition is echoed in recital 16 which mentions subliminal components individuals cannot perceive, whic h indicates that subliminal techniques cannot be perceived. The same is reflected in the Proposal s Explanatory Memorandum, which states that the prohibitions covers (sic) practices that have a significant potential to manipulate persons through sublimina l techniques beyond their consciousness . 38 However, apart from the fact that the prohibited practices are not perceivable and that they are beyond consciousness, it is not entirely clear what subliminal means in the context of AI systems. Subliminal is usually understood as referring to sensory stimuli which are too subtle to consciously perceive. Yet, AI systems can be manipulative and cause harm without relying on sensory stimuli which are impossible to perceive. Consider a chatbot which learns wa ys of tricking people into revealing their passwords, for example. This chatbot does not emit sensory stimuli which are impossible to perceive its statements can easily be seen, read, and consciously processed by humans. However, the result of those stat ements might still be that someone is manipulated into sharing their passwords. Another example is social media disinformation which plays on people s anxieties by misrepresenting facts with a view to influencing their behaviour is manipulative .39 Ostensib ly the individual can act as they please, but once a person s deeper concerns are triggered, it becomes less clear what that means. This is manipulation which may materially distort a person s behaviour in a harmful manner but which may not be subliminal at all. It is unclear whether such manipulative systems which do not rely on subliminal cues would be covered by the prohibition in Art icle 5((a). This could lead to legal uncertainty. Moreover, it would leave a gap in responsibility for manipulative A I practices which do not rely on subliminal cues. Consequently, we suggest that where AI systems employ technologies that 37 Mary Still and Jeremiah Still, Subliminal Techniques: Considerations and Recommendations for Analyzing Feasibility, International Journal of Human Computer Interaction 35, no. 5 (, 457 -See also Sid Kouider and Stanislas Dehaene Levels of Processing during Non -Conscious Perception: A Critical Review of Visual Masking, Philosophical Transactions: Biological Sciences 362, no. 1481 (, 857 -38 European Commission, The Proposal, Explanatory Memorandum, 39 Caroline Jack, Lexicon of Lies: Terms for Problematic Information, Data & Society Research Institute , August 9, 2017, -of-lies/. 23 manipulate individuals (including, but not limited to, systems that utilise subliminal techniques) in a way that causes harm or interf eres with their fundamental rights should be prohibited. Finally , there may be uses of subliminal technologies which influence people s actions, but which are not illegitimate or harmful. There are examples of subliminal manipulation which aid motivation a nd could be self -consciously chosen by the user with their full and informed consent .40 However, for this to be so, there needs to be a power relation ship between the manipulator and manipulated which is close to symmetrical, such that the manipulation is plainly transparent. With this in view, we recommend that subliminal techniques to which those affected have given their free and informed consent and which do not cause harm or fundamental rights i nterference are excluded from the prohibition, although they must be subject to the transparency and disclosure obligations stated in Title IV on Transparency Obligations for Certain AI Systems, Article c) The provisions on AI -enabled social scoring need to be clarified and potentially extended to private actors In addition to the problems regarding the fixed nature of the list of prohibitions and the seemingly arbitrary particulars of the prohibition on manipulation, the prohibition on social scoring also needs to be clarified and extended. Article 5((c) of the Proposal prohibits AI systems to be used by public authorities or on their behalf for the evaluation or classification of the trustworthiness of natural persons over a certain period of time bas ed on their social behaviour or known or predicted personal or personality characteristics, with the social score leading to either or both ( ) detrimental or unfavourable treatment of certain natural persons or whole groups ( ) and/or treatment ( ) that is unjustified or disproportionate to their social behaviour or its gravity. In short, it prohibits AI -enabled social scoring by public auth orities. However , social scoring is often conducted by private actors, who potentially have access to enormous amounts of personal data, covering many important areas of life, like job applications, hiring policies, and loan applications. Social scoring m odels in these domains could have devastating effects. Furthermore, private organisations are increasingly moving into areas of social policy which would have previously been occupied by the state, and employ social scoring models to identify areas of need , like the identification of children most at risk of being mistreated with a view to taking them into care against the wishes of their parents, or the children themselves . In this context, it is crucial to clarify what on behalf of public authorities means, as some social domains occupied by the state in one Member State could be in private hands in another. This could potentially undermine the Proposal s goal of a coordinated European approach to the risks associated with AI. Secondly, Article 5(( c) focuses on social behaviour or known or predicted personal or personality characteristics, while it is well documented that proxies may be employed for where such personal data is protected. These proxies could be drawn from factors not mentioned in t he Article, such geographical location (postcodes, etc.). Scoring on these measures can be as discriminatory and devastating for individuals as those drawn from the characteristics included in the Proposal. This should be reflected in the language of the A rticle. 40 Henk Aarts, Ruud Custers , and Martijn Veltkamp , Motivating Consumer Behaviour by Subliminal Conditioning in the Absence of Basic Needs: Striking Even While the Iron is Cold, Journal of Consumer Psychology 21, no. 1 (, 49 -24 In conclusion, we warmly welcome the list of prohibited practices in Article 5, as it is a crucial part of the prevention of harms and the allocation of responsibility under the Proposal , and therefore contributes to Legally Trustworthy AI . Howeve r, we recommend the inclusion of a procedure to enable review , consultation and revision to account for future uses of AI which are incompatible with fundamental rights. Moreover, the provisions regarding subliminal manipulation and social scoring must be clarified and strengthened . 4 The adverse impact of biometric systems needs to be better ad dressed AI-enabled biometric systems are one of the primary regulatory targets of the Proposal. While the moratorium on facial recognition technologies mentioned in a leaked version of the AI White Paper has not materialised ,41 we welcome the EU s determin ation to tighten the control over such intrusive technologies, even redlining some uses of biometric systems. However, as we will argue in this section, the Proposal does not seem to take the fundamental rights implications of biometric technologies sufficiently seriously. This is especially problematic because b iometric systems can pose threats to several fundamental rights enshrined in the EU Charter. These affected rights are not limited to the rights to privacy and data protection, which are straightforwardly implicated by the use of biometric systems. Less obvious implications of biometric systems include interferences with the right to non -discrimination which may b e contravened by biased systems , or systems which categorise people on the basis of physical features which are implicitly or explicitly related to protected characteristics such as age, sex, ethnicity or race. Moreover , widespread implementation of biometric systems in society also risks generat ing chilling effect s with significant implications for the rights to free speech and free assembly and therefore ultimately the foundations of democracy itself.42 In addition , the invisible or silent 43 nature of these systems and the difficulty of avoiding particular public spaces make it so that genuine transparency is hard to achieve. As discussed in section 1, fundamental rights have an enhanced moral and legal status, and cannot simply be overridden for the sake of convenience or efficiency. Indeed, fundamental rights are accorded strong presumptive legal protection, such that any interference with those rights must be justified by those seeking to interfere with them in accordance with heightened standards of scrutiny . Those wishing to employ biometric systems ought to justify the use of such systems with reference to their legitimate purpose, necessity, and proportionality. The burden of justification is on the providers and users of biometric syste ms it is not on those who are subjected to them to prove that their use was unjustified. Moreover, the final judgment on the adequacy of such a justification must lie with an independent judicial authority. While the Proposal seems to apply this logic t o a very limited category of biometric systems, it presumes that most uses of biometric systems are merely high -risk, meaning that the AI provider does not need to engage in any justificatory discourse, as long as the requirements for high-risk systems a re complied with. This results in a biometric -tolerant regime, which leaves 41 Samuel Stolton , LEAK: Commission Considers Facial Recognition Ban in AI White Paper. EURACTIV.Com , January 17, 2020, -commission - considers -facial -recognition -ban-in-ai-white -paper/. 42 The Guardian Editorial, The Gua rdian View on Facial Recognition: A Danger to Democracy, The Guardian , June 9, 2019, -guardian -view -on- facial -recognition -a-danger -to-democracy. 43 Lucas Introna and David Wood, Picturing Algorithmic Surveillance: The Politics of Facial Recognition Systems, Surveillance and Society 2, (, 177, 181 -25 excessive discretion to AI providers regarding most biometric systems and fails to offer meaningful and effective protection of fundamental rights . In what follows, we first explai n the distinctions made by the Proposal between the various kinds of biometric technologies (a). Then, we critique the approach taken by the Proposal regarding biometric systems, including the limited nature of the prohibition on certain uses of biometrics (b), the unjustified exclusion of biometric categorisation and emotion recognition from the prohibition and the high -risk category (c), and the distinction made between public and private use of biometric systems (d). a) Different types of biometric systems under the Proposal : an overview Before we critique how the Proposal regulates biometric technologies, it is useful to outline the different categories of biometric systems envisioned in the Proposal. First, the adjective remote is used to indicate only t hose biometrics that collect data in a passive, remote manner while excluding traditional ones requiring physical contact, e.g., fingerprints and DNA samples. Second, biometrics used for identification or verification (known as remote biometric identifica tion systems, or RBIS) are differentiated from systems which classify or categorise people (known as biometric categorisation systems, or BCS). Additionally, emotion recognition systems (ERS) are separately defined and regulated despite their reliance o n biometric data . Such a differentiation renders RBIS subject to regulation of a higher threshold whereas BCS and ERS are only subject to transparency obligations and not necessarily regarded as high -risk. A distinction is also made between post (retrospective) and real -time (live) use of biometric systems, defined in Article Both are classified as high -risk and subjected to requi rements such as logging capabilities and human oversight.44 Further, the uniqueness of their high risks renders them subjected to third party conformity assessment rather than to self-assessment , as is the case for other high -risk AI systems.45 The only no rmative difference is that real -time biometrics are considered particularly intrusive 46 and prohibited if used in publicly accessible spaces for the purpose of law enforcement, subject to several exceptions.47 b) The prohibition on biometrics is not a re al prohibition The use of RBIS in public spaces for the purpose of law enforcement is a prohibited practice under Article 5((d). However, the prohibition is so narrow and allows for such broad exceptions that it barely deserves to be called a prohibit ion. Firstly, the Proposal does not mention the use of live biometrics in public spaces by public actors for non -law enforcement purposes, such as intelligence gathering for the purposes of national security or resource allocation , or for migration management purposes . The prohibition therefore only covers a limited range of uses of biometric systems, and by a very limited range of actors ( i.e., law enforcement). Regardless of whether it is used by law enforcement or by other public (or pri vate) actors, the use of this technology implicates fundamental rights in a way that is almost inevitably disproportionate, as it requires the processing of a great amount of sensitive data of many people to enable the identification of few individuals. The prohibition should therefore be extended beyond the use of RBIS by law 44 European Commission, The Proposal, Recital 45 European Commission, The Proposal, 46 European Commission, The Proposal, Recital 47 European Commission, The Proposal, Recital 19 and Article 5((d). 26 enforcement, and at least also cover its use by public authorities (or private actors acting on their behalf) that have any coercive power over individuals. Secondly, broad exceptions exist to the already limited prohibition on the use of RBIS. Article 5((d) allows law enforcement to use RBIS in public spaces if such use is strictly necessary for one of the following objectives: ( the targeted search for specific potential victims of crime, including missing children; ( the prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons or of a terrorist attack; or ( the detection, localisation, identificati on or prosecution of a perpetrator or suspect of a criminal offence referred to in Article 2( of Cou ncil Framework Decision 2002/584/JHA and punishable in the Member State concerned by a custodial sentence of a detention order for a maximum period of at least three years, as determined by the law of that Member State. These exceptions are allowed as long as the seriousness, probability and scale of the harm caused in the absence of the use of the system and the consequences of the use of these systems for the rights and freedoms of all persons concerned are taken into account. Although we recognise that there m ight in principle be circumstances which render the use of RBIS by law enforcement in public spaces justifiable in a manner that conforms with respect for fundamental rights , the scope of the current exceptions does not meet the requisite standard . In particular , the third ground of exception, which allows for the use of such technology for the detection of perpetrators or suspects of criminal of fences covers a vast number of situations. This places excessive discretion in the hands of law enforcement agencies. Moreover, the consequences of permitting RBIS in public spaces means that infrastructures to enable the permitted use of this technology w ill be built and rolled out across Members States at scale. Once these infrastructures are built, function creep and potential misuses or abuses of such an infrastructure remain a very real concern. It is important to stress that the risks relating to this infrastructure go beyond an impact on individual privacy, but risks affecting collective values and the integrity of democracy at large. Merely knowing that the infrastructure exists, and not being able to ascertain whether it is currently being used , may lead to severe chilling effects on the exercise of political rights such as the freedom of speech and freedom of association. As the prohibition on biometric systems only applies to a very specific type of biometric system; a limited set of purposes for the system; and a limited set of actors, and as very broad exceptions to the prohibition exist, still enabling the constructio n of a biometric surveillance infrastructure, we believe that this prohibition barely deserves its title, and fails to provid e adequate fundamental rights protection. c) The Proposal does not take a principled approach to the risks of various biometric systems The prohibition on the use of RBIS by law enforcement in public spaces just discussed, is justifiable from a fundamental rights perspective. Yet, it is unclear why the prohibition in Article 5((d) does not include BCS or ERS. Firstly, the range of possible legitimate uses of remote live BCS for law enforcement purposes in public places is very difficult to imagine . Differently than in the case of RBIS, this technology does not aim to search for a specifically identified potentially at danger or dangerous individual, but for a group of people. Indeed, t he premise behind BSC is that people can be categorised based on their physical characteristics, such as visible gender, race, age, and the way they move their bodies. As these physical characteristics relate closely to the protected characteristics outlined in outlined in Article 21 of the Charter of the Fundamental Rights of the EU, any use of such technology for law enforcement purposes is suspect , and requires justification based on its necessity and proportionality in a democratic society, provided for by law . Moreover, t hese technologies present serious risks to minorities, as one 27 could easily imagine a system which is designed to recognise members of religious minorities, racial mi norities, or members of the LGBT+ community, which could be seen as useful information for law enforcement purposes.48 It is not clear why the Proposal presumes that RBIS is always more intrusive or damaging than BCS. Therefore , we recommend that the use of remote live BCS in public spaces for law enforcement purposes (and other public authorities with coercive powers) should be prohibited outright and admit of no exceptions . As regards the use of BCS ex post , given its intrusive and inherently discriminatory nature , it would be important to ensure that any permissible use thereof is subject to strict safeguards including ex ante control and ex post review by an independent authority. Therefore, all other uses of BCS which are not captured by the a fore-suggested prohibition should be included in the list of high -risk AI systems, and subjected to ex ante conformity assessment and additional safeguards in terms of necessity and proportionality, as well as transparency , rule of law and due process . Secondly, it is unclear why ERS are s een as less intrusive than RBIS. The premise behind ERS is that emotions can be read from human bodies, which can then be used for interventions. Yet the idea that emotions can be read from human bodies in any scientific and objective way has long been deb unked : it is based on pseudoscience rather than established scientific evidence .49 It is impossible to understand how a pseudoscientific technique could ever be properly regarded as necessary and proportionate for legitimate law enforcement purposes. Moreover, even if ERS were scientifically sound, automatically reading every single person s emotional state is a highly intrusive act, with potentially huge chilling effects, which again seems unjustifiable for law enforcement pu rposes. We therefore urge the Commission to prohibit any use of ERS for law enforcement purposes (and other public authorities with coercive powers), and admit of no exceptions given the lack of any scientific soundness of this technology and its extremely intrusive nature . As regards the use of ERS for other purposes for instance , deployed by private actors we believe they should at the very least be included in the list of high -risk AI systems . Moreover, the Commission should consider subjecting ERS intended to be used in one of the high -risk domains to ex ante conformity assessment and contro l, in particular when used in situations with power asymmetries or on vulnerable individuals and groups . d) The distinction between private and public uses of remote biometric systems requires justification Not only does the Proposal not seem to adopt a principled approach to the regulation of the different types of biometric systems, it also does not justify the distinctions it makes between different users of biometric systems. Currently, individuals are offered an asymmetrical level of protection against the use of AI - enabled biometric recognition systems. While the use of RBIS in public spaces by law enforcement authorities is prohibited, its use by private organisations is merely categorised as high -risk. The Proposal thus creates a two -tier system of regulation by which state 48 Joy Buolamwini and Timnit Gebru, Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, Proceedings of Machine Learning Research 81 (, 1 David Leslie, Understanding Bias in Facial Recognition Technologies: An Explainer, Alan Turing Institute , October 26, 2020, Evan Selinger, A.I. Can t Detect Our Emotions, OneZero , April 6, 2021, -i- cant-detect -our-emotions -3c1f6fce2539. Luke Stark and Jesse Hoey, The Ethics of Emotion in AI Systems, Octobe r 9, 2020, Douglas Heaven, Why Faces Don t Always Tell the Truth about Feelings Nature , February 26, 2020, -020-00507 -28 authorities are held to a higher standard, to the detriment of rights protections for citizens from private actors. Recital 18 ju stifies this on the basis that: The use of AI systems for real -time remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement is considered particularly intrusive in the rights and freedoms of the concerned persons, to the extent that it may affect the private life of a large part of the population, evoke a feeling of constant surveillance and indirectly dissuade the exercise of the freedom of assembly and other fundamental rights. However, these same problems may exist with private use of biometric technologies. It is not immediately clear why this approach differs from that of the General Data Protection Regulation, wherein no distinctions are made between public and private data controllers , recognising that AI systems expand and extend the powers of private companies and opening fundamental rights to new forms of abuse.50 This is not to say that the Proposal could not legitimately distinguish between the use of biometrics by pu blic authorities and by private actors. Nonetheless, we would expect at least a justification as to why such a distinction is necessary, and why government use of biometric systems poses a greater risk to fundamental rights than use by private organisation s. Otherwise, these restrictions seem to operate on the assumption that the use of biometrics systems by private organisations are somehow less intrusive, despite their potential for widespread use in semi -public spaces such airports, shopping centres, a nd sports stadiums. Personal data collected by private actors could be shared with law enforcement bodies, opening up a worrying loophole in the protections afforded by the proposed Regulation. The Proposal s approach seems to overlook the fact that priva te biometric systems are likely to be widespread and may also cause a feeling of constant surveillance ; and that private actor s may share information with law enforcement authorities or collaborate with them . It is true that the justification for the interference with fundamental rights traditionally comes from public actors. However, considering the enormous power of private actors in the space of AI and their ability to directly contribute to chilling effects on the exercise of fundamental rights, the Commission should consider tightening the rules for private use of biometric systems. In sum, t o ensure that the development and use of AI -enabled biometric technologies in the EU are Legally Trustworthy, the Pro posal needs to take seriously the duty to justify any interference with fundamental rights according to the principles of fundamental rights law. To enable such a discourse of justification , the Proposal requires a more principled approach towards biometri c technologies, which includes strengthening the current prohibition of RBIS, prohibiting live remote BCS in public places as well as the use of ERS by law enforcement and other public authorities with coercive powers , and reconsidering the existence of different regimes for public and private actors. Moreover, uses of ERS and BCS which are not included in the suggested prohibition should be added to the list of high -risk systems of Annex III , and subjected to strong safeguards . Only then can the Proposal move towards more adequate prevention of the harms associated with biometrics, and the appropriate allocation of responsibility for such potential harms. 5 The requiremen ts for high -risk AI systems need to be strengthened and should not be entirely left to self -assessment After having considered the proposed regulatory frameworks for prohibited systems and biometric systems, we now turn to high -risk AI systems as defined by the Proposal. 50 Linnet Taylor, Public Actors Without Public Values: Legiti macy, Domination and the Regulation of the Technology Sector, Philosophy and Technology (, -020-00441 -29 Title III of the proposed Regulation sets out a new regulatory regime with mandatory requirements for AI systems that pose a high risk to the health and safety or fundamental rights of natural persons namely high-risk AI systems .51 Rather than defining the concept of high risk, the Proposal specifically lists which systems fall under this category, which we have referred t o as a prescriptive list-based approach . High -risk systems include systems intended to be used as safety components of products which are subject to third party ex -ante conformity assessment covered by EU legislation listed in Annex II of the Proposal, a nd other stand -alone AI systems used in high -risk domains listed in Annex III. The former concern AI systems which are, for instance, covered by the Machinery Directive (2006/42/EC), the Toy Safety Directive (2009/48/EC) or the Medical Devices Regulation ( 2017/. The latter concern a limited list of AI applications in various areas, including in the field of education, HR, public services and law enforcement. The mandatory requirements for high -risk AI systems are broadly inspired by the requirements for Trustworthy AI listed in the HLEG s Ethics Guidelines and must be complied with prior to the system s placing on the market or putting into service. They pertain more particularly to data quality and data governance, documentation and recording keeping, transparency and provision of information to users, human o versight, robustness, accuracy, and security. The imposition of such mandatory requirements is a considerable step forward in advancing the protection against the adverse effects of AI systems. At the same time, the Proposal should still be substantially r evised regarding the way in which high-risk systems are defined via a prescriptive list -based approach , and the requirements which apply to high-risk systems. a) Outsourcing the acceptability of residual risks to high -risk AI providers is hardly acceptable As argued in section 1, the Proposal takes a rather techn ocratic approach to fundamental rights, imposing a list of obligations on the providers of high -risk AI systems, rather than making them engage with the justificatory discourse customa ry in human rights law. Not only does this choice poorly reflect the spirit of fundamental rights, it also confers undue discretion for the AI provider . For high -risk AI systems, Article 9 of the Proposal mandates the establishment, implementation and docu mentation of a risk management system to be maintained as an iterative process throughout the AI system s lifecycle. The risk management system should inter alia include the identification and analysis of known and foreseeable risks, an estimation and eval uation of risks which may emerge when the system is used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse, and the adoption of suitable risk management measures. These measures, according to Article 9(, s hall give due consideration to the effects and possible interactions resulting from the combined application of the requirements and take into account the generally acknowledged state of the art, including as reflected in relevant harmonised standards or common specifications. In short, the effectiveness of the mandatory requirements imposed on high -risk AI systems thus hinge s on the quality of this risk management system and the protection which it is intended to provide . Importantly, the Proposal goes beyond a mere requirement of risk management documentation, and also requires that systems are tested so as to identify the necessary risk management measures to ensure compliance with the various requirements (Article 9(). In addition, while the risk management process primarily falls upon the provider of the AI system, the provider 51 European Commission, The Proposal, Explanatory Memorandum, 13 . 30 needs to anticipate the knowledge, experience, education and training that can be expected from the user, as well as the environment in which t he system is intended to be used. At the same time, the Proposal appears to leave an unduly large amount of discretion to the provider of the AI system as regards the execution of the risk management process. Article 9( leaves it up to the AI provide r to determine which measures to take in order to ensure that any residual risk associated with each hazard as well as the overall residual risk of the high -risk AI systems is judged acceptable . This means that the decision about which risks are deemed acceptable is outsourced to the AI provider, who also seeks to put the system on the market or into service. Bearing in mind that this includes not only risks to the health and safety of individuals, but also risks to fundamental rights, this outsourcing appears seriously problematic. The fact that the AI provider needs to communicate those residual risks to the user does not offer much solace. At the very least, we wonder why there is no obligation for the AI provider to consult with stakeholders, such as those who will be subjected to the AI system or otherwise have a legitimate interest, about which level of risk may be deemed acceptable. Reference can be made to Article 35( of the GDPR which, in the context of a data protection impact assessment, states that where appropriate, the controller shall seek the views of data subjects or their representatives on the intended processing, without prejudice to the protection of commercial or public interests or the security of processing operations . By an alogy, a similar provision to ensure the involvement of those affected by the AI application when establishing the risk management system and setting out mitigation measures should hence also be considered here. In the same vein, the fact that testing proc edures shall be suitable to achieve the intended purpose of the AI system and need not go beyond what is necessary to achieve that purpose, and that testing thresholds should be set which are appropriate to the intended purpose of the high-risk AI syste m, leaves it up to the AI provider to decide how the words suitable and appropriate are interpreted. Given that the system is only (potentially) subjected to independent control ex post , this margin of discretion is difficult to justify , especially as it may pertain to sometimes difficult judgment calls in a fundamental rights -sensitive area. This is even more so for high -risk AI systems which are already on shaky grounds due to a lack of any scientific evidence for their appropriateness, such as emotion recognition systems, or systems which involve highly vulnerable individuals such as refugees and children . In sum, the discretion left for AI providers and deployers is reflective of th e Proposal s deficient approach to fundamental rights outlined above. b) It can be questioned why the listed high -risk AI systems are considered acceptable at all Besides the fact that providers of high -risk AI systems have too much discretion to decide on the acceptability of those systems, one could wonder why these systems with clear and potentially severe adverse implications for fundamental rights are considered to be acceptable at all. Their categorisation as posing a high yet nevertheless accept able risk (since if unacceptable, they would figure under the prohibited AI practices of Article is especially blatant given that no evidence is provided that their adverse impact on fundamental rights is necessary and proportionate in a democratic soc iety, as required by human rights law (see section . 31 In this regard, reference can also be made to the joint opinion of the E uropean Data Protection Board and the E uropean Data Protection Supervisor concerning the Proposal . In this opinion52, it is stressed that biometric categorisation technology in public spaces whether used by public authorities or private entities based on ethnicity, gender, political or sexual orientation, or other discrimination grounds prohibited under the Charter seems incompatible with the fundamental right to data protection and should hence be categorised as prohibited rather than high-risk. The same point is made regarding AI systems whose scientific validity is not proven or which are in direct conflict with essential values of the EU, 53 such as AI -enabled polygraphs, which are currently included under Annex III and hence deemed to pose a high yet acceptable risk to fundamental rights according to the Commission. Furthermore , and as already mentioned in secti on 4(c), it is also unclear why emotion recognition systems, which according to the Study supporting the Impact Assessment of the AI regulation lack scientific reliability and validity 54, so that any sentiment analysis software attempting to recognise human emotions is thus unproven , 55 are not even considered as posing a high risk despite the calls for a ban on such technology, unless when used by public authorities in migration cases or by law enforcement. It is, in our view, at least doub tful that this categorisation is reflective of the informed public opinion on this matter, and that the impact of such systems on fundamental rights can be legitimised as necessary and proportionate. To appropriately allocate responsibilities for the risk s of high -risk systems, the list of high -risk applications cannot include AI practices which are incompatible with fundamental rights and whose use cannot be reasonably justified, such as technologies which are manifestly discriminatory and/or lacking in a ny clearly established scientific basis . c) The adaptability of the Scope of Title III is too limited As noted above, the scope of Title III i.e., high -risk AI systems is limited to those systems which are covered by Annex II of the Proposal (intended to be used as a safety component of a product, or a product covered by the specifically listed Union harmonisation legislation) or by the list of stand -alone high-risk systems laid down in Annex III. This Annex provides a list of eight areas in which, according to the European Commission, high -risk AI applications are used. Within each of these areas, the Annex lists a limited number of AI applications which are considered as high -risk and hence subjected to the mandatory requirements laid down in Title III. Pursuant to Article 7( of the P roposal, the European Commission can amend the list in Annex III and include additional systems if it can demonstrate that those systems pose a risk of harm to the health, safety or fundamental rights of individuals which is in respect of its severity and probability of occurrence, equivalent to or greater than the risk of harm or of adverse impact posed by the high -risk AI systems already referred to in Annex III. Furthermore, Article 84( mentions that the Commission will review this list on a yearly b asis, in order to keep it up to 52 European Data Protection Board, EDPB -EDPS Joint Opinion 5/2021 on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act), 18 June 2021, -06/edpb - edps_joint_opinion_ai_regulation_en.pdf . 53 European Data Protection Board, Joint Opinion, 54 Andrea Renda et al., Study to Support an Impact Assessment of Regulatory Requirements for Artificial Intelligence in Europe, April 21, 2021, Publica tions Office of the European Union , -detail/ -/publication/55538b70 -a638 -11eb -9585 -01aa75ed71a1/language - en/format -PDF/source -55 Renda, Study, 32 date with technological developments. Crucially, however, the Commission is only empowered to add high -risk AI systems to Annex III if they are intended to be used in any of the areas listed in points 1 to 8 of Annex III. This means that the list of high -risk AI systems is only amendable to the extent that new high -risk applications fall under the already existing headings mentioned in Annex III. This is problematic for two reasons. Firstly, there are already categories of AI systems which should be classified as high -risk, but are not currently listed in Annex III, nor would they fall under any of the eight headings. For example, high -frequency trading algorithms have profound impacts on the market and have the potential to have destabili sing effects on economies.56 These algorithms do not fall under any of the listed areas and can therefore never be classified as high -risk systems. Secondly, there might be categories of high -risk systems which we are currently not aware of, considering that the field of AI moves fast and increasingly permeates other fields. For example, whereas the field of cyber security was not conventionally seen as an AI field, it is now flooded with AI research, and many cyber security solutions n ow rely on AI. There could be similar developments in other fields, which we might not currently be able to foresee. For these two reasons, it would be advisable to foresee the opportunity to include new domain categories to the list in Annex III. Final ly, while recital 85 of the Proposal seems to indicate that the Commission should also have the power to amend Annex II (namely the list of Union legislation covering systems that pose a risk to safety and hence fall under the high -risk category) no such powers seem to be granted to the Commission in the Proposal s articles. Instead, the Commission s delegated powers to amend the list of high -risk AI systems seem to be limited to an update of Annex III. This omission might be an oversight on behalf of the drafters of the Proposal and would need to be corrected prior its adoption. d) The list of high -risk AI systems for law enforcement should be broadened Even when leaving aside concerns about the future -proofness of the list -based approach to high-risk AI systems and about the potential miscategorisation of such systems and lack of policy -congruence ,57 questions already arise regarding the comprehensivenes s of Annex III, specifically in the domain of law enforcement. Annex III( lists the high -risk systems in the domain of law enforcement. The list is mostly focused on systems which have natural persons as their subjects ( individual risk assessments for natural persons; detect the emotional state of a natural person; profiling of natural persons ). By focusing on natural persons, the Annex fails to identify optimisation systems which use geospatial data to determine law enforcement resource deployment and/or prioritisation as high -risk systems (otherwise known as predictive policing, or crime hotspot analytics systems ). Generally, such technologies are used to produce statistical predictions regarding where future crime may take place, in order to enable law enforcement authorities to optimise where and how they deploy their resources for maximum benefit. Resource optimisation systems do not solely rely on personal data of natural persons, but draw from a wide range of data relating to a geographi cal location, including the occurrence and rate of occurrence of crimes in a specific geographical location. 56 See generally Maureen O Hara, High -Frequency Trading and Its Impact on Markets, Financial Analysts Journal 70, no. 3 (. 57 In this regard, see also Karen Yeung s submission to the public consultation on the European Commission s White Paper on Artif icial Intelligence of February 2020, which elaborates on the problems relating to a list - based approach to high -risk AI systems. The submission can be accessed here: Despite not relying on personal data of natural persons, the fundamental rights implications of these systems are important because they are used to determine who can be subject to increased police intervention (based on geographical location), how these interventions occur, and with what frequency. Used for law enforcement purposes, without due care, resource optimisation systems may contribute to over-policing and surveillance of specific geographical locations caused by data feedback loops, 58 and in doing so, may further exacerbate existing problems with systemic discrimination arising from historical racial and socio -economic biases in existing policing datasets (as geographical location is often a proxy for race or economic class), with little opportunity for redress, and no transparency for affected individuals. Drawing from Philip Alston speaking about the SyRI welfare fraud detection system in the Netherlands, targeting entire neighbourhoods as suspect and subject to special scrutiny with a combination of digital and physical methods threatens the very essence of privacy, by contributing to general unease, potential prejudice, and chilling effects on behaviour.59 It matters little whether the personal data of natural persons is implicated here. Were the final text of the Regulation therefore to exclude resource optimisation systems, this could amount to a significant failure to recognise the systemic social assumptions that are built into AI systems as socio -technical systems which mediate social i nstitutions and structures which by virtue of their implementation, further mediate the enjoyment of individuals fundamental rights, including respect for their human dignity, equality, liberty and other freedoms. We therefore recommend that geospatial AI systems are included under Annex III(, and that reference is made to AI systems which affect the distribution of law enforcement resources. e) The requirements that high -risk AI systems must comply with need to be strengthened and clarified Our final remarks on the proposed regulatory framework for high -risk AI systems concerns the strength and clarity of the requirements for such systems. While vague language which might cause legal uncertainty and a weak protection against AI s adverse effects can be found under several requirements for high -risk systems, we highlight a few questions and concerns regarding three requirements in particular: those pertaining to data governance , transparency and human oversight. Data governance obligations The first requirement which raises questions is data governance. Firstly, the large discretion for providers of high -risk AI systems mentioned earlier is also reflected in the requirements for data governance . Article 10 of the Pro posal, dealing with requirements of data quality and governance, also lets the term appropriate do some heavy lifting. Indeed, it requires that training, validation and testing data sets shall be subject to appropriate data governance and management pr actices, that the data sets shall have appropriate statistical properties, and that the processing of special categories of data to avoid the risk of bias is carried out subject to appropriate safeguards for fundamental rights. While the Article can be commended for specifying the minimal considerations that should be taken into account for the data management process to be considered appropriate, it leaves open what constitutes an 58 The Law Society of England and Wales, Algorithms in the Criminal Justice System, June 4, 2019, -use-in-the-criminal -justice -system -report, 59 Philip Alston, Brief by the United Nations Special Rapporteur on extreme po verty and human rights as Amicus Curiae in the case of NJCM c.s./De Staat der Nederlanden (SyRI) before the District Court of the Hague (case number: C/9/550982/HA ZA 18/, OHCHR , September 26, 2019, 34 appropriate statistical property. Does this require that the data be a representative sample of the entire population, or merely of the potential ad hoc groups that may be subjected to the AI system s analysis or scoring? This decision is left to the AI provider. Going the other extreme, the Article also seems to require tha t training, validation and testing data sets shall be complete and free of errors a potentially unrealistic obligation. Secondly, the data governance obligations themselves do not seem to prevent the use of bad proxies. Article 10( requires that t he assumptions about the information that the data supposedly measures and represents which we understand to include the manner in which the data is used as a proxy for something else be rendered explicit, and that potential data gaps or shortcomings a re identified at the outset. Rendering assumptions and proxies explicit is an important step forward. At the same time, this does not yet in and of itself prevent the use of misguided proxies, especially given that these are not part of the information tha t needs to be made public in the EU database for stand -alone high -risk AI systems according to Article Nothing in the Proposal seems to, for instance, prevent public authorities from using arrest data as a proxy for crimes committed (while not all arrested persons are charged or convicted, and many crimes occur for which no arrests are made). Given that these assumptions are not publicly accessible, their misguided nature may not easily come to light. It is hence difficult to argue th at this provision can ensure the adequate protection of fundamental rights. Accordingly, this provision should be strengthened with a requirement to ensure that the assumptions made are reasonable and adequate, and that they are part of the information tha t is communicated both to the user of the high -risk AI system as well as those who may be affected thereby. Finally, there are open questions about the new data governance requirements. For instance, interaction with the GDPR - and more specifically the applicability of the GDPR s requirements for collecting and processing data - is not addressed in the Proposal (except in the Explanatory Memorandum which states that the Proposal is without prejudice and complements the GDPR).60 Furthermore, while Articl e 10( states that training, validation and testing data sets shall be relevant, representative, free of errors and complete, no mention is made of data integrity (in terms of data provenance). One can hence wonder what the status is of data that has be en collection in violation of people s rights outside the EU such as data from the Chinese population with less extensive privacy rights and whether the use thereof would still be deemed appropriate by the proposed Regulation. We would hence advise the inclusion of a specific provision in the Proposal to exclude the use of such non -integer data. Transparency obligations Transparency regarding AI systems is intended to be one of the main fundamental rights protections afforded by the Proposal. Besides establishing a category of AI systems which require increased transparency, regardless of their risk -level (under Title IV), and the creation of the EU database for stand -alone high -risk AI systems that will be publicly accessible (under Title VII) , transparency requirements are also found in Title III on high -risk AI systems. Under this Title, Article 13 mandates that a high -risk AI system be designed in such a way as to be sufficiently transparent to enable users to interpret the system s output and use it appropriately 61 and that it should be accompanied by instructions for use with information about the system that is relevant, accessible and comprehensible to users. Pursuant to Article 13(, this information should inter alia include the id entity and contact details of the provider of the AI system, the technical specifications and details relating to the system s performance 60 European Commission, The Proposal, Explanatory Memorandum, See also section 2 further below, concerning the consistency of the proposed Regulation with data protec tion legislation. 61 European Commission, The Proposal, Recital 35 (including its limitations), the human oversight measures that were put in place, as well as residual risks to health , safety or fundamental rights based on the systems intended use or reasonably foreseeable misuse. However, there are three two significant problems with the approach taken to transparency here. Firstly, the Proposal focuses specifically on transparency fo r the user of the system rather than for the individual who is subjected to the system. There is hence no direct engagement with the needs of EU citizens and residents , or with those individuals coming into contact with providers and users of AI systems wh o are based within the EU. This point is further stressed under section 3 .2, which deals more extensively with the Proposal s deficiency in terms of transparency rights for individuals . Secondly, as is the case for the other requirements of Title III, the use of terms such as sufficiently transparent and appropriate type and degree of transparency, grants a lot of interpretative leeway to AI providers who will be self -assessing their systems level of transparency. As noted above, while Article 13( does mention the minimal elements which should be disclosed on to users, this does not concern information about the way in which those who are subjected to the system may be adversely imp acted by the system. Furthermore, the list of information to be disclosed seems to rely on the idea that results generated in the lab, for instance in terms of performance and accuracy, will suffice. It should however be kept in mind that these results may differ substantially when it comes to in the field settings that can be more unstable or different from the lab. It would hence be important to include a statement about the conditions in the field in which the AI system is intended to be used, as wel l as about the parameters of performance testing. The lack of standardised protocols for such testing and for quality and performance metrics is problematic, since these are essential to ensure that the imposed transparency obligations remain meaningfu l outside of the lab. Human oversight obligations In addition to the requirements above, t he requirement of human oversight also needs further clarification . Article 14 expresses an admirable desire to ensure that there is a human failsafe in the loop to prevent harm or rights violations. Yet in many instances, a meaningful failsafe is impossible to secure in practice, given that the entire premise of data mining is aimed at generating insight that is beyond the capacity for human cognition. This inevitably also means that the human being who needs to exercise oversight over the system will often not be able to second -guess the validity of the system s outpu ts, except in limited cases where human intuition may detect obvious failures or outliers. Moreover, the problem of automation bias is unlikely to be overcome through this provision . Further, it remains unclear whether the human oversight measures set out in Article 14 apply to the user, or someone independent of the user , or indeed whether the user refers to the organisation that uses the AI system on the whole, or to a specific individual who is responsible for a specific decision.62 Oversight is required for all action related to the development, deployment and use of AI systems, to ensure that fundamental rights are protected to the highest standard possible. This includes human oversight of the design process and regular independent human oversight of those humans -in-the-loop who are responsible for taking the final decision, informed by the output of an AI system. It is not enough to know that these individuals are aware of the potential for automation bias, it must be transparently demonstrated and ensured that decisions are not made through over -reliance on the output of an AI system. Therefore, we recommend that under Article 14(, a third category is introduced which recognises the need for users to implement organisa tional non -technical measures to 62 European Commission, The Proposal, Recital 36 ensure robust human oversight, which consists of at least: training for decision -makers, logging requirements, and clear processes for ex post review and redress. Finally, a clarification is needed regarding Article 14(, which imposes an additional oversight requirement when biometric identification systems are used. In such case, the system s user cannot take any action or decision based on the identification resulting from the system, unless the result was verified and confirmed by at least two natural persons. While such strengthened oversight sounds laudable, two points can be raised. First, for this provision to be meaningful, the confirmation given by the two natural persons should be based on a separate assessment (with one on the ground, for instance, to sight the individual in question) rather than being reduced to two people looking at the same computer screen. Second, reliance on human oversight as a safeguard should only be used as a last resort once the use of such intrusive systems has been proven to be necessary and proportionate in a democratic society, and not as a legitimation of the use of technologies that should in fact not be used in light of their rights - violating nature. Human oversight is not a p anacea for the problems that certain AI systems might introduce, and should hence not be used as an excuse for their deployment where there is no basis to do so. The danger of over -reliance on the outputs of an AI system is best evidenced through the Viog n system, used in Spain to predict and prevent the risk of domestic violence against women. Fourteen out of the fifteen women who were killed in a domestic violence incident in 2014 had previously reported their aggressor , yet had been classified by Viog n as being at low or non -specific risk.63 This shows that depending on the context of the system, even decisions resulting in low risk classification can produce significant dangers, where the decision -maker does not have the requisite technical knowledg e to robustly understand how the system works and to consider its limitations. In light of the above, we therefore suggest the Commission to strengthen the protection afforded by the mandatory requirements for high -risk AI systems, and to clarify the open questions they raise. The previous sections considered the ways in which the Proposal could be amended to come closer to attain ing the first element of Legally Trustworthy AI the appropriate allocation of responsibilities for harms caused by AI, particularly regarding fundamental rights. We addressed the Proposal s conception of fundamental rights, its scope, the content of the prohibitions, the regulatory framework around biometrics, and high -risk systems. The following sections address the second pillar of Legally Trustworthy AI: an effective enforcement framework which promotes the rule of law. 2 The Proposal does not ensure an eff ective framework for the enforcement of legal rights and responsibilities (rule of law) As explained in Chapter 3, one of the functions of law is to allocate and distribute responsibility for harms and wrongs in society. The previous sections explained tha t the Proposal does not allocate responsibilities in ways which adequately protect against fundamental rights infringements. Additionally, Chapter 3 argued that the distinctive character of legal (as opposed to ethical ) rules lies in an effective and legitimate framework through which legal rights and duties are enforced. The following sections therefore comment on the enforcement framework proposed in the proposed Regulation. 63 AlgorithmWatch, Automating Society Report 2020, AlgorithmWatch , 2020, 37 We argue that the current oversight, monito ring and enforcement regime falls woefully short of the standard that Legal Trustworthiness requires. As a result, the Proposal is in danger of providing the fa ade of legal protection, while in practice offering little meaningful and effective protection and collap sing into little more than self -regulation. Firstly, the enforcement architecture relies heavily on (self -) conformity assessments. This leaves too much discretion to AI providers in assessing risks to fundamental rights without meaningful ind ependent oversight, leaving many safety -critical and fundamental -rights critical AI applications without any ex ante review or systematic ex post review ( . Secondly, insufficient attention has been paid to the coherence and consistency of the Proposal, both internally and in relation to other legal instruments ( . Thirdly, the Proposal is completely silent on the enforcement of individual rights. No procedural rights are granted to individuals affected by AI systems (who are indeed rarely mentioned) and no complaints mechanism is foreseen ( . Finally, we argue that the enforcement mechanism relies too much on national competencies and is overly complex ( . 1 The Proposal unduly relies on (self -) conformity assessments It is w ell-recognised that the accordance of excessive discretion to governmental officials is a serious threat to the rule of law.64 This also applies when excessive discretion is delegated to those who are legal subjects in combination with insufficient guidance as to how to use such discretion leading to a potential deficit in Legal Trustworthiness. As already argued in mentioned i n section 4(((a), the Proposal nevertheless provides a high level of discretion not only to public officials using high -risk AI systems, but also to private actors as it depends almost exclusively on self -assessments to ensure that fundamental rights are complied with. Below, we raise concerns relating to the enforcement mechanism envisioned in the Proposal with regard to the overly broad discretion which the Proposal grants to AI providers (a), and the nature of the current CE marking regime, which lacks an ex ante control mechanism for fundamental right -sensitive applications (b). a) The Proposal leaves an unduly broad margin of discretion for AI providers and lacks efficient control mechanisms Section 1 argued that the Proposal understands fundame ntal rights protection in an overly techn ocratic manner and grants too much discretion to AI providers regarding the balancing exercise involved in fundamental rights protection. These concerns relate to the way in which the Proposal allocates responsibility, but they also relate to the way in which the Proposal is enforced. As argued section a, the Proposal gives AI providers and users very broad discretion to determine what they consider to be respect for fundamental rights, given that there is no requirement for ( ex ante ) independent verification and certification that the system is in fact fundamental rights compliant. Although independent authorities are empowered to review the training, validation and test data and associated technical documentation (per Article , t hey only have residual power to inspect and test the technical and organisational systems if the documentation is insufficient to ascertain whether a breach of fundamental rights has occurred. It is difficult to understand the basis upon which an authority could establish whether the data offered for inspection and associated documentation is in fact the basis upon which the AI 64 Per A.V. Dicey: No man is punishable or can be lawfully made to suffer in body or goods, except for a distinct breach of law established in the ordinary legal manner before the ordinary courts of the land, in A.V. Dicey, Introduction to the Study of the Law of the Constitution (New York: Liberty, , 38 system is configured and whether, in practice, the system operates in a manner that respects fundamental rights (other than in the case of manifest and egregious violations) in the absence of technical testing. Furthermore, many of the protections offered by the Proposal rely heavily on provider self - assessment. This includes, for instance, the conformity assessment practices of Ar ticles 19 and 43, and the quality management systems of Article As previously explained, the acceptability of adverse implications of AI systems on fundamental rights depends on proportionality and necessity tests through which a limited range of leg itimate justifications can be considered . Yet, it remains unclear who determines what is necessary and proportionate, whether this occurs during the certification and conformity assessment processes, and whether there are means to dispute the proportionali ty assessment of a given system. The Proposal offers little guidance on how proportionality is considered in the development, deployment and use of AI systems, other than the responsibilities of notified bodies in relation to the certification procedure (Article 44(). While much of the enforcement of the Proposal relies on self -assessment by AI providers, little information is given as to their responsibilities regarding fundamental rights proportionality and necessity tests. This lack of guidance on pr oportionality and necessity tests gives rise to significant ambiguity, for example in relation to the definition of AI systems as high -risk, which requires consideration of its intended purposes. The designation of risk for a specific technology involves c onsideration of (a) whether a system may cause harm, and (b) whether this is likely . The intended purpose is defined by the AI provider and reasonably foreseeable misuse refers to the activity of the user. Article 9 provides for a risk management system intended to be a continuous iterative process run throughout the entire lifecycle of a high -risk AI system, to minimise the risk posed to fundamental rights by a given AI system. However, several questions remain open regarding its application in practic e. For example, how does this approach prevent against unintentional misuse? Are there cases in which a user may legitimately misuse a particular AI system for to protect fundamental rights (could the user then change the intended purpose of an AI syste m without incurring the obligations of a provider under Article ? If so, who decides these thresholds? In cases where interferences with fundamental rights are in question, how are normative trade -offs negotiated to prevent any interference with, and co nflicts between, different fundamental rights? Consider, for example, a recidivism risk assessment system created for the purpose of identifying teenagers who require specific and heightened therapeutic intervention to reduce their future risk of re -offending. Yet in practice, the results of this risk assessment significantly impact their access to public assistance benefits, and/or vocational and educational institutions even though they have never been convicted of a crime thus generating a doubl e (or triple) risk with regard to those high -risk uses identified in Annex 33 Would this constitute misuse, if the provider identified the intended purpose as only being for use in recidivism risk assessment? Or would the authorities in this case be proportionately pursuing a legitimate aim? How is this tension and ambiguity resolved, given that the result significantly impacts the rights of the individual involved, including rights of the child, their right to privacy and the presumption of innocence ? Even a single AI system can raise serious questions regarding a wide range of potential scenarios, risks and adverse effects on fundamental rights. It is far from clear to us that the average AI provider has the expertise to consider all the complex ways in which their systems could affect fundamental rights. Considering the questionable ability and legitimacy of AI providers to engage in these complex balancing exercises, the self -assessment process risks amounting to a technical tick box exercise. In other words, it is not a guarantee of a high standard of fundamental rights 39 protection. It is a system through which the relative risks might be considered and registered, which heavily depends on the actions of individual actors, who may be incentivised t o accord heavier weight to economic concerns than to fundamental rights protections. More clarity is required as to how this risk would be averted. Finally, AI providers are responsible for regulation through post -market monitoring . It is unclear how th is would work in a law enforcement or national security context, where there are significant obstacles concerning confidentiality and the handling of complex and sensitive data. There are therefore very serious questions as to whether monitoring would be p ossible, given that providers simply may not be able to obtain access to the necessary information, and therefore, further processes that ensure independent oversight of sensitive AI systems may be necessary. In addition to AI providers, also users of hig h-risk systems are granted unduly wide discretion under the Proposal. Users are responsible for a similar self -assessment of risks under Article 29, which contains very little in the way of obligations beyond following the instructions of use given by th e provider and to maintain automatically generated logs for a period that is appropriate in the light of the intended purpose of the high -risk AI system and applicable legal obligations under Union or national law. 65 This says nothing of the problem of automation bias, or relative liability where something goes wrong. Automation bias is briefly mentioned in Article 14((b) but is not substantively addressed. In sum, the unduly broad discretion accorded by the Propo sal to AI providers and users should be addressed in order to enhance legal certainty and secure more effective protection against the adverse effects raised by the use of AI. b) The conformity assessment regime should be strengthened with more ex ante indep endent control In addition to granting excessive discretion for AI providers, the Proposal s enforcement mechanism puts undue faith in the effectiveness of conformity assessment and CE marking. The Proposal relies heavily on these instruments in relation to high -risk systems to provide the necessary level of assurance to individuals and the public that the system will not violate their fundamental rights or threaten their safety. Yet the CE certification system and conformity procedures involving notified bodies provides weak protection of human health and safety, judging by the experience of the Medical Devices regime which allowed fraud and corruption in the medical devices industry to occur undetected for a significant period of time, including the sale of over 400,000 PIP silicone breast implants worldwide which were manufactured using cheap industrial grade silicon in violation of the CE mark.66 Given this experience of conformity assessment conducted by Notified Bodies, it is seriously questionable wh ether reliance on self -certification provides meaningful legal assurance that the requirements to obtain a CE mark in relation to high -risk AI systems are properly met. At the very least, if the conformity assessment and CE marking is retained, ex ante verification by an independent body should be considered for a broader set of high -risk AI systems, beyond just biometric systems. If these systems are truly considered to pose a high risk to safety and fundamental rights, the lack of ex ante independ ent auditing and evaluation is an enigma. This is even more so given the severity of the impact of some of these systems on fundamental rights, especially when used in the context of vulnerable groups or when based on unscientific 65 European Commission, The Proposal, Article 29(. 66 Sylvia Kierkegaard and Patrick Kierkegaard, Danger to public health: medical devices, toxicity, virus and fraud, Computer Law and Security Review 29, no.1 ( , 13-40 approaches such as in the case of emotion recognition or polygraphs, as commented in section a. Exacerbating this problem is the fact that the documentation and logging requirements for high - risk AI systems risk being a mere ex post verification of a paper trail based on s elf-certification rather than an effective audit. This again underlines the need for a more fine -grained approach to the various risk -levels that AI systems pose, and heightened attention to the use of these systems by governments. The presently still over ly binary categorisation of a system as either high-risk or no-risk and except for biometric identification as either left to a self -assessment or no assessment at all, seems untenable. It should hence be explored whether certain high -risk systems list ed under Annex III would benefit from a conformity assessment carried out by an independent entity prior to their deployment to ensure that their trustworthy status hinges on more than a paper document. 2 There is currently insufficient attention to the coherency and consistency of the scope and content of the rights, duties and obligations that the framework seeks to establish The delegation of significant parts of the enforcement of the proposed Regulation to AI providers and users is not the only concern which affects the rule of law the second pillar of Legal Trustworthiness. The lack of effective and meaningful protection accorded to fundamental rights resulting from the Proposal s current formulation of a risk -based approach also fails to ensure that EU laws are internally consistent and coherent because the level of protection offered falls below the standards set out i n the EU Charter of Rights and Freedoms. Although we warmly welcome the attempts made in the proposed Regulation to ensure that there is clarity about how AI systems which already fall within existing EU laws will be treated, there are several legal instru ments and legal doctrines which are implicated by the development and deployment of AI systems , the relationship of which to the proposed AI Regulation is not always clearly identified. After outlining concerns regarding the Proposal s internal coherency (a), we briefly discuss the Proposal s relationship with the GDPR (b), the Law Enforcement Directive (c) and the MiFID II Regulation (d). a) The consistency of the Proposal with EU (fundamental rights) law should be ensured To ensure that the legal rights an d obligations arising under the proposed Regulation are coherent and consistent with existing laws, the locus of responsibility for the protection of fundamental rights must be clarified. The Proposal states that one of its primary aims is to ensure a high level of protection for fundamental rights across the EU. However, the Charter obligations are primarily binding on Member States and EU institutions while the proposed Regulation imposes various legal requirements on all those deploying or putting AI systems into service. There is a lack of clarity concerning the obligation to ensure that those deploying high -risk systems have quality management systems in place to ensure respect for fundamental rights extends to mechanisms that guard against rig hts interferences arising from the activities and actions other than those of Member States and EU authorities. In this respect, we recommend that the Proposal be amended along the lines of the GDPR, which explicitly confers legal obligations pertaining to the collection and processing of personal data on all data controllers irrespective of whether the data controller is a public or private person (see also section d). Unless the legal obligations to demonstrate respect for fundamental rights me an ensuring that all actors, whether state or non -state, are required to demonstrate respect for fundamental rights by all persons involved in deploying, using and putting on the market AI systems, the draft 41 Regulation is unlikely to provide adequate prote ction of fundamental rights. We therefore recommend that the proposed Regulation be amended to make it explicit that the obligation to respect fundamental rights when AI systems are put into service applies to private sector actors in a direct and unmediat ed way, and not merely as legal obligations against states accorded under conventional international human rights law. The imposition of legal duties on private actors is particularly important given the serious asymmetry of power between those who are directly affected by AI systems, and the organisations with the resources and expertise to deploy them, given the capacity of these systems to operate automatically, at scale and in real time. Additionally, it is unclear whether the proposed Regulation esta blishes a framework of maximum harmonisation binding in its entirety on Member States, or whether it is one of minimum harmonisation67 that allows Member State legislatures to decide whether to set more demanding standards than those stipulated for example, to decide to prohibit biometrics by law enforcement agencies and private use in public spaces given that there is no demonstrable and r obust evidence that the use of these technologies substantially improves public safety and security. b) The Proposal s relationship with the G eneral Data Protection Regulation should be strengthened The Proposal is designed to enable full consistency with existing Union legislation applicable to sectors where high -risk AI systems are already used or likely to be used in the future. 68 In addition to fundamental rights law, t his includes both the General Data Protection Regulation and the Law Enforcement Directive , both of which should complement with a new set of harmonised rules applicable to the design, development and use of certain high -risk AI systems and restrictions on certain uses of remo te biometric identification systems. 69 However, from a fundamental rights perspective, it remains to be seen whether these new rules complement existing standards of protection, or risk undermin ing them. For instance, the GDPR contains stringent protections for special category data for which collection and processing is conditional on significant thresholds of protection, while the Proposal does not offer analogous protection. Under the proposed Regulation, Article 1(b) prohibits the following: the placing on the market, putting into service or use of an AI system that exploits any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability, in order to materially distort the behaviour of a person pertai ning to that group in a manner harm. As already mentioned in section a, there is an open question about why this provision does not include all categories of special category data, recognised in the GDPR as requiring a higher standard of protection. Furthermore, questions can be raised about the way in which the Proposal deals with systems relying on biometric data which are also covered by the GDPR as discussed under section 4 above. At the same time, the European Commission did consider the relevance of the GDPR to this Proposal elsewhere. For instance, Article 10( which deals with data governance requirements 67 This point was also rightfully raised in Michael Veale and Frederik Zuiderveen Borgesius, Demystifying the Draft EU Artificial Intelligence Act, SocArXiv , July 6, 2021, doi:31235/osf.io/38p5f. 68 European Commission, The Proposal, 69 European Commission, The Proposal, 42 for high -risk AI systems does foresee a basis to process special categori es of personal data (referred to in Article 9( of the GDPR), to the extent this is strictly necessary to ensure the monitoring, detection and correction of bias, and subject to appropriate safeguards for the fundamental rights and freedoms of natural per sons. Moreover, the Proposal provides that u sers of high -risk AI systems shall use the information provided to them under Article 13 to comply with their own obligation to carry out a data protection impact assessment under Article 35 of the GDPR .70 Furthe rmore, in the context of the provisions on establishing regulatory sandboxes, the Commission explicitly foresees that the Proposal can provide the legal basis for the use of personal data collected to develop certain AI systems in the public interest with in the AI regulatory sandbox. 71 Given the reliance of many AI systems on personal data, it is recommended to strengthen the ties between the Proposal and the GDPR more consistently, in order to ensure a more coherent and comprehensive data protection framework for AI systems. More generally, consistency between these two regulatory instruments covering strongly related technological practices would contribute to the rule of law, and therefore to Legally Trustworthy AI. c) The Proposal s relatio nship with the Law Enforcement Directive should be clarified In addition to uncertainties stemming from the Proposal s relationship to the GDPR, there are questions regarding its relationship to the Law Enforcement Directive72 (LED), including (a) the duti es and obligations of private organisations which develop and deploy AI systems for law enforcement use; (b) the relationship between data protection impact assessments under the LED, and conformity assessments under the Proposal; and (c) the role of safeg uards provided for by the LED, which have no equivalent in the text of the Proposal. Firstly, the duties of private AI providers in the context of law enforcement must be clarified. Private actors acting on behalf of law enforcement authorities (for the p urposes of prevention, investigation, detection, or prosecution of criminal offences, or the execution of criminal penalties, including the safeguarding against and the prevention of threats to public security) are explicitly included as competent authori ties under Article 3((b) of the LED. This is of crucial significance considering that many law enforcement authorities may procure and use AI systems developed by private organisations . We must ask then, if providers (found in the Proposal) are captur ed by these same requirements, considering that the design of a given AI system significantly influences both (a) the procedure of law enforcement decision -making (e.g., providers of machine -learning recidivism risk assessment make important choices regard ing the relevance of specific data sets, the weighting of specific features, and the form and content of decision outputs provided to law enforcement, in addition to the level of technical transparency about the logic of the system itself) and (b) the kind s of decision that can be made (prospective, future -oriented predictions based on large data sets that would not be possible by human judgement alone). Similarly, considering the obligations of providers to ensure post -market monitoring which again, may significantly affect the form and content of decisions made is there a threshold at which this would be considered as the exercise of 70 European Commission, The Proposal, Article 29(. 71 European Commission, The Proposal, Recital 72 European Parliament and Council, Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA, OJ L 119, 2016, p. 89 131 ( LED ), 2016 . 43 public authority? The answers to these questions are vital for accountability for potential interferences with fundame ntal rights in the law enforcement context. Secondly, the relationship between data protection impact assessment under the LED and impact assessments under the Proposal would need some clarification . Article 27 of the LED states: where data processing involves new technologies, and taking into account the nature, scope, context, and purposes of the processing is likely to result in a high risk to the rights and freedoms of natural persons, data con trollers must produce a public Data Protection Impact Assessment which contains at least (taking into account the rights and legitimate interests of the data subjects and other persons concerned ):73 - a general description of envisaged processing operations; - an assessment of risks to the rights and freedoms of the data subjects; - the measures envisaged to address those risks; - safeguards; - security measures and mechanisms and to demonstrate compliance with this directive. Questions remain as to how the conformity assessments referred to in the Proposal differ from data protection impact assessments, the latter of which include mention of safeguards, and contribute to the ability of subjects to obtain remedies (laid out below), in the sense that data subjects have recourse to seek a judicial remedy where impact assessments (a) have not been completed; (b) are inco mplete; or (c) are complete but demonstrate incompatibility with data protection requirements. This possibility of remedy has not been replicated by the conformity assessment procedure, yet not alternative has been provided. Thirdly, the LED offers other remedies not available in the Proposal, where such assessments do not meet requirements, or obligations are not met in other ways. For example: Article 52 provides for a right for the data subject to lodge a complaint with the domestic supervisory authority where a breach of data protection occurs or is suspected; Article 53 provides for a right to an effective judicial remedy against a supervisory authority for data subjects who have lodged complaints under Article 52 and who disagree w ith the resultant decision or action; Article 54 provides for a right to an effective judicial remedy against a controller or processor for data subjects, where they believe that rights laid down in the LED have been infringed upon in a non -compliant fashi on. This right is provided without prejudice to any alternative administrative or judicial remedy. Nothing of the sort is offered by the Proposal, nor even suggested. It is assumed that such remedies can be accessed by other means, but this should at least be spelled out in the Proposal itself. Individuals whose fundamental rights are affected by AI systems must be provided with a clear path to access justice. As will be stressed further below, w ithout any mention of remedies, it is unclear whether the Proposal can achieve the second pillar of Legally Trustworthy AI . 73 European Parliament and Council, LED , Article 44 d) Concerns around the Proposal s implicit harmonisation with MiFID II should be addressed Finally, there is no direct indication of the Proposal s harmonisation with the Markets in Financial Instruments Regulation (MiFID II).74 This aspect risks represent ing a deficiency in the Proposal, given the growing use of algorithms in the context of multiple financial activities and services (e.g. , algorithmic trading and robo -advisory) and given the fact that practices like algorithmic trading are not listed as a high -risk domain under Annex III. The Proposal merely provides that financial authorities should be designated as compet ent authorities for the purpose of supervising the implementation of this Regulation and that the conformity assessment procedure should be integrated into existing procedures under the Directive on prudential supervision namely, Directive 2013/36 / EU ( with some limited derogations) .75 These indications do not seem sufficient to ensure legal certainty and consistency. Indeed, MiFID II contains several provisions that seem to overlap with the content of this Proposal. Article 17, for example, provides a n otion of algorithmic trading and delegates the preparation of particular risk management practices (not based on conformity assessment and CE marking) to the European Security and Markets Authority (ESMA) to protect public savings and market stability. In requesting that the conformity assessment takes place within the framework of the Directive on prudential supervision (focusing on the role of the European Banking Authority), the Proposal might create a conflict between different risk assessment procedure s and an overlap of functions between different European agencies. It would hence be important for the Commission to take this risk of inconsistency into consideration. 3 The l ack of individual rights of enforcement in the Proposal undermines fundamental rights protection In the sections above, we argued that the enforcement of the Proposal overly relies on conformity (self -) assessments and that its coherence with different parts of EU law must be ensured. In this section, we draw attention to the fact that a significant part of the enforcement mechanism is completely missing from the Proposal, namely individual rights of redress and an accompanying complaints mechanism. One of the P roposal s aim s is protect ing the fundamental rights of indivi duals, yet the se individuals do not feature in the Proposal at all . Its provisions instead focus on the obligations of the AI provider and user, who are often already in an asymmetrical power relation to those individuals whom they subject to their systems. This asymmetrical representation of the different stakeholders in the Proposal creates an enforcement architecture which potentially threatens the rule of law. We therefore argue that the Proposal must guarantee procedural rights to redress for individuals subjected to AI systems (a), a nd a complaint s mechanism dealing with potential violations of the Regulation or infringements of fundamental rights (b). Later, in section 2 , we also argue for more substantive rights for individuals in order to address the blatant absence of ordinary people from the Proposal. 74 European Parliament and Council, Regulation (EU) No 600/2014 of the European Parliament and of the Council of 15 May 2014 on markets in financial instruments and amending Regulation (EU) No 648/2012, OJ L 173, 2014, 2014, 84 75 See European Parliament and Council, Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the acti vity of credit institutions and the prudential supervision of credit institutions and investment firms, amending Directive 2002/87/EC and repealing Directives 2006/48/EC and 2006/49/EC, OJ L 176, 2013, 2013, 338 45 a) The Proposal does not provide any rights of redress for individuals Those whose rights have been interfered with by the operation of AI systems (whether high - risk or otherwise) are not granted legal standing under this Proposal to initiate enforcement action for violation of its provisions, nor any other enforc eable legal rights for seeking mandatory orders to bring violations to an end, or to seek any other form of remedy or redress. The Proposal instead focuses on the imposition of financial penalties and market sanctions where an AI system is non -compliant. F or example, Member States are empowered to implement rules on sanctions, and market surveillance authorities can require that a non - compliant AI system be withdrawn or recalled from the market.76 Yet, to reach the Proposal s goal of robustly protecting fundamental rights, individuals who encounter AI systems in the EU must be able to rely on a clear system of remedies protecting them from harms and fundamental rights interference caused by public o r private adoption of AI systems. As outlined in section 1 , an independent body must then have competence to assess whether any fundamental rights interferences are necessary and proportionate in a democratic society. Individual rights of redress would address the current gap in enforcement which is especially blatant in cases where individuals cannot reasonably opt out of certain outco mes produced by AI systems (as recognised by the Proposal). Recital 38 calls for effective redress in relation to procedural fundamental rights violations (and draw s attention to right to an effective remedy ), but classifying something as high -risk is itself not a remedy. Although the risk classification process for AI systems under Article 7((h)(I) inclu des considering whether processes for re medies are necessary for a particular AI system, there is no guidance on what an effective remedy looks like and no provisions through which individuals can access such a remedy. Additionally, the imposition of penalties for breaches of duties and obligations arising from the Proposal are not individual remedies sufficient for the ongoing robust protection of fundamental rights rather, they offer only a de terrence -based approach for which there is no guarantee of succes s, despite clear risks to the fundamental rights o f individuals. It is our understanding that the European Commission will soon publish a draft liability framework for AI systems which could potentially strengthen the procedural rights of individuals who are adversely impacted by AI systems. It is, however, regrettable that this framework was not published simultaneously with the Proposal, since this renders it difficult to holistically assess the protection offered to individuals. We hope th at the concerns raised above are either reflected in the revised AI Regulation, or in its accompanying liability rules or preferably both. The inclusion of individual rights to redress would contribute substantially to an enforcement framework which empowers all relevant stakeholders, and therefore to Legally Trustworthy AI. b) The Proposal does not provide a complaint s mechanism In addition to a lack of rights which grant individuals standing , the Proposal currently also do es not provide the possibility for individuals to file a complaint with the national competent authority even though the Proposal renders this authority the sole actor who ensure s compliance with the Proposal. This absence of a complaint s mechanism stands in sharp contrast to the mechanism provided under the GDPR, whereby each supervisory authority has the task to handle complaints lodged by a data subject, or by a body, organisation or associatio n in accordance with Article 80, and investigate, to the extent appropriate, the subject matter of the 76 European Commission, The Prop osal, Article 65(. 46 complaint and inform the complainant of the progress and the outcome of the investigation within a reasonable period, in particular if further investigat ion or coordination with another supervisory authority is necessary . 77 Article 77 of the GDPR further provides for a right to lodge a complaint with a supervisory authority for data subjects. While the Proposal does not preclude national competent authorities from establish ing a complaint s mechanism on their own initiative, the absence of harmoni sation of such initiative s renders it likely that individuals in different EU Member States face different levels of protection . The inclusion of a provision which mandates a complaint s mechanism , similar to the GDPR s, would be beneficial to the Proposal s aim of ensuring the protection of fundamental rights. Moreover, it would contribute to the rule of law across the Union. Firstly, it would provide individuals with a clear procedure to follow in case they suspect that an AI system they are subjec ted to does not meet the requirements of Title III or operates in contravention of Title II of the Proposal. Second ly, it w ould help national competent authorities to fulfil their task s, since these complaints can lead to more effective monitoring and eval uation of problematic AI practices . 4 The Proposal s enforcement mechanism is inadequate In addition to the absence in the Proposal of the individual affected by AI systems , the Proposal s enforcement architecture copes with several practical problems. For the proposed Regulation to be Legally Trustworthy, it must conform to rule of law standards, and i ts enforcement must therefore be congruent with the promulgated norms. This may be undermined by the fact that the current enforcemen t structure is relatively complex and heavily relies on the competenc ies of national authorities , which may be uneven and under -resourced (a). Furthermore, the role of notified bodies and the scope of the procedural rights conferred on the Proposal s legal subjects could be further clarified (b) . a) The enforcement structure hinges too much on national competenc ies The enforcement of the Proposal risks being undermined by the practical aspects of Member State competencies. Article 59 of the Proposal enable s Member States to designate or establish national competent authorities for the purpose of ensuring the ap plication and implementation of this regulation. 78 Unless otherwise provided for by the Member State in question, these authorities will be required to act as both notifying authority and market surveillance authority as part of a combined national supervisory aut hority . 79 Notified bodies are responsible for verifying the conformity of high -risk systems, and market surveillance authorities are responsible for the evaluation of high -risk AI systems in respect of its compliance with all the requirements and obligations of t he Proposal.80 If a given system is not compliant, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system's being made available on its national market, to withdraw the product from that mar ket or to recall it. 81 There are three practical concerns about this setup. 77 See European Parliament and Council, Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Dat a Protection Regulation), OJ L 119, 2016 , 2016, 1 88 ( GDPR ), Article 78 European Commission, The Proposal, Article 59(. 79 European Commission, The Proposal, Article 59(. 80 European Commission, The Proposal, Article 65( 81 European Commission, The Proposal, Article 65(. 47 Firstly, t his envisaged enforcement architecture remains a work in progress. As it stands, much work is required to determine the contours of the enforcement practices which these bodies must undertake to ensure full compliance with the Proposal . It also raises questions about how these bodies should cooperate with existing entities, from data protection authorities to (notifying ) bodies active in the do mains and sectors listed in Annex II of the Proposal. Secondly, there is a risk of uneven implementation across Member States. T he Proposal places the burden on national competent authorities to effectively design the proposed regulatory mechanisms at a domestic level . However, the uneven availability of resources in Member States, and different manners in which national authorities could concretise the implementation of the Proposal, risk impeding the development of a common level playing field of fundamental rights protection vis-a-vis AI -generated risks across the EU. This also extends to the development of rules in relation to penalties for non -compliance.82 It is the role of the European Artificial Intelligence Board83 to harmonise different approaches to the Proposal s implementation, yet it remains to be seen how this Board will operate in practice. Finally , the Proposal must acknowledge concerns raised in the context of the GDPR relating to over-reliance for enforcement at the domestic level . Three years after the GDPR s implementation , it has become evident that the protection is significantly weakened due to lack of resourc es across the EU.84 It is important not to replicate this mistake with the Proposal . Any enforcement architecture implemented as a result of the Proposal should be developed with careful attention, ensuring th at it is (a) appropriately resourced, and (b) that these bodies are staffed with people who have the appropriate skills and knowledge with regard to both technical aspects of AI and the social and legal aspects of fundamental rights. Reliance on Member States to complete the work of developing the Proposal s enforcement architecture may be inadequate if not backed up by sufficient financial and human resources, and harmoni sed implementation guidance . b) The enforcement powers conferred upon supervisory authorities should be clarified There are also a number of outstanding issues in relation to specific powers of enforcement conferred upon supervisory authorities. Firstly , the powers of the supervisory authorities provided for by the Proposal focus on ex post controls, yet it remains t o be seen on which basis and at which scale these controls will be carried out. The EU database of stand -alone high -risk AI systems, in which those systems need to be publicly registered, could provide a helpful tool to enable supervisory authorities to prioritise their inevitably limited resources . Nevertheless, a risk of under -enforcement remains , particularly given the lack of a complaint s mechanism that allows individuals to flag potentially problematic AI practices , as mentioned above . Secondly , Member States can deviate from the protection afforded by the Proposal if they find there is ground to do so : for exceptional reasons, including public security, the protection of life and health of persons, environmental protection and the protection of key industrial and infrastructural assets . 85 This exception is broad in scope and should be clarified further to 82 European Commission, The Proposal, Article 71(. 83 This Board, established by Article 56 of the Proposal, shall be chaired by the European Commission and composed of the national supervisory authorities (represented by the head or equivalent high -level official of that authority) and the European Data Protection Supervisor. 84 Estelle Mass , Two years under the EU GDPR: An implementation progress report - state of play, analysis and recommend ations , Access Now , May, 2020, -Years -Under -GDPR.pdf. 85 European Commission, The Proposal, Article 47 . 48 prevent unjustified infringement of fundamental rights by systems which are otherwise considered high -risk. This is particularly important given that several high -risk AI systems are operated by the authorities of the Member State itself , and granting Member States the power to derogate from the protection offered by the Proposal with regard to their own AI systems risks incentivising abuses of power . Finally, due attention should be given to the procedural rights conferred on AI providers . In the course of their activities, supervisory authorities have wide access to data and documentation relating to high -risk AI systems. While this is needed to ensure that the requirements can be checked and enforced, the Proposal s procedural limits to these powers are currently relatively scarce . In providing the right of the notified body to have full access to training, certification and test data and to request access to source codes, the Proposal generates a tension between (a) the need to regulate the activities of organisation s responsible for the development of high -risk systems, and (b) the need to protect the intellectual property of said organisation s, in line with the freedom to conduct business and the right to protection of intellectual property, which are both protected by the EU Charter .86 It should be ensured that the know -how of businesses is adequately protected , with sufficient confidentiality requirements , and any access requests should be targeted and proportional to the specific task at hand. The rule of law, as the second pillar of Legally Trustworthy AI, requires that the legal framework for AI is consistent, coherent, and that it is effectively and legitimately enforced. The previous sections argued that the Proposal falls short in this regard, as it relies too much on the instrument of conformity assessments; does not pay sufficient att ention to coherence with fundamental rights law and other EU legal instruments; does not grant individuals any procedural rights in its enforcement mechanism; and still requires guidance as regards the practicalities of the Proposal s enforcement. The next sections address the extent to which the Proposal s to safeguard the third pillar of Legal Trustworthiness: democracy. 3 The Proposal fails to e nsur e meaningful transparency, accountability and rights of public participation (democracy) As the final pillar of Legal Trustworthiness, democracy requires that members of communities are entitled to participate actively in determining the norms and standards that apply to the life of that community . This pertains particularly to activities whic h have a direct impact on the rights, interests and legitimate expectations of its members, and the distribution of benefits, burdens and opportunities associated with new technological capacities. The inevitability of normative trade -offs in the design an d deployment of many existing and anticipated AI applications makes meaningful participation rights especially important. Put simply: democratic participation in the formation of the legal framework around AI contributes to its trustworthiness. Closely rel ated to this idea of Legal Trustworthiness through democracy is the right for individuals to be informed about decisions that affect them in a substantive manner, and of the way in which the commonly agreed upon standards are accounted for. Indeed, meaning ful transparency and accountability are essential requirements of a well -functioning democratic society. We argue that the current Proposal does not do enough to acknowledge the importance of democracy for Trustworthy AI, nor does it provide the institut ional framework to robustly incorporate the value of democracy in AI governance. Public consultation and participation rights are entirely absent from the Regulation (. Moreover, as highlighted earlier, the Proposal does not provide individuals with any substantive rights, and only formulates 86 See European Union, EU Charter, Articles 16 and 17 . 49 transparency obligations , which currently fail to ensure meaningful accountability (. Indeed, the obligations envisaged in the Proposal are primarily operationalised via bureaucratic management techniques. Finally, the Proposal s reliance on standard -setting mechanisms to operationalise the technical requirements meant to protect fundamental rights risks further increasing this democratic deficit (. 1 The Proposal does not provide consultation and participation rights The opportunity for meaningful public deliberation on the basis of which legislation can be enacted is especially important for politically controversial matters such as the AI prohibitions or a high-risk list . Yet the Proposal s lack of consultation and participation rights seems to fall short of this requirement . First, it is questionable whether the Proposal s content reflects the concerns that citizens have about prohibited and high -risk AI systems (a). Second, the Proposal does not foresee any p articipation and consultation rights for stakeholders in the context of future revisions (b). a) The scope of the public c onsultation prior to the Proposal s drafting would have be nefitted from more targeted questions regarding prohibited and high -risk applications From the outset, the European Commission aimed at ensuring that the voice s of citizens and relevant stakeholders could be heard in order to give shape to the proposed Regulation. Thus, a public consultation was organised on the draft Ethics Guidelines of the HLEG (published in December , which served as an inspiration for t he Commission s further work on AI, as well as on its subsequently refined Assessment List in the form of a broad piloting phase (until December . Furthermore, public consultations were also launched after the Commission s publication of the White Pap er on Artificial Intelligence (in February , and on the Inception Impact Assessment of the proposed AI Regulation (until September . Moreover, we warmly welcome the opportunity provided by the European Commission to provide our feedback on the pu blished Proposal for an AI Regulation in the context of the current public consultation, running until August While these efforts of the Commission should be commended, the scope of the last consultations on the White Paper and Inception Impact Asses sment would have benefitted from more targeted questions regarding the stance on practices that should be prohibited and practices that should be considered as high -risk AI systems. The structure of the current Proposal accords great importance to an AI sy stem s status as either no -risk, prohibited, or high -risk. It is unclear whether the current lists under Title II and Title III duly reflects popular concerns about AI systems particularly in light of the impact they can have on fundamental rights. We doubt whether the lack of emotion recognition systems and biometric categorisation systems under the list of high -risk AI systems (or even prohibited practices) truly echoes the opinion that European citizens have towards such practices. In any case, i t is not clear that democratic deliberation is at the foundation of these distinctions. b) Insufficient opportunities for consultation and participation enshrined in the Proposal itself The risk that popular opinion on the harms associated with AI is not reflected in the content of the Proposal is exacerbated by the fact that the Proposal itself does not provide consultation rights for the future revision of its content despite the E uropean Commission s competence to update the Proposal s various Annexes. The provision of such rights is especially important given that the many current and anticipated AI applications, particularly those which entail the collection and analysis of biometri c data, claim to offer myriad benefits without any robust 50 evidence to justify their highly intrusive interference with the fundamental rights of individuals, in real time and at scale. While provisions are made for the Commission to consult with expert s if it wants to expand the list of high-risk systems, or the requisite technical documentation, this is no substitute for public participation by lay members of the community. The current Proposal fails to recognise that one of the legal system s most im portant roles is to provide an institutional framework through which a community can discuss and determine its own collective values, and how it decides on inescapable conflicts between individual and collective values, and between collective values inter se. Consider in this regard the insistence of the HLEG, in its Policy and Investment Recommendations, that: Questions about the kinds of risks deemed unacceptable must be deliberated and decided upon by the community at large through open, transparent and accountable deliberation, taking into account the EU s legal framework and the obligations under the Charter of Fundamental Rights.51 Accordingly, when the Commission revises the list of high -risk AI systems, it is crucial that participation of the public at large is ensured within this revision exercise. Additionally, the absence of any form of public consultation rights is problematic in relation to the determination of what constitutes an acceptable residual risk in the context of high -risk AI system s, which is treated in the Proposal as a matter which developers and deployers are free to determine themselves. As noted in section a, any residual risk is currently dealt with by requiring that provider communicates these risks to the user, and t hat the user complies with the stated instructions. Given that the user is the organisation deploying the AI system, this gives so little protection to individuals and communities that it cannot be properly described as inviting democratic deliberation o r as respectful of fundamental rights. More generally, as stressed in section a, the protection of individuals and the general public against the threats posed by high -risk AI systems offered in the current Proposal relies heavily on the effectivenes s and legitimacy of the AI provider s compliance with the proposed mandatory requirements, and falls far short of living up to the aspiration of respecting fundamental rights and safety, which requires, at minimum, open public discussion and consultation i n relation to what constitutes an acceptable residual risk. 2 The Proposal l acks meaningful substantive rights for individuals The fact that the Proposal fails to provide robust frameworks for public consultation and participation might be a symptom of the wider problem that the Proposal seems to have forgotten about the persons subjected to AI systems, or simply put, ordinary people. The absence of rights for ordinary people is not only potentially detrimental to the rule of law, as argue d in section 3, it also directly contributes to the Proposal s democratic deficit. The procedural individual right to redress and the individual right to a complaints mechanism argued for in section 3 (in the context of the Proposal s enforcement) would not only strengthen the Proposal s enforcement mechanism, they would also contribute to the Proposal s democratic legitimacy. This is because granting ordinary people a more substant ive role in the enforcement of the Proposal might make it more li kely that the concerns of those people reach the authorities and are dealt with accordingly. Yet, the Proposal would further benefit from adding substantive rights for individuals. These substantive rights would make the ordinary person more of a central figure in the proposed Regulation, and would provide very specific grounds for them to exercise the procedural rights ar gued for in section Two substantive rights which should be granted to individuals are the right not to be subjected to AI system s that disproportionally affect their fundamental rights (in particular, prohibited AI 51 practices and high -risk AI systems that do not conform to the requirements set out in this Proposal) (a), and information rights which allow individuals to effectively exercise their existing rights (b). a) The Proposal does not provide any substantive rights for individuals The lack of substantive individual rights in the Proposal reduces individuals to entirely passive entities, unacknowledged and unaddressed in the regulatory framework. The Proposal s silence on individuals is especially striking considering that one of the primary reasons why AI is being regulated at all is to protect those very individuals from the risks generated by AI systems. The absence of procedural rights of redress and a right to file a complaint with a national supervisory authority was already set out in section In addition, the Proposal does provide individuals with any new substantive rights either. It only consists of obligations imposed on AI providers - and to a lesser extent, on users and other actors. Indeed, as remarked earlier, the Proposal offers no equivalent to the data subject rights which can be found under the GDPR. Consider Chapter III of the GDPR, which offers citizens whose personal data has been processed a set of rights which they can exercise against data controllers. T hese include a right to be informed about personal data collected, a right of access to information regarding why and how their data is being processed, a right to rectification of e.g. , incomplete or incorrect data, a right to be forgotten, and rights relating to automated decision -making, including profiling. In addition, data controllers must facilitate the exercise of these rights, without undue delay, unless they can demonstrate that they cannot identify the data subject in question. No similar ri ghts are offered in the Proposal, wherein responsibility is left mainly to providers and users to monitor the design, implementation and use of AI systems. One can wonder why the Commission did not see the need for any substantive rights in the context of AI, and whether it considers data subject rights as sufficient to tackle relevant dangers posed by AI to fundamental rights more generally. If the Proposal assumes that data protection law provides sufficient individual rights to protect against the risks of AI systems to individuals, it is not clear why, and the equivalence between AI systems and general data processing must be justified. Concretely, at the very least, the Proposal should grant individuals a right not to be subjected to prohibited AI pr actices as listed in Title II, and a right not to be subjected to high -risk AI systems that do not meet the conformity requirements of Title III. This would transform the obligations of AI providers and users from merely obligations towards the regulators of the market to obligations towards specific individuals. Making the individual a central figure in the Regulation would reintroduce the ordinary person both as an explicit beneficiary of the Regulation, and as an empowered legal actor in the regulatory framework around AI. b) The Proposal does not provide meaningful information rights for individuals For a democracy to flourish, the public must be adequately informed to be able to participate in the political life of their community and to plan their own individual lives. In this context, it is essential that individuals are provided with sufficient in formation about technological developments which directly or indirectly affect their health, safety, and fundamental rights. Additionally, democracy requires that the public can challenge those in power to keep a check on their actions, which can also only be done if sufficient information is available to the public. Both these democratic desiderata require a governance framework which affords an 52 appropriate degree of transparency which is itself a prerequisite for meaningful democratic accountability . The Proposal addresses the issue of transparency in three ways, which are yet cumulatively insufficient. Firstly, particular AI systems (like emotion recognition systems) are subject to transparency obligations specified in Title IV. These obligations amount t o informing the individuals subjected to such systems that they are , in fact, being subjected to them. As mentioned in section c, merely telling someone they are being subjected to a system does not amount to protecting that person against the advers e effects of that system and could even cause chilling effects. Secondly, under Article 13 of the Proposal, AI providers have certain information obligations towards AI users as regards the AI system s features. As outlined in section e, these transp arency obligations are hence owed to the user of the AI system, not the individuals exposed to them. Thirdly, under Title VII of the Proposal, a publicly accessible EU database is established for stand -alone high -risk AI systems , yet the information that needs to be provided in that database is fairly limited, and does not provide individuals with sufficient information to potentially question and contest the AI system s impact . In sum, while the Proposal does explicitly address the need for transparency, it does not guarantee that the general public receive sufficient information to understand the risks which they are being subjected to. Moreover, these transparency obligations are not grounded in a framework which gives individu als clear pathways for contesting the existence or operation of certain AI systems and thereby using the obtained information in a way which contributes to fundamental rights protection. Transparency is best used as a method of exposing AI systems to publi c scrutiny and fundamental rights analysis, as a crucial element of a wider system of rights protection, but it does not equate to protection in and of itself. Transparency with a view of protecting both democracy and fundamental rights should ( primari ly focus upon the information needed to expose potential risks to, and violations of, fundamental rights; and ( should thus be tied to clear actionable means by which to remedy such occurrences. This means that transparency rights and obligations need to be tied directly to the needs of affected individuals to understand precisely: (a) How they are affected by the AI system in question , including how the AI system generates and arrives at outputs from a given set of inputs which then directly affect them and in which ways ; (b) How any effects incurred may interact or interfere with their fundamental rights (including the provision of a reasoned explanation for any relevant adverse decisions); (c) How they may take action and obtain remedies where they are concerned that their fundamental rights have been unduly or disproportionately affected or where AI providers and users may have otherwise failed to fulfil legal obligations. The Proposal currently does not deliver these elements, and should therefore b e strengthened in the following manner: First, in line with the recommendation in section 5, the information obligations currently imposed on AI providers in Title III should be extended to individuals subjected to the AI system, rather than merely tar geting its commercial users. This can also be reflected in a substantive right for affected individuals to request such information from the provider or user of the AI system. Second, the transparency obligations of Title IV imposed on certain AI systems in light of the risk of manipulation they pose, should be complemented with a substantive right to be informed about the use of those systems. This should be the case for all three of the AI systems 53 covered by Title IV, and is particularly important for BC S and ERS, considering the intrusive and risky nature of these systems. The fact that these systems are currently only covered by transparency obligations in Title IV is emblematic of the Proposal s poor recourse to transparency obligations to protect fu ndamental rights. Merely mandating AI providers to inform people that they are being subjected to intrusive technologies which are unjustified by scientific evidence, and potentially discriminatory, does not address the chilling effects of these technologi es, but rather enhances them. Therefore, in addition to including a right to be informed about the use of these systems rather than only relying on an information obligation in Title IV, in section 4 we also argued that BCS and ERS should be subject to the requirements for high -risk AI systems of Title III and, in some instances , fall under the prohibited AI practices of Title II. Third, the information that should be included in the EU database of stand -alone high -risk AI systems (as per Annex VIII) should be extended. It offers, for instance, little comfort to an individual who suffered unjust interferences from the use of an AI system to detect and prevent irregular immigration (identified in Annex III as a high -risk system), if they can see that this system has been included on the database without having access to information that helps them to understand and interrogate its legalit y. Such information could include, for instance, the characteristics, capabilities and limitations of performance of the high -risk AI system, including not only its intended purpose, but also its level of accuracy, known or foreseeable circumstances relate d to the use of the high -risk AI system which may lead to risks to the health and safety or fundamental rights, the underlying assumptions on which the system is operating, the system s performance as regards the persons or groups of persons on which the s ystem is intended to be used and, when appropriate, relevant information about the training, validation and testing data sets used, taking into account the intended purpose of the AI system .87 Fourth, to complement the right to a complaints mechanism argue d for in section 3, individuals should be able use the information they receive to challenge and contest the problematic use of AI systems. Decisions resulting from these systems can implicate a range of fundamental rights (in the above example, for in stance, the right to liberty and security, the right to conduct business, non -discrimination - and where transparency is lacking - the right to an effective remedy). In sum, transparency rights and obligations must be embedded in a larger framework of rig hts to achieve what they claim to achieve. Merely informing an individual that a risky AI system has been used offers little protection if that individual does not have rights to essential information about that AI system, and does not have a procedure thr ough which to use that information to contest the system before an independent authority. If the Proposal can be strengthened to ensure that each of the elements set out above are provided for, this would demonstrate a much stronger commitment to empower ing individuals in exercising and safeguarding their fundamental rights, and ensuring accountability for any adverse impacts caused by the use of AI systems. Not only would this afford a more central role for ordinary people in the regulatory framework and therefore contribute to the third pillar of Legal Trustworthiness, it would also strengthen the protection of fundamental rights and the rule of law. 87 Inspiration can inter alia be found when considering the transparency obligations under Article 13 of the Proposal, which are currently solely directed to the commercial user of the AI system. 54 3 The Proposal suffers from a democratic deficit in standard -setting and conformity assessment The final aspect of the proposed Regulation which presents a potential threat to the third pillar of Legal Trustworthiness is the process by which the implementing standards and conformity assessments are shaped. As noted above, t he regulatory f ramework laid down by the Proposal embraces the EU s New Approach for goods. This means that the general requirements for high -risk AI systems are laid down in the Proposal (the so -called essential requirements ), while the detailed technical requiremen ts will be set out primarily in European standards developed through European standardisation. Even though the detailed technical standards have already been ascribed a large role in Title III s Chapter 5, they are largely still lacking today. Their develo pment will be crucial for the effective implementation and enforcement of the proposed Regulation. However, as already noted by Veale and Zuiderveen Borgesius, while standardisation bodies such as CEN/CENELEC can be empowered by the Commission to develop t he standards, this approach is not without controversy .88 Standards creation in principle constitutes an open process, yet consumer representatives and civil society organisations are typically underrepresented therein due to a lack of resources and domain knowledge. Coupled with the more general absence of public participation mechanisms in the Proposal as outlined above, this approach risks leading to a process which provides insufficient democratic input in a process which significantly affects the pract ical application of the proposed Regulation. It will hence be crucial that the Commission take measures to ensure that not only private entities, but also organisations which represent public interests are involved in the creation of standards which might be technical, but value -laden and contested nonetheless. This point can also be made more generally regarding the implementation of the Proposal s conformity assessment mechanism. The conformity assessment of AI systems is performed according to technical rules which are entirely defined by notified bodies i.e. private bodies which in principle receive fees for their activities. It is hence of utmost importance to ensure that to the extent possible national authorities are nevertheless empowered to e xercise democratic control on how these entities perform their activities, and on how the Proposal s standards are implemented concretely. To conclude, the third pillar of Legal Trustwor thiness requires that the regulatory framework around AI provides amp le opportunities for citizens to influence the priorities of the legislation , its regulatory design , and the details regarding its implementation. This requires that the ordinary person is given a central role in the proposed Regulation. Accordingly, t he Proposal could be significantly strengthened if it provided explicit procedures for public participation and consultation ; meaningful substantive rights for individuals ; meaningf ul transparency obligations embedded in a larger framework of fundamental rights protection; and procedures which encourage and enable democratic input for the setting of technical standa rds. OUR KEY RECOMMENDATIONS In this document, we contextualised the proposed AI Regulation against the background of previous efforts by the EU to develop a framework for Trustworthy AI. We identified the attainment of Legally Trustworthy AI as the main goal of the Proposal , in ord er to fill the gap of the High -Level Expert Group s Ethics Guidelines which explicitly only dealt with Ethical AI and socio -technically Robust AI . We argued that the concept of Legal Trustworthiness consists of three pillars fundamental rights, rule of law, and democracy and showed how 88 Veale & Zuiderveen Borgesius , Demystifying the Draft. 55 these can be attained (i.e. , through appropriate allocation of responsibility, adequate enforcement mechanisms and coherence, and processes which encourage public participation). In the sections detailing how the Pr oposal falls short of these three pillar s, we made numerous recommendations for the Commi ssion to consider. For the sake of convenience, our explicit recommendations are listed below, organised by the Title and Article they pertain to. 1 Recommendations for Title I of the Proposal Article 2 and Article 3 (scope and definitions) as discussed in section 2 : Consider broadening the scope of the Proposal to explicitly include all computational systems used in the identified high-risk domains, regardless of whether they are considered to be AI. This might require some revision of the logging requirements to make them more inclusive of systems w hich do not necessarily rely heavily on data. Such broadening would make the application of the Proposal more dependent on the domain in which the technology is used (as specified in Annex III) and the fundamental rights -related risks related thereto , rather than on the specific computational technique used to engender the risks . Clarify the position of university researchers undertaking academic research under the Proposal. Ensure equal levels of fundamental rights protection for research done by university researchers in academic institutions and in corporate R&D departments. Also explicitly include in -the-wild trials or experiments under the definition of putting into service of Article 3(. Remove the condition where that use falls un der the exclusive remit of the Common Foreign and Security Policy regulated under Title V of the Treaty on the European Union (TEU) from recital 12, in order to prevent the establishment of different legal regimes for military AI systems depending on the context in which they were developed OR repeat that condition in article 2(, meaning that all military AI systems developed outside of the CFSP would fall under the scope of the Proposal, and include military AI as a high -risk category. If all military AI is to be excluded from the scope of the Proposal, this leaves a gap in legal protection against the large risks associated with military AI. Ensure that military AI is regulated to the extent that the Union has legal competence (at least the use of mil itary AI in the context of the CFSP), if necessary, under a different legal basis than the current Proposal. Include the use of AI systems for national security and intelligence agencies as a high - risk domain and expand Article 3( to include these agencies in order to prevent a loophole undermining the prohibition on real -time remote biometric identification. 2 Recommendations for Title II of the Proposal Articl e 5 (prohibited practices) as discussed in section 3 Add a procedure which enables the Commission to add prohibited practices to Article 5 of the Proposal after review and consultation . Include a set of criteria which the Commission should use to determine whether a particular AI practice should be prohibited, similar to the list in Article 7( for high -risk systems , to provide legal certainty . Ensure that the list of high -risk systems does not inclu de AI practices which are incompatible with fundamental rights and whose use cannot be reasonably justified, such as technologies which are manifestly discriminatory and/or lacking in any clearly established scientific basis. Consider moving such AI practi ces to the list of prohibited practices. 56 Expand the scope of Article 5((a) and (b) to include harms other than physical and psychological harm. Remove references to physical and psychological harm and replace them with harm and fundamental rights inte rference , and consider expanding the scope to also include harms to groups and to EU values as listed under Article 2 TEU . Expand the scope of Article 5((a) to include manipulative AI practices which do not rely on subliminal cues, but which nevertheless cause harm or interfere with fundamental rights. Expand the references to vulnerable groups in Article 5((a), (b) and (c) to include all protected characteristics listed in Article 21 of the C harter of Fundamental Rights of the European Union . Consider expanding the scope of the prohibition on social scoring in Article 5((c) to private actors which operate in social domains with significant effects on individuals lives. Add a reference in Ar ticle 5((c) to proxies for personal characteristics used for social scoring practices. Expand the prohibition on remote biometric identification systems in public spaces of Article 5((d) to non-law enforcement public actors , and at the very least to a ctors with coercive powers . Consider extending this prohib ition to private organisations, or at least , subject these organisations to obligations that are more robust and significant than the current high -risk requirements. Consider the effects of the exc eptions to the prohibition on law enforcement use of remote biometric identification systems for the construction of technological infrastructures which in principle allow for such practices. These infrastructures could cause chilling effects on fundamenta l rights without even being used, and could be subject to function creep. Prohibit both the use of remote live biometric categorisation systems in public places and the use of emotion recognition systems , by law enforcement and other public actors with coercive power . Add biometric categorisation systems and emotion recognition systems that would not fall under th is suggested prohibition to the list of high -risk systems . Consider subjecting these systems to independent ex ante control , especially when they are used in safety -critical and fundamental rights -critical domains, on vulnerable individuals and groups, and in situations with power asymmetries . 3 Recommendations fo r Title III of the Proposal Article 6 (classification of high -risk systems) as discussed in section 5 Include law enforcement AI systems which do not explicitly use personal data (e.g. crime hotspot analysis based on geospatial data) in the list of h igh-risk systems in Annex III( . Consider extending the list -based approach with an approach based on broader risk criteria , as can also be found under the GDPR. Consider ex ante verification by an independent body for a broader set of high -risk AI systems, beyond just biometric systems especially when used in the context of 57 vulnerable groups or when based on unscientific approaches such as in the case of emotion recognition or polygraphs. Article 7 (amendments to Annex III) as discussed in section 5 Empower the Commission to add new AI points (in addition to points 1 - to the list of high-risk AI systems. Ensure that the adding of high -risk categories is done through robust consultation with stakeholders , including citizens and public interest representatives. Article 10 (data and data governance) as discussed in section e Add to Article 10( a requirement concerning data provenance and data integrity , to enable checks on the legitimacy of the origins of the data. Article 13 (transparency and provision of information) as discussed in section e and section 2 Add to Article 13( the requirement to provide inform ation about the ways in which those subjected to the system may be adversely impacted by the system. Include a statement about the conditions in the field under which the AI system is intended to be used, as well as the parameters of performance testing. Include individuals subjected to AI systems as beneficiaries of the information obligations imposed on AI providers in Title III. Reflect this inclusion in a substantive information right for affected individuals to request the specified information from either the provider or user of the AI system. Article 14 (human oversight) as discussed in section e Add to Article 14( an organisational, non-technical category of human oversight measure which consists of at least: training for decision -makers, logging requirements, and clear processes for ex post review and redress. Ensure that human oversight does not legitimise rights -violating uses of AI. Article 16 (obligations for providers of high -risk systems) and Article 29 (obligations for users of high -risk systems) as discussed in section c Clarify the relationship between data protection impact assessment s under the Law Enforcement Directive and impact and conformity assessments under the Proposal. Additionally, clarify the duties of private AI providers in the context of law enforcement activities in light of Article 3((b) of the LED. Article 43 (conformity assessm ent for high -risk systems ) as discussed in section a Expand the list of high -risk systems which are subject to prior independent conformity assessment control. This should particularly be considered for AI systems that are used in contexts of asymmetry of power (such as, for instance, migration management and law enforcement), systems used for the biometric categorisation of individuals (which are currently not listed in Annex III), and systems relying on unscientific methods (su ch as polygraphs and emotion recognition systems, regardless of their deployment by a private or public actor). Ensure that conformity assessment does not amount to mere tick -box exercises. Incorporate a requirement for the AI provider to engage in a discourse of justification regarding potential fundamental rights interferences, requiring that providers engage 58 robustly with proportionality and necessity assessment, in plain language and available for independent review. Article 47 (derogation from conf ormity assessment procedure) as discussed in section b Clarify the circumstances under which Member States may derogate from the conformity assessment procedure. In doing so, ensure that this provision prevents abuses of power in cases where Member States themselves wish to deploy high -risk systems. 4 Recommendations for Title IV of the Proposal Article 52 (transparency obligations) as discussed in section b Subject AI systems which rely on subliminal cues without causing any harm or fundamental rights interferences to the transparency requirements of Title IV. Add an explicit right for individuals who are subjected to the AI systems falling under transparency obligations of Title IV to be informed about th e use of such systems. 5 Recommendations for Title s VII and VII I of the Proposal Article 60 (EU database for stand -alone high -risk AI systems) as discussed in section b Consider expanding the amount of information required to be provided by the AI provider in the context of the EU database ( as per Annex VIII), to grant individuals access to information which helps them interrogate the legality of the high -risk systems they are subjected to. Such information could i nclude the characteristics, capabilities and limitations of performance of the high -risk AI system, known or foreseeable circumstances related to the use of the high -risk AI system which may lead to risks to the health and safety or fundamental rights, the underlying assumptions based on which the system is operating, the system s performance as regards the persons or groups of persons on which the system is intended to be used and, when appropriate, relevant information about training, validation and testi ng data sets used. Title VIII ( post-market monitoring) as discussed in sections b, a and b Add a provision which mandates a complaints mechanism before the national supervisory authority for individuals who suspect that an AI system they are subjected to does not meet the requirements of Title III or operates in contravention of Title II of the Regulation. Define the procedural limits of the supe rvisory authorities in a manner which upholds intellectual property rights and protects AI providers from undue interference by national authorities. Ensure that Member States authorities have access to sufficient financial and human resources to effectively implement and enforce the proposed Regulation and provide harmonised implementation guidance to Members States, to prevent uneven or unreliable enforcement at the national level. 6 Other Fundamental Rights Recommendations , incl uding redress a nd participation Justificatory discourse for fundamental rights interference as discussed in section a Align the binding content of the Proposal with existing fundamental rights legislation and practice which establishes substantive and procedural requirements for potential 59 interferences with fundamental rights, including the principles of proportionality, necessity, and independent oversight . Explicitly impose an obligation on private sector actors to respect fundamental rights when AI systems are put into service in a direct and unmediated way, and not merely as legal obligations that imposed upon states. Rights for individuals as discussed in section 3 and 1 Clarify the relationship between the rights afforded to indi viduals under the GDPR and the Proposal. If the Proposal assumes that data protection law provides sufficient individual rights to protect against the risks of AI systems to individuals, the equivalence between AI systems and general data processing must be justified. Add an explicit right for individuals not to be subjected to the prohibited AI practices listed in Title II, and the right not to be subjected to high -risk AI systems which do not meet the conformity requirements of Title III. Add an explici t right of redress for individuals who are subjected to non -compliant AI systems, similar to the rights of data subjects under data protection law. Ensure that the Regulation, combined with the accompanying liability rules, provides adequate remedies for i ndividuals where the exercise of fundamental rights is implicated by the development or use of AI systems. Add an explicit information right for individuals who are subjected to high -risk AI systems to be granted the information provided to the users of the AI systems under Title III. Add public participation rights for EU citizens regarding the decision to amend the list of high -risk systems in Annex III. Add public participation rights for those subjected to high -risk AI systems to be involved in the determination of the acceptability of the residual risk . Ensure that not only corporate and expert groups are involved in the setting of technical standa rds for conformity assessments of high -risk AI systems, by actively involving organisations which represent public interest s.",en,"This approach is in line with the HLEG s Policy Recommendations which supported the introduct ion of precautionary measures when scientific evidence about an environmental, human health hazard or other serious societal threat (such as threats to the democratic process), and the stakes are high. Secondly , Member States can deviate from the protection afforded by the Proposal if they find there is ground to do so : for exceptional reasons, including public security, the protection of life and health of persons, environmental protection and the protection of key industrial and infrastructural assets .",risk
E.ON SE (Germany),F2665468,05 August 2021,Company/business,Large (250 or more),Germany,"E.ON consultation response : Artificial Intelligence E.ON welcome s the opportunity to actively participate in the consultation process on the Artificial Intelligence (AI) Act. E.ON generally supports the plans of the European Commission to create a uniform framework for AI with the primary goal to strike a risk-based balance between the fundamental right s of European citizens and enabling competitive AI systems to develop . In order to ensure effective and fair regulation in this field , E.ON propose s inter alia the following: Definition , Art . 3 ( + Annex I (c) The proposed definition of AI is too broad. In particular, the techniques listed in Annex I (c) to the AI Act include conventional software coding techniques . The definition should therefore include the ability of software to self -learn. As far as the output aspect is concerned, at least content seems too extensive and will lead to, combined with the techn iques set out in Annex I (c), broad ranges of software being considered as AI that are currently considered software only . The definition should not go beyond the internationally accepted definition of the Organization f or Economic Co -operation and Development (O ECD ) which, too, itself is very broad . The aspect of self - learning ability should be added. Risk -based approach / High -Risk , Art. 6 ( + Annex III In principle, we welcome the risk -based approach with regulation focusing on high -risk technologies giving more freedom to make use of harmless technologies. However, it is important to ensure that there is no over -regulation if industries or technologies are identified as high -risk. The individual AI technologies must be understood and not assessed acro ss the board : Regulate the application, not the technology . Regarding the existence of an abstract high risk of AI, it is decisive in particular whether a decision affecting the environment it is interacting with is autonomously left to the AI itself or not. The latter is not the case if technical safety devices monitoring the output of AI, such as circuit breaker s or the like , or human control , stand as a corrective between the AI and a decision /an action and exercise control over the AI , in particular in case of deviation from regular course of operations . E.ON therefore proposes that AI should not be considered to entail high risk if (i) due to human oversight a natural person remains in charge for decision -making or (ii) safety devices exercise control over an AI system . With these additional measures and controls in place the situation becomes comparable to other technologies, including physical components currently used in the operation of critical infrastructures such as a valve in a gas grid or a switch in an electrical grid . For both cases there are safety precautions in place as standard operating procedures. Development of AI The mere development of an AI system should not fall under the AI Act and not be subject to any of its restriction s or requirement s. Only once an AI system is placed on the market or put into service, its inherent risks , if any, may materiali se. This is not the case where , following the completion of the development of an AI system , such system subsequently is never put in use . In the definition of provider set out in Art . 3 (, with a view should be replaced with and places it on the market or puts it into service ( ) . Compliance cost & Measures In particular with regard to high -risk AI , the statutory requirements regarding risk management , other organi sational measures , associated implementation and ongoing compliance costs impose too high a hurdle on the introduction of such AI systems. There should be synergies with other measures already established in other areas such as data protection management systems or cyber security standards established for critical infrastructure. At least, there should be concrete harmonised technical standard s recogn ised officially by the EU prior to the AI Act entering into force . Their fulfilment should lead to compliance with the requirements of the AI Act . Otherwise, not only will there be a risk of legal uncertainty but also of escalating costs for businesses contrary to the goals of Europe s digital decade. Technical Documentation / Art. 11 It must be ensure d that the trade secrets , including AI algorithms, of any company remain confidential and secure from third party access at all times. Against th is background, the scope of information to be provided in accordance with Art . 11 is too broad ; at least, any in formation provided must be subject to appropriate technical and organi sational security measures in line with state -of-the-art technology to be established by the recipient . Delegated Acts The draft AI Act contains numerous authorisations to adopt Delegated and Implementing Acts. Both the number and the regulatory content to be addressed by these legal acts are not appropriate from our perspective and not in line with (TFEU scope whereby delegation should be used only to supplement or amend non -essential parts of legislation, for example to define detailed measure . This does not appl y in particular to the definition of AI which we believe should be the subject of a full debate among co -legislators . No retroactive effect According to Art. 83 ( of the AI Act , its provisions sh all also apply to high -risk AI systems that were placed on the market or put into operation before application of the AI Act if their ""design or purpose has been significantly changed thereafter"". E.ON rejects a subsequent inclusion of already existing AI systems especially using such subjective criteria (i.e. significantly changed ). T his would entail disproportionately high adaptation efforts for the providers of corresponding AI systems. What if the potential of AI is not fully used? E.ON considers that the draft AI Act takes a one -sided approach, i.e. to mitigate potential risks stemming from the use of AI. However, we bel ieve the counterfactuals should be equally considered. Can Europe s targets be achieved if AI is not deployed in due course? The Act should focus as well on leveraging the use of AI to mitigate the risks of climate change. The German Federal Constitutiona l Court has just established in its recent verdict on German climate law that failure to achieve the Intergovernmental Panel on Climate Change s climate targets will inevitably lead to restrictions on people's fundamental rights of freedom. There is genera l agreement that the energy transition requires the use of AI as well as other innovative technologies. Technology therefore not only has the effect of putting freedom at risk, which seems to be the prevailing view in the debate about AI, but can also have the effect of preserving freedom. E.ON calls for a priority focus on any AI use case with a benefit for climate and the environment.",en,The Act should focus as well on leveraging the use of AI to mitigate the risks of climate change. The German Federal Constitutiona l Court has just established in its recent verdict on German climate law that failure to achieve the Intergovernmental Panel on Climate Change s climate targets will inevitably lead to restrictions on people's fundamental rights of freedom. There is genera l agreement that the energy transition requires the use of AI as well as other innovative technologies. E.ON calls for a priority focus on any AI use case with a benefit for climate and the environment.,risk
"Arthur's Legal, Strategies & Systems (Netherlands)",F2665467,05 August 2021,Company/business,Small (10 to 49 employees),Netherlands,"v20210805 / Arthur Strategies & Systems is part of Arthur s Legal B.V. The content of this document is provided as -is and for general information purposes only. Page 1 of 10 OBSERVATIONS & RECOMMENDATIONS v20210805 regarding the Proposal (dated 21 April for a Regulation of the European Parliament and of the Council laying down Harmonised Rules on Artificial Intelligence and Amending Certain Union Legislative Acts I. INTRODUCTION About Arthur Strategies & Systems Arthur Strategies & Systems is part of Arthur s Legal . It has h andpicked experienced interdisciplinary experts to focus on a bility to navigate, enable, facilitate as well as execute and systemize. Our c ore team consists of a ttorneys at law, senior legal counsels, governmental advisors, strategists, innovation, policy & standardisation experts, community & competence builders, all well- connected in the world of human values , societal values, ecological values and economical values, as well as related technology, strategy, ethics, policy , legal matters & global business. Our daily domains are, among others, help ing to create, design, architect, build, deploy , succeed and sustain human -centric 21st century cyber -physical and other digital e cosystems, societal challenges, data strategies, trusted data sharing, safety, c ybers ecurity, privacy, sustainability, transparency, t rust, IoT, robotics, autonomous systems, AI, RPA, cognitive systems, attributed - and other evidence- based trust and trustworthiness , human -centric digital transformation, combinatoric applied innovation, competence and capability building, impact -based deployments, resilience, human values, accountability and dynamic assurance. We are already active in most these domains for over 20 years. We are member of the AI Alliance , consortium partner to projects such as STAR -AI.eu , AI4PublicPolicy.eu , ASCAPE -project.eu and CONCORDIA -h2020.eu, and advisory board member to projects such as h2020 -AVENUE .eu and many others. As always, we provide our inputs fully independently, where we are aiming for building human -centric and future -proof ecosystems that are addressing societal challenges while being transparent , trustworthy , transformative yet inclusive, and where all stakeholders are accountable and co-accountable, for people, planet, prosperity, peace and partnership. We Support & Endorse We welcome the opportunity to provide our observations and recommendations . We strongly support and endorse the initiative by the European Commission for the current proposed Artificial Intelligence Act1. We find it a very impressive (draft) act already. This initiative will help ensure that AI is safe, lawful and in line with EU fundamental rights , and related acc ountability (and liability) attribution . 1 EU Commission (. Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts. COM( 206 fin al, 21 April 2 -regulation/have- your-say/initiatives/12527 -Artificial -intellig ence-ethical - andlegalrequirements_en v20210805 / Arthur Strategies & Systems is part of Arthur s Legal B.V. The content of this document is provided as -is and for general information purposes only. Page 2 of 10 The overall goal is to stimulate the uptake of trustworthy and accountable AI in EU society and econom y. We underline the importance of establishing such framework to foster safe, trustworthy and accountable AI -supported innovation while address ing Societal Challenges . We also strongly support the proportionate, risk -based approach. Adding AI to a process, technology or (eco)system could strengthen the capacity to do good , in an accountable way, and address societal challenges of which also the EU and its member states, communities and citizens have plenty , both short term, mid term, long term and extreme long term , However, adding AI to the equation also can increase risk , and augment or trigger potential material detrimental impact. It is impo rtant to ensure that these risks are mitigated , and organisations (whether providers, users or otherwise) are held ethically , socially and legally responsible, accountable and liable. We believe that the current proposed AI Act is generally well -developed; however, some parts are not yet sufficiently developed or otherwise deserve further considerations, conditions, q ualifications, detailing and other improvements. II. OBSERVATIONS & RECOMMENDATIONS Multiplicity; human (value) centric Digital technology, climate, pandemics, and other dynamic changes the world at a fast pace. Yet, humans are underrated. Build, enhance and retain trust with the combination of human brain power, purpose & passion, machines, algorithms, data and accountabil ity. We always aim for the human -centric, trusted and trustworthy Multiplicity Approach: a certain, dynamic symbiotic combination of diverse groups of people with diverse gender and diverse backgrounds that work together with diverse groups of human -centric machines, algorithms and capabilities to identify, address and solve problems, make and where appropriate execute decisions, and double- looping capabilities to never stop learning. Technology has outstripped the (universal) human values, societal, economical and legal frameworks; how to catch up, and keep up? Most of the particular domains, risk areas and other dimensions that the proposed Artificial Intelligence Act focuses on , can also happen, be met or achieved (or impacted, either positively or negatively) without the use of AI as defined in the proposed AI Act in general and its Annexes in particular. It is very good to use a technology domain such as AI as a use case and scenario plotting and mapping catalyst, in order to identify, classify and address the various additional risks (either new, combined, au gmented or otherwise) . However, this will not address, mitigate or solve the challenges that existing data analytics, algorithms, software , computing , platforms and other technology can already provide, closely mimic k AI capabilities (regarding functionality and outcomes) . v20210805 / Arthur Strategies & Systems is part of Arthur s Legal B.V. The content of this document is provided as -is and for general information purposes only. Page 3 of 10 The focus on AI therefor e is good for purposes of this proposed AI Act, but let s not forget the many other technical layers, systems, products, services, convergence thereof and related opportunities and challenges that AI aimes to address, and risk s that are either already happening, accruing , evolving , emerging (and creating problems including concentration risks) every single day. Our conventional ways to organise and govern those are too slow to catch up and keep up. The instruments we deploy (paper versus digital) also are not sufficient enough in this 21st Century noting that we are already for 21% in th at century . Digital policy instruments to help keep AI and other technology and digital ecosystem transparent, trustworthy and under meaningful control with clear (timely) accountability should be part of the equation. Diverting, mandating or otherwise referring enforcement and the like to member states and public sector organisations in the traditional way means: (public sector on behalf of the) people v ersus digital . That has already for at least 15 years been proven an unequal match where people (and the citizens , societies , economies and our planet that depend on swift, proper and clear enforcement, redress and remedies) lose. This needs to be taken into account when trying to rule, govern, control, monitor and otherwise organise AI and AI systems, whether high risk or not. For once, a principle -based, technology -neutral, dynamic , hybrid and lean approach is one of the main success factors. Holistic Involvement & Approach As mentioned in the proposed AI Act , the definition of AI system aims to be as technology neutral and future proof as possible, taking into account the fast technological and market developments related to AI. We appreciate that, as mentioned above, although the AI Act is one of the least technology neutral EU regulations of the past 7 years . Artificial intelligence (AI) supports, as a mere tool and, when designed, deployed and maintained well a great tool, next to the many others. AI is not a separate, stand- alone technology that makes everything happen. Focusing too much on anything means staring at one thing , and also in this Digital Age this means one misses everything else (and the point ). Furthermore , artificial intelligence systems , or better: artificial intelligence- supported systems ( AI systems ) are way more than part of the software layer. It can be part of a services; it can be part of ecosystems of systems. These (eco)systems are converging, meaning that verticals merge, submerge and otherwise integrate into other sectors, markets and domains whether new or not AI can also be part of a backend system; so not marketed in that sense, but still highly impactful to people, health and fundamental rights. As mentioned above already as we ll, a very small piece of AI capabilities in large (eco)systems, placed either upstream, midstream or downstream, can have potential high value, but also high risks. It is quite contextual. In any case, it is essential to have a horizontal, cross -cutting and cross-sector approach. AI will almost never be a stand -alone technology. It will be part of one or multiple technical stack s, in one or multiple sectors, markets and domains, and these are not isolated or otherwise silo -ed. v20210805 / Arthur Strategies & Systems is part of Arthur s Legal B.V. The content of this document is provided as -is and for general information purposes only. Page 4 of 10 As per the ongoing convergence these are becoming more and more connected, interconnected and hyperconnected, bringing greater opportunities and cross -fertilization, but on the other end also can infect and affect each other easily, and augment known and unknown threats, vulnerabilities and other risks. Furthermore, various life -cycle factors need to be considered right from the design phase all the way th rough the end -of-life phase to ensure transparency, trustworthiness, safety, resilience, accountability , remedies and redress . We believe that it should be set very clear in the AI Act that this all is part of the term AI systems, and in scope of the AI Act . Intended, Expected & Actual Purpose , and (Actual) Use Where the current proposed AI Act only focuses on intended purpose (which will be defined by the provider, accordance to the current definition in Article 3(, and the reasonable forseeable misuse (as currently defined in Article 3(), also as per the fact that AI systems can be quite extensive and AI functionality may have been defined by the provider as stand- alone where it already know s or can expect in advance that it will be actually deployed or otherwise used in an AI system (that, in this example, is not stand - alone), it is not clear enough in the current proposed AI Act whether this is in scope of either intended purpose or reasonable forseeable misuse . One could argue, it falls under the latter (having a formal view that anything that is outside the intended purpose is misuse ), but it leads to providers defining the intended purpose extremely narrow, where the provider knows the expected use as well as the a ctual use is (far) outside such narrow definition. This for instance could lead to defining the intended purpose to a non- high-risk purpose where it is well -known it will be used in a high risk environment. Defined intended use versus the real, complex and already and ever increasing connected, interconnected and hyperconnected worlds and ecosystems balances out to a win for the definition (and another loss for people, society and others), if this is kept too rule -based and not principle -based. This, is a lawyer s paradise, and although Arthur s Legal is a law firm as well, we are very keen and determined to avoid this, and further optimize policy instruments such as the proposed AI Act where and when we can. A classification of various abstract purpose s and use can be: a. intended purpose b. intended purpose is also the expected purpose/use c. intended purpose is also the expected purpose but not the actual purpose/use d. intended purpose, but not the expected purpose/use e. intended purpose, but not expected and actual purpose/use f. unintended use g. unintended but expected purpose/use h. unintended but expected purpose and actual purpose/use, or i. unintended and unexpected purpose j. unintended, unexpected yet actual purpose/use. v20210805 / Arthur Strategies & Systems is part of Arthur s Legal B.V. The content of this document is provided as -is and for general information purposes only. Page 5 of 10 Insights in risks, whether high or otherwise, do not merely result from the intended purpose, as these only reveal hypothetical risks (including levels of probability and levels of impact and consequences). Even more so, risks happen (or can happen or othe rwise reveal themselves) from the expected purpose, actual purpose and most importantly, to the actual use; in the real world. The main point we are trying to make regarding to focus on intended purpose , is that is to be avoided only to look at well -drafted definitions of what the intended purpose would be. It would be otherwise too easy to avoid being accountable and co -accountable, and push away responsibilities (and liabilities). The inclusion of the more subjective and less -manipulable term of reasonably foreseeable misuse already helps a lot but does not cover the relevant purposes set forth above. Risk (Segmentation) in AI and AI -supported Systems It is important not to percieve risk as always something necessarily negative. It is an integral part of the equation and with that an enabler and facilitator of anything that works in a trusted, trustworthy and accountable way. It gives essential and valuable insights to what may happen or may go wrong, what people or society like or fear, et cetera. For instance, being an entrepreneur or director in the private sector is all about risk -analysis and well - informed decision making. For sure, in the AI or AI -supported domain risk an essential success factor as well . The magnitude of risks, determined by the probability as well as the impact thereof, is very much context and application dependent. To prepare for and mitigate the potential harm, to embed preparedness for foreseen and unforeseen situations, and to make it resilient and future -proof, it is necessary that AI systems are des igned and deployed guided by trust principles. These non -functionals are principles that consistently preserve trust, trustworthiness and engagement of all relevant stakeholders , and the currently proposed AI Act already has embedded the most relevant ones , such as security, safety, privacy, accountability and robustness. There are several hundred of these trust principles ; to date, our firm has identified almost 500 unique trust principles . These can be found in best practices, guidelines, white papers, st andards, regulations but also in common practice and nature. Two major challenges in the A I design and deployment are ( to map the relevant risks accurately and comprehensively throughout the system s entire lifecycle, and ( to incorporate non- functio nals by design. It is at least useful to segment the various AI- related dimensions of this Digital Age in order to get some relevant oversight and insight. For purposes of this (relatively brief) contribution , the initial segmentation is however done in four ( segments as set forth below: a. Non-connected , which is a stand -alone device, tool, machine, appliance or application that does not have connectors or connectivity that can connect to the internet or other external network or resourc es. v20210805 / Arthur Strategies & Systems is part of Arthur s Legal B.V. The content of this document is provided as -is and for general information purposes only. Page 6 of 10 b. Connected , where a device, tool, machine, appliance, application or system may be connected to, via the internet, a centralised databases, cloud infrastructure and other centralised systems; c. Inter -connected , where several edge devices, tools, machine s, appliances, applications or systems are connected with each other, either via orchestrated, federated systems, and; d. Hyper -connected , where numerous far edge and other IoT devices, tools, machines, appliances, applications or systems are directly connected with each other via distributed (computing and related) ecosystems of ecosystems. For each of these segments, various value cases, business models, feasibility models and therefor e use cases can be identified and created in the AI & AI- supported systems. Each segment has its own values, benefits, efficiencies, inefficiencies, et cetera. The segmentation set above obviously is not the only one possible. Various other segmentations are relevant to consider as well, such as for instance real -time, n ear-real-time or not. This segmentation may be relevant when near -real-time autonomous 3D printing is considered, or real -time prognostic health monitoring or related integrated logistics support is relevant. Other segmentations that can be considered are single- vendor, multi -vendor, OEM, public, private, public -private, et cetera. Risk Classification Spectra: A Multi -Layered Approach When going back to the above- mentioned segment, Hyper -Connected devices, and taking a risk-perspective to those, a methodology to do high -level quality risk classification is to have a multi- layered approach and do such risk classification per spectrum, starting with the risk classification of the connectors and connectivity of, for instance an IoT device itself. Even though AI capabilities may not yet be in the equation, it is essential to understand the various risks that are embedded in or could arise from such AI -supported IoT device. Subsequently, other risk spectra should be considered and risk classified, as visu alised below. v20210805 / Arthur Strategies & Systems is part of Arthur s Legal B.V. The content of this document is provided as -is and for general information purposes only. Page 7 of 10 Especially more downstream there may be risk spectra that may not be relevant; however, if such spectrum may become relevant later in the life cycle of the IoT device it is recommendable to keep it in and already do the spectrum risk classification. In general, three categories of main risk levels are used: low, medium and high. Based on the outcome of (i) a risk classification for each spectrum, and (ii) the interim outcome of the various risk classifications up to Spectrum 13 (A I Capabilities), the baseline risk classification can be established. Based on that baseline, the AI Capabilities risk classification can be done, and the subsequent risk spectra; the holistic perspective constitutes the Combined Risk Classification, on which one can consider and organise technical & organisational security, safety, privacy and related technical and organisational measures. Any technical and organisational measures taken or to be taken can include, cause or otherwise trigger risk by itself or as a trigger consequence. It is therefore recommended to double -loop the particular set of measures, for once to initially assess if and to what extent these may have a detrimental impac t. As per the dynamics of IoT and even more so AI- supported IoT and IoT ecosystems any of the risk classification spectra can be expected to trigger, change or otherwise show relevant dynamics, such as (A) technical or other threats and vulnerabilities, (B) actors and other stakeholders anomalies, updates or upgrades in code, datasets or attributes, or (C) changes in regulatory standards, policies or other relevant best practices, it is recommended to double -loop as well, including those spectra that are or may be related or otherwise are (inter)depended on the particular spectra. Therefore, it is recommended to continuously monitor the risks, and where necessary or otherwise double- loop thereafter to keep the security measures up to date and resilient. In any case, the segments, whether non -connected, connected, inter -connected or hyper - connected, that have AI capabilities of any kind, are for sure game changing, where non - functional and functional requirements have to be addressed together. Biometrics in the (semi) Public Domain On another topic that is addressed in the currently proposed AI Act and we would like to make an observation and recommendation about , it is not clear why real -time remote biometric identification systems used by private sector organisations in publicly or se mi- publicly accessible spaces have not been prohibited (with clear yet strict exceptions) . This, as the consequences of misidentification in the case of individual persons can be far-reaching and highly -impactful . Reference is made to the joint EDPB and EDPS Opinion on the AI Act of June 2021 2 which nicely clarif ies that the use of these technologies may easily result in the end of anonymity in these spaces . 2 EDPB -EDPS Joint Opinion 5/2021 on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) of 18 June -work -tools/our - documents/edpbedps- joint-opinion/edpb -edps-joint-opinion -52021- proposal_en v20210805 / Arthur Strategies & Systems is part of Arthur s Legal B.V. The content of this document is provided as -is and for general information purposes only. Page 8 of 10 Law Enforcement While we have a high level of confidence in the legal systems in each of the member states, including law enforcement, it needs to be very clear that law enforcement agencies and related organisations can only use AI or AI -supported remote real time biometric identification systems or similar, in case of a serious crime . Most exceptions in the proposed AI Act seem reasonable, considering their narrowly defined scope, except for the one set forth in Article art. 1(d) under iii , as it allows such technologies to be used in the context of an arrest - and surrender order regarding offences to which maximum punishments of 3 years or more can be imposed. This is not a proper threshold for serious crime , giving too much room for misinterpretation and potential misuse, and therewith to little factual clarity and legal surety to people, organisations and society . In this context, i t is also unclear how non -suspects will be protected against the (mis-)use of biometric systems targeting suspects of serious crimes . Furthermore, it should be questioned how citizens will be protected by use of biometric infrastructure by authorities, that does not align with the fundamental rights under EU law. Currently Underdeveloped: Market Authority Discretion Similar to the last paragraph, we are concerned about the current structure, qualifications and conditions set forth in Article 67 about compliant AI systems which present a risk. Assuming that one has a highly ethical, safe and trustworthy AI system that is fully compliant to the requirements set forth in the current proposed AI Act. A market authority in a member state can under A rticle 67 still argue and as per the current wording quite subjectively and randomly if it may think such compliant AI syst em should be withdrawn, recalled or corrected . Next to the question about legal uncertainty and potential random and bias acts and behaviour that can occur (for political , local or national competitive, economical or any other reason), it would lead to the great minds, researches, entrepreneurs and (municipality, regional) leaders and other lead- users in any sector not to invest time, money and other resources in the development , marketing, deployment, use and feed -looping of highly ethical and comp liant AI systems. This, as the risk that its expensively design, build and maintained AI system may be banned without the ability of making prior informed decisions. Therefore, even though most of the proposed AI Act in our view is well -developed, this topic and Article needs to be further developed, and nuanced, also to protect investors, entrepreneurs, providers , users and society. Facilitating (and Automating) Conformity Assessments for SMEs As one does generally not outsource its own thinking, and not outsource making its own decisions, it can be expected that AI and AI system will be developed and used, by the users themselves. v20210805 / Arthur Strategies & Systems is part of Arthur s Legal B.V. The content of this document is provided as -is and for general information purposes only. Page 9 of 10 A director or Chief Executive Officer of a SME, Midcap compan y or large corporate also does not make the decisions solely itself but jointly with the various board members, staff and experts. Where large corporates may have the capability to do conformity assessments of AI and AI systems itself, for the, also in the EU essential SMEs and Midcap companies, it will probably be too expensive and otherwise burdensome. Therefor e, we would recommend that such assessment of AI created, provided or used by SME s are facilitated (for instance through automated assessments or dynamic assurance) or otherwise supported3. The same goes for municipalities and other organisations who do generally not have the resources for these efforts. Private Law Remedies & Redres s While the currently proposed Artificial Intelligence Act lists numerous stakeholders and human -centric, the citizens and related organisations are not stakeholder in the current proposed AI Act . There are no direct remedies, redress or similar legal instruments available for the people of the EU themselves. This was also the case in the 1995 Privacy Directive and has been corrected in the current GDPR. It is recommended to provide for legal instruments in the AI Act that EU citizens (either indiv idually or collectively, via an EU legal entity such as a foundation or association or not) can enforce themselves. This, next to the administrative instruments that the current proposed AI Act caters for, to the member states. III. IN CONCLUSION Especially the past five ( years, t he Commission respectively European Parliament ha ve been consistently taking initiatives in the form of strategies, action plans, legislations and other instruments to both grasp the opportunities of data, technology, and human- centric digital capabilities as well as identify and address the risk and related responsibilities, accountability a nd liability. This is highly appreciated, also regarding AI and AI( -supported) systems . We are for sure moving in the right direction . Also this initiative c ould mean leadership of these opportunities and challenges on a to quite some extent global sca le. The current proposed AI Act is a very impressive regulation. As any other draft/proposal, it does however need some further considerations, improvements and other optimisation , including the ones related to the observations and recommendations set for th in this document. While doing that, it is important to have a holistic approach, both from perspectives of ecosystems of systems, society, communities and economies it can influence, impact, manipulate, lead, steer and otherwise affect (both positivel y, negatively and from a net - benefit point of view), as well as life -cycles. This, to ensure that one does not view AI and AI systems in a linear and stand -alone manner but instead in a holistic and future -proof manner. 3 Currently proposed Artificial Intelligence Act, A rticles 19, 26 and 43 v20210805 / Arthur Strategies & Systems is part of Arthur s Legal B.V. The content of this document is provided as -is and for general information purposes only. Page 10 of 10 We believe th at our limited and not exhaustive observations and recommendations mentioned above support the Commission in its mission to successfully develop and later on passes a high -quality and highly- impressive and positively impactful AI Act. There is no denying that the EU has been a front -runner in ensuring that the different facets pertaining to the human -centric digital domain s are regulated with introduction of the General Data Protection Regulation, ePrivacy Directive, Regulation on the F ree Flow of D ata, NIS Directive, Cybersecurity Act, and upcoming regulations and directives in this Digital Age. We continue to be dedicated to this and are as always ready to further help and engage and keen to further elaborate on the above at the Commission s request. Amsterdam, 5 August 2021 / Arthur s Legal, Strategies & Systems",en,"Our c ore team consists of a ttorneys at law, senior legal counsels, governmental advisors, strategists, innovation, policy & standardisation experts, community & competence builders, all well- connected in the world of human values , societal values, ecological values and economical values, as well as related technology, strategy, ethics, policy , legal matters & global business. Our daily domains are, among others, help ing to create, design, architect, build, deploy , succeed and sustain human -centric 21st century cyber -physical and other digital e cosystems, societal challenges, data strategies, trusted data sharing, safety, c ybers ecurity, privacy, sustainability, transparency, t rust, IoT, robotics, autonomous systems, AI, RPA, cognitive systems, attributed - and other evidence- based trust and trustworthiness , human -centric digital transformation, combinatoric applied innovation, competence and capability building, impact -based deployments, resilience, human values, accountability and dynamic assurance. OBSERVATIONS & RECOMMENDATIONS Multiplicity; human (value) centric Digital technology, climate, pandemics, and other dynamic changes the world at a fast pace.",risk
Novartis International AG (Switzerland),F2665464,05 August 2021,Company/business,Large (250 or more),Switzerland,"Novartis feedback on Artificial Intelligence Act - 5 August 2021 Novartis welcomes the European Commission ambi tious proposal for a comprehensive legal framework regarding the use of Artificial intelligence (AI) . As a global healthcare company we are u sing this innovative science area and the latest technologies to discover and develop medicines and other treatments . For us and for many of our peers and other companies in nearly all industries artificial intelligence (AI) is transforming the way how we innovate and operate. AI technologies offer tremendous potential for improving health care quality . Novartis has been using AI extensively in drug discovery for some time and, more recently, is working to apply AI to drug development in addition to other applications to improve our business processes . With respect to the development of medical products, we view the applicability of AI in two ways. One way includes the horizontal capabilities and tools that could help increase our understanding of unstructured data from various sources , such as images and text s for application s across the medical products we are developing. The other way is the application of AI algorithms and methodologies for product -specific use s across the life- cycle from early discovery, to development , manufacturing and deployment of our products and services . At N ovartis , we have defined and published our commitment to ethical and responsible use of AI in alignment with our code of ethics. This includes a commit ment to deploy AI systems in a transparent and responsible way and to ensure that the use of AI systems has a clear purpose that is respectful of human rights, is accurate, truthful, not misleading, and appropriate for their intended context. We are in agreement that a well- designed AI regulation is supportive of the protection of fundamental rights, ensuring safety and attributing liability. W e have identified 3 key areas in the proposed AI Act that we would like to comment on in more detail: Definition of scope In terms of material scope, the proposed AI Regulation would apply to the placing on the market, putting into service and use of "" AI systems"". AI systems covered by the proposed AI Act are defined broad ly to include : (i)s oftware developed in accordance with the first Annex to the proposal this Annex covers notably AI based on machine- learning approaches, logic and knowledge- based approaches and statistical approaches(ii)software that can, for a given set of human- defined objectives, generate outputs influencing the environments they interact with N ovartis believes that the scope of proposed regulation should be more precisely and narrowly focused on AI created through machine- learning approaches. In contrast, the application of more general terms such as rules -based and statistical AI could also be inferred to apply to applications which are not new, their underlying logic being completely transparent due to their explicit programming and/or curation. It is our concern that this regulation could otherwise potentially place undue burden on long- utilized techniques and approaches, which have been agreed among regulators and the healthcare industry to support decision- making today . Risk assessment of AI syst ems The current proposal introduces new oversight for high-risk AI systems which will require a case- by- case assessment from AI providers, based on other Annexes of the proposal and a series of criteria. Among the identified high risks are harm to health and safety that could result from application to human beings , the risk of negative impact on fundamental rights and the potential for discrimination. We are aligned with the need to address the aforementioned risks, but note that this proposal creates additional challenges for the development of AI -driven software, especially if a different risk -level is applied in comparison to the Medical De vice Regulation (MDR, 2017/ and n-vitro Diagnostic Regulation (IVDR, 2017/ . We would welcome clarificat ion on the proposed criteria to specify malfunctioning of an AI algorithm . Harmonization of approaches We understand the proposed AI Act will set forth obligations for manufacturers, importers and distributors that are in addition to obligations set forth in applicable regulations such as the Medical Device Regulation (MDR, 2017/ or In- vitro Diagnostic Regulation (IVDR, 2017/. Therefore, we would appreciate harmonized regulatory definitions for AI & related terms and aligned risk classification with the MDR and the IVDR and clarification on the responsibilities related to distributors who incorporate an algorithm vs. manufacturers of the algorithm . Also we would appreciate more clarification on the range of non- medical device uses of AI in drug development and their classification for risk under the AI Act. For example, would AI uses to categorize patients based on radiology scans or genetic profiles fall into thi s category? This data could be captured from medical devices , in alignment with current GDPR , in the context of clinical trials and would optimize data analytics. In terms of risk classification, would the following examples be classified as high or low r isk? Machine Learning (ML) algorithm used to identify patients for inclusion in studies based on prognostic/ predictive features Cases where processing of patient data may constitute biometric identification in alignment with (EU) Nr. 910/Given th e international interest in Artificial Intelligence, we also would appreciate a r isk-based and global ly harmonized approach to AI regulation including Member States and intra- European region alignment , with other countries/ regions such as US Food and Drug Administration . We welcome a transparent and predicted approach to the further development of the appendices. We commend the EC for drafting this first of its kind legislative proposal which, together with related legislative initiatives, w ill contribute to positioning the EU as the leader of trustworthy AI and a key player in the digital space. As Novartis we look forward to further engag ing with you to shape a system that is fit for the future and adapted to support and foster innovation . Novartis' commitment to the ethical and responsible use of Artificial Intelligence (AI) Systems. A human-centered approach in using Artificial Intelligence to reimagine medicine.2 | ETHICAL USE OF AI SYSTEMS- Introduction Page 5 Themes and respective principles Page 8 Empower Humanity Page 9 Accountability Page 10 Mitigate Bias Page 11 Respect Privacy Page 12 Transparent and Explainable Page 13 Safe and Secure Page 14 Environmental Sustainability Page 15 Review, Learn and Adapt Page 16 Glossary Page 17 References Page 19T able of | ETHICAL USE OF AI SYSTEMS- BACKGROUND Background The pharmaceutical value chain provides medicines companies with opportunities to gather meaningful data at every touchpoint, from early biomedical research, to clinical trials and medicines production, through to patient and healthcare community engagement. Leveraging Artificial Intelligence (AI) at scale enables the industry to unlock the power of this data to establish valuable insights. The ability to use these insights to inform and accelerate decision making is what makes AI a transformative technology for the pharmaceuticals industry.1 This paper outlines Novartis commitment to leveraging AI responsibly and ethically, in line with our overall purpose of improving and extending people s lives. As a leading global healthcare company, powered by advanced therapy platforms and data science, Novartis is undergoing a digital transformation to embed cutting-edge digital technologies and data science into all parts of its business. We have applied AI broadly across Novartis. With over 100 use cases already developed, we are transforming the way we: Innovate across R&D to develop novel therapies and drugs Optimize business processes, operations and commercial activities Engage with patients, healthcare professionals and partners Since 2018 we have been committed to progressing our ambitious enterprise transformation, aimed at answering three big what if questions: What if we could bring medicines to patients two years faster by transforming how we innovate in R&D; What if, with an eye to reinvest in R&D, we could significantly reduce our costs by $1-2 billion by revolutionizing the way we work optimizing and automating processes to drive breakthrough innovation; What if we could reach twice as many patients twice as fast, by rethinking traditional approaches to customer engagement and creating more personalized experiences?4 | ETHICAL USE OF AI SYSTEMS- BACKGROUND These technological developments come with both opportunities and challenges, leading to important questions which, as a leading pharmaceutical company, we need to address thoughtfully and affirmatively. With AI playing such a critical role in helping us to achieve our digital transformation goals, we recognize the need to define clear ethical principles around AI. In full alignment with the principles and commitments within our Code of Ethics2, this paper highlights our commitment to and ambition for responsible and ethical use of AI across our business. Specifically, on AI: Our commitment: To deploy AI systems in a transparent and responsible way. We will ensure that the use of AI systems has a clear purpose that is respectful of human rights, and is accurate, truthful, not misleading, and appropriate for their intended context. Why it matters: AI can help Novartis increase patient access, improve customer experience, drive automation, provide predictive analytics and detect potential misconduct. It also has the potential to be used to improve the speed and accuracy of diagnosis, treatment protocols, drug discovery, drug development, patient monitoring, and patient care, among other applications that will improve patients lives and optimize the healthcare ecosystem. Our Methodology: We engaged a team of leading ethicists and data privacy, legal and AI specialists both from within Novartis and externally. We developed an inventory of our current practices in AI and designed the following principles in line with our wider corporate Code of Ethics. Commitment has undergone rigorous review with the Independent Bioethics Advisory Committee (IBAC)3 and has been approved by Novartis Trust and Reputation Committee, which is chaired by the CEO.Artificial Intelligence, as discussed in this document, at the most fundamental level refers to intelligent agents that receive percepts from the environment and take actions that affect that environment4, often implemented as software programs5. In order to affect actions, AI systems aim at performing machine simulations of human intelligence processes such as learning, reasoning and self-correction.6 5 | ETHICAL USE OF AI SYSTEMS- INTRODUCTION Introduction Novartis is harnessing the power of data and digital in reimagining medicine, employing data science and Artificial Intelligence (AI) in three broad areas: Generative Chemistry We use generative chemistry to augment chemistry teams with well-annotated, high-quality ideas in a seamless fashion for our end users. We use Machine Learning to scan billions of molecules in our compound library and propose virtual molecules with a desired target profile, as defined by our drug discovery experts. It efficiently reports the multi- parametric ideation process every medicinal chemist undertakes daily. The output is a manageable set of optimized compound suggestions that can be readily synthesized. Discovery scientists can either directly choose from select compounds or be informed to come up with related, yet novel ideas. MELLODDY7 (Machine Learning Ledger Orchestration for Drug Discovery) The MELLODDY project (Innovative Medicines Initiative consortium, of which we are part) has created an AI platform that learns from proprietary compound assay data (>one billion for 10 million small molecules) contributed by multiple pharmaceutical companies, while maintaining confidentiality through blockchain-based encryption. Companies maintain control over their own data and resulting Machine Learning models. The models learn correlations between chemical substructures and activities in biological assays of disease relevance and benefit from techniques such as transfer learning , the principle that prediction accuracy may be enhanced by learning from models in adjacent areas. MELLODDY will enable cheaper, faster and higher-throughput drug discovery by providing structure-activity information for legacy and current assays in our drug discovery pipeline.The development of novel therapies and drugs The use of AI is being explored in the pre-clinical phase to understand disease biology and drug candidates; in the clinical phase to help target populations and to design intervention studies; and in the development of digital therapeutics and devices to enable continuous monitoring. 6 | ETHICAL USE OF AI SYSTEMS- INTRODUCTION The optimization of business processes and operations The use of AI is being explored and may improve processes in clinical development, manufacturing, and supply chain by automating, optimizing and re-engineering processes. In the business services area, we use AI to ensure efficiencies, effectiveness and drive operational excellence and compliance.AE Brain: Automating repetitive processes AE Brain improves the quality of our safety information and also reduces the burden of manual repetitive work. AE Brain processes messages to identify potential adverse events and technical complaints in these messages. The system ingests textual data from multiple sources and applies Natural Language Processing (NLP) technology to understand the | ETHICAL USE OF AI SYSTEMS- INTRODUCTION Engagement with patients, healthcare professionals and partners The use of AI is being explored to enhance engagement with stakeholders and participants in the healthcare systems with the objective of supporting patients and generating insights. For Novartis to improve and extend peoples lives in a sustainable manner, we must collaborate with trusted partners in tech, academia and other areas. Hence, in collaboration with its AI partners, Novartis is committed to using AI systems responsibly and in full alignment to the commitments and principles articulated in our Code of Ethics: Empower Humanity Hold Ourselves Accountable Mitigate Bias Respect Privacy Be Transparent and Explainable Assure Safety and Security by Design Prioritize Environmental Sustainability Review, Learn and Adapt Ai Nurse: empowering patients Novartis partnered with Tencent to develop a WeChat mini-app, called Ai Nurse, for patients diagnosed with heart failure. The patient engagement platform is designed to empower patients and their healthcare providers to be more aware of their condition and to take appropriate actions to improve their health and wellbeing. The app uses multiple AI-driven algorithms to transform voice to text and text to voice. Algorithms are used to anticipate disease progression, recommend activities and provide targeted coaching and education. All of this data is continuously assimilated and interpreted to assess a patient s improvement or worsening condition. Accordingly, nurses and physicians can remotely track patients, with full consent and privacy protections as discussed in this paper, and provide additional continuity of care recommendations.Novartis is harnessing the power of data and digital in reimagining medicine, employing data science and Artificial Intelligence (AI) in three broad areas: 8 | ETHICAL USE OF AI SYSTEMS- THEMES AND RESPECTIVE PRINCIPLES Themes and respective principles Novartis believes that any development, application or use of AI systems should be governed within the following ethical principles which are fully aligned to the respective Novartis Code of Ethics principles and commitments. 9 | ETHICAL USE OF AI SYSTEMS- THEMES AND RESPECTIVE PRINCIPLES Empower Humanity8: At Novartis, our values and culture are driven and defined by our purpose to reimagine medicines to improve and extend people s lives. Our everyday decision making is based on our ethical principles, as outlined in our Code of Ethics. These values and ethical principles form the basis from which we design, implement, and deploy AI. Novartis is committed to: Enforcing human-centric design in the deployment and use of AI systems; Building a mutually beneficial relationship between human knowledge, expertise and decision-making and the computational machinery which provides inferences and connections between data at scale. Respecting the rights and dignity of all people, and striving to prevent and mitigate identified adverse human rights impacts that may arise through our use of AI; Continuously assessing AI advances to ensure they proceed from within Novartis context and are determined by Novartis, rather than influenced by external factors; Monitoring the impacts of AI to evolving human and societal values.PRINCIPLE 10 | ETHICAL USE OF AI SYSTEMS- THEMES AND RESPECTIVE PRINCIPLES Accountability9: As an accountable organization, Novartis is committed to establishing robust governance over the design and use of AI. Such rigorous governance includes appropriate leadership and oversight, risk and impact assessments, appropriate policies and procedures, transparency, training and awareness, monitoring and verification, response and enforcement. Therefore, Novartis is committed to: Maintaining human accountability in decision- making processes of designing, delivering and operating AI systems; Providing autonomy to associates in the controlling, creation, training, deployment and operation of AI systems; Performing business and regulatory impact assessments of AI systems within the Novartis value chain before integration and deployment; Applying Novartis Information Technology (IT) and Operation Technology (OT) controls and processes to plan, implement and continuously monitor AI systems, in alignment to the commitments in the Code of Ethics; Proactively monitoring and mitigating potential negative AI consequences; Enabling the auditability of the AI systems via validation and verification functionalities and keeping an audit trail in line with best practice. We are building capabilities to elevate the practice of data science and AI across the enterprise and sparking a mindset shift so associates feel empowered by data science and AI, not threatened by it. In collaboration with the Novartis Learning organization, we have established the Data Science Academy that brings online and in-person education to Novartis associates. We are working towards annual accreditation for both data scientists and executives who have to enable data science. PRINCIPLE 11 | ETHICAL USE OF AI SYSTEMS- THEMES AND RESPECTIVE PRINCIPLES Mitigate Bias11,12,Data and algorithms used in AI systems need to meet Novartis strong commitment to fairness and non-discrimination detailed, inter alia in our Code of Ethics; particularly where AI systems are used in sensitive areas that closely touch critical decisions regarding drug development, socio-economic benefits, hiring and matters that relate to human behavior. We are committed to mitigating the risk of bias throughout the process, from data gathering, model creation and application of the model. To that end, we will strive to: Design, develop, test, train and operate AI algorithms based on inclusive and representative data to eliminate possible biases and known discriminatory aspects such as race, gender, ethnicity, sexual orientation, political or religious beliefs; Use data samples that are representative of the studied and analyzed population to eliminate or prevent unconscious bias; Perform a risk impact assessment on the AI systems before their use in production to eliminate the risk of bias or discrimination; Develop and use AI systems in ways that reflect the social and cultural diversity of Novartis; In the short-term, assess, acquire or develop tools and establish techniques to assess statistical bias in data-sets from external sources mitigating bias in all data sourced from outside of Novartis; Ensure the responsible use of AI when applied to the real world, as outlined in our Empower Humanity Principle. PRINCIPLE 12 | ETHICAL USE OF AI SYSTEMS- THEMES AND RESPECTIVE PRINCIPLES Respect Privacy14: In some instances, AI systems are trained on and use personal information. Outputs of AI systems may also impact the privacy of individuals. Novartis has established and implemented Global Data Privacy Principles15 that govern the use of personal information. These Principles apply without exception to the design and use of any AI system. The Principles are: Transparency: We are transparent about what personal information we process, how and why we collect it, use it, and who we share it with. We explain this in clear and simple language. Legitimate and Meaningful Collection: We connect all collection and use of personal information to specific business purposes related to how we operate, innovate or engage. Responsible and Sustainable Processing: We use personal information only in ways compatible with the purposes for which it was collected. We facilitate Individuals to exercise their rights with regards to their personal information. Security: We protect personal information by using reasonable safeguards to prevent its loss, unauthorized access, use, alteration or unauthorized disclosure. Integrity and Quality: We take appropriate steps to keep personal information accurate and up to date. Minimal Retention: We keep personal information only for as long we can legitimately use it.Designing and training viable AI models requires large samples of real-world data. In order to preserve transparency and explainability, while correcting for bias, the data sets used in the process may need to be preserved, creating tension with Privacy Principles, in particular the principle of minimal retention. Therefore, whenever possible, we are committed to finding alternatives to the use of personal information when designing AI. Such alternatives may be synthetic or anonymized data, or documentations of our approaches that do not require the retention of the training data sets. For Novartis full and detailed approach to Respecting Privacy, including details of implementation and tangible use cases of these principles in our business, please refer to the Novartis Global Data Privacy Policy.16 PRINCIPLE 13 | ETHICAL USE OF AI SYSTEMS- THEMES AND RESPECTIVE PRINCIPLES Transparent and Explainable17: Novartis strives to create transparency around the design and use of AI systems to explain how such systems work through: Short term: Openly disclosing / informing end-users when they are interacting with an AI system; Mid-term: Enabling the auditability and traceability of the decision pathways taken by AI systems using IT tools and infrastructure; Mid-term: Transparently communicating and explaining the limitations, purpose, decisions and capability of AI systems as new visualization models are developed; Ensuring the use of AI systems has a clear purpose that is accurate, truthful, not misleading, and appropriate for their intended context; aligning with the principles of Beneficial AI.18 PRINCIPLE 14 | ETHICAL USE OF AI SYSTEMS- THEMES AND RESPECTIVE PRINCIPLES Safe and Secure19: AI systems need to be safe, performing as intended, secure and resistant to compromise via unauthorized parties. Hence, in the design, implementation and use of AI systems, Novartis commits to the following: Technically robust systems that translate in-depth human understanding to stable operations based on a review of impact assessments and the specific context of the use-case; If the AI systems are deployed in relation to products and manufacturing environments, we are committed to reporting adverse events within 24 hours of discovery to the Novartis Safety Department and quality complaints to Quality Assurance, and then transparently communicating the risks of our medicines and devices to regulatory authorities; In relation to confidentiality, Integrity and Availability of Novartis Information, we hold ourselves accountable for the information and technology that we handle, with an obligation to safeguard our patients and partners information. Clinical Trial Procedures The Novartis clinical trial procedures are aligned with the CONSORT20 2010 and SPIRIT21 2013 statements, which are evidence- based guidelines to ensure transparent evaluation of new interventions in clinical trials - in study design, methodology and reporting. Since publication, both statements have had AI extensions, developed through international multi-stakeholder consensus, to ensure the safe use of AI in clinical trials. Novartis follows both AI extensions to guide the safe and ethical use of AI in clinical trials, and ensure transparency in the reporting of AI-specific information. Nerve Live Data and analytics platform, Nerve Live, harnesses past and present operational data, providing access to decades of drug development experience buried across multiple sources. The platform enables the systematic application of machine learning and predictive analytics to generate intelligence : new insights across multiple functional areas. To action the insights and create value, we crafted skillfully designed end-user applications for domain experts to plan, track, predict, compare and monitor domain activities, optimize costs, and maximize quality.22 PRINCIPLE 15 | ETHICAL USE OF AI SYSTEMS- THEMES AND RESPECTIVE PRINCIPLES Environmental Sustainability23: AI systems need to be designed sustainably, inter alia, assessing the resource usage and energy consumption to limit the risks to the environment. To address the environmental footprint of AI systems (e.g. assessing the resource usage and energy consumption), the Environmental Sustainability principle within the Code of Ethics would apply. This principle lays out that Novartis is committed to minimizing the environmental impact of our activities and products over their lifecycle. Novartis is aiming for carbon neutrality across the supply chain by In AI, this means addressing three broad areas: Short term: Partnering with like-minded sustainable technology platforms. Novartis will introduce sustainability as a key component in procurement of the computational infrastructure required for AI solutions and services; Mid-term: Ensuring optimal use of algorithms with internal implementation of AI, by training data scientists to be selective about the algorithms they want to train upfront, before committing them to the computational power required to deploy deep learning; Long-term: Reviewing internal operations, such as Novartis Technical Operations (NTO) to assess how AI can be used to reduce carbon footprint. PRINCIPLE 16 | ETHICAL USE OF AI SYSTEMS- THEMES AND RESPECTIVE PRINCIPLES Review, Learn and Adapt24: AI systems need to support and enable professional standards. As such, Novartis is committed to: Ensuring and maintaining professionalism and accountability in the creation and deployment of AI systems; ensuring that associates have the necessary depth of understanding of the ethical implications; Implementing and using AI systems that augment, complement and empower human capabilities and skills to improve speed, quality and maximize impact in a positive way; Enhancing the offering of our Data Science Academy to educate data scientists as well the broader group of Novartis associates on the use of AI; Empowering, educating and training associates in the short-term to have the right ethical professional awareness (knowledge, experience and required skills) as they use or operate AI systems, to ensure that ethical commitments (as laid out in the Code of Ethics) are not compromised; moving in the mid- term to a system of certification; Disclaimer: Novartis recognizes that AI is evolving rapidly in our industry and in society overall. We also recognize that we are at the beginning of the journey to embed these principles for responsible and ethical use of AI in our governance structures, our operations, and our businesses. Novartis is committed to becoming a leading and responsible voice in helping shaping and governing AI. We believe that our leadership, and willingness to adapt and learn as AI evolves, will also build critical trust with our patients, our associates, our partners, and other stakeholders as the benefits of AI are realized in the years ahead. PRINCIPLE 17 | ETHICAL USE OF AI SYSTEMS- GLOSSARY Glossary This glossary is meant to help in the understanding of the terms used in this paper. Communication AI systems should not represent themselves as humans to users; humans have the right to be informed that they are interacting with an AI system. This entails that AI systems must be identifiable as such. Beyond this, the AI system s capabilities and limitations should be communicated to AI practitioners or end-users in a manner appropriate to the use case at hand. This could encompass communication of the AI system's level of accuracy, as well as its limitations. Explainability Explainability concerns the ability to explain both the technical processes of an AI system and the related human decisions (e.g. application areas of a system). Technical explainability requires that the decisions made by an AI system can be understood and traced by human beings. Moreover, trade-offs might have to be made between enhancing a system's explainability (which may reduce its accuracy) or increasing its accuracy (at the cost of explainability). Whenever an AI system has a significant impact on people s lives, it should be possible to demand a suitable explanation of the AI system s decision-making process. Such explanation should be timely and adapted to the expertise of the stakeholder concerned (e.g. layperson, regulator or researcher). In addition, explanations of the degree to which an AI system influences and shapes the organizational decision- making process, design choices of the system, and the rationale for deploying it, should be available (hence ensuring business model transparency). Good Machine Learning Practices (GMLP) GMLP are those Artificial Intelligence or Machine Learning practices (e.g. data management, feature extraction, training and evaluation) that are akin to good software engineering practices or quality system practices. Artificial Intelligence or AI systems Artificial Intelligence (AI) systems are software (and possibly also hardware) systems designed by humans that, given a complex goal, act in the physical or digital dimension by perceiving their environment through data acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or processing the information, derived from this data and deciding the best action(s) to take to achieve the given goal. AI systems can either use symbolic rules or learn a numeric model, and they can also adapt their behavior by analyzing how the environment is affected by their previous actions. As a scientific discipline, AI includes several approaches and techniques, such as Machine Learning (of which deep learning and reinforcement learning are specific examples), machine reasoning (which includes planning, scheduling, knowledge representation and reasoning, search, and optimization), and robotics (which includes control, perception, sensors and actuators, as well as the integration of all other techniques into cyber-physical systems). AI systems life cycle An AI system s life cycle encompasses its development (including research, design, data provision, and limited trials), deployment (including implementation) and use phase. Beneficial AI Ensuring that AI is always grounded in the principle of improving and extending human life. Instead of building systems that optimize arbitrary objectives, we need to learn how to build systems that will, in fact, be provably beneficial for us. 25 Bias Bias is an inclination of prejudice towards or against a person, object, group or position. 18 | ETHICAL USE OF AI SYSTEMS- GLOSSARY Recommender Systems A recommender system is one that uses active information-filtering techniques to exploit past user behavior to suggest information tailored to an end user s goals.29 Trustworthy Trustworthy AI has three components, which should be met throughout the system s entire life cycle: ( it should be lawful, complying with all applicable laws and regulations ( it should be ethical, ensuring adherence to ethical principles and values and ( it should be robust, from a technical and social perspective since, even with good intentions, AI systems can cause unintentional harm. Transparency Transparency is closely linked with the principle of explicability and encompasses transparency of element relevant to an AI system: the data, the system and the business models. The Berkman Klein definition is as follows: Principles under this theme articulate requirements that AI systems be designed and implemented to allow for oversight, including through translation of their operations into intelligible outputs and the provision of information about where, when, and how they are being used. 30 Traceability The data sets and the processes that yield the AI system s decision, including those of data gathering and data labelling as well as the algorithms used, should be documented to the best possible standard to allow for traceability and an increase in transparency. This also applies to the decisions made by the AI system. This enables identification of the reasons why an AI-decision was erroneous which, in turn, could help prevent future mistakes. Traceability facilitates auditability as well as explainability.Human-centric AI The human-centric approach to AI strives to ensure that human values are central to the way in which AI systems are developed, deployed, used and monitored, by ensuring respect for fundamental rights, including those set out in the Treaties of the European Union and Charter of Fundamental Rights of the European Union, all of which are united by reference to a common foundation rooted in respect for human dignity, in which the human being enjoy a unique and inalienable moral status. This also entails consideration of the natural environment and of other living beings that are part of the human ecosystem, as well as a sustainable approach enabling the flourishing of future generations to come. Machine Learning (ML) The scientific study of algorithms that build a mathematical model of sample data to make predictions or decisions without being explicitly programmed to perform the taskxli. ML is often considered to be a branch of AI.26 Natural Language Processing (NLP) A subfield of AI concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. NLP draws from many disciplines including computer science and computational linguistics.27 Prevent Misconduct Novartis intend to use AI in a legal and responsible way to detect and prevent potential misconduct within its business practices Time spans Short term refers to 1-2 years; mid-term to 3-5 years; long-term to 5+ years. Synthetic Data Microdata records created to improve data utility while preventing disclosure of confidential respondent information. Synthetic data is created by statistically modeling original data and then using those models to generate new data values that reproduce the original data's statistical properties. Users are unable to identify the information of the entities that provided the original data. 19 | ETHICAL USE OF AI SYSTEMS- REFERENCES References ethics-english.pdf - IBAC- provides analysis and recommendations on Novartis guidelines and policies for the ethical conduct of clinical research, and on selected ethical challenges which may arise in clinical trials, development programs, managed access programs and other areas across the Novartis enterprise. IBAC is comprised of bioethicists, clinicians, healthcare practitioners, patient advocates and other domain knowledge experts appropriate to the problem at hand. Human Compatible: AI and the Problem of Control, Russell, S.J; 2020; Penguin Books, Limited. See also: books/307/307948/human-compatible/html Artificial Intelligence in Healthcare, Nature Biomedical Engineering, Vol 2, Kun-Hsing Yu, Andrew L. Beam and Isaac S. Kohane; See also: Artificial Intelligence for clinical trial design, Trends in pharmacological sciences, Stefan Harrer, Pratik Shah, Bhavna Antony, Jianying Hu; See also: files/2019-07/piis0165614719301300.pdf To conduct our business in a manner that respects the rights and dignity of all people. We will strive to prevent, mitigate and remedy adverse human rights impacts throughout our workplace, business operations and in the communities in which we work. We want to protect people from abuse by those who are more powerful. To deploy Artificial Intelligence (AI) systems in a transparent and responsible way. We will ensure that the use of AI systems has a clear purpose that is respectful of human rights, and is accurate, truthful, not misleading, and appropriate for their intended context Artificial-Intelligence-and-Machine-Learning-Discussion-Paper.pdf We will educate our people on inclusivity and provide all associates with equal opportunities to contribute to our company and advance their careers. We will listen to different communities with a learning mindset, to do what we can to contribute to building a world that is safer and more inclusive To conduct our business in a manner that respects the rights and dignity of all people. We will strive to prevent, mitigate and remedy adverse human rights impacts throughout our workplace, business operations and in the communities in which we work. We want to protect people from abuse by those who are more powerful. We will respect the rights, safety and dignity of individuals and communities, protect scientific integrity and strive to advance the practice of medicine. We will make sure that any data or information that we create or are responsible for, is true, accurate and fair. We do not make false or misleading statements. To use personal information that we are entrusted with in a responsible way. We will adhere to our Data Privacy principles and ensure our external service providers also commit to these principles. privacy-principles.pdf privacy-principles.pdf To deploy Artificial Intelligence (AI) systems in a transparent and responsible way. We will ensure that the use of AI systems has a clear purpose that is respectful of human rights, and is accurate, truthful, not misleading, and appropriate for their intended context Human Compatible: AI and the Problem of Control, Russell, S.J; 2020; Penguin Books, Limited. See also: books/307/307948/human-compatible/html To protect our data and technology and ensure that information is kept safe from theft, loss, misuse or disclosure. We will take accountability for the information and technology we handle. Leading a Digital Transformation in the Pharmaceutical Industry: Reimagining the Way We Work in Global Drug Development, Clinical Pharmacology & Therapeutics, Luca A. Finelli, Vas Narasimhan; See also: To minimize the environmental impact of our activities and products over their lifecycle. We will strive for a positive effect on climate, by reducing our carbon footprint, waste and water usage and making efficient use of natural resources. To maintain high standards of ethical business conduct. We are committed to the same high standards of ethical business conduct wherever we do business. Human Compatible: AI and the Problem of Control, Russell, S.J; 2020; Penguin Books, Limited. See also: books/307/307948/human-compatible/html Artificial Intelligence for clinical trial design, Trends in pharmacological sciences, Stefan Harrer, Pratik Shah, Bhavna Antony, Jianying Hu; See also: files/2019-07/piis0165614719301300.pdf Artificial Intelligence for clinical trial design, Trends in pharmacological sciences, Stefan Harrer, Pratik Shah, Bhavna Antony, Jianying Hu; See also: files/2019-07/piis0165614719301300.pdf Gadepally.pdf Principled Artificial Intelligence, Berkman Klein Center. See also https:// cyber.harvard.edu/publication/2020/principled-ai",en,"A human-centered approach in using Artificial Intelligence to reimagine medicine.2 | ETHICAL USE OF AI SYSTEMS- Introduction Page 5 Themes and respective principles Page 8 Empower Humanity Page 9 Accountability Page 10 Mitigate Bias Page 11 Respect Privacy Page 12 Transparent and Explainable Page 13 Safe and Secure Page 14 Environmental Sustainability Page 15 Review, Learn and Adapt Page 16 Glossary Page 17 References Page 19T able of | ETHICAL USE OF AI SYSTEMS- BACKGROUND Background The pharmaceutical value chain provides medicines companies with opportunities to gather meaningful data at every touchpoint, from early biomedical research, to clinical trials and medicines production, through to patient and healthcare community engagement. Hence, in collaboration with its AI partners, Novartis is committed to using AI systems responsibly and in full alignment to the commitments and principles articulated in our Code of Ethics: Empower Humanity Hold Ourselves Accountable Mitigate Bias Respect Privacy Be Transparent and Explainable Assure Safety and Security by Design Prioritize Environmental Sustainability Review, Learn and Adapt Ai Nurse: empowering patients Novartis partnered with Tencent to develop a WeChat mini-app, called Ai Nurse, for patients diagnosed with heart failure. To action the insights and create value, we crafted skillfully designed end-user applications for domain experts to plan, track, predict, compare and monitor domain activities, optimize costs, and maximize quality.22 PRINCIPLE 15 | ETHICAL USE OF AI SYSTEMS- THEMES AND RESPECTIVE PRINCIPLES Environmental Sustainability23: AI systems need to be designed sustainably, inter alia, assessing the resource usage and energy consumption to limit the risks to the environment. To address the environmental footprint of AI systems (e.g. assessing the resource usage and energy consumption), the Environmental Sustainability principle within the Code of Ethics would apply. This principle lays out that Novartis is committed to minimizing the environmental impact of our activities and products over their lifecycle. Novartis is aiming for carbon neutrality across the supply chain by In AI, this means addressing three broad areas: Short term: Partnering with like-minded sustainable technology platforms. Novartis will introduce sustainability as a key component in procurement of the computational infrastructure required for AI solutions and services; Mid-term: Ensuring optimal use of algorithms with internal implementation of AI, by training data scientists to be selective about the algorithms they want to train upfront, before committing them to the computational power required to deploy deep learning; Long-term: Reviewing internal operations, such as Novartis Technical Operations (NTO) to assess how AI can be used to reduce carbon footprint. Leading a Digital Transformation in the Pharmaceutical Industry: Reimagining the Way We Work in Global Drug Development, Clinical Pharmacology & Therapeutics, Luca A. Finelli, Vas Narasimhan; See also: To minimize the environmental impact of our activities and products over their lifecycle. We will strive for a positive effect on climate, by reducing our carbon footprint, waste and water usage and making efficient use of natural resources.",risk
BEUC - The European Consumer Organisation (Belgium),F2665432,05 August 2021,Non-governmental organisation (NGO),Small (10 to 49 employees),Belgium,"1 AI Act BEUC s preliminary assessment DISCLAIMER : this document is BEUC s preliminary assessment of the AI Act . A fully fledge d position paper will be published soon . On 21st April 2021, the European Commission published a proposal for a Regulation laying down harmonised rules on artificial intelligence ( Artificial Intelligence Act ). AI has the potential to bring many positive things for consumers . It can power new products and services and help make consumers lives easier and more convenient. For example, banks use AI to track suspicious activities and prevent fraud; public authorities use it to process and answe r citizen requests; smart phones integrate virtual personal assistants that resort to AI; and social media applications use AI to organise , moderate and personalise their content feeds . However, AI also comes with significant risks and challenges for consumers . For example, there is a risk of bias and unfair discrimination among different groups of people on the basis of economic criteria, gender or a person s health . More broadly, the use of AI can negatively affect consumers autonomy and freedom of choice.1 We welcome that the European Commission has put forward a legal instrument aimed at regulating AI . We strongly regret, however, that the AI Act proposal generally fails to address consumers main concerns and expectations. This is particularly due to its narrow scope, focused on so-called high risk AI applications , and the fact that the need to strength en consumers rights is neglected . Moreover, the prop osal relies heavily on industry s own unvetted assesment of compl iance with the legislation and does not set up a suffici ently strong governan ce and public and private enforcement system. Finally, the methodology used to list specific AI system s within the scope of application of regula ted AI systems does not seem to be suffici ently future -proof. We hope that the EU s co-legislators ( i.e., the European Parliament and the EU Member States) will address these shortcomings and improve the proposal so that it provides the rights and protection s that consumers need while enabl ing the path for innovation that respects our fundamental values . Below are some preliminary recommendations to address key issues of the proposal from a consumer perspective. 1 You can find more information about consumer perceptions about AI in this survey: -x-2020-078_artificial_intelligence_what_consumers_say_report.pdf 2 The prohibited AI practices need to be clarified and strengthened Article 5 of the proposal establishes a list of four AI practices that should be banned. BEUC strongly welcome s this regulatory approach. Certain AI practices represent such a n important risk for consumer rights, fundamental rights and our societal values that the most adeq uate regulatory solution is a clear prohibition. However, several elements need to be improved to better take into account consumers interests and ensure a broad and clear application of these rules: - Art. 5 ( a) is limited to AI that is used intentionally to materially distort behaviour, causing physical or psychological harm to someone. Yet, AI that manipulates , discriminates, misleads or otherwise harms consumers in a manner that causes economic and societal harm is not covered. These harms should also be included . - Some terms used in this Article need to be modified as they would effectively limit the scope of this provision too much (e.g., it should cover any technique , be it subliminal or not ). - Art. 5 ( b) is limited to AI that exploit s the vulnerabilities of specific groups such as children or mentally disable d persons , with the specific intention to materially distort their behaviour leading to physical or physiological harm . While ensuring protection to the most vulne rable groups, this provision does not take into account the constant state of vulnerability of all individuals created by exposure to black box technology and economic practices and consequences that consumers cannot grasp . There is a structural and architectural unbalance ( digital asymmetry that should be addressed in the formulation of Article - In Arts. 5 ( a) and b), the wording in order to limits the application of this provision to AI whose intended use is to cause physical or psychological harm , thus excluding the potential use or foreseeable use of the AI .3 We strongly reject t he requirement to prove intent which is not required under EU consumer law in case of unfair commercial practices and would be a significant and inacceptable step backwards with regards to the level of protection that consumers need and are entitled to expect under EU law. If not changed, it would also mean that these pro visions are in practice non -enforceable. - Whereas the use of social scoring by public authorities is prohibited , which is welcome, its use by private entities , and thus commercial uses , is not regulated adequately by the AI Act.4 This highly problematic gap must be addressed. In this regard, it must be noted that t he European Data Protection Board (EDPB) and European Data Protection Superviso r (EDPS) 5 are requesting a ban on any type of social scoring . 2 -x-2021-018_eu_consumer_protection.0_0.pdf , at 46 et seq. 3 See Recital 16 of the proposal ; 4 We can envisage certain types of social scoring used by private bodies being regulated as high -risk AI (e.g. Annex III Points 3 and ; 5 -edps-call-ban-use-ai-automated -recognition -human -features - publicly -accessib le_en 3 - Remote biometric identification systems (Art. 5 ( d)) o Under the current proposal, t he use of real -time remote biometric identification system s (e.g. facial recognition AI) in publicly accessible spaces7 for the purpose of law enforcement would be prohibited but se veral exceptions are foreseen. o The use of real -time and post remote biometric identification systems by private entities are only classified as high -risk AI and thus permitted (Annex III Point . o Due to its intrusiveness and the risk it represents for fundamental rights and core democratic principles, the use of remote biometric identification systems by private entities in public spaces should be regulated as strictly as any other use. In this sense, s everal organisations, regulatory authorities and policy makers8 are already requesting a blanket ban on the us e of this technology. If, nevertheless, certain exceptions are envisaged for public entities , such exceptions must be accompanied by specific safeguards to guarantee the respect of the necessary, proportionality and fairness principles as well as fundamental rights. - AI used to identify emotion recognition is either classified as high -risk AI (if used by public bodies for law enforcement pu rposes )9 or subject to weak transparency obligations (if used by private actors ).10 This is not sufficient . Researchers have demonstrated that ""it is not possible to confidently infer happiness from a smile, anger from a scowl, or sadness from a frown, as much of current technology tries to do when applying what are mistakenly believed to be the scientific facts "".11 The use of emotion recognition systems should be more strictly regulated and only permitted in very specific cases .12 The proposal needs to regulate all AI and not only high -risk AI. The scope of the proposal is too limited and is not future -proof: First, it has a strong focus on high -risk AI , thus almost completely excluding from the rules applications to be considered low- and medium - risk. Only Art. 5 (prohibited practices) and Art. 52 (transparency measures for certain AI) reach beyond high -risk AI . As a consequence, we can envisage that a lot of AI applications that consumers use in their everyday lives and that can have an important impact on consumers and on our societies 6 remote biometric identification system means an AI system for the purpose of identifying natural persons at a distance through the comparison of a person s biometric data with the biometric data contained in a reference database, and without pri or knowledge of the user of the AI system whether the person will be present and can be identified (Art. 3 ( of the proposal) ; 7 publicly accessible space means any physical place accessible to the public, regardless of whether certain conditions for access may apply (Art. 3 ( of the proposal) ; 8 E.g. See footnote 6 and ; 9 See Annex III Point 6, b); 10 AI used by public authorities for emotion recognition purpo ses is considered high -risk (Annex III Point 6 (b)) 11 Pag. Barrett, L. F., Adolphs, R., Marsella, S., Martinez, A. M., & Pollak, S. D. (. Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements. Psychological Science in the Public Interest , 20(, 1 This is also the position of the EDPB and the EDPS, who are calling for the prohibition of AI used to infer emotions, except for very specified purposes (e.g. in the health sector, there could be appl ications where patient emotion recognition detection can be important); 4 will be excluded from the scope. This is for example the case of online profiling and personalisation techniques , content recommender systems that select what people see in their social media feeds . Also, a significant number of connected devices embedded with AI (e.g. smart meters, conn ected toys, virtual assistants) are not classified as high -risk AI.13 The risks created and potential for harm stemming from the myriad of AI systems that are and will increasingly be present in consumers lives are not properly addressed by the regulation. Even if there is a basic requirement of transparency in Article 52 , the scope of this provision is limited to certain AI systems and transparency alone is not sufficient. Secondly, the high-risk category is too narrowly defined as we ll.14 The risks taken into consideration are limited to those of health and safety and the protection of fundamental rights15, leaving out basic consumer rights, societal effects, impact on democracy, rule of law or environmental impact , as well as the potential for economic harm. For example , AI used to assess the eligibility of someone for a health or car insurance , or the cost of such insurance, would not be considered as high -risk AI . The Commission has competence to update the list of high-risk AI falling under Annex III . However, several strict conditions need to be fulfilled , making it very difficult to make use of such possibility in practice .16 For example, it limits the possibility of expanding the scope to the areas already listed in Annex III. BEUC in contrast supports a risk -based approach where all AI (and not only high -risk) are subject to a minimum set of rules ( starting with basic principles of transparency, fairness, accountability, non -discrimination, security , etc. ). Then, the higher the risk, the stricter the specific requirements should become. A broader , more inclusive approach is necessary in the proposed Regulation. For high risk AI applications, a conformity assessment by third parties should be the rule, not the exception The proposal provides for far too much reliance on industry self -assessing that it complies with the rules .17 This approach is not adequate as it does not take in consideration the complexity of the risks posed by AI and is likely not to be effective to protect consumers . First, there is an evident conflict of interest : the entity assessing whether a certain product is in com pliance with the rules is the same company who has an interest in placing the AI on the EU market as quickly as possible. Second , a survey18 about independent third -party testing shows that the compliance and safety of independently -checked products can be considerably higher than for products that rely simply on manufacturer s self -declaration of conformity. 13 According to Article 44 ( of the proposal, the manufacturer of these devices may need to apply AI standards under certain conditions. 14 See Article 6 of the proposal; 15 See Article 7 ( b) of the proposal; 16 See Article 7 of the proposal; 17 See Article 43 of the proposal; 18 -federation.org/content/wp -content/uploads/2016/11/Consumer -Products -Safety -Study - pdf 5 Therefore, a conformity assessmen t by independent third parties should become the rule, not the exception . AI rights for consumers need to be added. Overall, the consumer perspective is lacking in the proposed AI Act. Remarkably , user in the proposal is only defined as business user .19 More importantly, the proposal does not include any specific citizen or consumer rights such as a right to transparency , right to explanation , the right to contest an algorithmic decision and obtain human oversight , or the right to submit a complain t in the case of a suspected infringement . When it comes to transparency, except for high -risk AI20, the proposal only envisages transparency obligations in very limited circumstances. In short, users n eed to be informed when dealing with AI such as chatbots, deepfake s or emotion recognition technology .21 Consumers should have a strong set of rights , given the risks that AI technology brings for them. Existing legislation, such as the GDPR , does not provide sufficient protection in an AI context . Key rights such as the right to obtain an explanation of how a decision based on AI affecting them has been taken are not included in the proposed legislation but need to be added. Consumers should not depend on the processing of personal data as this is the case under EU data protection rule s.22-23 Effective redress mechanisms for consumers are needed . The proposal does not envisage any redress possibilities for consumers . As mentioned in the previous point, t here is no t even a right to complain before a supervisory authority nor an obligation for companies to provide for a complaint mechanism . For example, if a consumer is harmed by non -compliant high-risk AI or by the use of AI prohibited under Art. 524, the proposed rules do no t foresee any rights or mechanisms for consumers to obtain redress. In consequence, the entity which is the most vulne rable to harms caused by AI is also the least protected. The AI Act must envisage a structure that would furnish individuals with horizontal rights to seek remedies (e.g., compensation for damages ) and to protect them. This should also include a revers al of the burden of proof for all AI interacting with or affecting individuals. Reversal of the burden of argumentation/proof establishes that it would be for the supplier of a n AI to demonstrate that it complies with the law .25 19 See Art. 3 ( of the prop osal; 20 See Art. 13 of the proposal; 21 See Article 52 of the proposal; 22 Article 22 of the GDPR sets out that the data subject has the right not to be subject to automated decision - making. However, this right is restricted to fully automated processing of data and does not apply in a number of cases, such as when the decision making is based on use rs consent or if there is not a legal or similar significant impact on the individual ; 23 Opinion of the German Data Ethics Commission, Executive summary, 44, p . 21; 24 See Chapter 1 above; 25 -x-2021-018_eu_consumer_protection.0_0.pdf , at 75 6 Furthermore, t he proposal does not include the possibility of harmed consumers to be represented by an NGO, including consumer organisations, in the exercise of their rights. An article similar to Art 80 GDPR (Representation of data subjects) or Article 68 of th e proposal for a Digital Services Act (Representation) should be introduced. In the context of the AI Act , this representation shall not be subject to a mandate. Also, t o enable collective redress actions, t his proposal should be included in the Annex of the Representative Actions Directive26. A similar provision was included in the Commission s proposal for a Digital Services Act.27 The governance and enforcement structure must be clear and ensure an effective and consistent application of the rules at European and national level The proposed governance and enforcement structure mainly rests at national level and raises issues in relation to the obligations, competences and powers of the different actors involved28 and the different processes envisaged . For example : - The need to ensure a coherent and coordinated EU-wide enforcement. While the Commission plays a central role if a national supervisory authority notifies its intention to adopt measures against an AI system in its territory29, the Commission has no powers to proactively take the lead in case of inaction by national authori ties. The autonomy and powers of the European AI Board30, comprised of high-level representatives of the national supervisory authorit ies and chaired by the Commission , also seem quite limited. - There is a need to clarify potential overlaps with existing bodies such as the European Data Protection Board , as well as the role of Data Protection Authorities a view that other stakeholders also share31. - It is also important to ensure a co mmon approach when it comes to the cooperation between the different competent authorities and supervisory authorities at national level, to ensure effective and swift enforce ment as well as to avoid different approache s by Member States . - Given the lack of redress mechanisms for consumers in the proposal , there is no authority that is entrusted with dealing with consumer complaints in case of breaches of the regulation. 26 Directive (EU) 2020/1828 on representative actions for the pr otection of the collective interests of consumers and repealing Directive 2009/22/EC ; 27 See Article 72 of the Proposal for a Regulation on a Single Market For Digital Services ( Digital Services Act ) and amending Directive 2000/31/EC ; 28 Namely: the European Artificial Intelligence Board (EAIB), the national competent authorities, the national supervisory authority, the Commission, the European Data P rotection Supervisor (EDPS) and the AI system providers ); 29 See Article s 65 ( and 66 of the proposa l; 30 See Articles 56 58 of the proposal; 31 See, for example, Access Now: -minimal -steps -to-regulate -harmful -ai- systems/ . 7 - Providers of high risk AI systems are only obliged to report serious incidents or malfunctioning which constitute a breach of Union law intended t o protect fundamental rights . This leave s out breaches of fundamental rights stemming from non -high risk AI systems, as well as any serious incidents or malfunctioning which are not directly related to fundamental rights . For example, a faulty AI used in an energy distribution grid can have a serious impact on the finances and wellbeing of consumers by inadvertently cutting off their energy supply or overc harging them. This would not be covered under the proposal . It is key to have a clear and coherent oversight and enforcement structure to guarantee an effective and consistent protection of consumers all across the EU .",en,"Secondly, the high-risk category is too narrowly defined as we ll.14 The risks taken into consideration are limited to those of health and safety and the protection of fundamental rights15, leaving out basic consumer rights, societal effects, impact on democracy, rule of law or environmental impact , as well as the potential for economic harm. For example, a faulty AI used in an energy distribution grid can have a serious impact on the finances and wellbeing of consumers by inadvertently cutting off their energy supply or overc harging them.",risk
Zwizek Pracodawcw Business & Science Poland (Poland),F2665431,05 August 2021,Business association,Small (10 to 49 employees),Poland,"We connect Polish Business an d Science with the EU Page 1 of 4 EU Tran sparen cy Register Number: 76-89 Business & Science Poland (BSP) welcomes the possibility to comment on the proposal for a Regulation of the European Parliament and of the Council on laying down harmonized rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain union legislative acts (COM ( 206 final). We represent leading Polish entities particularly from energy -intensive industr y, air transport, cyber and IT sector , postal services and financial mark ets employing over one hundred thousand employees in Poland, other E urop ean Union (EU) Member States and outside the EU. BSP is also in a partners hip w ith R&D , academic and SME organizations. Topics related to Artificial Intelligence are within the sco pe of our interests. We welcome the intention of ma king the E U a leader in the developme nt of safe and trustworthy Artificial Intelligence (AI). The new regulations are aimed, on the one hand, at building trust in Artificial Intellig ence systems to reduce the associated risk . On the other hand new regulations are focused on supportin g investment in Artificial Intellig ence and further development of this technology. We would like to focus on several issues identified in the analysis of the proposed regulation s. AI is a rapidly growing group of techno logies that can deliver many d ifferent socioeconomic benefits across all indu stries and areas of social activity. Due to the pace of technological changes and in the light of potential challenges, the EU is striving to develop a well -balance d approach. It is in the interest of the Euro pean Union to maintain the EU's technological leadership and ensure that Europeans can benefit from new technologies d esigne d and operated by following EU values, fundamental rights, and principles. We understa nd that the aim of proposed regulations is to en sure security and respect the fundamental righ ts and values of the EU. We agree with this and consider actions i n this regard as necessary. At the same time, we would like to note that the mechanism proposed in this regu lation is not the most optimal and may result in a series of undesirable side effects in the form of freezing the development of AI, as well as shifting AI development centres outside the E U. Therefore, we propose several improvements in this area. #BSP_ Paper August 2021 Artificial Intelligence Act We connect Polish Business an d Science with the EU Page 2 of 4 EU Tran sparen cy Register Number: 76-89 In particular, we would like to highlight the following aspects of the AI regulat ion: 1/ High -risk AI systems In the discussed regulation proposal, it is recognized that the application of a risk -based framework is a better solution th an the general regulation of all AI systems. Risks and hazards should be determined on a per -sector an d case-by-case basis. The r isk meas ures should also give the consideration to the legal and safety implicati ons. In our opinion, t he category of high -risk algorithms is too broad. Deeming all algorithms applied to the critic al infrastructu re as high -risk (Annex III) will paralyze the development of th e domain in the currently least digi tized industry, such as energy and petrol sector. Algorithms allowing for internal optimization of raw material consumpt ion, predicting hydro carbon quality as well as allocation of means of transport for fuel delivery to terminals or stations should not be the subject of these regulatio ns. They are an element of the improvement of operational excellence in enterprises . High -risk algorithms used in the energy and pe trol sec tor do not concern the end customer an d do not cause the risk of discrimination on hum an r ights . We therefore strongl y recommend that this category be excluded from the definition of ""high -risk"" algorithms. 2/ Record -keeping of the high -risk algorithms We would like to lay down another solution than an automatic registration of all high -risk algorithms . In our opinion, the reverse mechanism should be used i.e. allowing users to request conformity assessment and verifi cation by audit bodie s through raised objections if needed . We find that enab ling to use only the ""certified"" algorithms will significantly slow down the development o f AI in the EU and at the same time it will cause uncontrolled migration of algorithm develop ment centers out side t he EU, in particular to the US and China, and con sequ ently will not solve the problem posed by regulations. The proposed provisions impose an excessive administrative burden on the suppliers of the algorithms and in consequence it will delay the technological d evelopment of many industries, thus cause subop timal resource management by enterprises. We connect Polish Business an d Science with the EU Page 3 of 4 EU Tran sparen cy Register Number: 76-89 3/ The requir ement to confirm the compatibility of algorithms in each case (conformity assessment ) The proposed requirement to confirm the compliance of the algorithm in case of every subs tantial update of the al gorithm does not bear in mind that the algorithms require , since their fundamental assumptions, regular and systematic updating, ""learning"" and tuning on the basis of real time data. As a result, an indented and targeted purpose that an algorithm could be frozen , certified and implemented may never be reached. On the contrary, the algorithms are constantly evolving. As a result, issuing consent only for a specific ver sion of the algorithm, without consider ation of its future modifications, will result in the u se of an outdated SI algorithm (developed on the basis of historical, not real time data). This may result in duplicati ng the algorithmic bias as the algorithm would be tra ined on a limited data pool. With this in mind , we acknowledge that reporting standard devi ations would be more e ffective solu tion than issuing approvals for release to use. 4/ Penalties for non-com pliance with the re gulation In our opinion, the fines for the non-compliance of a high -risk Artifici al Intelligence system with requ irements under the prop osed regulation are too high. Businesses or public authorities that develop or use AI applications that constitute a high risk for the safety or fundamen tal rights of citizens would have to comply wi th sp ecific requirements and obligations. Compli ance with these requirements would imply costs amounting to approximately EUR 6000 to EUR 7000 for the supply of an average high -risk AI system of around EUR 170000 by For AI users, there would also be the annual cost for the time sp ent on ensu ring human oversight where this is appropriate, depending on the use case. Those have been est imated at approximately EUR 5000 to EUR 8000 per year. Verificati on costs could amount to another EUR 3000 to EUR 7500 for suppliers of high -risk AI . Businesse s or public authorities that develop or use any AI applications not classified as high risk would only have minimal obligations of information. However, they could choose to join others and together adopt a code of conduct to follow suitable r equirements and to ensure that their AI sy stems We connect Polish Business an d Science with the EU Page 4 of 4 EU Tran sparen cy Register Number: 76-89 are trustworthy. In such case, costs would be at most as high as for high -risk AI systems, but most probably lower. We recommend considering provi sions applying warnings for non -compliance with the regulat ion and lower level of financ ial fines than those proposed for a given entity . 5/ High -risk AI system -based places - data storage We recommend that the servers proces sing high -risk data , for cybersecurity pur poses , should be located only with in the European Union . 6/ Confo rmity asses sment for AI systems at the national level There is a defined line for t he exchange of information and good practices among domestic operators to carry out t hird-party conformity assessment s for AI systems intend ed to b e used for the remo te biometric identification of persons. 7/ Document retention Please note that in the event that the company will exist for less than 10 years, the supplier should not keep the data at the disposal of the competent national authorities for 10 years after placing the AI system on the market or for personal use. 8/ App lications using AI We fully agree that applications using AI and applications that viola te the fundamental rights and affect the rights of child ren, should be banned in t he EU. We suggest that the EU requires companies to demonstrate that their claims an d assumptions are backed by science, that they do not violate any of the fundamental rights , and do not discriminate against users, before the market launch of their applications or products that use artificial intelligence. We connect Polish Business an d Science with the EU Page 5 of 4 EU Tran sparen cy Register Number: 76-89 9/ Bayesian techniques The list of techniques in the Annex I include technologies that are not normally considered to be Artificial Intelli gence ( e.g. traditional data analysis tools). For example, Bayesian inference is not an Artificial Intelligence, but prove n mathematical formula . 10/ The error free data As we know fr om working with large data sets, complete error -freeing and c ompleteness of the data obtained are not possible. Data are i nterpreted by experts, and even in their line of reasoning, error s can creep in. However, t his does not mean that the dat a should not be as accurate as possible. Therefore, the recommendation s hould be broader or contain various types of labels to disting uish data - from false, artificial, with minor errors , to those closest to the ideal. About BSP Business and Sc ience Poland (BSP) connects the experience of leading Polish enterprises with t he E U agenda. We represent the knowledge and interests of succe ssful entities, which employ over 100 000 workers in Poland, EU and globally. We are c ommi tted to advancing the va lues of EU Common Market in sync with the needs to transform it responsibly and effectively.",en,"We represent leading Polish entities particularly from energy -intensive industr y, air transport, cyber and IT sector , postal services and financial mark ets employing over one hundred thousand employees in Poland, other E urop ean Union (EU) Member States and outside the EU. Deeming all algorithms applied to the critic al infrastructu re as high -risk (Annex III) will paralyze the development of th e domain in the currently least digi tized industry, such as energy and petrol sector. Algorithms allowing for internal optimization of raw material consumpt ion, predicting hydro carbon quality as well as allocation of means of transport for fuel delivery to terminals or stations should not be the subject of these regulatio ns. High -risk algorithms used in the energy and pe trol sec tor do not concern the end customer an d do not cause the risk of discrimination on hum an r ights .",risk
Federation of German Consumer Organisations (vzbv) (Germany),F2665425,05 August 2021,Consumer organisation,Medium (50 to 249 employees),Germany,"Impressum Verbraucherzentrale Bundesverband e.V. Team Rudi-Dutschke -Stra e 17 10969 Berlin Bundesverband der Verbraucherzentralen und Verbraucherverb nde Verbraucherzentrale B undesverband e.V. 05 August 2021 digitales@vzbv.de Digital and MediaARTIFICIAL INTELLIGE NCE NEEDS REAL WORLD REGULATIO N Position paper of the Federation of German Consumer Or- ganisations (vzbv) on the European Commission s proposal for an Artificial Intelligence Act (AIA) Artificial intelligence needs real world reg ulation 2 l 27 Verbraucherzentrale Bundesverband e.V. II. INTRODUCTION 6 III. PROPOSALS FOR I NCREASING THE AIA S CONSUMER FOCUS 7 Definitions ................................ ................................ ................................ 1 Article 23 ( Artificial intelligence system ................................ 2 Article 3 ( User ................................ ................................ ................................ 3 Article 3 ( Emotion recognition system ................................ The Scope is too n arrow Part I: Neglect of Economic harms and violations of consumer rights ................................ ................................ ................................ 1 AI applications with large economic/financial impact or effects on consumer rights must be regarded as high -risk ................................ ................................ 2 Future proofing AIA: Considering consumers rights and economic risks when determining the updating modalities of the list of high -risk applications The Scope is too narrow Part II: List of Prohibited AI ................................ 1 Art. 5 par. 1 (a) Prohibition of Dark Patterns ................................ 2 Art. 5 par. 1 (b) Prohibition of exploiting weaknesses to influence behaviour 3 Art. 5 par. 1 (c) Prohibition of general social scoring by private entities 4 Art. 5 par. 1 (d) Prohibition of remote biometric identification in the public space by private entities ................................ ................................ ................................ 5 Prohibition of emotion recognition system b y private entities More Transparency for consumers ................................ ................................ 1 Art 52 Labelling obligation ................................ ................................ 2 Individual explanation for consumers ................................ ................................ 3 Information for the general public ................................ ................................ IV. PROPOSALS FOR EN SURING EFFECTIVE IND EPENDENT ASSESSMENTS OF AI SYSTEMS 23 Ensuring Consumer trust with independent Assessments ................................ 1 Conformity assessment ................................ ................................ 2 Title VIII, Chapter 3 Enforcement must be complemented with independent assessments ................................ ................................ ................................ V. ENSURING PRIVATE ENFORCEMENT 26 VI. TRADE AGREEMENTS MUST NOT HINDER AN E FFECTIVE TRANSPARENC Y AND MONITORING OF AI SYSTEMS 27 3 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation I. SUMMAR Y vzbv welcomes that the European Commission proposes a regulation laying down har- monised rules on artificial intelligence (AI) in the (Artificial Intelligence Act (AIA)). The AIA must mitigate AI -related risks for consumers. To achieve this goal and accomplish the European Commission s stated objective of a trusted AI , vzbv recommends to in- crease the focus on consumers of the AIA and to strengthen the possibilities for inde- pendent assessments of high -risk AI systems. Consumers must receive individualised explanation s AI systems must be transparent and comprehensible in order to enable sovereign con- sumer decisions. Regrettab ly, the draft AIA provides no transparency towards consum- ers beyond a labelling obligation for three types of AI systems (Art . . The AIA must contain a provision mandating providers or users of high -risk AI sys- tems to inform consumers and explain the result of the individual case in a compre- hensible, relevant and concrete manner (upon their request). Such information rights are central for consumers to be able to understand and individually review an AI sys- tem s decision. Only then can consumers can exercise their rights. This must include information on the input data on the basis of which an AI application made/prepared a decision about the individual, the logic of the model , the criteria against which the AI system optimises, measures of fairness /bias , robustness , and accuracy as well as the purpose of the use of the AI system. The general public need s information Trust in AI systems can only emerge on the basis of an informed public debate and an assessment of the risks and opportunities of these syst ems. Providers of high -risk AI systems must provide the public with meaningful information that is relevant for an in- formed debate and understanding of an AI system. It should include for example infor- mation on the characteristics, capabilities and limitat ions of performance of the system as well as information on human oversight, corresponding to the information obligations towards professional users (Art . . Complement the list of h igh-risk AI systems The current AIA proposal focuses on AI -risks related to (product -)safety, health and fundamental rights linked to the use of AI systems. It thereby neglects that AI systems can cause significant economic/financial welfare losses for consumers or lead to viola- tions of consumer rights. AI systems operat ing in the following areas must be included in the list of high -risk applications in Annex III as they can cause serious economic/financial harm to con- sumers or severely violate consumer rights: AI systems intended to be used in the area of insurances , consumer -facing AI applications for financial investment or portfolio management , payment and debt collection . Also for scoring and profiling of consum- ers when they determine consumers access to services or markets and AI systems de- termining consumers access to the housing market should count as high-risk. Future high -risk applications: Consider consumers rights and economic risks To make the AIA future proof , legislators must be able to add new high -risk AI sys- tems from other areas of application than thos e already listed in Annex III . When up- Artificial intelligence needs real world regulation 4 l 27 Verbraucherzentrale Bundesverband e.V. dating the list of high -risk AI applications in Annex III legislators can only add AI sys- tems that pose a risk of harm to the health and safety, or a risk of adverse impact on fundamental rights (Art. 7 par. (b)). To ensure the AIA is future proof, legislators must be able to declare an AI systems as high -risk when it poses significant risks with respect to the violation of consumer rights , as well as social and economic harms for individual s and (social) groups. Prohibit dark patterns and exploitation of consumers vulnerabilities Art. 5 par. 1 (a) should in general prohibit that AI systems exploit so -called dark pat- terns by presenting end user choices in a non -neutral manner, or by otherwise sub- verting or impai ring user autonomy, decision -making, or choice via the structure, func- tion or manner of operation of a user interface or a part thereof. Also Art. 5 par. 1 (b) should in general prohibit AI systems from exploiting vulnerabilities of consumers . It should no t be limited to young, old and persons with disabilities , but include all con- sumers. Every person can find itself in very vulnerable positions temporarily . AI sys- tems must not exploit vulnerabilities caused by emotional distress, exhaustion, tired- ness, gri ef, sorrow, physical pain or the influence of medication. The AIA should not require intentionality as a precondition for prohibiting dark pat- terns or for exploiting weaknesses and vulnerabilities of consumers . It is near to im- possible to prove that provid ers of AI systems intended harm. Also, the harm caused by these systems should not be limited to physical or psychological harm, but also in- clude socio -economic welfare losses , violations of fundamental rights (e.g. dis- crimination) and consumer rights (e.g. deception). Prohibit general social scoring by private entities Art. 5 par. 1 (c) bans general scoring by public authorities under certain circumstances. General social scoring undertaken by private entities can also have a large negative im- pact on indiv iduals or entire groups. It can lead to unjustified exclusion of consumers from entire markets or services, discrimination, economic and financial harm. The AIA must prohibit that private entities use social scoring for the evaluation or classifica- tion of people s trustworthiness based on their social behaviour or to predict personal or personality characteristics. Prohibit remote biometric identification in public spaces by private entities The use of biometric identification systems in publicly accessibl e spaces can cause sig- nificant harm to consumers, including severe violations of the right to privacy and of their autonomy. Examples of potentially harmful applications include smart glasses, augmented reality applications on mobile phone s or analysis of shopping centres sur- veillance camera footage. The AIA must prohibit the use of biometric identification systems in publicly accessible spaces by private entities (not only public authorities). The ban must include real -time as well as retrospective bio metric identification. Prohibit emotion recognition system by private entities The AIA does not protect consumers effectively from private entities using emotion recognition systems. Companies can exploit these to decei ve and manipulate users, to undermine or subvert consumer autonomy and decision -making , using automated recognition and analysis of human features, facial expressions, voice, keystrokes and other biometric or behavioural signals. Art. 5 must ban the use of AI-based emotion recognition systems and the analysis of consumers emotions by private entities . Exception from the ban should be granted for specifically defined purposes that are to 5 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation the clear and proven benefit of the consumer (such as for medical or research pur- poses in the public intere st) in strict compliance with applicable data protection law and subject to appropriate safeguards. vzbv joins the EDPB s demand for a general ban on any use of AI for automated recognition of human features in publicly accessible spaces . Independent c onformity a ssessment for all high -risk AI systems Legislators must not leave the conformity assessment of high -risk AI systems to the AI providers self -assessment. Only independent audits can create consumer s trust and foster the acceptance of AI in gene ral. All high -risk applications must be subject to in- dependently verified conformity assessments as laid out in Annex VII when the AI system is brought to market for the first time . Also the AIA should demand independ- ent conformity assessment in case of we ll funded indications that high -risk AI systems are not in conformity with the AIA s requirements (e.g. when the system has been changed, or is employed in another context). Complement enforcement with independent assessments on request of civil society or ganisations Limiting market surveillance to public authorities and institutions (Art. 64 par. is not sufficient . Civil society organisations must have the right to request audits of high-risk AI systems by notified bodies when there are reasonable indic ations that the high-risk AI system violates European or Member States legislation, has a significant negative impact on the social, economic, physical or psychological wellbeing or the se- curity of persons or social groups, or poses significant environme ntal risks. Legislators must establish due process obligations for providers of high -risk AI sys- tems so that notified bodies, on the request of civil society organisations, can conduct independent audits. This must include obligations for providers to giv e auditors access to all data, documentation and records needed to assess the AI systems risks to the social, economic, physical or psychological wellbeing and security of persons or groups as well as its potential environmental impact. The notified body must publish the find- ings of the audit in a report . Ensure private enforcement of the AIA Consumers greatly benefit when consumer org anizations enforce their rights comple- mentary to enforcement by competent authorities . To ensure that consumer organiza- tions can enforce the AIA provision in courts legislators must add the AIA to Annex I of the European Directive on representative actions for the protection of the collective interests of consumers ((EU) 2020/. Trade agreements must not hinder an effecti ve transparency and monitoring of AI systems Current EU trade negotiations might restrict the E urope Union s ability to regulate in the field of AI in the future, in particular with regard to independent assessments and au- dits. Legislators must enact trade rules that do not impede on future AIA rules . Artificial intelligence needs real world regulation 6 l 27 Verbraucherzentrale Bundesverband e.V. II. INTRODUCTION This statement provides the Federation of German Consumer Organisations (Verbraucherzentrale Bundesverband - vzbv) feedback to the European Commission s proposal for a regulation laying down harm onised rules on artificial intelligence (Artificial Intelligence Act (AIA)).1 vzbv welcomes the opportunity to comment on the European Commission s proposal, as artificial intelligence (AI) increasingly shapes consumer markets and our societies. AI systems in consumer markets undoubtedly benefit consumers in some areas . This is for instance the case when improving personalis ation of services and thus increasing convenience for instance in e-commerce , individualis ing health care , auto mation of ve- hicles and providing driver support systems in cars . On the other hand, obscure AI also increases risks for consumers, for example, by enabling undertakings to exploit per- sonal vulnerabilities or infere with consumer preferences , by enabling discriminatory practices, unfair treatment or violations of people s privacy. The AIA must strike a bal- ance to mitigate these AI-related risks so consumers can reap its benefits. To achieve this goal and accomplish the European Commission s stated objective of a trusted AI , vzbv recommend s to increase the focus on consumers of the AIA and to strengthen the possibilities for independent asse ssments of high -risk AI systems. ____________________________________________________________ _______________________________ 1 European Co mmission: Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts (COM( 206 final) (hereafter AIA ) (202 , URL: -lex.europa.eu/legal - content/EN/TXT/?qid=1623335154975&uri=CELEX%3A52021PC0206 [Access: 2021]. 7 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation III. PROPOSALS FOR INCREASING THE AIA S CONSUMER FOCUS DEFINITIONS 1 Article 23 ( Artificial intelligence system vzbv welcomes t he broad definition for artificial i ntelligence system (AI system) in Arti- cle 3 ( and the list in A nnex I. Annex I lists a range of algorithm -based decision mak- ing Systems (ADM) and their underlying techniques. These are embedded in a wide range of AI-driven applications where they prepare or take decisions about consumers or on their behalf . These in turn can have a substantial (negative) impact on peoples lives in various contexts. The d efinition of AI systems as provided in Article 3 ( in conjunction with the list of ap- proaches and techniques in Annex I is appropriate, and not too broad. It corresponds to the definition of ADM used in the scientific community2 as systems that prepare or even make decisions over the treatment of cons umers. It is also in line with the recommen- dations of the Data Ethics Commission for the German Federal Government3 which re- fers to algorithmic systems . The actual goal of the AIA is r egulating and protecting consumers from harms caused by such s ystems . The broad definition of AI ensures that the AIA will be future proof. Restricting AI -rules on a narrow set of machine learning systems misses the point. A narrow definition of AI systems risks outdating the AIA soon, when new AI techniques emerge, as new forms of fast development of AI techniques in the past have demonstrated .4 European legislators should keep the broad definition of AI systems laid out in Ar- ticle 3 ( and Annex I . They reflect algorithm -based decision -making systems that underlie many c ritical AI systems that prepare or make vital decisions on consumers. 2 Artic le 3 ( User Article 3 ( defines the user of an AI system as professional users only. Art. 3 lacks a definition for the non -professional user, e.g. people using AI driven h ealth application giving them health advi ce5, or consumers affected by AI -driven decisions of systems employed by professional users. The omission illustrates the lack of consumer focus that runs throughout the whole AIA draft. ____________________________________________________________ _______________________________ 2 See: Kilian Vie th and Ben Wagner , Teilhabe, Ausgerechnet - Wie Algorithmische Prozesse Teilhabechancen Beeinflussen K nnen , 2017 < -stiftung.de/de/publikationen/publikation/did/teilhabe - ausgerechnet>. 3 Data Ethics Commission. p. 59 -62, p. 4 For example , see the tremendous advancements of neural netw ork-based AI systems over the past decade: Pitchford, Daniel. Forbes: A Decade Of Advancements As We Enter A New Age Of AI (, URL: -decade -of-advancements -as-we-enter -a-new-age-of- ai/?sh=5c65d3cb4055 [Access: 2021]. 5 Ada Health GmbH: Ada Website, URL: [Access: 2021]. Artificial intelligence needs real world regulation 8 l 27 Verbraucherzentrale Bundesverband e.V. Legislators must complement Article 3 with a definition for non-professional user s of AI systems . This must entail p eople using AI systems in their capacity as con- sumers and citize ns. It must also consider consumers who are affected by AI sys- tems employed by professional users . 3 Article 3 (3 Emotion recognition system The definition of emotion recognition systems in Art. 3 ( is too narrow. The defini- tion relies on the definition of biometric data as defined in Art. 3 (33 ), which itself is taken over from the General Data Prot ection Regulation (GDPR) .6 It holds that bio- metric data must allow or confirm the unique identification of that natural person. As a consequence emotion recognition systems that do not rely on data allowing the unique identification of a natural person , will fall out of the scope of the AIA. However, vzbv holds that these types of systems should also fall under the AIA s scope. This could include systems that rely only on the analysis of clicking, typing and cursor movement data for example. Also, for an AI system supporting a retail salesperson in a shop , it is not important to know th e identity of a potential customer entering the shop. The AI system can provide the shop personnel with valuable real time personality/emo- tion analysis data, based on the customer behaviour. For example inferences from measures on the relative tone/height, rhythm, and the speed of a voice, but not the voice itself. The definition of emotion recognition systems in Art. 3 ( should not refer to bio- metric data but to pers onal data . Otherwise , there is a significant risk for circumven- tion of the legislation . THE SCOPE IS TOO NARROW PART I: NEGLECT OF ECONOMIC H ARMS AND VIOLATION S OF CONSUMER RIGHTS In general, the s cope of the proposed AIA is too narrow and the legislation does not fo- cus on consumers . The E uropean Commission s proposal f ocus es on problems of (product -)safety, health and fundamental rights linked to the use of AI systems . It mostly deals with high risks to people in their capacity as citizens and employees , ne- glect ing that AI systems can lead to significant economic/financial welfare losses for consumers or to violations of consumers rights. 1 AI applications with large economic/financial impact or effects on consumer rights must be regarded as high -risk The European Commission s proposal sees high-risks of AI systems nearly exclusively in the areas of (product -)safety, health and fundamental rights. The draft AIA focuses on mitigating risks to people in their capacity as citize ns, patients, employees and stu- dents ( education ). However, m ost AI systems in these areas are already subject to European legislation . Therefore, in practices, it can be doubted that consumers will benefit much from the draft AIA in these areas . ____________________________________________________________ _______________________________ 6 Compare Art. 4 (: GDPR: European Parliament: EU General Data Pro tection Regulation (GDPR) Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Direc tive 95/46/EC (General Data Protection Regulation, OJ L 119, 2016, 9 l 27 Verbraucherzentrale Bundesverband e.V. Artificial int elligence needs real world regulation The proposed AIA widely neglects consumer rights outside these areas . In particular , AI systems employed by private entities that c an lead to significant economic and finan- cial welfare losses in consumer market s are out of the draft AIA s focus. Credit scoring is the only classic consumer marke t high -risk AI application listed in Annex III and typ- ically provided by private companies .7 In vzbv s view, t he AIA must include more areas of applications whe re AI systems are employed by private entities and can violate consumer right s or cause signifi cant eco- nomic and financial welfare losses. Legislators must classify some applications that fall into these areas of application as high -risk systems. For others it will be sufficient to as- sign them to a to-be-defined medium risk category, with appropriat e requirements and obligations. In this respect , vzbv regards Art. 52 as insufficient as a medium -risk cate- gory, as it merely entails labelling obligation s for three selected areas of application . The European Commission framed the AIA as a legislation f ostering a trusted AI . Un- fortunately, neglecting a wide range of AI applications that can cause significant ec o- nomic and financial harm for consumers or violations of consumer rights will further un- dermine peoples low trust in this technology as recent surveys suggest.8 AI-systems used in the area of insurances are the most obvious example for AI sys- tems that should be added to the list of high -risk AI systems . With the increasing spread of telematics insurance schemes in car insurance but also individua l behaviour - based tariffs for life insurance (e.g. the Vitality programme at Generali), a new quality in the structure of insurance relationships is reached. For the first time, data about the in- dividual behaviour of consumers is monitored and included in the pricing of net insur- ance premiums.9 AI-driven individualised behavioural tariffs, e.g. in telematics -based motor insurance or in health insurance , are likely to become much more widespread and transform the entire insurance industry . Other areas of app lication include AI-based risk categorisation of consumers for individually determining insurance premiums for li- ability and household content insurances10 and behaviour -based bonus calculations for ____________________________________________________________ _______________________________ 7 Other areas AI-applications in in Annex III include infrastructure (Annex III ) or education (Annex III ). However, these are regularly provided by public institutions or highly regulated (e.g. emergency services). 8 T V-Verband: Verbraucher wollen Sicherheit und Transparenz bei K nstlicher Intelligenz (, URL: -verband.de/IG -NB/vdtuev -startseite/news/ki -studie?context=e3068ebc9b4940b 0b56ad4576ca633bd [Access: 2021]; BEUC: Artificial Intelligence: what consumers say - Findings and policy recommendations of a multi -country survey on AI (, URL: -consumers -see-potential -artificial - intellig ence -raise -serious -concerns/html [Access: 2021]. 9 Compare: Generali Vitality GmbH: Generali Vitality (, URL: -our-program/ [Access: 2021]. 10 Taulli, Tom: Lemonade IPO Shows The Power Of AI (Artificial Intelligence) (, URL: -ipo-shows -the-power -of-ai-artificial - intelligence/?sh=152fd0f83aeb [Access: 2021]; Lemonade: Lemonade Cont ents & Personal Liability Insurance | Protect The Stuff You Love, URL: [Access: 2021]. , Artificial intelligence needs real world regulation 10 l 27 Verbraucherzentrale Bundesverband e.V. life insurance11, AI-based claims handling for car -12, liabi lity and household content in- surances13. Insurers can already use in dividual behavioural -based bonus programs as vehicle to price particular consumer groups out of the market , thereby undermining the principle of solidarity.14 Thereby AI can lead to signific ant unjustified treatment, discrimi- nation and financial harm for individuals or groups of consumers.15 Another area in which AI systems can obviously lead to significant financial harm for consumers are consumer -facing AI applications intended to be used a utomated finan- cial investment or portfolio management.16 AI-driven payment and debt collection services are another area of concern. The online shopping boom during the corona pandemic led to an increasing number of consumer complaints that revealed underl ying problems of these services17: A large provider raised vzbv s attention as he reject s consumers money transfers allegedly, because the stated purpose for the transaction does not exactly correspond to its specifications. The rea son for th e rejection is presumably a fully automated process. Consequently, the provider uses a debt collection agency to handle the case and charges the con- sumer for the additional costs. The rejection of payments and the corresponding addi- tional costs are obviously unjustified, as the provider can nonetheless assign the pay- ment to the right consumers: he informs them that their transfer was rejected. vzbv, in line with the German Data Ethics Commission (DEK) and the academic com- munity, points out that it is not sufficient to simply refer to GDPR when it comes to the protection of personal data in the context of AI: the GDPR s scope is limited and does for example not regulate profiling or scoring per se or the automated preparation of hu- man decisions. One core function of AI applications in consumer markets is the classifi- cation and prediction of user behaviour based on profiles/scores in order to prepare or make/prepare decisions about consumers. Furthermore, GDPR only covers personal data. However, AI applications increasingly rely on non -persona l data, also when pre- paring or t aking decisions about co nsumers, which leaves consumers unprotected. It ____________________________________________________________ _______________________________ 11 Generali Vitality GmbH (see FN. . 12 In the car insurance sector, AI is employed to exam ine photos of damages and to de cide on the coverage of the dam- age or repair costs.The Allianz Schaden Express App in Austria also automatically decid es on cases but us ually with a human in the loop Frankfurter Allgemeine Zeitung GmbH: Geld in 30 Sekunden?: Der vollautomatische Kfz - Sachverst ndige (, URL: tuell/finanzen/meine -finanzen/versichern -und- schuetzen/kuenstliche -intelligenz -in-der-kfz-versicherung -eine-revolution -html [Access: 2021]. See also: SVRV - Advisory Council for Consumer Affairs: Consumer -friendly scoring. Report of the Advisory Council for Consumer Affairs (, URL: -verbraucherfragen.de/en/ [Access: 2021]; Insurance Journal: Tokio Marine Uses Tractable's Artificial Intelligence Solution for Auto Claims in Japan (, URL: [Access: 2021] 13 Lemonade: How Lemonade's Tech -Powered Claims Work | Lemonade Insurance (, URL: [Access: 2021]. 14 Another subject for discussion is whether bonus programmes are used as a vehicle for indirect risk selectivity if those who are healthy anyway and those who are health -conscious are the main beneficiaries. The survey shows that some health insurance funds deli berately set out to appeal to health -conscious individuals. This may be interpreted as an attempt to recruit and retain the youngest and healthiest possible clientele . , SVRV - Advisory Council for Consumer Affairs ( (see FN. , p. 15 The report of the Adviso ry Council for Consumer Affairs at Germany s Federal Ministry of Justice and Consumer Protection cites an insurer warning: Behavioural tariffs may result in individual groups of insured persons ex ploiting them at the expense of people whose illnesses are not lifestyle -related. We therefore take a very critical view of these tariffs. ebd. p. 86 16 Compare: Frankenfield, Jake: What Is a Robo -Advisor? in: Investopedia (, URL: -roboadviser.asp [Access: 2021]. 17 Verbraucherzentrale Bundesverband: Beschwerden zu digitalen Bezahldiensten nehmen zu (, URL: -zu-digitalen -bezahldiensten -nehmen -zu [Access: 2021]. 11 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation also becomes increasingly difficult to clearly distinguish between personal and non -per- sonal data. This underpins the urgent need to supplement GDPR with specific rules on profiling/scoring and automated prep aration of human decisions. The AIA is a legisla- tion well suited to define minimum legal requirements for profiling and scoring . AI systems intended to be used in the following areas must be included in the list of high-risk applications in Annex III, bec ause they can a) cause serious economic/fi- nancial harm to consumers or severely violate consumer rights: - AI systems intended to be used in the area of insurances including but not lim- ited to AI systems employed to determine individual behaviour -based insurance premiums and rates, risk categorisation of consumers and handling of insurance claims. - Consumer -facing AI -based applications intended to be used in the area of auto- mated financial investment or portfolio management . - AI-based applications intended to be used in the area of payment and debt col- lection services. - AI systems intended to be used for scoring and profiling of consumers when the scores and profiles are used to determine consumers access to services or markets. The AIA must regulate th ese AI systems for scoring and profiling of consumers as such and not just decisions based on it, as defined in GDPR. - AI systems intended to be used to determine access to the housing market in- cluding but not limited to (pre -)selection of and assigning score values to poten- tial tenants and buyers (if they are consumers, not professionals or legal per- sons).18 - AI systems intended to be used to determine access to social security ser- vices , including reimbursements. 2 Future proofing AIA: Considering consumers rights and economic risks when determining the updating modalities of the list of high -risk applications Art 7 par. (a) determines that updating the list of high -risk AI systems in Annex III is limited to adding new AI applications within the existing eight areas in Annex III (para- graphs 1 -. Consequently, when updating the list in the future, the European Commis- sion cannot complement the list with high -risk AI applications from other areas of appli- cations . This focus on the present state of AI develop ment endangers the application of this legislation in the future : In the years to come , potentially harmful AI applications could emerge outside the areas listed in Annex III. These would then not fall under the scope of the draft AIA. To make the AIA future proof, legislators must ensur e that they can add new high- risk AI systems as well as areas of application in Annex III outside those eight areas listed in Annex III . Therefore Art. 7 par. (a) must be deleted. Art. 7 par. (b) holds that when updat ing the list of high -risk AI applications in Annex III legislators only take AI systems into account that pose a risk of harm to the health ____________________________________________________________ _______________________________ 18 Discrimination risks in the housing market are various. One example is AI -driven advertisement, potentially discrimi- natory along racial or religious lines: U.S. Department of Housing and Urban Development: HUD Charges Facebook with Housing Discriminat ion Over Company's Targeted Advertising Practices (, URL: [Access: 2019]. Artificial intelligence needs real world regulation 12 l 27 Verbraucherzentrale Bundesverband e.V. and safety, or a risk of adverse impact on fundamental rights . There is no reason for not considering violations of consumer rights and economic and financial harms when updating the list in Annex III . These risks can also have significant and long-term nega- tive impact on individuals, groups and their social and economic welfare. AI-based (personality ) profiling of co nsumers can lead to the exploitatio n of individual vulnerabilities and the unjustified treatment of persons and groups , leading to economic welfare losses . Examples for AI systems likely leading to unjustified discriminations in- clude AI systems developed t o determine consumers access to markets (e.g. the de- nial to use platforms or services like AirBnB19). Another are a of concern are AI systems leading to higher prices in form of unjustified high insurance premiums20 or rejection of or insurance claims on the basis of faulty AI -driven lie detectors Art. 7 par. (b) must be complemented so that del egated acts, updating the list of high -risk AI systems in Annex III , also take into account the risk of violation of consumer rights and social and economic har ms for individua l persons and (so- cial) groups. This will make the AIA future proof, as future AI applications might emerge in new areas of life and significantly affect social and economic welfare of individuals and groups. THE SCOPE IS TOO NARROW PART II: LIST OF PROHIBITED AI The proposed prohibited practices in Art 5 par. 1 (a) , (b), (c) and (d) are too narrow in scope. They leave consumers unprotected from potential harm in various areas . Poten- tial h arms to consumers include deception, manipulation und subversion of consumer decisions and their autonomy , leading to economic harm and discrimination as well as potentially substantial violations of privacy. 1 Art. 5 par. 1 ( a) Prohibition of Dark Patterns Art. 5 par. 1 (b) aims at prohibiting AI systems ex ploiting so-called dark patterns22 to the detriment of consumers. It is well justified and urgent that the AIA addresses the harms caused by dark patterns . Dark pattern refers to unfairly subverting or impairing user autonomy, decision -making, or choice v ia the structure, function or manner of operation a user interface or a part thereof. Dark pattern -tactics are well -documented and widely used to manipulate users causing financial harm. For example, when they are used to ____________________________________________________________ _______________________________ 19 See for example Business Insider: Airbnb has patented software that digs through social media to root out people who display narcissism or psychopathy (, URL: -software - predicts -if-guests -are-psychopaths -patent -2020 -1/?r=US&IR=T [Access: 2020]. 20 Compare: McKinsey & Company: Insurance 2030 - The impact of AI on the future of insurance (, URL: -services/ our-insights/insurance -2030 -the-impact -of-ai-on-the-future -of- insurance [Access: 2021]; SVRV - Advisory Council for Consumer Affairs ( (see FN. . 21Quach, Katyanna: Insurance startup backtracks on running videos of claimants through AI lie detector (, URL: https:/ / [Access: 2021] Bittle, Jake: Lie detectors have always been suspect. AI has m ade the problem worse., URL: -lie-detectors -polygraph -silent -talker -iborderctrl -converus - neuroid/ [Access: 2021] 22 Martini, Mario u. a.: Dark Patterns 01 (, in: ZfDR - Zeitschrift f r Digitalisierung und Recht, H. 1, URL: -document -library/zfdr_heft_20 21-pdf [Access: 2021]. 13 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation keep them from unsubscribing from services23 or to push gamers during a video game to in-game purchases.24 About 70% of gamers spent money on in -game purchases. A survey found that 20% of them spent money without realising that the purchased items would not give them any [..] advantage. 25 AI systems can be exploited to fine tune and personalise manipulative to incentives for continuous in -game spending. Given that in Germany alone gamers spent almost 8 billion on in -game purchases in pre-pan- demic 2018, the economic welfare losses to European consumers caused by dark pat- terns via in -game purchases can be expected to be significant.26 Art. 5 par. 1 (a) mi ght be well-intended, unfortunately, it will not address many of the most harmful dark patterns as its scope is far too narrow. It prohibits AI systems that deploy subliminal techniques beyond a person s consciousness only if they intention- ally ( in orde r to ) distort a person s behaviour in a manner that causes or is likely to cause harm. It is near to impossible to prove that providers of AI systems intended harm, as they will hardly admit to this 27 (e.g. when software for stalking is disguised and marketed as child tracking software28). Also In real life, harm can accumulate without a single event tripping a threshold of seriousness, leaving it difficult to prove. These cu- mulative harms are reinforced over time by their impact on individuals enviro nments [ ] Consequently , Art. 5 par. 1 (a) will probably , in practice, not be appli cable and enforce- able at all . Art. 5 par. 1 (a) is limited to dark patterns that cause physical or psychological harm. But AI systems, by exploiting dark patterns , can cause other significant harms as well, e.g. financial loss, violation of privacy and violation of consumers rights . Consumers should be protect ed from these kinds of harm as well . The AIA should in general prohibit that AI systems exploit dark patterns by pre- senting end user choices in a non -neutral manner, or by otherwis e subverting or im- pairing us er autonomy, decision -making, or choice via the structure, function or man- ner of operation of a user interface or a part thereof. ____________________________________________________________ _______________________________ 23 For example, the recent Norwegian Consumer Counsel recent report shows how Amazon seems to deliberately ob- struct consumers who wish to unsubscribe from its Amazon Prime service. In the process of unsubscribing from Ama- zon Prime, the company manipulates consumers to continue using the service in what seems like a deliberate attempt to confuse and frustrate customers. See Forbrukerradet: Amazon manipulates customers to stay subscribed, URL: -in-english/amazon -manipulates -customers -to-stay-subscribed/ [Access: 2021] . 24 E.g. A [ ] federal lawsuit asserts that Electronic Arts unlawfully increases its sports games difficulty in order to in- duce gamers into paying the video game publisher addi tional money. See Sportico: Federal Law Suit: This Video Game is too Da mn Hard (, URL: -sports -its-in-the-game - / [Access: 2021]. 25 Taylor Wessing LLP: In -game purchases (, URL: d/article -ingame - purchases.html [Access: 2021]. 26 Germany Trade and Invest - Gesellschaft f r Au enwirtschaft und Standortmarketing mbH: Gaming Industry (, URL: -en/invest/industries/creative -industries/gaming -65554 [Access: 2021]. 27 Compare: Veale, Michael; Borgesius, Frederik Zuiderveen: Demystifying the Draft EU Artificial Intelligence Act, SocArXiv 2021, URL: https ://osf.io/preprints/socarxiv/38p5f/download?format=pdf. , p. 28 Harkin, Diarmaid; Molnar, Adam; Vowles, Erica: The commodification of mobile phone surveillance: An analysis of the consumer spyware industry 16 (, in: Crime, Media, Culture: An International Journal, H. 1, p. 33 60, URL: [Access: 2021]. 29 Veale, Mic hael; Borgesius, Frederik Zuiderveen (see FN. , p. Artificial intelligence needs real world regulation 14 l 27 Verbraucherzentrale Bundesverband e.V. Art. 5 par. 1 (a) should not require intentionality as a precondition for the prohibi- tion of dark patterns, as it is near to impossible to prove . The definition of harm should not be limited to physical or psychological harm, but also include socio -eco- nomic welfare losses, violations of fundamental rights (e.g. discrimination) and consumer rights (e.g. deception). 2 Art. 5 par. 1 (b) Prohibit ion of e xploiting weaknesses to influence behaviour Just like in Art. 5 par. 1 (a) the European C ommission might have good intentions with this article, but due to its narrow scope it will hardly provide any meaningful benefits for consumers in practi ce. Art. 5 par. 1 (b) suffers from the same flaw as Art. 5 par.1 (a): It prohibits AI systems exploiting people s weaknesses only if the exploitation happ ens intentionally ( in order to ) to materially distort the behaviour which leads to consumer harm . Again, t he inten- tion of the provider of the AI system will be near to impossible to prove in practi ce, even if it exist ed30. Consequently , meaningful applica tion of Art. 5 par. 1 (a) in practice must be doubted . Also, just like Art. 5 par. 1 (a), Art. 5 par. 1 (b) is limited to exploitations leading to ""physical or psychological"" harm. However, AI systems, by exploiting vulnerabilities , can cause other signifi cant harms as well, e.g. financial harms, violations of privacy and consumers rights. Consumers should be protect from these kinds of harms as well. Art 5 par. 1 (b) aims at prohibiting AI systems that exploit a person s or groups weak- nesses (""vulnerabilit ies""), weaknesses of children, the elderly, and the physically or mentally disabled . These groups are certainly particularly vulnerable and worthy of pro- tection. But the problem also exists beyond these groups for other consumers, too. Every person can be temporarily in a very vulnerabl e position : emotional ly, psychologi- cally or physical ly. For example, AI systems can exploit vulnerabilities caused by emo- tional distress, pressure, exhaustion , inattention, tiredness, grief, sorrow, mental agita- tion, physical pain and injuries or influence of medication or medical treatments . All consumers should be protected from AI system s exploiting these situational or tem- porary vulnerabilities . Especially when providers use marketing techniques exploiting consumers perso nal - if only temporary - weaknesses in order to overcharge consum- ers or sell items or services to consumers they would not buy otherwise .31 For example, A I systems can be used to personalise and further optimise current tech- niques for exploit ation vulne rable gamers.32 These include children, but also , problem- ____________________________________________________________ _______________________________ 30 ibid. 31 Kietzmann, Jan; Paschen, Jeannette; Treen, Emily: Artificial Intelligence in Advertising: How Marketers Can Leverage Artificial Intelligence Along the Consumer Journey 58 (, in: Journal of Advertisin g Research, H. 3, p. 263 267, URL: [Access: 2021]. 32 For example in -game purchases . See: Daniel L. King u. a.: Unfair play? Video games as exploitative monetized services: An examination of game patents from a consumer protection perspective 101 (, in: Computers in Human Behavior, p. 131 143, URL: [Access: 2021 ]. 15 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation atic adult gamers with a propensity for game addiction into in -game purchases : Sys- tems also pair in -game purchase offers with known triggers for an individual player or known triggers for similar pla yers. 33 True, also a human salesperson can try to spot customers weaknesses and exploit them. But , in contrast to the human, AI -driven marketing system s can draw on large amounts of granular data from various areas in real time to influence consumer deci- sion making : this includes data on individual consumers as well as behavioural patterns of others consumers . Providers can scale up AI systems and systematically exploit indi- viduals weaknesses and manipulate many consume rs individually34. Marketers already use AI systems for real time analysis of consumer behaviour to influence individual de- cision making along the entire value chain . This ranges from targeted advertisement, to individually curated content and personal rebates influencing consumers evaluatio n and purchase decision35. The trend for individualisation of AI -driven marketing will in- crease even further the existing imbalance of power and knowledge between consum- ers and providers. Examples for this AI-driven hyper -personalisation of marketing are already in use to- day are various : E-commerce, websites use AI-driven tools, like Prudsys36, to analyse user browsing behaviour in real time in order to offer them personalised prices in the form of personalised discounts . In order to push consumers to make purchases , e-Spirit provides AI-driven E -commerce tools to personalis e multiple sales channels layout, menu bars, display ed ads, pop -ups, text and CTAs (so-called Calls to action , meaning designs or phrases intended to prompt an immediate sale37). Personalisation can be based on each consumer s online behaviour or profile. The Canadian car insurer Ka- netix used integrate.AI38 systems to detect and t arget undecided consumers and in- creased convergence rate by 13%Firms can use these tools to exploit consum ers vulnerabilities like illnesses, exhaustion or other personal difficulties people are struggling with . This can lead to welfare losses when marketers use this technology to overcharg e consumers or manipulate them to make purchases they would not do oth erwise. The AIA should in general prohibit AI systems from exploiting weaknesses and vulnerabilities of consumers . This protection should not be limited to young, old and persons with disabilities , but include all consumers, even if their weaknesses or vulnerabilities are temporary or situational . ____________________________________________________________ _______________________________ 33 Markle, Tracy; Kennedy, Brett: In -Game Purchases: How Video Games Turn Players into Payers. in: Digital Media Treatment (, URL: -game -purchases/ [Access: 2021]. 34 Kietzmann, Jan; Paschen, Jeannette; Treen, Emily (see FN. . 35 Davenport, Thomas u. a.: How Artificial Intelligence Will Change the Future of Marketing 48 (, in: Journal of the Academy of Market ing Science, H. 1, p. 24 42, URL: -019-00696 - pdf [Access: 2021]; Kietzmann, Jan; Paschen, Jeannette; Treen, Emily (see FN. ; Prudsys: Price Optimization: Intelligent and Personalized Couponing (, URL: - optimization_promotion -pricing/ [Access: 2021]. 36 Prudsys (see FN. . 37 Call to action (marketing) - Wikipedia. in: Wikipedia (, URL: [Access: 2021]. 38 Integrate.ai (, URL: [Access: 2021]. 39 Adriano, Lyle: Kanetix leverages AI technology to optimize consumer experience (, URL: mag.com/ca/news/digital -age/kanetix -leverages -ai-technology -to-optimize -consumer - experience -aspx [Access: 2021]. Artificial intelligence needs real world regulation 16 l 27 Verbraucherzentrale Bundesverband e.V. Art. 5 par. 1 (b) should not require intentionality as a precondition for the prohibi- tion of dark patterns, as it is near to impossible to prove. The definition of harm should not be limited to physical or psy chological harm, but also include socio -eco- nomic welfare losses , violations of fundamental rights (e.g. discrimination) and violation of consumer rights (e.g. deception). 3 Art. 5 par. 1 (c) Prohibition of general social scoring by private entities vzbv we lcomes that Art . 5 par. 1 (c) prohibits public authorities from employing AI sys- tems for social scoring under certain conditions. The prohibition of social scoring for the evaluation or classification of people s trustworthiness based on their social behav iour or to predicted personal or person- ality characteristics under certain conditions should also include private entities and not be limited to scoring undertaken by public authorities . The prohibition of so- cial scoring by private entities should also be subject to the two conditions Art . 5 par. 1 (c) I, namely that the underlying data stems from unrelated social contexts. General social scoring can have a large negative impact when used by private entities. It can lead to unjustified exclusion of consume rs from entire markets or services ( or ser- vice levels), discrimination and economic, financial, and social harm to consumers or entire groups of consumers. For example, AI -driven prediction of personality traits from patterns of behaviour col- lected from sm artphone usage40 or analysis consumers voices (already used in HR contexts) , can allegedly reveal personality traits . AI developers provide systems for per- sonality analysis for marketing purposes in E-commerce41. A large share of firms say they are eager to employ AI for individually target ing consumers42. There are well-funded doubts about the reliability of some currently marketed AI -based systems for personality analysis43. Nonetheless, the AIA must regulate these systems, ____________________________________________________________ _______________________________ 40 Stachl, Clemens u. a.: Predicting personality from patterns of behavior collected with smartphones 117 (, in: Proceedings of the National Academy of Sciences, H. 30, p. 17680 17687, URL: E.g. A field of application includes psychological analysis based voice samples, used for HR recruiting context. How- ever, this techn ique can be applied in consumer contexts as well. Compare Precire: Precire f r M arketing und Sales - Zielgerichtete Kundenkommunikation (, URL: -und-sales/ [Access: 2021] see also Krahmer, Caroli n. Recrutainment Blog: Pers nlichkeitsprofil aus der Analyse von Sprache: Einfach nur creepy oder die Technologie von morgen? Interview mit Mario Reis von Precire und Britta Nollmann von RANDSTAD (, URL: nlichkeitsprofil -aus-der-analyse -von-sprache -einfach -nur-creepy - oder-die-technologie -von-morgen -interview -mit-mario -reis-von-psyware -und-britta -nollmann -von-randstad/ [Access: 2021]. 42 A salesforce study reveals the AI s potential for marketers to scale up personalized marketing tools in the area of per- sonalization improvement of lead genera tion, customer acquisition and upselling . Compare Salesforce: State of Marketing Report - Fifth Edition (, URL: -releases -fifth- edition -of-state -of-marketing -report -marketers -prioritize -ai-powered -personalization -and-emphasize -customer -trust/ [Access: 2021 ]. 43 Compare Thiel, Veronika: Sprachanalyse: Wunschdenken oder Wissenschaft? AlgorithmWatch (, URL: -hr/ [Access: 2021]. 17 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation in order to be future proof and to prevent discrimination and unfair treatment of con- sumers due to flawed inferences. Scoring can be useful for some narrowly defined specific purposes and when subject to strict rules, e.g. credit scoring. Even then it must be subject to strict rules, such a s transparency for consumers and scrutiny by independent experts (which is often not the case , as the Schufa c redit rating example illustrates44). Both conditions specified in Art 5 par. 1 (c) i and ii seem appropriate to capture the in- stances when social s coring by private entities becomes overly prone to misjudge- ments, unjustified outcomes and discrimination : When scoring can lead to detrimental or unfavourable treatment of consumers that is unjustified or disproportioned in social contexts that are unrela ted to the contexts in which the data was originally generated or collected. This applies for example to a patent for social scoring hold by AirBnB45: Its score aim s to predict consumers trustworthiness based on a variety of social me- dia/online data . This is likely to lead to unjustified discriminatory exclusion and mis- judgements of consumers in t he vacation hous ing market. Such misjudgements or dis- criminations can lead to large individual and aggregated financial and social harm for individu al consumers or entire social groups. 4 Art. 5 par. 1 (d) Prohibition of remote biometric identification in the public space by private entities The use of biometric identification systems in publicly accessible spaces can cause sig- nificant harm to consumers, including se vere violations of the right to privacy and of their autonomy. AI systems that could be used for biometric identification in publicly ac- cessible spaces could occur via smart glasses (e.g. by Facebook46) or mobile phone augmented reality applications that ca n be used to recognise objects in public spaces (e.g. Google Lens47 or Google maps48). These could theoretically also be used for in- stance to identify passengers in public transport. Other examples are AI systems em- bedded in cameras in shopping centres. Whet her biometric identification happens in real -time or retrospective often makes no difference with respect to the potential harm e.g. privacy violations or data breaches.49 The AIA should prohibit the use of biometric identification systems in publicly ac- cessible spaces by private entities (not only public authorities). The ban should in- clude real -time as well as retrospective biometric identification. ____________________________________________________________ _______________________________ 44 Compare: AlgorithmWatch: Blackbox Schufa: Auswertung von OpenSCHUFA ver ffentlicht -. in: AlgorithmWatch (, URL: -schufa -auswertung -von-openschufa -veroeffentlicht/ [Access: 2021]. 45 Booker beware: Airbnb can scan your online life to see if you re a suitable guest (, URL: -software -scan -online -life-suitable -guest -a4325551.html; Business Insider (see FN. . 46 Compare wearable.com: The best smartglasses and AR specs Snap, Amazon and more (, URL: -best-smartglasses -google -glass -and-the-rest [Access: 2021]. 47 Google: Google Lens search what you see (, URL: -GB/ [Access: 2021]. 48 Dass.: Augmented Reality (, URL: [Access: 2021]. 49 For Example Clearview amassed one of the largest -known repositories of pictures of people s faces a database of more than 3 billion images scraped without permission from places such as Facebook, Instagram, and LinkedIn. BuzzFeed: Surveillance Nation (, URL: -ai-local-police - facial -recognition [Access: 2021] . Similar biometric information could be collected from cctv footage or images captured by smart mobile devices in public places. Artificial intelligence needs real world regulation 18 l 27 Verbraucherzentrale Bundesverband e.V. The use of biometric identification systems by private entities falls under the GDPR, but it should never theless be banned outright by the AIA, as the risk s to fundamental rights are significant . vzbv joins the EDPB s50 demand that a ban must not be limited to biometric identifi- cation but that there should be a general ban on any use of AI for automated reco gnition of human features in publicly accessible spaces, such as recognition of faces, gait, fingerprints, DNA, voice, keystrokes and other biometric or behav- ioural signals, in any context. In addition to biometric identification and analysis, the collec tion and analysis of so - called metabolites 51 provides particular risks for consumers. It involves the analysis of particles that people leave behind/give off, for example sweat, dust, breath, etc. The analysis of metabolites can allow to draw conclusions about individual behaviour, con- sumption and habits , and is thus highly sensitive. Therefore, the collection and analysis should be covered by the scope of the AIA regulation. The AIA should ban the use of AI -based analysis of metabolites by private enti- ties in consumer facing markets, unless it is to the clear and proven benefit of the consumer (for example health applications in clin ical contexts). 5 Prohibition of emotion recognition system by private entities The proposed AIA considers AI -based emo tion detection tools a high risk only in the context of law enforcement (this includes polygraphs and similar tools or tools to detect the emotional state of a natural person (Annex III, (b)). Except for the labelling obligation (Art. which will probab ly not protect consumers effectively in practi ce, the draft AIA neglects the issue of emotion detection and analy- sis in consumer markets . The automated recognition of human features and expressions (e.g. of faces, mimic, gait, voice) keystrokes and other b iometric or behavioural signals by private entities leaves all consumers vulnerable to exploitation, deception and manipulation . Emotion recognition systems can severely harm consumer s in commercial contexts for several reasons: Biometric analysis by compa nies widens the existing asymmetry in infor- mation and power between consumers and companies . It greatly enhances the com- pany s position and power in negotiations with consumer s. Companies can exploit biometric emotion recognition to undermine or subvert c on- sumer autonomy and decision -making. This could be done for example by targeting ____________________________________________________________ _______________________________ 50 EDPS call for a general ban on any use of AI for automated recognition of human features in publicly accessible spaces, such as recognition of faces, gait, fingerprints, DNA, voice, keystrokes and other biometri c or behavioural sig- nals, in any context. : EDPB & EDPS: EDPB -EDPS Joint Opinion 5/2021 on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) (, URL: -work -tools/ our-documents/edpbedps -joint-opinion/edpb -edps -joint- opinion -52021 -proposal_en. 51 The Economist: Metabolites and you - People leave molecular wakes that may give away their secrets, in: The Economist, 19 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation them with personalised offers52, possibly exploiting current emotional states of consum- ers (like pain, sorrow, stress, fatigue) to increase the chance of overcharging for good s and services53. For example, ShelfPoint54 introduced real time emotion recognition of fa- cial expressions of customers in retail stores. It assess the shoppers emotional state as well as demographics like age, gender and ethnic background and ulitmately aims at personalisation of customer engagement via shelf displays increasing convergence55. Customer service or sales employees in the markets for high -value goods (e.g. cars) could exploit similar biometric emotion analysis to manipulate consumers and subver t their choices56. Insurance Start up lemonade already developed an AI-driven lie detec- tor . Consumers had to make their claim in form of video. Lemonade s AI would use fa- cial analysis of non-verbal cues to indicate fraudulent consumers claims . In respo nse critique, Lemonade claims it now uses its facial -recognition algorithms to prevent the same person from making multiple claims57. In principle, the use of such lie detectors could be employed in other areas as well: Imagine landlords interview ing potential tenants using of AI-driven remote lie detec- tors 58 to improve their negation position or put pressure on consumers. Art. 5 must be complemented with a provision that bans the use of AI-based emo- tion recognition systems and the analysis of consumers emotions by private enti- ties, except for clearly defined purposes (such as for medical or research purposes in the public interest) in strict compliance with applicable data protection law and subject to appropriate safeguards. Biometric emotion recognit ion of consumers should only be allowed if it is to the clear and proven benefit of the consumer . These include, for example, AI systems in health care or medical contexts, such as systems to monitor p atient s or elderly people in single households. Other us eful applications include systems designed to prevent physical harm from consumers , like AI systems that provide clear benefits i n related to health or security, like fraud detection. MORE TRANSPARENCY FOR CONSUMERS The proposal s mandatory transparency re quirements towards consumers merely in- clude the labelling of some AI applications (Art . Other than this, the draft AIA pro- vides no transparency for consumers, except a CE marking . ____________________________________________________________ _______________________________ 52 For example: Kanetix uses AI to categorise and target customers with incentives to buy insurance. Adriano, Lyle (see FN. . 53 See for example O'Shea, Dan: How re tailers can tell stories by reading emotions (, URL: -retailers -can-tell-stories -by-reading -emotions/542298/ [Access: 2021] ; An- other Use case for in -store AI systems: Einzelhandelslabor S dwestfalen: Ein Roboter als Kundenberater?, URL: de/praxisbeispiele/ein -roboter -als-kundenberater/ [Access: 2021]. 54 McManus, Ashley: Partner Spotlight: shel fPoint Retail Solution Adapts to Shopper Emotions. in: Affectiva (, URL: -spotlight -shelfpoint -retail -solution -adapts -to-shopper -emotions [Access: 2021]. 55 Levine, Barry: Cloverleaf s new grocery shelf displays watch shoppers, track their emotions (, URL: -new-grocery-shelf -displays -know -whether -youre -happy/ [Access: 2021]. 56 Davenport, Thomas, et al. (see FN. . 57 See: Quach, Katyann a (see FN. . 58 Bittle, Jake (see FN. . Artificial intelligence needs real world regulation 20 l 27 Verbraucherzentrale Bundesverband e.V. Legislators must ensure that the AIA includes more requirements for tra nsparency to- wards consumers. Consumers must obtain the information necessary to make informed decisions and exercise their rights when necessary. They need to know about the risks and the reliability of an AI application, the data that is underpinning the decision and how a specific decision came about. Developers and operators of AI systems must ex- plain to consumers how their systems work to ensure traceability (and accountability). 1 Art 52 Labelling obligation The l abelling obligations in Art . 52 are go od in principle59. vzbv wants to point out the risk, that the proposed labelling obligations in Art. 52 could be circumvented. For exam- ple, the l abelling of AI systems interacting with persons or the labelling of emotion recognition systems could become ine ffective if the labelling is hidden or hardly recog- nisable. Also, vzbv points out that produce rs or provide rs of deep fake video/audios with malicious intends will probably not comply with the AIA s labelling obligation any- way. Therefore, an effective enfo rcement of Art. 52 is necessary. The challenge con- sists of striking a delicate balance between effective enforcement and the right to free- dom of expression. Legislators must avoid t he misconception of the EU copyright re- form that incentivises over-blocking of content by platforms .60 vzbv recognises that the proposed labelling obligations in Art. 52 are important. Leg- islators must ensure that the labelling obligations will not be circumvented . 2 Individual explanation for consumers The draft AIA includes a num ber of transparency obligations for providers of high -risk AI systems. These include transparency vis - -vis professional users of the system (Art. , supervisory authorities (Art and notified bodies within the context of conformity assessments (annex IV). Regrettably, there is no provision obliging providers or professional users of AI systems to provide meaningful information to consumers beyond a labelling obligation for a lim- ited set of AI applications (Art . The AIA must contain a provision m andating providers of high -risk AI systems to inform consumers and explain the result of the individual case in a comprehensible, relevant and concrete manner (upon their request). (In contrast to the general duty to inform under the GDPR, where the functi oning of an AI or algorithmic system is explained in general terms). The information must be provided in a comprehensible, relevant and concrete man- ner and include: - The input data on the basis of which an AI application made/prepared a deci- sion about the individual. The data must be provided in plain language and a commonly used and machine -readable format. That i nformation must also cover ____________________________________________________________ _______________________________ 59 85% of German consumers say A I systems should be labelled, see: T V-Verband: Sicherheit und K nstliche Intelligenz - Erwartungen, Hoffnungen, Emotionen (, URL: -verband.de/IG -NB/vdtuev - startseite/dok_view?oid=777991 [Access: 2021]. 60 Verbraucherzentrale Bundesverband: Nutzerrechte sind ein Must -Have - Stellungnahme zum Reg ierungsentwurf f r ein Gesetz zur Anpassung des Urheberrechts an die Erfordernisse des digitalen Binnenmarktes (, URL: -sind-ein-must -have [Access: 2021]. 21 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation the sources from wh ich the data has been obtained and where and by whom the data was originally collected. - Informatio n about the underlying logic of the model and the criteria against which the AI system optimises. - Measures to ensure the fairness /bias, robustness , and accuracy of outcomes of the final AI System. - The purpose and goal of the use of the AI system . Such inf ormation rights are central for consumers to be able to understand and individ- ually review an AI system s decision. Only then can consumers exercise their rights as laid down in the GDPR, for example and challenge a decision on a well -founded basis , for example, to defend themselves against discrimination or erroneous deci- sions.61 The draft AIA obliges providers of AI systems to generate much of this information in the context of the conformity assessment s anyway (e.g. data on robustness and fair- ness of the model etc.). Therefore, providers will incur no significant extra cost were they obliged to provide this information to consumers, too. This provision is explicitly not aiming at the disclosure of trade secrets but at addressing the legitimate right t o in- formation of consumers. Individual explanations of a high-risk AI application s decision on consumers also pro- vide large benefits for providers of AI systems: Transparency create s trust among consumers in AI -based systems. This increases consumers ac ceptance of the use of these systems in larger areas of life.62 Transparency also enable s consumers to check the accuracy of AI-based decisions when they can have a significant i mpact on them personally, e.g. by checking whether a decision about them is based on correct and up-to-date data. This feedback in turn can help improv ing the accuracy of AI systems outcomes. Transparency enables consumers and citizens more widely to exercise their rights and challenge an AI -based decision on a well -founded basis , for example, to defend them- selves against discrimination or erroneous decisions. This benefits consumers directly. And consumers taking action against providers of faulty, discriminatory or illegal AI sys- tems provide important benefits to providers of high -quality systems: This exposes the black sheep in the market and redirects demand to wards providers who seriously in- vest in the quality and accuracy of their systems. 3 Information for the general public The implementation of high-risk AI systems can have broad and deep social and eco- nomic implications. For example, AI-based selection s of job applicants or the determi- nation of insurance premiums can have a profound social and economic imp act on the life of many people. With AI systems making or preparing s uch vital decisions , social ____________________________________________________________ _______________________________ 61 Compare: dass.: White Paper on Artificial Intelligence - Proposals o f vzbv (, URL: [Access: 2021]. 62 For consumers demand for transparency and independent audits of AI systems compare recent surveys by BEUC (see FN. ; T V -Verband (see FN. . Artificial inte lligence needs real world regulation 22 l 27 Verbraucherzentrale Bundesverband e.V. trust in AI can only emerge on the basis of an informed public debate and an assess- ment of the risks and opportunities of these systems. This will in turn encourage uptake and dissemination of AI technolog ies. The information on high-risk AI systems pub- lished in the planned EU database for stand -alone high -risk AI systems (Art. is too superficial to fulfill this purpose63. An informed public debate can also serve as guidance for policy -makers regarding the ethical and social i mplications when deciding about the rules for AI systems in specific sectors or areas of application. Provider s of high -risk AI systems must provide the public with information that is relevant for an informed debate and understanding of an AI system. Thi s must entail the information spe cified in Art 13 par. (b) (characteristics, capabilities and limita- tions of performance) and Art 13 par. (d) (human oversight) . The information must be provided in a comprehensible manner and include: - General inform ation about the training data and the input data (e.g. the catego- ries of data). - General information about the logic/ methodology of the model . - Measures of fairness/bias , for the training data and the input/output data (e.g. with respect to gender, ethnicit y and other possible grounds of prohibited dis- crimination). - Information and measures on the robustness and accuracy of the model. Providers must generate this information anyway to ensure compliance with Art Therefore, this information can be provided at negligible costs. This provision is not aiming at the disclosure of trade secrets but at addressing the legitimate right to infor- mation of the general public. ____________________________________________________________ _______________________________ 63 For an overview of the transparency obligations in the draft AIA see Veale, Michael; Borgesius, Frederik Zuiderveen (see FN. . P.12 23 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation IV. PROPOSALS FOR ENSURIN G EFF EC- TIVE INDEPENDENT ASS ESSMENTS OF AI SYSTEMS ENSURING CONSUMER TRUST WITH INDEPENDE NT ASSES SMENTS Consu mers must be able to trust that all AI systems , especially the high -risk AI sys- tems, comply with all EU legislation when entering EU markets, or when providers change existing systems significantly or employ them in oth er contexts. Unfortunately, this is currently not the case. C onsumers mistrust AI systems, as a recent BEUC64 con- sumer survey on perceptions on AI in eight EU Member States shows: Although con- sumers are generally in favour of AI development, they have serio us concerns in rela- tion to AI systems . While consumers see benefits of AI, they have low trust in AI and its added value . This is displayed in concerns ranging from the lack of transparency, unintended consequences or the abuse of personal data. A majority of consumers strongly agree that companies use AI to manipulate consumer decisions (e.g. 64 % in Belgium, Italy, Portugal and Spain65 and even 71 % in Germany66) Most consumers think that current rules are not adequate to effectively regulate AI- based acti vities (50% in Sweden and 55% in Portugal). Around 56% of all EU consum- ers have low trust in authorities to exert effective control over AI .67 To foster consumers trust in AI , legislators must ensure that independent experts can audit all high -risk AI systems with respect to compliance with (all) EU legisla- tion and their potential negative impact on consumers. 1 Conformity assessment Legislators cannot leave the assessment of the complex impact of high-risk AI systems to the AI providers self-assessment. The draft AIA foresees harmonised standards as the basis for the conformity self -assessment. But these standards cannot capture the fine facets of the complex social and economic impact that design and data choices of AI systems have . For example, the social and economic implications and effects of the underlying data, how the data has been collected, interpreted or manipulated (e.g. ag- gregated , cleaned or combined with other data ) are highly complex . It requires inter- pretation and scrutiny from different data and social sciences perspectives to reveal hidden discrimination of some consumer groups. Legislators must recognise that subtle and indirect forms of discrimination and potential harm to consumer s cannot be miti- gated by standards in the form a self -assessment. In the case of most high -risk AI systems, the draft AIA allows that the providers carry out the conformity assessment in the form a self -assessment .68 Provider s must ensure ____________________________________________________________ _______________________________ 64 BEUC, 2020, Artificial Intelligence: what consumers say: Findings and policy recommendat ions of a multi -country survey on AI, -x-2020 -078_artificial_intelligence_what_consumers_say_re- port.pdf [download 2020] 65 BEUC (see FN. . 66 T V-Verband (see FN. . 67 BEUC (see FN. . 68 The exceptions are AI systems for the real -time and post remote biometric identification of natural perso ns. Here the conformity assessment involves a notified body (see conformity assessment procedure in Annex VII) and the high - Artificial intelligence needs real world regulation 24 l 27 Verbraucherzentrale Bundesverband e.V. compliance with standards or by common specifications and verify these themselves . Consequently, consumers sh all trust that the respective AI system compl ies with the re- quirements for high -risk AI systems laid out in Title III, Chapter vzbv points out that self-assessment s does not create consumers trust, if they cannot be verified by independen t auditors . Certification mark s relying on self -assessment, like CE marking s, are susceptible to misuse and fraud. This is illustrated by cases where producers have affixed CE markings to products that do not fulfil the legal requir e- ments69, some of which eve n endangered the health and life of consumers70. Therefore it is well justified, that 85% of German consumers say that AI systems should only be brought to the market if the ir safety has been assessed by independent auditors. Only 17% say that self -assessments by the providers are sufficient. 71 Therefore, the AIA must not leave the conformity assessment of a high -risk system to the provider s of high-risk systems . Only independent checks and audits of high -risk AI systems can create consumer s trust and foster the acceptance of AI in general . All high -risk applications must be subject to independently verified conformity assessment s as laid out in Annex VII when a) the AI system is brought to market for the first time and b) in case there are well funded indications that the AI system is not in conformity with the requirements in Title III, Chapter 2 any more (e.g. when the system has been changed, or i s employed in another context). Relying on common specifications (Art is no t sufficient . Established s tandards should be the basis for a conformity assessment.72 2 Title VIII, Chapter 3 Enforcement must be complemented with independent assessments vzbv welcomes that t he draft AIA ensures that a uthorities may have access to data, documentation, etc. for monitoring purposes. Art. 64 also allows other public institutions to act as surveillance authorities: National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundament al rights [...] . This could for example include Germany s Federal Anti-Discrimination Agency. ____________________________________________________________ _______________________________ risk-AI systems listed in Annex II (e.g. for product safety, medical products etc.), which must adhere to the procedures and externa l controls laid out in the respective legislation. 69 See: Europe an Parliament: Answer Given by Mr Verheugen on Behalf of the Commission to Question No P -5938/07 (, URL: -6-2007 -5938 -ASW_EN.html?redirect [Access: 2021] and T V Rheinland: Why Manufacturers Lie About CE Marking (, URL: -manufacturers -lie-about -ce-marking [Access: 2021]. 70 A detailed article on hazards found due to poor -quality AC adapters: ""The good news for the consumer is that there appears to be a cheap charger for any make or model of mobile phone, toy or hand -held games consoles that you might require the bad news is that it could kill you!"" Buckinghamshire Trading Standards: What s in your socket? ( , URL: webarchive.nationalarchives.gov.uk/20140713175508/http%3A/ oof.pdf [Access: 2021]. 71 T V-Verband (see FN. . 72 DIN - Deutsches Institut f r Normung: Standards als zentraler Baustein der europ ischen KI -Regulierung (, URL: -und-seine -partner/presse/mitteilungen/standards -als-zentraler -baustein -der- europaeischen -ki-regulierung -800318 [Access: 2021]. 25 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation Civil society organisations have many competence s when it comes to the identif ication of potential risks of AI systems in their specific area of expertise. It would increase trust among consumers if civil society organisations could request and initiate independent assessment s by notified bodies on high-risk AI systems if there are reasonable indica- tions that they do not comply with EU legislation or violate consu mer rights . This right to request such assessments should be granted to civil society organisations who have the required expertise . They should include human rights - labour -, consumer - and envi- ronmental organisations. Such audits would provide an independ ent assessment of a high -risk AI system. They could include an assessment of the legality of the AI -System (e.g. with respect to dis- crimination or consumer rights ), their social , economic and environmental risks and benefits as well as their impact on the psychological and physical health of individual persons, social groups and society . Such independent assessments can be the basis for an informed public discussion on the risks and benefits of a high -risk AI System and serve as a watchdog . Independent auditors scrutinising high-risk AI systems on the initiative of civil society organisations will increase people s trust in the respective AI systems.73 Limiting market surveillance to public authorities and institutions as Art. 64 par. 3 is not sufficient . Civil society organisations must have the right to request au- dits of high -risk AI systems by notified bodies when there are reasonable indications that the high-risk AI system violates E uropean or Member States legislation , has a significant negative impact on the social, economic, physical or psychological well- being or the security of persons or social groups, or poses significant environmental risks. Legislators must complement the draft AIA with due process obligations for provid- ers of high -risk AI syste ms so that notified bodies, on the request of civil society or- ganisations , can conduct independent audits. This must include obligations to give the auditors access to all data, documentation and records ( as laid out in Art. 64 par. needed by the audito rs to assess the high -risk AI systems risks to the social, eco- nomic, physical or psychological wellbeing and security of persons or groups as well as its potential environmental impact . The notified body must publish the findings of the audit in a report . Policy -makers should establish safeguards, ensuring that confidential information is protected (for example via confidentiality agreements). However, policy -makers must ensure that providers of AI applications do not advance the protection of trade secre ts for with- holding information from scrutiny by the auditors or exclude it from the published re- port. ____________________________________________________________ _______________________________ 73 Consumers demand for independent scrutiny of AI systems see: BEUC (see FN. ; T V -Verband (see FN. . Artificial intelligence needs real world regulation 26 l 27 Verbraucherzentrale Bundesverband e.V. V. ENSURING PRIVATE ENF ORCEMENT Private enforcement of EU legislation complements the enforcement efforts by compe- tent authorities . Consumers greatly benefi t when consumer organisations enforce their rights in courts complementary to enforcement by competent authorities :74 Consumer organisations are well aware of the detriments consumers are facing in various mar- kets. Consumer organisations like vzbv can take proactive action. In doing so, they pre- vent consumer harm from occurring in the first place and avoid disputes as far as possi- ble. The European Commission points to the large number of injunction procedures in Germany and Austria which both traditionally rely on the private enforcement of con- sumer law initiated by the consumer and business organisations .75 As a result, one can expect a reduction in the number of infringements by companies, leading to a reduction in related consumer detriment .76 For example, more than half of vzbv s legal actions are successfully settled out of court with companies issuing cease -and-desist declarations.77 vzbv s successful procedures against Volkswagen resulted in a 830 million settlement for German consumers.78 Also, vzbv pro ceedings against Facebook79 illustrate the benefits of consumer organisations enforcing consumer law. In addition , enforcement of consumer rights by consumer organisations relieves the enforcement burden on competent authorities. It frees up authorities re sources allowing them to concentrate scarce resources on strategically important cases. To ensure that consumer organis ations can enforce the AIA provision s, legislators must add the AIA to Annex I of the European Directive on representative actions for the protection of the collective interests of consumers ((EU) 2020/ .80 ____________________________________________________________ _______________________________ 74 Verbraucherzentrale Bundesverband: Mehr Sammelklage wagen - Kurzpapier des vzbv (, URL: -sammelklage -wagen [Access: 2021] 75 European Commission: Report of the Fitness Check SWD(209, URL: -registe r/detail?ref=SWD(209&lang=en [Access: 2021]. 76 Ebd. p.103 77 Verbraucherzentrale Bundesverband: Brosch re: Recht durchsetzen, Verbraucher st rken (, URL: -recht -durchsetzen -verbraucher -staerken [Access: 2021]. 78 See: Volkswagen AG: European Directive on representative actions for the protection of the collective interests of consumers ((EU) 2020/ (20 , URL: -and-volkswagen - agree -on-a-fair-settlement -solution.html# [Access: 2021]; Verbraucherzentrale Bundesverband: vzbv -Klage gegen VW f hrt zu Deutschlands gr tem Massenvergleich (, URL: htt ps:// -klage - gegen -vw-fuehrt -zu-deutschlands -groesstem -massenvergleich [Access: 2021]. 79 Verbraucherzentrale Bundesverband: Facebook verst t gegen Datenschutzrecht - Kammergericht Berlin gibt Klage des vzbv in vielen Punkten statt (, URL: -verstoesst -gegen -datenschutzrecht [Access: 2021]. 80 European Parliament: Directive (EU) 2020/1828 of the European Parliament and of the Council of 25 Novem ber 2020 on representative actions for the protection of the collective interests of consumers and repealing Directive 2009/22/EC (, URL: -lex.europa.eu/legal -content/EN/TXT/?uri=CELEX:32020L1828 [Access: 2021]. 27 l 27 Verbraucherzentrale Bundesverband e.V. Artificial intelligence needs real world regulation VI. TRADE AGREEMENTS MUS T NOT HIN- DER AN EFFECTIVE TRA NSPARENCY AND MONITORING OF AI SYS TEMS vzbv wants to highlight the importance of consis tency of European AI -related policies. It is of particular importance to ensure the compatibility of the AIA with trade commit- ments to which the EU is binding itself by international law , especially as AI technolo- gies and the understanding of risks is still nascent and will likely be evolving in the years to come. A vzbv study81 recently found that current EU trade negotiation s might significantly restrict the EU s ability to regulate in the field of AI in the future , in particu- lar with regard to independent assessments and aud its. The study finds tha t trade rules could impede on future EU rules on transparency, certification and accountability. Po- tential rules on the non -disclosure of source code currently under discussion in the World Trade Organisation (WTO) would hinder effective transparency provi sions within the AIA. Legislators must enact trade rules that do not impede on future AIA rules on trans- parency, certification and accountability. Potential rules on the non -disclosure of source code currently under discussion in the World Trade Organisati on (WTO) must not hinder an effective transparency, en- forcement , monitoring and independent assessments of AI systems under the AIA. ____________________________________________________________ _______________________________ 81 Irion, Kristina: AI Regulation in the European Union and Trade Law: How Can Accountability of AI and a High Level of Consumer Protection Prevail over a Trade Discipline on Source Code? (, URL: _id=3786567 [Access: 2021].",en,"This must include obligations for providers to giv e auditors access to all data, documentation and records needed to assess the AI systems risks to the social, economic, physical or psychological wellbeing and security of persons or groups as well as its potential environmental impact. with respect to dis- crimination or consumer rights ), their social , economic and environmental risks and benefits as well as their impact on the psychological and physical health of individual persons, social groups and society . Civil society organisations must have the right to request au- dits of high -risk AI systems by notified bodies when there are reasonable indications that the high-risk AI system violates E uropean or Member States legislation , has a significant negative impact on the social, economic, physical or psychological well- being or the security of persons or social groups, or poses significant environmental risks. needed by the audito rs to assess the high -risk AI systems risks to the social, eco- nomic, physical or psychological wellbeing and security of persons or groups as well as its potential environmental impact .",risk
BIL Sweden (Sweden),F2665297,04 August 2021,Business association,Small (10 to 49 employees),Sweden,"1( BIL Sweden, Box 26173 SE -100 41 Stockholm, Sweden, Telephone +46 8 700 41 00 , Telefax +46 8 791 23 11 Bes ksadress: Storgatan 19 114 82 Stockholm VAT No. SE 556077- 4886, bil@bilsweden.se, 2021- 08-04 To: European Commission BIL Sweden s response to EUCOM s public consultation on Proposal for a regulation of the European Parliament and of the Council laying down harmoni sed rules on artificial intelligence (A rtificial Intelligence Act) and amending certain union legislative acts , 2021/0106 (COD) BIL Sweden is thankful for the opportunity by the Commission t o submit comments on the proposed AI Act. BIL Sweden s views BIL Sweden choose to limit our comments to th ose proposals that we predominately perceive concern vehicles. Here, b y ""vehicle s"" is meant th ose vehicles that our members manufacture, i.e. passenger cars, buses, trucks and certain types of work machines. BIL Sweden s members have also contributed to and support ACEA s response to EUCOM s public consultation on AI Act. Sectorial approach BIL Sweden welcomes the EU Commission's sectoral approach where the technical requirements for automotive products are integrated within existing frameworks for type approval of vehicles, EU regulations 2018/858 and 2019/2144, thereby avoiding duplication of governance mechanisms. However, the European Commission has taken the liberty of allowing itself to make changes through delegated acts, continuously. This approach makes it very difficult for the automotive industry to provide our views and comments . Long lead times Vehicles probably have a significantly longer development cycle than many other products that contain AI. When EU Regulation 2018/858 will be amended, when a future delegated act that includes AI will be adopted, an appropriate time frame / lead time for a phased implementation is needed for both new vehicle types and all vehicle types to meet the new requirements. Technology neutrality The technology development is very rapid , and the AI Act s ambition is to affect all sectors, thus it must be technology neutral. 2( BIL Sweden, Box 26173 SE -100 41 Stockholm, Sweden, Telephone +46 8 700 41 00 , Telefax +46 8 791 23 11 Bes ksadress: Storgatan 19 114 82 Stockholm VAT No. SE 556077- 4886, bil@bilsweden.se, Harmonis ation with UNECE The automotive industry is global. The Swedish vehicle manufacturers, as well as the European vehicle manufacturers, export their vehicles bot h to EU countries and to countries outside the EU. It is important that the AI Act does not create trade restrictions and/or reduce the competitiveness of European companies. BIL Sweden advocates that it s important that the AI Act is harmonized with the w ork carried out at UNECE level for vehicle regulation, for example with the working groups FRAV (Functional Requirements for Automated and Autonomous Vehicles) and VMAD (Validation Method for Automated Driving) with regard to automated vehicles. The AI Act in relation t o existing reg ulations concerning AI Before new EU -regulations are drawn up, existing regulations concerning the area of AI should be reviewed, to identify where there is overlap and what is not already covered, the ""white spots"". Often, it s easier and faster to propose additions/adjustments to existing regulations than to create a completely new one. There may also be sectors that have already regulated an aspect that concerns AI, where experience could be used by other sectors. Withi n the automotive industry, for example, there is a new legislation that regulates the right to introduce software updates, UNECE -R156 ""Software update and software update management system"", which may interest other sectors using software updates. The AI Act in relation to the GDPR ( and the forthcoming regulation on ePrivacy) etc. Vehicle manufacturers also see a significant risk that the extensive and highly set requirements set out in the proposal, in combination with other existing legislation such as t he GDPR (and also forthcoming regulation on ePrivacy), could lead to companies in some cases refraining from deploy AI and thus loses the positive effects, both for the users and the people ""exposed"" to AI. BIL Sweden's view is that this may affect the competitiveness and attractiveness of the EU vehicle manufacturers, its suppliers and society as a whole. It also appears that the AI Act, at least in part, overlaps with the framework for data protection, product s afety, discr imination and liability. In this consultation response, we have not immersed into this, but it needs to be reviewed to avoid double regulation and ambiguities. The definition of AI The AI Act introduces a very broad definition of AI. It would be gained by even greater clarity, primarily with regard to the definition of high- risk AI systems and the demarcation for which AI systems that are c onsidered to entail high risk. Risk- base d approach BIL Sweden support the logic behind the risk -based approach set out in the proposal, this should ensure that the requirements in the AI Act are in proportion to the level of risk for AI applications and that they are not so burdensome for companies throughout Europe. However, it is important that high -risk is well defined. Article 9 on Risk Management Systems contain some vague wordings, which may cause legal uncertainties, and should therefore be clarified and simplified. 3( BIL Sweden, Box 26173 SE -100 41 Stockholm, Sweden, Telephone +46 8 700 41 00 , Telefax +46 8 791 23 11 Bes ksadress: Storgatan 19 114 82 Stockholm VAT No. SE 556077- 4886, bil@bilsweden.se, The definition of high -risk The vehicle industry is a part of Annex II through WVT A and GSR and has, thus by definition, been classified as high -risk. BIL Sweden advocates that clarification is needed, regarding whether it s all systems falling under EU Regulation 2018/858 Type Approval and GSR that are by definition classified as high risk. The v ehicle manufacturers see a significant risk, if so, that AI will not be used in vehicle s. In that case, it would result in the a utomotive industry not being allowed to contribute as much to increased road safety as they could have done. BIL Sweden also advocates that clarification is needed regarding the definition in Article 3 ( of ""safety component in a product or system"". Data and data governance Article 10, 3, states that Training, validation and testing data sets shall be relev ant, representative, free of errors and complete. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons on which the high- risk AI system is intended to be used. . BIL Sweden believes that data, usually and in practice, can never be guaranteed to be completely ""free from errors"" and ""complete"", validation of such is not practically possible. It is also not possible to fully ensure that all data has ""the appropriate statistical properties"". Record -keeping Article 12, 2, states that The logging capabilities shall ensure a level of traceability of the AI system s functioning throughout its lifecycle that is appropriate to the intended purpose of the system. . BIL Sweden advocates that a clarification is needed as to what is meant by ""lifecycle"", if it means that events (logs) must be stored for the entire life of a product. It s important to keep in mind that storing logs for a longer period of time requires large storage capacity, which can lead to large consequences in terms of increased costs for data storage, increased energy consumption and maintenance. Overall, this would be inconsistent in view of the goals of climate neutrality set out in the European Green Deal bu t also in view of the goals of strengthening European competitiveness. Innovation and AI regulatory sandboxes BIL Sweden welcomes the opportunity for AI regulatory sandboxes but advocates that much sharper proposals are needed for experimental opportuniti es that benefit technology development and innovation. In regulated sandboxes, companies should be able to test ideas under limited responsibility and without being affected by the burdensome market access system that assessments of conformity constitute. The innovation capacity is central to competitiveness and should therefore enable in a clearer and simpler way. The right to experiment under certain conditions should be included directly in the proposal and not handed over to the respective member state since it could lead to different interpretations and thus conditions within the internal market. When it comes to experiments for new AI solutions, there are today many limitations, for example due to the GDPR's rules. BIL Sweden is pleased to be available for further discussions or any clarifications. Maria Backlund Technical Coordinator : Vehicle regulations and Research BIL Sweden",en,"It s important to keep in mind that storing logs for a longer period of time requires large storage capacity, which can lead to large consequences in terms of increased costs for data storage, increased energy consumption and maintenance. Overall, this would be inconsistent in view of the goals of climate neutrality set out in the European Green Deal bu t also in view of the goals of strengthening European competitiveness.",risk
"Data, AI and Robotics aisbl (Belgium)",F2665296,04 August 2021,Other,Micro (1 to 9 employees),Belgium,"Data, AI and Robotics (DAIRO) aisbl Avenue des Arts, 56 1000 Brussels Belgium E-Mail: info@core.bdva.eu Web: BDVA /DAIRO position paper Response to the European Commission s proposal for AI Regulation 04th August 2021 BDVA /DAIRO welcomes the opportunity to provide feedback to the European Commission s proposal for AI Regulation as there is a clear need for a solid AI European approach based on European values . BDVA/DAIRO supports the idea that there should be a balance between regulation and innovation , and that new rules should facilitate investment and innovation . For this reason, the specific objective of the proposed regulatory framework to ensure legal certainty to facilitate investment and innovation in AI is highly supported by the BDVA/DAIRO community. However, the cost, time, infrastructure and knowledge needed to implement this regulation may be burdensome. Small companies, research and education organizations might not be able to easily follow the regulatory developments and might be affected negatively e.g. by a lack of sufficiently qualified personnel , resulting in hard implementation that can hinder innovation and competitiveness. Very s olid investments for the implementation of this regulation are needed and in this respect BDVA/DAIRO observes that the European Commission s Coordinated Plan on AI considers and supports the implementation of the future AI Regulation. To accelerate the adoption of Industrial and Trustworthy AI in E urope , the association stresses the need to develop and invest in engineerin g and standardisation frameworks for Industrial and Trustworthy AI (such as the one BDVA is developing in collaboration with the Franco -German initiative ) and calls for strong collaboration with national initia tives working on Industrial AI to align and coordinate efforts on this matter . The association supports and highlights the importance of Data quality and Data governance for AI as paramount, and underlines the fundamental connection between AI and Data. BDVA/DAIRO welcomes also the link to other legislative acts, such as the GDPR , and underlines the alignment needed with the Data Governance Act and the upcoming Data Act. In relation to Data quali ty, the BDVA/DAIRO community has also emphasized the topic as a mean to, among other benefits, achieve fairness and avoid discrimination . The need for different metrics has been stressed by the community in areas such as Data quality, interpretability, exp lainability , predictive accuracy, robustness, fairness, computational complexity and very importantly in risk -assesment in alignment with standardisation efforts and results . In addition to the above -mentioned topics ( Data and Data Governance for AI, the need to develop frameworks and tools to speed up the adoption of industrial AI, and the balance innovation -regulation ) the BDVA/DAIRO community has stressed the importance of and provided feedback and recommendations to i) the scope and impact of the d raft AI regulation; ii) the notion and components of trustworthiness in the regulation; iii) risk assessment and classification: iv) life -cycle of products, and v) opportunities for SMEs. The recommendations consider also the need for the establishment of safe environments for better testing . Access to knowledge, testing , experimentation and ecosystem are key factors for success in particular for SMEs . The association acknowledges that the proposed plan for an AI Regulation comes with a new updated Coordinated Plan on AI addressing these topics, but calls up for the importance of investing in federated experimental networks such as the European Federatio n of i - Spaces or Big Data Innovation Hubs. These networks can not only provide access to knowledge, innovation, testing and ecosystem to SMEs and startups in context of the new AI regulation , but can offer opportunities to legislators in supporting collabo ration between innovators and standardisation experts. Data, AI and Robotics (DAIRO) aisbl Avenue des Arts, 56 1000 Brussels Belgium E-Mail: info@core.bdva.eu Web: Building on these considerations and on a series of workshops held with the association s members , BDVA /DAIRO wishes to underline a few important elements concerning both identified challenges and recommendations from the research & innovation community linked to the European Big Data industry. This response to the feedback request covers in particular eight aspects of the proposal for AI Regulation . The document is therefore structured as follows : Scope and impact of the draft AI Regulation Data and Data governance for AI Notion and components of trustworthiness in the AI Regulation Risk-assessment and risk classification Balance innovation - regulation and elements of supporting innovation How the regulation lower barriers, opportunities for SMEs Life-cycle of products Engineering and Standardisation frameworks for Industrial and Trustworthy AI Other topics Scope and Impact of the Draft AI Regulation BDVA/DAIRO appreciates that t he proposal addresses the basic concept of sustainability for the technology field of AI, as it includes elements of environmental conditions , social aspects, and governance . This is considered to support adjustments that may become necessary in the future. For the time being, however, the association sees also a number of issues. The definition of AI is generally quite broad , and could create issues or future uncertainty an d lack of clarity . First, it is important to note that the Regulation should address AI Systems and not AI as a field. The notion of A I Systems is present and addressed all the way through the EC Proposal. However, the clear distinction between AI at large and AI Systems seems to be implied and never explained. For this reason , we would strongly recommend to add a clear definition at the begininning of the AI Regulation proposal explaining the semantic and conceptual ch oice. The definition of AI systems shall be reviewed , possibly taking into account how current standardization efforts define AI systems1. Second, t he definition of AI system also covers far more than what is subjectively understood as AI . Based on such definition , even the simplest search, sorting and routing algorithms could be considered AI systems , or being subject of this Regulation, risking the creation of a set of rules for high - risk software systems. Third, i t is not clear how components of larger AI systems should be treated , like pre -trained AI components from other manufacturers, or components that are not released independently (e.g would such components need an independent certification? ) We would recommend this part to addressed with m ore clarity . BDVA/DAIRO acknowledges the importance of creating an a nnex to the Regulation listing the technologies affected by the regulation and welcomes the p ossibility to amend it as foreseen in paragraph ( . In fact, the association supports the idea of annexes which are also reviewed periodically given that the choice to horizontally regulate technology leads to some issues . When updating the Annex I of the Regulation, p roactivity of policymaker s shall be needed in order to protect citizens from new threats of a very dynamic technology . This would cover AI system technologies and also specific application areas. Regularly review and evaluate these annexes with stakeholders from sectors and on national level. 1e.g. -plan-ict-standardisation/artificial -intelligence Data, AI and Robotics (DAIRO) aisbl Avenue des Arts, 56 1000 Brussels Belgium E-Mail: info@core.bdva.eu Web: In some verticals there may be different developments, effects or regulations . It should be possible to create sector -specific exceptions , which could also be included in an annex that is reviewed and updated regularly. In both cases, the review of the appendix could contain a pro active element so that citizens are protected from new dynamically evolving potential threats. The AI Regulation proposal does not seem to give any exception for research . Many researchers develop AI systems and publish them in open access journals or in a repository, such as GitHub , free of charge and with an open license, in order to disseminate knowledge and to let other researchers improve them. Those models are not intended to be commercialized, but , due to the nature of the licenses most commonly used, it is always possible that the models are used by other entities, even incorporated in other models, and put into the market . There should be a provision similar to art . 89 of GDPR, in order to allow researcher to publish their model for academic purposes without falling into the definition of putting into the market/ into service . The situation in which researchers collaborate with industry, hospitals or other entitie s should be regulated separately . Regarding startups and SMEs, o ne source of concern is the Commission's power2 to unilaterally extend the list of sectors covered by the regulation by delegated acts . Industry - including professional associations - should be consulted before any decision is taken that may have an impact on the scope of the Regulation. Data and data governance for AI BDVA/DAIRO highlights the importance of Data Quality and Data governance as paramount and welcome s article 10 of the draft regulation. The association also welcomes the r eference to the Data Governance Act (in preparation) and the ethical guidelines of the HLEG (as reported) . The definition of data quality in the proposal for regulation is too absolute and misle ading although BDVA/DAIRO agrees that it is important to have one. In Article 10 , the Commission does not define the criteria for measuring the quality of the data sets . There should also be an e xplicit mention of a need for data quality metrics in place (e.g. predictive accuracy, robustness, fairness of trained machine learning models) . The r esponsibility and/or accountability for data quality (in testing, experimentation, and operation) may need to be clarified . It is impossible to guarantee that training datasets are completely free of errors and biases, and the obligation in this respect must necessarily be a best effort obligation , taking into account the state of the art and industry practices, as is envisaged in te rms of security in Article The following wording would be more appropriate: ""Adequate efforts must be made to ensure that the data sets are sufficiently relevant, representative, error -free and complete . Our community believes also that more emphasis should be placed on achieving fairness through data quality in the regulation although the achiev ement of fairness through data quality is true for some types of systems (such as facial recognition systems) but not for others (such as classification of satellite images ). Efforts should be made to ensure representativeness achieving fairness through data quality . Contextual assessment could be used, meaning that data quality should be coupled to a certain application/goal/task and only be assessed in that respect . Also it should be c onsider ed the potential use of low-quality data derived from socio -technical inequalities in implementation in social contexts as well as algorithm/model deployment . The establish ment of testing facilities, methodologies and standards should be foreseen for better testing (also with data) and MLOps (Paragraph . Another weakness is that no explicit provisions for the validation of testing and experimentation data sets (and related metrics) are provided (Paragraph 45 , 46 and Article 10 ). Also , implications of the utilization of aggregated data (from 2 -making -process/adopting -eu-law/implementing -and-delegated -acts_en Data, AI and Robotics (DAIRO) aisbl Avenue des Arts, 56 1000 Brussels Belgium E-Mail: info@core.bdva.eu Web: different providers) or implicit data (as from third party suppliers of AI elements in embedded systems) need to be addresse d. The Regulation shall also ensure consistency with existing EU legislations . BDVA/DAIRO welcomes that the p roposed Regulation complements the GDPR , although overlaps in scope and application should be avoided. Today, m ost AI systems are based on the processing of massive data so, it is essential that the AI Regulation is aligned with the GDPR . As it stands, this text represents new risks of legal ins ecurity due to its difficult reconciliation with existing regulations. The principle of quality of data sets , which is essential for the development of efficient and unbiased AI, is difficult to reconcile with certain principles of the GDPR , in particular the principle of data minimization, the limitation of retention periods, the consent to collect certain types of data and the principle of informing users about the purpose of the processing at the time of collection. Finally, the end-to-end lifecycle appr oach is needed to ensure that the Regulation is considering mitigations beyond data quality, such as how the model is designed and built, and whether developers and researchers should use blocklists or other technical measures to disallow inappropriate content Notion and components of trustworthiness in the AI regulation The notion of trustworthiness of AI is thoroughly present in the proposal for AI Regulation. This notion, as a general concept, is welcomed by BDVA/DAIRO . A supporting framework to explicitly and practically define trustworthiness of Industrial AI in different (high risk or not ) sectors and foster the adoption of norms, can really promote and accelerate adoption of Industrial AI. This reduces risks and will also enable sector -specific optimizations. BDVA/DAIRO is currently conducting several activities in relation to Industrial and Trustworthy AI3 to set out a comprehensive industrial and trustworthy AI framework that clusters the priority area for AI research, innovation and deployment and calls for strong collaboration with national initiatives working on Industrial and Trustworthy AI to align and coordinate efforts on this matter . Also, adequate standa rds supporting industrial AI and trustworthiness are fundamental such as adequate metrics for interpretability , explainability , predictive accuracy, robustness, fairness, computational complexity . Regarding the management of complex systems composed of several AI systems (especially using different underlying AI methods) , BDVA/DAIRO believes that the proposal for a Regulation should be more clear on whether separate components would be individually required to conform to the AI Regulation, or if that would be considered only as part of the system as whole . Moreover, there is an open question on whether there w ould be implications for liability when components are not compliant . When referring to the link between trustworthines s and labellin g, it is important to consider digital literacy as very important . Additionally, a label is always context dependent , and it should contain at least two dimensions : technology and organisation . Some AI applications may seem to be unfair or discriminatory, but that depends on the contexts of deployment. Therefore, rules to distinguish between legitimate bias and unfair bias may be considered by the legislator. Measures to address aspects like non -discrimination and fairness are not explicitly required for high risk AI in the proposed regulation. This may be necessary because biases do not merely result from low -quality data, but also from socio -technical inequalities in implementation in social context s as well as algorithm/model deployment 3 See example from BDVA/DAIRO collaboration with the Fran co-German initiative on speeding up industrial and trustworthy AI at: Data, AI and Robotics (DAIRO) aisbl Avenue des Arts, 56 1000 Brussels Belgium E-Mail: info@core.bdva.eu Web: Finally, an explicit mention of a need for trustworthiness metrics in place (e.g. predictive accuracy, robustness, fairness, interpretability, computational complexity) of machine learning models shall be added . Risk -assessment and risk classification BDVA/DAIRO welcome s the risk -based approach that aims to look at the effects of AI rather than trying to formulate bottom -up technical requirements. Balancing innovation, human rights and strategic autonomy is a delicate exercise and the current proposal is a big step in the right direction . The proposal of a Regulation on AI is in line with our concerns about the need for risk -assessment and risk classifications. Th e association welcomes the idea of having an Annex (i.e ., Annex III to the Regulation) , for listing the risks and apply a set of criteria and risk assessment methodology. It would be very helpful to include a definition of the respective levels and applications and the respective risk levels . The AI risk classification and assessment criteria and metrics could be aligned explicitly with i nternational standardiz ation results. The standard risk metrics could be used to make Article 9 (Risk Management system) more specific, especially for article 2 (risk assessment process). Some issues should be considered: on the one hand, a more detailed classification of ris k could be necessary for industry to perform self -assessment of risk associated with their products. Risk and levels cannot be easily generalized over many application of AI . Moreover, risk granularity is coarse (High Risk, Low Risk etc). On the other hand, risk management need standard metrics , as in finance and other regulated fields . Additionally, the issue of bias shall be looked at closely. BDVA/DAIRO underlines that the re is still no established practice in research to deal with bias in general, and that a framework would be welcome. Finally, t he current text does not specify the means that the European Commission intends to deploy to ensure the compliance of high -risk A I developed outside the EU . In general, r egulation would need to constantly move with research and innovation efforts , in particular, for the definition of risk -assessment and risk classification patterns and the Regulation would need to be based on the continuously updated standards based on ongoing research . Balance innovation -regulation and elements supporting innovation BDVA/DAIRO is very much aligned with the idea that the regulation should not hinder innovation, but rather faci litate investment and innovation . For this reason , it supp orts the specific objective of the proposed regulatory framework on AI to ensure legal certainty to facilitate investment and innovation in AI (page 3 of the Proposal) . However, t he regulation might create difficulties to establish balance innovation -regulation on a constant basis. At the moment it might be an administrative burden especially to universities, research centers and SME. On the other hand , BDVA/DAIRO agrees that a Regulation can drive innovation . Thus, regulatory actions and innovation shouldn't view these as antithetical to each other . The Regulation should come along with strong investments in implementation, definition of European frameworks to support adoption , skills development programmes and support to SMEs. BDVA/DAIRO welcomes t he propo sal of Regulatory s andboxes as to establish protected environment s to experiment innovation . This is a major advance in European law that will bring the EU in line with the standards of its international competitors. A close monitoring to assess whether such regulatory sandboxes and the way those are implemented in practice, actually facilitate innovation is needed and strongly recommended. Moreover, the conditions for acces s to and use of sandboxes need to be clarified . BDVA/DAIRO recommends that for startups the scheme should be strengthened with a precise definition of eligible actors , the duration of the scheme and, above all, the list of obligations from which the scheme should exempt them. The Commission should not delegate Data, AI and Robotics (DAIRO) aisbl Avenue des Arts, 56 1000 Brussels Belgium E-Mail: info@core.bdva.eu Web: all decisions on the implementation of sandboxes to Member States , in order to avoid fragmentation of regulatory regimes and a possible race to the bottom between countries competing to attract AI innovators. The idea would be to avoid the concentration of companies in countries with the least restrictive rules, a problem that has already emerged with tax rules and the GDPR with Ireland and Luxembourg. Art 10 could be ""suspended"" for short period of time to allow the introduction of new AI -based services and to assess if bias exists . This , as to promote a kind of safe conduct for a predefined and fixed time . ""Clinical trials"" might give a good reference framework for suspensions . A proposed soluti on would be to put easily accessible ethics commissions first. Th ose, could also give guidance to SMEs . Lowering barriers and increasing opportunities for SME s In line with the previous section BDVA/DAIRO agrees with the idea that a clear and consistently applied and interpreted Regulation sets a common ground that can facilitate innovation particularly in SMEs . In other words, Regulations should be consider ed as guideline s instead of a set of constrains, like for GDPR or PSD2 . Under this light, it can be considered as a source of technological innovation, opening new business opportunities and competitive advantages against less agile entities (in particular those not used to EU ethical principles). New business roles can potentially be generated, opening the playing field for new service and solution providers. Access to knowledge, testing , experimentation and ecosystem are key factors for success in SMEs. The association acknowledges that the proposed plan for an AI Regulation comes with a new updated Coordinated Plan on AI addressing these topics, but stresses the importance of investing in federated experimental networks such as the European Federation of i-Spaces or Big Data Innovation Hubs4. These networks can not only provide access to knowledge, innovation, testing and ecosystem to SMEs and startups in context of the new AI regulation, but can offer opportunities to legislators in supporting collabora tion between innovators and standardisation experts. The establishment of a European Artificial Intelligence Board is also positively considered in order to ensure the consistent application and interpretation of the Regulation among the different national authorities. Howe ver it is not clear what the role of private entities (and in particular SMEs and startups) within the European Artificilal Intellingence Board will be, and how their voice will be caught and brought into the board . Also, a s previously mentioned, an element of notice is the establishment of regulatory sandboxes to foster AI innovation and to accelerate the access to markets by reducing potential barriers for SMEs and startups. However , the conditions by which SMEs (but in gene ral all the businesses) will be allowed to access and use the regulatory sandboxes is not clear. The AI field is characterized by a high variability and new developments , so part of the Regulation could be affected by obsolescence in respect to the actual evolution of the field and cons quently a sense of feeling lost from the SME perspective and, even worse, high risk of investiment waste . This is particularly critical during the boosting phase of start -ups. Support to SMEs and startups needs to go beyond ""preferential access"". Best available technology principles should be shared so SMEs can understand and contribute to the SOTA . Also, SMEs should have easy access to the market , and hurdles to enter it being lowered. As d ata is the key element for many A Is and access to data is paramount it is fundamental an alignment between the proposal for AI Regulation and the Data Act. Additionally, repositor ies of AI trustworthy capa bilities can be created (models, patterns, source code) and shared. 4 For example, see the federation of Big Data Innovation Hubs created by the EUHubs4Data project: Data, AI and Robotics (DAIRO) aisbl Avenue des Arts, 56 1000 Brussels Belgium E-Mail: info@core.bdva.eu Web: Life-cycle of products BDVA/DAIRO highly appreciates the Commssion s awareness regarding the specifics of the AI systems life -cycle and the efforts to integrate proper risk management of high-risk AI systems , including the creation of synergies with other instruments such as the Proposal for a Regulation on Machin ery Products. The creation of dynamic, evolutionary state of the art, relevant for the legal assessment of disruptive technologies such as AI is of great value for the industry as well as the society, ensuring de facto legal certainty and reinforcing the notio n of trustworthiness. The Engineering and Standardisation framework for Industrial and Trustworthy AI should be taken into account and enhanced. As stated in previous BDVA/DAIRO s position papers, s tandards and standardisation can be employed as a mechani sm to leverage international best practice to build trust and confidence in AI products, services, tools, and processes5. BDVA/DAIRO stresses upon the need to leverage international best practice to build trust and confidence in AI products, services, to ols, and processes . Regarding the life -cycle of products, the association recognises that AI engineering is not sufficiently established in science and industrial practice . Also, t he multi -technology and multi -disciplinary requirements should be considered. Furthermore, there are open questions such as h ow to cope with context changes and what is the ' best before ' date of a trained model . It is also important to take such approach when designing and implementing innovation facilitation solutions, such as regulatory sandboxes, which in their traditional shape and format concentrate predominantly on the risks posed by products, services and business models prior their access to the market. It is highly recommendable to broaden the scope of the performed tests and evaluation in order to mitigate risks for consumers and businesses associated with the requirements aligned with the greater awareness of the AI systems lifecycle. Additionally , there is a n eed to define what a failure is , since a wrong prediction has a probability of happening . This, links to the concept of e xplainable AI: How to evaluate the accuracy of an explanation and how to integrate it into the AI-lifecycle for decision making? Finally, in addition to the product as a whole, a t least 3 different elements must be taken into account in a life -cycle assessment of a product or service : the training system and associated training data , the resulting AI intelligence model , the application which uses AI and associated applicat ion data . Towards an Engineering and Standardisation Framework for Industrial and Trustworthy AI As stated in paragraph 61 of the Regulation proposal, standardisation should play a key role to provide technical solutions to providers to ensure compliance with this Regulation . The association agrees also with the recognition, by the European Commission, to the need of adopting common technical specifications in areas where no harmonised standards exist or where they are insufficient (paragraph 61, pag e . For this reason, the BDVA/DAIRO community proposes that the European Commission shall consider the following tools and methodologies : To support the design, test, validation, verification, and maintainability of AI -based functions and system s. To address the development of AI -based process and systems to demonstrate its integration into new products and services 5 Source: BDVA s response to the European Commission s Whitepaper on Artificial Intelligence A European approach to excellence and trust May 2020 Data, AI and Robotics (DAIRO) aisbl Avenue des Arts, 56 1000 Brussels Belgium E-Mail: info@core.bdva.eu Web: To do model evaluation (currently own datasets are being used , but trusted independent datasets for stakeholders is something Member States and/or sector oversight bodies will need to develop ) On a governance perspective, a coordinat ed organisation to promote common standardisation, technical framework and roadmap s shall be considered. A successful example that shall be considered is the Franco -German initiative on speeding up industrial AI and trustworthiness6. BDVA/DAIRO has joined forces with this initiative with the objective of developing a European Engineerin g and Standardisation Framework for Industrial and Trustworthy AI and calls for national initiatives on Industrial AI to engage in this initiative. BDVA/DAIRO also recommends supporting process -based certification scheme s instead of re - assessing high -risk AI systems . In other words , according to Article 43 paragraph 4, high-risk AI systems should undergo a new conformity assessment every time they are subject to substantial modifications . This would lead to excessive admin istrative stress as well as increasing costs, especially for SMEs. This, would lead to processes hindering innovation due to excessive testing and audits, discouraging innovators and undermining competition in Europe. For this reason , a process -based certification scheme for high -risk AI systems should be put in place. Such certification scheme would focus on the AI system and its coherence with the ethical development, deployment and operation with regard to the effectiveness of the company or the wide AI system process . Hence, the best practices of AI ethics can be provided and therefore applied to the development and deployment activities of each AI system. The definition of the evaluation criteria and meth odology shall be create d as to make a process -based certification scheme the baseline for transparent and effective processes for developing trustworthy AI systems across industries. Other topics BDVA/DAIRO believes that t he Regulation should concentrate on fully auditable AI . This means that the question of IP rights on models and managing them legally as data products should be addressed. The d efinition of IP rights on models and the legal manag ement of them as data products would be an impor tant first step to preserve IPR . BDVA/DAIRO also calls the attention to take into account relevant EU legislation such as the GDPR in the case of requesting providers to keep documentation for 10 years, the Trade Secrets Directive in the case of granting market surveillance authorities access to datasets, source code of high -risk AI systems and the EU Cyber security Act in the case of requesting cyber security measures and incident notifications from providers. Moreover, t he Regulation should adop t a balanced approach to human rights and technology, ensuring that no right or freedom would be limited unless provided by law and with appropriate guar antees that the essence of these rights would be preserved according to Article 52 of the Charter of Fu ndamental Rights of the European Union. Finally, BDVA/DAIRO welcomes the arrangement of the European Commission for the Reg ulation to be reviewed and evaluated after 5 years , as outlined in Chapter 1 of the Proposal. 6 Data, AI and Robotics (DAIRO) aisbl Avenue des Arts, 56 1000 Brussels Belgium E-Mail: info@core.bdva.eu Web: About this Document Main editors o f this documentare (in alphabetical order ): Natalie Bertels , Valorisation Manager & Researcher, imec/ KULeuven , representing BDVA TF5 Policy and Societal Freek Bomhof, Senior Data Science consultant , TNO , representing BDVA TF5 Policy and Societal Roberto Di Bernardo , Head of Open Government R&D Group, Engineering Ingegneria Informatica S.p.A. representing BDVA TF7.SG8 Smart Governance and Smart Cities Ana Garc a Robles, Secretary General BDVA/DAIRO Norbert Jastroch, Head of Research MET Communications, BDVA /DAIRO member Tjerk Timan, Researcher TNO, BDVA /DAIRO member Mattia Trino, Operations Manager BDVA/DAIRO Ray Walshe , Lecturer D ublin City University , repre senting BDVA /DAIRO TF6.SG6 Standardisation Katerina Yordanova, Researcher imec/ KULeuven representing BDVA TF5 Policy and Societal Sonja Zillner , Lead of Core Company Technology Module Trustworthy AI at Si emens AG, SRIA Lead at BDVA /DAIRO This paper is the result of a c ooperative work that gather inputs from the almost 150 BDVA /DAIRO members that participated in the following : Workshop with BDVA /DAIRO members Feedback to AI Regulation proposal (07th June Workshop on BDVA/DAIRO Position Paper on Industrial and Trustworthy AI (18th June Consolidation Workshop on BDVA/DAIRO Feedback to AI Regulation proposal (30th June The final document has been drafted during the month of July 2021, thanks to a join effort of the main editors in coordination with the BDVA/DAIRO Secretariat. About BDVA /DAIRO The Big Data Value Association BDVA, (from 2021, DAIRO - Data, AI and Roboti cs aisbl), is an industry -driven international not for-profit organisation with more than 230 members all over Europe and a well -balanced composition of large, small, and medium -sized industries as well as research and user organizations. BDVA/DAIRO focuse s on enabling the digital transformation of the economy and society through Data and Artificial Intelligence by advancing in areas such as big data and AI technologies and services, data platforms and data spaces, Industrial AI, data -driven value creation, standardisation, and skills. BDVA/DAIRO has been the private side of the H2020 partnership Big Data Value PPP, it is a private member of the EuroHPC JU and is also one of the founding members of the AI, Data and Robotics Partnership. BDVA/DAIRO is an open and inclusive community and is always eager to accept new members who share these ambitious objectives Contact for further information: info@core.bdva.eu",en,"The document is therefore structured as follows : Scope and impact of the draft AI Regulation Data and Data governance for AI Notion and components of trustworthiness in the AI Regulation Risk-assessment and risk classification Balance innovation - regulation and elements of supporting innovation How the regulation lower barriers, opportunities for SMEs Life-cycle of products Engineering and Standardisation frameworks for Industrial and Trustworthy AI Other topics Scope and Impact of the Draft AI Regulation BDVA/DAIRO appreciates that t he proposal addresses the basic concept of sustainability for the technology field of AI, as it includes elements of environmental conditions , social aspects, and governance .",risk
5Rights Foundation (United Kingdom),F2665266,03 August 2021,Non-governmental organisation (NGO),Small (10 to 49 employees),United Kingdom,"1Disrupted Ch ldhood The Cost of Persuasive Design Authors Baroness Kidron Alexandra Evans Jenny AfiaWith contributions from Prof. Joanna R Adler Dr. Henrietta Bowden-Jones Dr. Liam Hackett Anisha Juj Prof. Andrew K Przybylski Dr. Angharad Rudkin Young Scot 5Rights Youth Leadership Group Acknowledgements We would like to thank all contributors, including the many academics upon whose work we have built: the 5Rights network of companies, organisations and individuals who support the rights of children in the digital environment and, most of all, the children and young people with whom we work. Particularly the Scottish 5Rights Youth Commission convened by Young Scot; their insights are endlessly thoughtful and pragmatic. We would also like to thank Parentzone and the PSHE Association for canvassing the voices of parents and teachers; the 5Rights team; the law firm Schillings, who generously supported the publication of this report; and also Headland Consultancy for their support for its launch. June 2018Executive summary 04 Foreword 10 Introduction 11 Chapter One 12 Children are struggling to put down their devices Chapter Two 15 The commercial imperative Chapter Three 20 The strategies to keep users online The rush (dopamine) Popularity contest Summonses: buzzes, pings, vibrations and the colour red Losing time Social obligation Emotional highs Chapter Four 27 Impact of persuasive technologies on childhood Chapter Five 36 Seeds of change Endnotes 39 Authors & contributors 43 About 5Rights Foundation 4604 Executive summary The battle for children s attention has been characterised by how much time they spend online. As online and offline lives are increasingly blended, it is no longer helpful or feasible to distinguish between the two. What we must now urgently consider, is what children are doing and why . If we continue to allow the persuasive design features to dominate the decisions children make online, we are in danger of stunting the creativity and development of a generation. This has far-reaching consequences for individual children, families and society. We urgently need to consider whether children are autonomous, respected and protected online. The Disrupted Childhood Report explains commonly-used strategies, and highlights how automated technology both leverages and reinforces human instinct, in order to trigger habits and behaviours. Variously called reward loops , captology , sticky , dwell features and extended use strategies , persuasive design strategies are deliberately baked into digital services and products in order to capture and hold users attention and imprint habitual behaviours. Habits and behaviours formed before 9 years old take significant intervention to change.1 The costs for children, who in the report call for fairer treatment, are palpable. They include personal anxiety, social aggression, denuded relationships, sleep deprivation and impact on education, health and wellbeing. At the same time, the current regime of data surveillance, fuelled by persuasive design, raises ethical, moral and legal questions. The Disrupted Childhood Report, published by 5Rights Foundation , explains how persuasive design strategies, deployed to maximise the collection of personal data, impact on children s social, mental and physical development. It questions the legitimacy of commoditising childhood and sets out a series of practical and immediately applicable recommendations for the tech sector, Government, parents and investors. The system is failing social networks they are man-made. If they are not serving humanity, they can and should be changed. Sir Tim Berners-Lee 05 The report concludes that:Executive summary Industry insiders, unhappy with compulsive strategies, demand that the technology sector operate within a fully-described set of ethical and social standards . Their powerful words indicate a broader discontent throughout civil society. The seeds of change are seen in the increasing focus of policy makers, the media and concerned adults on the costs for children of persuasive and habit-forming design. Digital technology promises unlimited potential for children and society. To fulfil its promise it must be deployed in a way that is accountable and proactively meets the needs of its child users. Services and products should be required to anticipate the vulnerabilities associated with the different ages and developmental stages of childhood in order to fully realise their potential. Whilst acknowledging the recent steps taken by some technology companies on behalf of younger users, it remains the case that digital services have consistently failed to prioritise the needs of children over those of shareholders. The report argues that self-regulation has proven inadequate and that standards delivered universally in the best interests of the child ,2 should be set by society, not Silicon Valley. The passage of the Data Protection Act 2018 (DPA), with its Age-Appropriate Design Code, offers a unique opportunity to create a children s data protection regime that offers children respect and protection. The question remains whether it will be fully embraced by all stakeholders and robustly enforced. As long as the digital environment deploys persuasive strategies for primarily commercial purposes, it will fail to live up to its promise of progress, creativity and knowledge. It is unreasonable to design services to be compulsive, and then reprimand children for being preoccupied with their devices. Children are overwhelmed3 and require more intentional use of digital technologies,4 and more time out. Services must be designed to anticipate the rights and needs of children. The development of a global governance system for the digital technology sector must be a priority for governments and international institutions. The Disrupted Childhood Report highlights the urgent need to ensure that the design of digital services and products is appropriate for children. It proposes the following recommendations to make that possible. 06 Executive summary Recommendations We call on industry to 1 Recognise compulsive use of technology as a public health issue. 2 Design all services to make it as frictionless to get offline as it currently is to get online. For example, by designing into services; Autoplay default off, and if changed, switch back to off once a child logs out or navigates away. Notifications and summonses default off, such as buzzes, read receipts, pings and all other non-specific alerts. Default streak holidays (and temporary absences from streak-type settings). Save buttons (so children are not forced to stay online to complete a task). Time out and disengagement opportunities; standardised, easily accessible and frequently offered, even if it is not in services commercial interests.5 Including regular reminders of time spent. A barrier to software upgrades that automatically enhance or switch persuasive design features back on. Alternatives to data collection as a price of entry. And, stop gathering children s data for the purpose of personalising services to simply extend use. 3 Proactively support children who are struggling to manage their use with clearly signposted access to services and technical solutions, for example (but not limited to); Health-based informatics should be used to prompt positive behaviour.6 Embed disengagement strategies in a voice tested and desired by children.7 Children should be encouraged to think about self-care and downtime, i.e. self-soothing, good sustenance, going to bed, sleep.8 IOS and Android Systems must be required to give systems access to services (e.g. Apps) that help tackle compulsive use to enable them to be integrated into a child s user experience. The above lists are non-exhaustive, there are as many solutions as there are strategies and it requires culture change, technological change and a commitment to change that puts the best interests of children first.9 Which is why we also recommend;07 Executive summary 4 Develop and undertake Childhood Impact Assessments on: Existing services and products. Future services and products before roll out in particular noting the ethical implications of emerging technologies and consider their impact on children. 5 Work to an ethical framework built on principles of transparency, accountability, responsibility in the best interests of children (see Recommendation . 6 Clearly and succinctly inform children (and parents of younger children) when persuasive design features are being used, and outline possible impacts, including sleep deprivation, loss of concentration, educational outcomes and impact on emotional state and behaviour. 7 Provide online services and products that prioritise children s best interests over commercial considerations don t simply lock them out. 8 Listen to the demands of children who are asking for more control, fairer treatment and more peace. We call on parents to 9 Talk with children about the value of independence and how persuasive design strategies undermine autonomy. 10 Help children disable persuasive design features that can currently be switched off. 11 Agree boundaries based on activity and intentional use, not time limits. 12 Support the introduction of Recommendation 2 by: Using services that proactively design their services with children in mind. Not using services that continue to design their services to be compulsive for children. 13 Put your own phones down.08 Executive summary We call on Government to 14 Define compulsive use as an internet harm for children, and to provide advice, information and adjust policy accordingly. 15 Require industry to characterise, name, label and grade the impact of the persuasive design features they are using, in order that children can make effective choices about digital use. 16 Publish a fair game charter that sets out an ethically child- centric set of standards for games and gaming. Setting out ethical rules against behavioural mechanics that try to draw children into addictive behaviours or exhortations.10 17 Undertake a public health campaign that explains the dangers of compulsive design strategies and their effects on children, as they move from primary to secondary education. Aimed at Year 6 children and those that care for and teach them, to counteract the 'cliff edge explosion of usage identified by the Children s Commissioner for England.11 18 Support digital literacy as part of the curriculum to equip children to help them navigate the digital world and understand how they are being influenced, and mandate that computer studies and PSHE in schools, and Computer Science Degree courses include modules that explore ethical design, including issues of data harvesting and impact of persuasive design strategies. 19 Ensure frontline professionals (for example, teachers, social workers, health and legal professionals) have appropriate training, and a broad understanding of the full range of opportunities and risks in the digital environment, including compulsive use. Include training as part of degree accreditation and professional standards. 20 Publish an ethical framework to govern all digital interactions with children and young people, based on principles of transparency, accountability and responsibility in the best interests of children. In doing so, consider the legality, ethics and safety of creating digital habits in childhood. 21 Support the new Age-Appropriate Design Code by providing sufficient resources for the regulator (ICO) to ensure that the Code is fully applied and enforced. 09 Executive summary 22 Create an independent, overarching centre of excellence, research and policy for all interventions relating to children in the digital environment. It should have powers and resources to compel attendance and demand accountability, and be a voice for children in all areas of Government including health, justice, education, home affairs and digital. 23 Actively work to implement a global framework that sets the ethical, governance and legal boundaries for the technology sector. And finally We call on institutional and individual shareholders to follow the action of Apple Investors, JANA Partners LLC and the California State Teachers Retirement System, and demand that companies you invest in design devices, services and products that accommodate the needs and rights of childhood.12 10 Foreword The internet is an extraordinary force for good but it was not designed with children in mind. Despite this, it is now part of every aspect of children s lives used to socialise, play, create and learn. Given this huge generational social change, it is everyone s responsibility to ensure that this interaction is positive and healthy rather than negative, destructive or dehumanising. This responsibility lies with parents, teachers, Government and importantly, technology companies themselves. While progress has been made on issues such as parental control features, age verification and promises of improving digital education (although we still await the details including available funding) the fact remains that children are still nowhere near suitably equipped with the skills they need to navigate their way online. Over the last two years, I have worked with a group of technology, legal and policy experts including the authors of this report to tackle these problems. I have called for three interventions from Government: the creation of a digital citizenship programme, to be compulsory in every school; implementation of the intent of the General Data Protection Regulation, by introducing simplified Terms and Conditions for digital services offered to children; and a new Children s Digital Ombudsman to mediate between under 18s and social media companies. While I am pleased to see that progress has been made around digital education and the implementation of the GDPR, an innovative and bold new approach is needed if we are going to help children develop a healthy relationship with their smartphones, tablets and laptops, and rebalance the playing field between them and the internet giants. Offline, adults, especially parents, must aim not just to educate children as they grow up, but to help them develop resilience and the ability to interact critically with the world. Without this, children fail to develop as agents of their own lives. We now have to recognise that for today s children, the online and offline worlds have all but merged, and we must equip them to negotiate both, with equal knowledge, preparation, confidence and skill. I welcome this new addition to the growing literature about this issue and applaud the great contribution 5Rights makes in articulating the rights and needs of children in the digital environment. In publishing the Disrupted Childhood Report, it continues to lead the way. Anne Longfield Children s Commissioner for England11 In the UKIntroduction The demand to create a better digital environment for young people is often synonymous with a call to curtail access. 5Rights Foundation does not support this view. Children and young people consider access to the digital environment as both desirable and essential. The ability for digital technologies to offer prosperity, communality and social benefit must be celebrated. 5Rights believes that the rapid growth of the digital environment must be on terms that meet the needs of children and young people and that every child should be able to access the digital world creatively, knowledgeably and fearlessly . 86% of three to four-year-olds have access to a tablet 83% of 12 to 15-year-olds own a smartphone 64% of children aged 12 to 15-years-old own three or more devices14 It is hard to overestimate the importance of digital devices in a child s life. Since the introduction of smartphones and tablets in the mid to late 2000s,13 the speed of adoption has been rapid and personal devices have become ubiquitous. In 1965, Gordon Moore, co-founder of Intel, predicted that the memory capacity of devices would double every year as the chip size simultaneously shrank (Moore s Law). While processing power has increased, devices have become cheaper and more available. A child with a smartphone now has processing power more than one thousand times greater than that of Apollo 11 in their pocket.15 Whilst the pace of technological change has run far ahead of public understanding, for children the digital environment is not an optional extra. Devices are used to study and learn, share opinions and interests, make arrangements and consume entertainment and news. They are also used as a gateway for creativity and building relationships. Children are growing up in a rapidly changing society where smart toys, smart cars, smart homes, smart cities (cashless and cordless) are increasingly the norm. Concerned debate accompanies each technological invention. Digital technologies, unlike previous inventions, not only enhance real world existence, but offer parallel alternatives. Infinitely portable and powerfully designed alternative and augmented realities are on offer 24/The impact of this persistent interactivity, fuelled by a gold-rush for children s attention, has yet to fully penetrate public consciousness: it demands concerned debate and urgent change. This report seeks to help policy makers, parents and children understand how persuasive design works, its impact on children s health and wellbeing and how it might be addressed. 5Rights exists to ensure that children s rights are upheld in the digital environment. The Fourth Right is The Right to Informed and Conscious Use . It is the right for young people to engage with the digital environment intentionally, free from deliberately orchestrated pressure to extend use. At 5Rights we work closely with children and young people. This report reflects their experiences and captures their voices. All I want to do is disconnect from my phone for a long period of time, perhaps weeks, but there are always pressures preventing me. I love the way the internet allows for lots of new opportunities, yet it prevents me from doing a lot of things. 16 Aged 17 Scrolling forever gives me a sick feeling in my stomach. I m so aware of how little control I have and the feeling of needing to be online and always consuming. Aged 1812 1Chapter Children are struggling to put down their devices The preoccupation of young people with their phones is a new norm. It is visible in public spaces, the subject of media headlines and increasingly a cause of familial conflict.18 In this chapter we look at the attitudes to time spent online. 1 Children Children are inherently optimistic about the opportunities that the digital environment offers and believe that it adds significant value to their lives.19 There are, however, multiple indications that their digital interactions can feel overwhelming or do not offer a meaningful way to connect to others. When you re not on your phone or social media you feel as if you don t know what s happening. Also, because of social media, people now struggle to function in a social area when you can t use your phone. Aged 16 Internet Matters ( revealed that 40% of secondary school-aged children and 34% of primary school-aged children feel worried that they are addicted to the internet .20 Often children display absolute devotion to their devices, on the one hand saying they could not do without their mobile phone for a day ,21 that they are best friends with their phone22 or don t feel right without it .23 At the same time, they report being addicted , attached , distracted , obliged , always consuming , having no control and feeling panicked .24 A Guardian newspaper study ( found that teenagers aged between 13 and 18-years-old experienced anxiety when asked to detox from social media. The participants said they hated not knowing what was going on , reported reaching for their phones in the middle of the night before realising what they were doing and said they couldn t unwind without social media.25 The technologies we use have turned into compulsions, if not full-fledged addictions. It s the impulse to check a message notification. It s the pull to visit YouTube, Facebook or Twitter for just a few minutes, only to find yourself still tapping and scrolling an hour later. It s the urge you likely feel throughout your day but hardly notice The products and services we use habitually alter our everyday behaviour, just as their designers intended. Our actions have been engineered. 17 Nir Eyal, author, Hooked: How to Build Habit-Forming Products Tech companies don t seem to think about how hard they are making parents lives. A parent34 of a five and nine-year-old13 In the UK 40% of parents of children aged eight to 15-years-old worry their child spends too much time gaming.35 One third of parents of children aged five to 15-years-old and 40% of parents of children aged between 12 to 15-years-old struggle to control their child s screen time .36Chapter One Children are struggling to put down their devices In the same year, Common Sense Media found that one third of American children aged between 12 and 18-years-old struggled to cut down time spent on devices; half said they felt addicted to their mobile devices .26 The tension between being governed by and devoted to their device is, in part, a result of the persuasive strategies baked into the digital services that children use. 2 Parents and carers Parents are often told that their children are digital natives ,27 which implies that children are in control. In reality, research consistently shows young people do not climb far up the digital ladder of opportunities ,28 but instead spend most of their time on a handful of platforms, predominantly social media-based.29 Meanwhile, headlines scream about bullying, sexual content and grooming,30 to which the policy response is to teach children to be resilient.31 This suggests that children are able to make effective decisions regarding their own safety and wellbeing, in an environment over which they have little control and which has not been designed to meet their needs or serve their best interests.Parents are also told that their child s life prospects are dependent on technology and that only those competent and confident with digital technologies will survive the radical changes in the job market. Last year, professional services firm PwC predicted that around 30% of jobs in the UK are potentially at high risk of automation by the early 2030s.32 The World Economic Forum advises equipping children and students with skills to harness the power of technology, to ensure that current and future generations are not left behind in the global digital skills race .33 These mixed messages that children are simultaneously in charge, that they are unsafe and that they must have digital skills leave many parents confused. Neither separately nor together, do they account for the full range of opportunities on offer, nor the difficulties that the digital environment presents for children, among them the impact of persuasive design. We set boundaries and when he is at home we can enforce them not easily but eventually. When he is out of our sight it is a whole other issue and I resent having to police him all the time. That isn t the sort of trusting parent I want to be. Parent of a 12-year-old While trying to manage their children s use, parents struggle with their own digital use. Web-based research platform Dscout found that the average adult smartphone user touched their device 2,617 times each day.37 Digital technology brings benefits, but it s being used in the wrong way. Secondary School Teacher39 14 The Canadian Paediatric Society ( concluded that parents found that shifting attention between screens and family life [is] stressful, tiring and reduces their ability to interact in the moment with children .38 Parents are so hypocritical about young people online often we are told that we spend too much time on our phones, just to watch them do the same. Aged 17 Sometimes [there are arguments] between my husband and me, sometimes between us and the children. It s usually because someone is on a device when someone else wants to talk to them or it s dinner time or some other family situation where the tech is getting in the way. Parent of an 11 and 16-year-old 3 Teachers Teachers are vocal about their pupils compulsive use of devices. During a session at education technology show Bett 2018,40 led by Lord Knight, chief education adviser at TES Global, more than three quarters of the teacher- packed audience voted to ban smartphones in the classroom, reflecting their frustration at trying to teach children who are surreptitiously using their smartphones. While education policies tend to embrace digital delivery, there is no overarching policy about the use of devices in educational settings. This means that it is up to individual head teachers to determine school rules and to class teachers to police them,41 setting up a frontline battle of attrition with technology designed to distract. The Association of Teachers and Lecturers ( reported: Children are coming to school with poor speech and significantly reduced language skills. They have poor social skills and their motor skills are underdeveloped. Schools are having to address this which seems to be a knock-on effect of too much screen time at home! 42 I find they [devices] are a huge distraction and students rely on them too much and don t engage with the teacher on the same level. 43 Post-primary school teacher A longitudinal study co-led by Harvard Medical School, the University of Alberta and Boston Children s Hospital in 2016 found that students ability to focus on educational tasks has decreased.44 In the same study, a teacher observed: I see youth who used to go outside at lunch break and engage in physical activity and socialisation. Today, many of our students sit all lunch hour and play on their personal devices. 45 What children, parents and teachers are experiencing is not the result of intentional use, but the consequence of deliberate design strategies that train device users to remain engaged and interactive, at any cost. Deployed singly and in consort these aspects of design are collectively known as persuasive design strategies.Chapter One Children are struggling to put down their devices The thought process that went into building these applications, Facebook being the first of them was all about: How do we consume as much of your time and conscious attention as possible? God only knows what it s doing to our children s brains. 46 Sean Parker, co-founder, Facebook15 2Chapter The commercial imperative Many aspects of the digital environment that were conceived as free and open are increasingly privately-owned and tightly controlled. Services that look free, especially to children, are predicated on a service contract paid for with the currency of personal data. The value of this data and the lengths to which the digital environment is designed to gather it are opaque to most users, and nearly all children. 1 The attention economy The Washington Post ( revealed the 98 that Facebook used to profile its users that it offered to advertisers. The included information on whether a user was expecting a baby, the type of car they drove, their political affiliations, religious beliefs, net worth and the number of credit lines they had. 47 Research by Cambridge University s Psychometrics Centre in collaboration with Microsoft Research Centre (, found that with nothing more than the Like button, a user s sexuality (88% and 75% accuracy for men and women respectively), drug use (65% accuracy), parental relationship status (60% accuracy), ethnicity (95% accuracy) and political views (85% accuracy) were revealed. 48 Many users, particularly children, have limited understanding of how much information they are revealing. Data gathered from multiple digital activities provides a profile of the user that can be used to ascertain or predict social and market trends. It may be used as part of a big data set that helps data analysts and computer scientists (including medical) to understand patterns of behaviours and outcomes or to gauge whether an individual has the appropriate attributes and skills for a job vacancy or a place in an educational establishment. Such data may be used for direct marketing purposes or to calculate an individual s social status or financial assets. In recent months, concerns have also been raised about the extent to which personal data has been used for profiling purposes during political campaigns and how surveillance data gathered by government agencies may have been sold or shared with artificial intelligence developers,49 whose purposes and impacts are not yet known. The current generation of children are the first to have data collected about them at every stage of their life. Professor Deborah Lupton and Dr. Ben Williamson in The Datafied Child ( point out that many parents start constructing a digital profile before their child is even born.50 81% of children have a digital footprint before they are two years old.51Digital technologies can be put to an infinite number of tasks and uses, but the last decade has seen the digital environment become increasingly commercialised. It makes me angry that businesses use specific designs to keep young people on their app/website. They are exploiting unknowing, young people so that they are able to build up ad revenue. Aged 1716 Chapter Two The commercial imperative This runs counter to social norms offline where it is understood that children have a right to privacy, and that the vulnerabilities associated with childhood make it inappropriate to profile children. Data is cheap to collect and can be easily shared. Those who control data repeatedly extract value at minimal cost, resulting in high-value companies with lower costs than traditional industries. The Big Five Apple, Amazon, Alphabet, Facebook and Microsoft or Seven, if you include Chinese giants Alibaba and Tencent, control the bulk of devices and services in the data collection value chain; they are also among the most valuable companies in the world.52 Central to this value chain are persuasive design strategies that entice and keep the user online in order to create more data.2 Persuasive design Never before in history have such a small number of designers had such a large influence on two billion [now three billion] people s thoughts and choices. 53 Tristan Harris, ex-Google Ethicist, founder of the Centre for Humane Technology Persuasive design, a term coined by psychologist BJ Fogg, combines the theory of behavioural design with computer technology. Behavioural design uses a system of rewards and punishments to determine human behaviour patterns. Both persuasive and behavioural designs can be used to increase wellbeing for personal and social good. However, it is arguably more often used to manipulate human behaviour so that users subconsciously act in the commercial interests of others. What is characterised as a struggle for attention is, in fact, deliberately orchestrated, engineered and designed. Persuasive design strategies are deployed for commercial purposes to keep users online. You lose precious time with your friends and family that you cannot get back. Aged 13 It is not reasonable to design services to be compulsive and then reprimand children for being preoccupied with their phones.17 A brief history of behavioural design At the beginning of the 20th Century, Russian physiologist Professor Ivan Pavlov discovered how to get dogs to produce an instinctive salivating response to a stimulus that bore no relationship to food. Having observed that dogs naturally salivate in anticipation of food, Pavlov experimented by ringing a bell whenever he fed the dogs. He then stopped bringing food and only rang the bell. The dogs, conditioned to associate the ringing with food, continued to salivate at the sound of the bell. This is known as classical conditioning. In the 1940s, psychologists BF Skinner and Charles Ferster built on Pavlov s work introducing schedules of reinforcement whilst experimenting with pigeons. They found they could teach the pigeons that their behaviour had consequences. This form of reinforcement, operant conditioning , requires the deployment of both reward and punishments to be effective. Classical and operant conditioning are acknowledged as having strengths and weaknesses, but others have gone on to build on the key insight that human and animal behaviour can be conditioned (trained) to change. In the 1990s, neuroscientist Wolfram Schultz demonstrated that once the brain receives a cue or trigger to behave in a way that is rewarded, it will automatically seek out further rewards. His findings implied that the human brain could be trained to repeat reward seeking actions. Schultz concluded that the use of reward signals was so powerful that they constrained free will to act.In the late 1990s, Professor BJ Fogg set up the Persuasive Design Lab and soon after published Persuasive Technology: Using Computers to Change What We Think and Do . 54 By 2009, he had developed The Fogg Behavior Model , combining advances in technology with behavioural science. The Behavior Model enabled computer scientists to build software that reward or punish certain behaviours in order to elicit desired changes in behaviour.55 Whilst Fogg s Persuasive Design Lab was set up with the intention of combining technology and behavioural science for social good (for example by developing programmes that use persuasive design to help people stop smoking or resolve conflict), the Lab became a hothouse for Silicon Valley. Alumni include Mike Krieger, co-founder of Instagram; Tristan Harris, ex-design ethicist at Google; and Ed Baker, head of growth at both Facebook and Uber, among others.56 Fogg s is not the only theory of behavioural design; another notable example is Professors Richard H Thaler and Cass R Sunstein s Nudge Theory. Their model uses choice architecture to ask questions in a way that nudges individuals behaviour in beneficial directions without restricting freedom of choice .57 The Nudge Theory found favour with Britain s former Prime Minister David Cameron who set up The Behavioural Insights Team within the Cabinet Office in July 2010 to enable people to make better choices for themselves .58 Separately and together these theories build on the proven concept that human behaviour can be manipulated by priming and conditioning, i.e. by manipulating human instincts using rewards and punishments.Chapter Two The commercial imperative18 If it s free, how do they make money? By collecting and selling your data. The US Federal Trade Commission lists the following data as routinely gathered.Chapter Two The commercial imperative Our ability to live the lives we want to live through technology is a design problem, not just a personal responsibility problem 62 Tristan Harris19 3 The zero-sum game for our attention The power to design user behavior ought to come with a standard of ethical limitations. 59 Nir Eyal This year, former Google design ethicist Tristan Harris launched the Center for Humane Technology, which describes the challenge of persuasive design in the following terms: There s an invisible problem that s affecting all of society Facebook, Twitter, Instagram, Google have produced amazing products that have benefited the world enormously. But these companies are also caught in a zero-sum race for our finite attention, which they need to make money. Constantly forced to outperform their competitors, they must use increasingly persuasive techniques to keep us glued. They point AI-driven news feeds, content and notifications at our minds, continually learning how to hook us more deeply from our own behaviour. Unfortunately, what s best for capturing our attention isn t best for our wellbeing: Snapchat turns conversations into streaks, redefining how our children measure friendship. Instagram glorifies the picture-perfect life, eroding our self-worth. Facebook segregates us into echo chambers, fragmenting our communities. YouTube autoplays the next perfect video, even if it eats into our sleep. These are not neutral products. They are part of a system designed to addict us.60 The commercial imperative of Big Tech to design compulsive use into digital products and services conflicts with the needs and rights of children.61 In considering how to fulfil those needs and rights, we must first understand persuasive design strategies. I worry that he seems to be overwhelmed with so many messages and constant communication from his friends. The alerts go off constantly. I couldn t cope as an adult, it is overwhelming for children. Parent of a 12-year-old Even though social media can be great, it can be like a contagious disease where people can t stop looking at their phones and spreads the word of oh you need to look at this . Aged 12Chapter Two The commercial imperative The short-term, dopamine-driven feedback loops that we have created are destroying how society works No civil discourse, no cooperation, misinformation, mistruth. 63 Chamath Palihapitiya, former vice-president of User Growth, Facebook20 3Chapter Strategies that keep users online Persuasive design strategies and techniques may be used singly or in concert but they all follow Professor Fogg s understanding that human instincts can be accelerated, nudged and determined by technology that, in turn, changes or trains human behaviour. In this chapter we explain the power of some of the most commonly used persuasive design strategies. 1 The rush (dopamine) Human beings respond to the promise of a reward by releasing a chemical in the brain known as dopamine.64 In some settings the reward is obvious; for example, an affirmation, such as a Like, from another user. Others are less understood; for example, typing bubbles or a read receipt. The anticipation triggers a small release of dopamine, which technology theorist Dr. Michael Chorust has described as the brain s reward-seeking drug .65 Once the reward has been absorbed, the dopamine fades leaving the desire for more. Children s predilection to seek immediate gratification makes them particularly susceptible to habit-forming rewards.66 This makes it difficult for them to ignore the prospect of a dopamine reward, even when this conflicts with other essential daily activities, such as sleeping or eating.67 Variable rewards hold a special thrill, as the user anticipates a reward that they know could come but is tantalisingly just out of reach. A gambler waiting to see where the roulette wheel will stop or a viewer watching a presenter s dramatic pause before they announce a winner. In both cases, the individuals experience a dopamine rush as they anticipate the unknown outcome. Professor Adam Alter explains: ...it s not guaranteed that you re going to get Likes on your posts. And it s the unpredictability of that process that makes it so addictive. If you knew that every time you posted something you d get 100 Likes, it would become boring really fast. 68 Online services are littered with these apparently benign reward features.Case study Angry Birds You feel the need to use social media all the time in order to be social or popular. Aged 1421 Chapter Three Strategies that keep users online Angry Birds is just one example of thousands of games that use reward loops to make playing compulsive. What is less apparent is that the same persuasive strategies are woven into most other digital services, such as social media, shopping, news, education or even entertainment. They prime users to repeat behaviours; as the loop becomes ingrained so the action becomes a habit. Neuroscientist Norman Doidge explains that the brain is not static but that conditioning (repeated activities) alters it. The ability for the brain to change is called neuroplasticity ;72 this makes it more capable of adapting to a changing environment, but also more vulnerable to outside influencers.73 A user s device is the means to access rewards; an equally integral part of the loop. Users habitually touch their pocket, bag or phone to check for their smartphone and then reactivate it in order to generate new rewards.Dr. Chorust describes the game Angry Birds as a terrific manipulator of the brain s dopamine system .69 The game involves firing cartoon birds from a slingshot to knock down precariously built towers. Launched in 2009, it has since been downloaded more than 7 billion times.70 The dopamine action in your brain makes you want to know, urgently, what will happen when you fire the bird. And it s extremely easy to get yourself in a position of wanting, because the game is so simple. It gives you intermittent but extremely satisfying rewards. So you pull the slingshot again and again and again. And again and again and again and AGAIN . 71 2 Popularity contest Human beings are social beings. Our identity is defined by and measured against other group members. Persuasive design strategies exploit the natural human desire to be social and popular, by taking advantage of an individual s fear of not being social and popular in order to extend their online use. For young people, identity requires constant attention, curation and renewal. At key development stages it can be overwhelmingly important to be accepted by your peer group.74 Quantifying friends, Likes, retweets or followers creates a public metric of personal value. At a glance, one user can see how many connections or responses another is getting and measure themselves against that. Posts ranked by popularity in a newsfeed are given pride of place on the screen, algorithms designed to promote the already popular, help them travel further, while constantly building the statistics. Quantity not quality of interaction is the metric that is measured. Sometimes using Snapchat can feel like you have achieved something when all you receive is a number. Aged 14 Fear of missing out, which even has its own widely used acronym FoMO , is the inverse of the popularity contest. Professor Andrew Przybylski et al describes FoMO as a pervasive apprehension that others might be having rewarding experiences from which one is absent .75 Those who regularly experience FoMO display a slavish need to stay online just in case they miss an opportunity for personal validation, or to confirm their own low status by passively watching others more popular than they are, exacerbating their experience of missing out. Such pervasive apprehension is fuelled by automated and targeted messages pointing to the activity of other users in an individual s network (and the network of their network) revealing a vast swathe of activity from which they, the non-active user, is excluded. Companies target your paranoia to make you feel you re missing out and that if you re not online something drastic concerning you may happen. Aged 1622 Those with FoMO use social media much more compulsively, including checking social media accounts as soon as they wake up, during meal times and last thing at night.76 The need to quantify relationships, and the associated pressure to not miss something, compels a child to enter a cycle in which they act and share continuously, thereby extending their time online. 3 Summonses: buzzes, pings, vibrations and the colour red When I walk around and see people staring at their phones often it s because they ve taken out their phones to look at Facebook notifications, that s something I feel is not going in the right direction for society. 77 Justin Rosenstein, co-designer of the Like button Human beings respond to noises, movements and light. It is a necessity borne from our hunter-gatherer forebears who needed to be alert to the presence of predators or other dangers.78 Summonsing is one of the most powerful strategies of persuasive design. Summons come in many forms. It might be a pop up or a locked screen message; short, long or insistent vibrations; surges of light or sharp sounds or a personalised call. All are designed to create a sense of urgency, which acts as a powerful summons. Often the only way to stop the influx of notifications is to comply with the call for attention. You can feel weak emotionally and vulnerable after spending too much use. Aged 16 You can t leave it because you d be up all night answering the old messages and the new ones asking why you didn t answer the first message sometimes when I get back to my phone then I get LITERALLY hundreds. Aged 14 Children are less able to make a hierarchy of demands so tend to answer the newest first instilling a habit of responding to the new .79 This has profound implications since routines and habits formed before the age of nine are unlikely to change.80 In 2017, Ofcom reported that 53% of three to four-year-olds were online, a statistic that rose to 79% of five to seven year-olds and 94% of eight to 11- year-olds.81 Habit-forming summons are further enhanced by machine learning and artificial intelligence systems82 which are able to learn when a user is most likely to respond, so send notifications at an optimal time .83 Leanplum, a mobile marketing platform, recommends services push messages between 8pm and 10pm to get the most impact.84 Re-engagement with gaming apps following push notifications peaks between 11am and 1pm, while social media peaks at 10am and then again at 11pm. 85 Whilst some settings can be switched off, this action almost always triggers warnings to users that they risk missing out on new content.86 Users are then forced to weigh up the intrusiveness of the notifications against their personal FoMO. For many children, this represents an impossible choice. Tristan Harris highlights the power of the small red circle. Red is a trigger colour, he explains. That s why it is used as an alarm signal. 87 The small red circle that appears on apps, messages and updates, implies urgency and encourages the user to respond by checking, thereby re-engaging with the service. The persuasive strategy of constant summons creates an exhausting level of demand that exploits a child s human instinct to respond.Chapter Three Strategies that keep users online Using technology often makes me feel like I could be missing out on time actually with my friends. Aged 1523 4 Slowing progress These barriers force a user to do or consume something before they can access the information they are seeking. They might take the form of sponsored ads above searches, videos that break up news articles or shopping sites that prevent precision searches. Each barrier is small, but as the user swipes, removes and negotiates the barriers, their time online is extended before getting to what they initially sought. Anthony Wagner, associate Professor of Psychology at Stanford University, explained: Where there are multiple sources of information... [users] are not able to filter out what s not relevant to their current goal. That failure to filter means they re slowed down by irrelevant information. 89 5 Pace of play Users are more likely to stop if the pace becomes predictable. Games partition progress into levels and change the pace and intensity of play, to offer the prospect of resolution while obscuring the fact that the game is designed to be played indefinitely, or at least as long as possible. Varying pace of play is not restricted to games. On social media, users can be swiftly dragged into a high speed retrospective of the last 24 hours in the life of someone they may or may not know 90 before introducing a slower pace as they scroll through endless feeds whilst notifications offer pacy interjections. 6 No save Some games prevent users from saving progress until they reach a predetermined point. If they break away before this point, all previous progress is lost. So, players play on. Each of these design features creates a pull towards extended use. The ubiquity of these strategies results in a very real sense of having lost time doing you are not quite sure what.4 Losing time I have two kids now and I regret every minute that I m not paying attention to them because my smartphone has sucked me in. 88 Loren Brichter, designer of the pull-to-refresh mechanism Routinely, the amount of time required or spent doing online tasks is concealed. The decision to continue watching, playing or scrolling is designed into the service. Examples of this are ubiquitous in the digital environment, but among them are: 1 Auto play, auto suggestion and infinite feeds Each one automatically replaces the next piece of content or action before the previous one has finished, thereby minimising or eliminating breaks during which a user might decide to disengage. 2 Creating a bubble Music or sounds are introduced to desensitise the user to their immediate real-life surroundings. These are combined with sharp intrusive sounds that make the player hyper-aware of the screen. Such techniques are particularly used in gaming. 3 Small demands These are invitations, such as click here , watch video , accept invitation , Like , agree , post or read message . Each demand seems small but will frictionlessly lead to further demands for action.Chapter Three Strategies that keep users online24 In a recent 5Rights workshop, we met a nine-year-old boy who spoke about his arguments with his mum and dad when they asked him to stop playing a particular game. He described screaming matches, missed dinners, exasperated parents and his own personal regret. This child was using a game that took three hours to complete, designed with no save function so that there was no alternative to the deliberately orchestrated desire to complete. As a result, he neglected offline aspects of his life including sleep, meals and friends, and was at loggerheads with his parents. He was visibly upset. All he wanted was a pause or save button, but it was not designed into the game, in order to make the pull to play greater than the opportunity to stop.No save button3.5 Social obligation A social validation feedback loop exactly the kind of thing that a hacker like myself would come up with, because you re exploiting a vulnerability in human psychology. 91 Sean Parker, co-founder, Facebook Part of being a social animal is a sense of obligation weighted against the nature and depth of the social bond. Most people feel a greater obligation to their trusted circle of family and friends than to the broad network of people they know less well, and significantly more than their obligations to those at the furthest fringes of their community. By contrast, online reciprocity frequently extends indiscriminately to as many people as possible in the user s network so that, in addition to intentional acts of social validation or communication, it can require large numbers of responses that do not acknowledge the complexity or limits of the relationships. Young people engaged in swiftly changing friendship patterns are held to old obligations, or made to feel guilty about moving on. This presents a perfect scenario for social anxiety. The obligations baked into services are presented in a manner that deliberately punish inaction, for example, by letting the sender know when the recipient has received or read a message or text. Knowing someone knows that you are online creates a heightened obligation to respond. Creating large quantities of social obligations within online relationships offers not only the exhausting prospect of constant social management, but can prevent the development of more nuanced and satisfying relationships driven by personal choice not numerical highs. For children at different development stages, their peers represent a powerful mirror of status and identity. Persuasive design strategies that emphasise quantity over quality create the backdrop for social anxiety and issues of self-esteem.Chapter Three Strategies that keep users online (There is a) pressure of losing your friends and ending lifelong friendships if you forget to send a streak one day. Aged 1325 6 Emotional highs Highly emotive content attracts and holds attention and increases engagement. As Jaron Lanier, the inventor of virtual reality, explains, companies hold our attention best by making us angry, insecure or scared: The most effective situation is when users get into weird spirals of mob-like agreement or disagreement with other users. 92 Polarised and/or extreme content keeps users online as they click through to the next equally emotive and extreme story, creating a cycle of activity. Clickbait is a term used for a title or heading that is deliberately written in a manner that entices a user to click on a link. Journalist Arwa Mahdawi describes YouTube as a terrifying cesspit of clickbait content , with one YouTuber feeding a homeless man biscuits filled with toothpaste and another killing her boyfriend in a prank gone wrong.93 Clickbait articles tend to disappoint, but by using punchy emotions such as anger, humour or inspiration and by promising to satisfy users curiosity, they offer the promise of something rewarding. Bryan Gardiner, a lecturer in computer science at Ulster University, writes: How many cheap emotional ploys, false promises and empty listicles and quizzes can a person endure?... research has shown that humans are quite willing to put up with massive amounts of disappointment and frustration, so long as there is an occasional pay-out. 94 A variable reward. Whilst clickbait offers a clear example of extreme content it comes in many forms. Newspapers have followed the success of clickbait by introducing their own screaming headlines online. True stories take six times as long to reach people than fake ones filled with outrageous claims,95 and comment boxes are filled with aggressive and counter aggressive opinion. When two users each send the other a snap a day for three days in a row, they start a streak. The goal is to keep the streak going for as long as possible. As the unbroken streak rises, it becomes a way of quantifying a friendship. It is common for children to maintain multiple streaks with competing friendships or to build streaks with children they don t know well to appear popular. Streak management can be time-consuming and distracting. The user is notified each time they receive a snap, which acts as a prompt to reciprocate. The compulsion to maintain the streaks, coupled with the need to not let others down, means that it is not unusual for children to get friends or siblings to babysit their streaks at times they are unable to access their phones. Breaking a streak is viewed as an indictment of a friendship. To avoid these socially awkward events, users are obliged to send multiple snaps a day irrespective of the quality of the relationship or the content of the communication. This cycle of obligation is deliberately designed to encourage repeat visits to Snapchat. The maintenance of children s streaks can run into many hours a week. In a recent 5Rights workshop, children were astonished by their weekly total time spent on Snapchat. One boy discovered that he had spent 32 hours on the app effectively four working days during each of the previous three weeks.Case study Snapchat streaksChapter Three Strategies that keep users online I m happy as long as she is occupied [but] every day [there are arguments]! Usually when I m trying to get her to do something else like go to bed or do her homework. Parent of a 10-year-old26 Professor John Suler discusses the toxic online disinhibition effect, where people are more likely to share personal information or display more intense behaviour than they would offline, including rude language and harsh criticisms.96 Young people, particularly early teens (aged between 13 and are characterised by idealism and tend towards polarised thinking making it more likely that they will respond to emotionally charged content.97 Professor Harry Dyer from the University of East Anglia explains that the mounting pressure to outdo oneself and others demands more extreme content, until eventually inevitably a line is crossed. Whether by embodying beauty ideals or eliciting laughs, everyone in the omniopticon [an increasingly powerful form of social surveillance where the many are watched by the many]98 is scrambling to be at the centre of attention people are pushing the limits in order to get noticed and this includes doing bizarre and even deadly things like eating laundry detergents. 99 Not all extremity is hateful, bullying or violent. Emotional reactions also come from the cute and the fuzzy. Parents, keen to occupy their children, put on nice content to keep them entertained, priming children to expect a cycle of emotional reward and the desire for more. Case study Sneezing pandas and dancing kittens Among the most watched and shared videos on YouTube are those that feature cute animals. In 2014, internet data from video marketer ReelSEO showed that two million cat videos were posted on YouTube, collectively getting more than 6 billion views.100 Humans are instinctively drawn to people, animals and even cartoon characters with infantile features (such as disproportionately big eyes, chubby cheeks and large foreheads) as they trigger the user s baby schema or cute response. That is our evolutionary instinct to nurture and care.101 The baby schema response emerges early during development. Children as young as three to six-years-old are drawn to, and will spend longer staring at, images of animals that have high levels of childlike features.102 Triggering a child s cute response keeps children, especially very young children, engaged for longer. Cute images release dopamine, encouraging users to seek out further images. Yale psychologist Oriana Aragon says: We want our cute fix. 103 The use of cute as a reward appears as one of the least problematic persuasive strategies. But as we outline in Chapter Four, a key issue for children, even very young children, is the opportunity cost what they are not doing while watching cat videos.Chapter Three Strategies that keep users online The more time you use social media the more addicted you are and there is no control over it. Aged 1527 4Chapter Impact of persuasive technologies on childhood Access to digital technologies for life, learning, social and entertainment is crucially important to children and young people, for the development of communities and for the future of society as a whole. The digital environment is entirely man and woman made. Any, or all, of the persuasive design strategies described in Chapter Three could be abandoned, recalibrated or redesigned to meet the needs of children and young people. 1 Anxiety and aggression Encouraged by persuasive loops of reward, reciprocity, obligation, heightened emotion and automated spread of content (particularly highly-charged content), children and young people are sharing their photographs, opinions, personal information and vulnerabilities on an unprecedented scale. 104 We have explained how the need for validation creates a habit of needing more. Managing such public and frequent interactions creates enormous pressures for young people, and with it comes anxiety, low self-esteem and mental health issues105 at ever-increasing levels.106 Professor Jon Elhai et al in the Journal of Affective Disorders (, is one in a long line of academics who link elevated levels of depression to excessive social media use.107 Stanford University s Professor Clifford Nass agrees. He warns that children have an unrealistic world view provided by the overwhelmingly happy curated postings they see online. He argues that this leads to the erroneous conclusion that everyone is happy, except me .108 An excessive amount of sharing also translates into exaggerating, polarising and aggressive behaviour, fuelled by the need to get noticed. Ditch the Label s Annual Bullying Survey last year found that 69% of respondents admitted to having done something abusive towards another person online,109 while 35% of respondents had sent a screenshot of someone s status or photo to laugh at them in a group chat.110 Girls are disproportionately affected. In 2017 Plan International UK found that almost half of girls aged between 11 and 18-years-old had experienced online abuse.111 The young contributors to The Internet on Our Own Terms focused on the personally damaging digital content which left them feeling highly vulnerable online .112 The participants recounted that other children had been bullied, forced to move schools, and that police had even become involved when a child s private content had been shared more widely.113 The culture of excessive sharing, fuelled by persuasive technologies, has resulted in an epidemic of self-doubt, anxiety, low self-esteem and correspondingly aggressive behaviour among the young.114 28 The pressure to be popular online is reinforced by the social networks with their invasive and frequent notifications, guilt-trip emails and emotional account deactivation processes. Social media outlets are quick to glamorise their most popular users and showcase them to global audiences as a tool to encourage deeper content consumption and creation. In order to counteract the damage that social media is having on the lives of young people, social networks must be transparent about their models of revenue generation and algorithm changes. The dangerous trend of seeking external validation online must be counteracted with self-esteem training for children. Furthermore, young people must be empowered to make educated decisions about the ways in which technology weaves into their lives. They should not be guilt-tripped into creating content or keeping their social media accounts active with automated emails and emotional processes.Social media addiction and personality augmentation online Dr. Liam Hackett, Chief Executive, Ditch the Label A recent Ditch the Label survey of more than 10,000 young people aged between 12 and 20-years-old found that 61% of respondents would struggle to last longer than 24 hours without access to their social media, with many suggesting that it would make them anxious, lonely or distressed.115 We are increasingly a society of social media addicts, with young people being some of its biggest consumers. Against a backdrop of media rhetoric that tells us, often in the most nuanced of ways, that we aren t quite good enough, many young people are seeking external validation online through the use of social media. Part of the appeal of social media is the ability to publish an augmented version of one s reality in an attempt to be the person with the most Likes or the most followers. Increasing amounts of young people are using social media versions of reality as benchmarks against their own lives; inevitably leading to significant rises in the rates of depression, anxiety and body dysmorphia, amongst many other health and behavioural issues.Chapter Four Impact of persuasive technologies on childhood Often, we can be right next to each other and they will still be Snapchatting each other. Aged 1529 2 Quality of relationships The quality of social interactions diminishes the more devices are used.116 Brown, Manago and Trimble ( found that the sheer volume of people and events, and the emphasis on popularity numerically quantified, lowered the quality of communication.117 Dr. Caroline Fisher similarly argues that pathological internet use affects an individual s sense of wellbeing and can lead to social withdrawal, self-neglect, poor diet and family conflict.118 The PISA Wellbeing report found that children who are extreme internet users (defined as more than six hours) were 1% less likely than moderate users (1 2 hours) to report a sense of belonging, and more likely to report feeling lonely, at school.119 Meanwhile a Common Sense Media report found that 70% of American teenagers, aged between 12 and 18 years, fight with their parents about their devices; 32% on a daily basis.120 The Education Policy Institute evidence review found that excessive internet use is preventing young people from developing strong relationships offline.121 Online relationships can enrich a child s social and emotional life, especially those who may be isolated in other settings. However, the persistent demands to interact often diminish the quality of relationships, levels of emotional understanding and create conflict. 3 Opportunity cost There is an undeniable truth that if you spend (or lose) a great deal of time doing one thing, something else must give . This is the opportunity cost. Creativity, autonomy, memory The potential to access information, creative activities, undertake research or build and maintain important relationships online must not be ignored. But creative activities of UK children only occupy around 3% of their total time online,122 meanwhile UK teenagers123 are spending less time on informational, civic and creative activities now, compared with a few years ago.124 I love reading, but by the time I ve spent an hour too long on my phone, I can no longer read my book. Aged 17 MIT Professor Sherry Turkle notes: The capacity for boredom is the single most important development of childhood. The capacity to self-soothe, go into your mind, go into your imagination. Children who are constantly being stimulated by a phone don t learn how to be alone, and if you don t teach a child how to be alone, they will always be lonely. 125 Development of memory is another opportunity cost. Dr. Benjamin Storm s ( research on internet use and memory found that when participants were allowed to use Google to answer questions, they used it even when they already knew the answer. 126 He commented: Memory is changing. Our research shows that as we use the internet to support and extend our memory we become more reliant on it. Whereas before we might have tried to recall something on our own, now we don t bother. 127Chapter Four Impact of persuasive technologies on childhood It is hard to live in a technological society and not get trapped in social media. You find that someone does it so you do too, and after a while you are addicted and miss out on things in life. Aged 1430 Memory and imagination share the same set of development and cognitive needs as agency that is, children making choices based on information that they can understand in conditions that allow for those choices to be meaningful.128 The development of memory is a key component of creating an individual s identity, holding shared experiences and therefore forming a group identity a necessity for building and maintaining communities and society. Sleep and sleep deprivation Perhaps one of the most publicised opportunity costs of compulsive device use is sleep deprivation. In a large-scale 2016 survey for JAMA Paediatrics, academics from King s College London found: Bedtime use of media devices doubles risk of poor sleep in children. 129 Specifically, it leads to inadequate sleep quantity, poor sleep quality and excessive daytime sleepiness because bedtime use disturbs sleep patterns of children and stimulates the brain s production of melatonin. 130 I spent 14 hours on the computer in one day learning [a computer game]; I was up until 3am the next day. Aged 17Results from a three-year pilot programme in Canada that developed a school-based sleep promotion programme for students, found that children who don t get enough quality sleep are more likely to have excess body weight, poorer diet quality, and lower physical activity levels.131 Education London School of Economics ( research found that student performance in exams significantly increased post-mobile phone bans. Specifically, LSE researchers working with a group of low-achieving, low-income students found smartphone use in the classroom exacerbated existing educational inequalities.132 Students using phones during class time affect whole classrooms as well as individual academic performance. Daniel Pulliam, author of Effect of Student Classroom Cell Phone Usage on Teachers ( found that 87% of teachers were distracted by students using phones. He added that teachers cognitive processes, such as working memory and the ability to stay focused and maintain awareness, decreased and led to weakened classroom performance. 133 The opportunity cost of attracting and keeping children online impacts on their creativity, autonomy, memory, sleep and education.Chapter Four Impact of persuasive technologies on childhood 31 Some children and young people have a higher propensity towards using smartphones and tablets to excess. This changes over time and is closely linked to resilience as well as the ability to withstand environmental stimuli in a positive and constructive way. From my experience working with young people who experience significant compulsive behaviours, their inability to manage the amount of time they spend online playing games, watching YouTube or being on social media is often closely linked to emotional states that may feel overwhelming. These are normally negative ones such as low mood, anger, feelings of abandonment and fear of social exclusion. At times, these behaviours worsen as the young person disengages from previously rewarding activities and relationships in the real world. Someone may stop attending netball team practice or their music lessons and thus cut themselves off from a whole series of nurturing and positive relationships fuelled by shared interests to seek out online relationships with fellow gamers or friends. The more isolated the person becomes, the more likely they are to turn towards online activities to supplement the loss of interaction. Many end up as recluses in their bedroom when their activity has intensified in terms of hours. There is an ongoing debate as to whether some of these intense compulsive behaviours can be deemed to be addictions. For example, if someone is gaming 14 hours a day, he may be defined as suffering from Gaming Disorder. A young person playing online poker all night may be suffering from Gambling Disorder. However, there are many more young people who use social media and gaming in an excessive way who, while not addicted, still use their mobile phones and tablets too intensely. This problematic use is often an attempt to navigate the difficulties of growing up in contemporary society. Driving users to understand the need for screen-free time, for exercise and for real life interactions is part of a stimulus control approach to shaping behaviour that will benefit everyone, whatever their age.When excessive behaviour becomes pathological Dr. Henrietta Bowden-Jones, Imperial College London, expert on behavioural addictionsChapter Four Impact of persuasive technologies on childhood 32 We are entering unprecedented territory when it comes to parenting children in the digital world. When television first made its appearance in the 1950s there was widespread concern about the effect this would have on the way children learnt and played. While today s narrative about the digital world is not so full of suspicion and fear, we are dealing with a far more ubiquitous issue with children as young as three-years-old having frequent access to smartphones and tablets. The research is growing but still lags behind the rapid pace of technological development. What is clear, however, is that we have to understand each child s developmental needs in order to truly get to grips with the opportunities, as well as the risks, of digital engagement. For example, pretending or role playing is an essential past time for two to five- year-olds. The opportunity to play at being a grown up or to pretend to be a superhero serves an important function in terms of cognitive and social development, of identifying with others and building self-identity. Vast increases in digital use by pre-schoolers leaves less opportunity for important self- propelled and imaginative play. The increase in digital play in this age group means that pre-schoolers are engaging in different types and quantity of pretend play, with, as yet, unknown consequences.Let s pretend: Creating opportunities to role play Dr. Angharad Rudkin, Children s Clinical Psychologist and Associate Fellow of the British Psychological SocietyChapter Four Impact of persuasive technologies on childhood 33 Most of the rhetoric about risks and harms of a digital childhood is based on a relatively mechanistic understanding of human behaviour, it is concerned principally with nudges and primitive models of influence. This is not fit for purpose. The most promising alternative to this mechanistic thinking is Self-Determination Theory (SDT). SDT provides a motivational continuum that runs from extrinsic motivation, carrot and stick prods, to autonomous self-regulation, behaviour that is guided by values and interests. Decades of analogue research indicates that autonomous motivation reflects an individual s internal desire to complete a task, while external motivators represent factors outside an individual s control, including rewards that may encourage tasks or punishments that limit them. Satisfaction from completing a task is higher when young people are autonomously motivated. Quality of engagement across school, social and family is also higher. Put simply, the more autonomous the motivation, the greater the likelihood that resilience and positive behaviour will be sustained once the external motivator disappears. If we are going to move beyond the risks and harms framework we will need to replace it with something better. For young people to be autonomously motivated online, we need transparent science, grounded in established psychological models. This will shift us from a reactive to a proactive understanding of digital childhood and provide a framework to fruitfully engage policy, industry, and charity stakeholders in order that children have agency in the digital environment.Prof. Andrew K. Przybylski Director of Research, Oxford Internet Institute4.4 Profiling, personalisation and surveillance Arguably a persuasive design strategy in itself, personalisation is a powerful tool by which a user is persuaded to extend use. Algorithms follow user behaviour patterns on such tight loops that they know the exact personalised mix of strategies that will work for each specific user. When it comes to children, these algorithms collect extremely intimate personal data. Lawyers Joe Newman, Joseph Jerome and Christopher Hazard explain that the move from standalone games to interactive online games brings with it a significant shift in the ability for game designers to: ...collect and generate enormous amounts of information about their players, much of which may be considered highly sensitive. This data includes information relating to the real world, ranging from a player s voice or physical appearance to [their] location or social network. It also includes detailed information from the player s actions within the game world, which may be analysed to create in-depth profiles of a player s cognitive abilities and personality. 134 This loop of data gathering and profiling is a norm across all sectors of the digital environment and creates super-charged personalised profiling, described by Professor Lupton and Dr. Williamson in their paper The Datafied Child as dataveillance . Dataveillance (an amalgam of data surveillance) is defined as: ...the monitoring or evaluation of children by themselves or others that may include recording and assessing details of their appearance, growth, development, health, social relationships, moods, behaviour, educational achievements and other features. 135Chapter Four Impact of persuasive technologies on childhood Putting people into virtual worlds can be incredibly effective at changing their behaviour, and those changes can happen without the person s awareness... What Facebook is today, with where virtual reality might go in the future, could be so destructive of a sense of truth, a sense of free will, the sense of the civil project. It could be really the destruction of us all. 140 Jaron Lanier, inventor of virtual reality34 This surveillance codifies presumptions and assumptions about a child s nature, their characteristics and ambitions at a time when children and young people are experimenting with, and exploring, their own identities. Professor Lupton and Dr. Williamson express concern that unless scientific neutrality is imposed, children s life chances and access to opportunities will be increasingly shaped by social sorting that has little or no oversight and is constructed to gather highly sensitive personal information that is extremely valuable for marketing and other commercial or as yet unknown purposes.136 The power of personalisation and potential for social, personal control is not limited to commercial considerations. For example, in China the government is developing a Social Credit System to rate and rank the trustworthiness of all Chinese companies, legal entities and its 3 billion citizens. Data will be collected as users participate both as citizens and consumers. The system not only investigates behaviour, it shapes it. 137 The complex algorithm that rates citizens has not been divulged, but factors such as credit history, personal characteristics, and behaviour and preferences are all taken into account. The government is attempting to make obedience feel like gaming. It is a method of social control dressed up in some point- reward system. It s gamified obedience. 138 Device dependence, the formation of hard-to-break habits, feelings of addiction and compulsion are all widely reported by children.139 There are also questions about the legality, ethics and safety of creating dependence and habits at a time of immaturity and rapid development. 5 The near future There is no single vision of the future of tech, but the ethical and social issues raised by persuasive design will be magnified by the emerging dominance of Artificial Intelligence, Machine Learning and the Internet of Things. This inevitable amplification of impact has led to calls for more oversight. The New York Times journalist and tech analyst, Farhad Manjoo, speaks for many when he says: My default position about whether this stuff [technology] is going to be good or bad in the world has changed. So in the past, my reflexive bias of a new piece of technology tended toward optimism it s going to make us more efficient or help us connect with people and that has to be good But I think we should all be more sceptical of the unseen and longer- term potential dangers of these technologies before we rush to embrace them. 141 As the digital environment becomes integrated with the physical environment, users will be automatically plugged in: not merely for extended use, but for permanent use. This will create a de facto situation where users are guided through life along algorithmically- determined pathways acting in the best interests of whoever owns or pays to use their data.Chapter Four Impact of persuasive technologies on childhood35 Imagine a near future in which refrigerators can sense when a child is hungry and offer snacks based on how much a company has paid for their product to be suggested. Who has responsibility for the nutritional needs of that child? Parent or carer? The Government? Or the company who controls the data gateway to the fridge? Or should the perfect nutritional balance be built into the artificial intelligence? If so, should perfect be set against income, ethnicity, an ecological footprint, a daily read out of the child s state of health, their family s traditions, ethics of food production, or simply based on what they ate yesterday? What if it doesn t spot the diabetic, a religious dietary requirement, or a life-threatening allergy? And what if that hungry child yearns occasionally for a chocolate bar but is only ever offered a carrot stick?The Fridge ProblemSome of the questions raised by the Fridge Problem resemble existing ethical questions. Some are new. But a small set of questions about a smart fridge quickly amplifies into profound questions about self-determination, rights, liability and agency. The advent of smart homes, smart schools and smart cities, creating a world where your television knows when you have sat down, or where homework is shared with future employers, and a car is designed to decide who to save you or the pedestrian at the moment of a malfunction, means that human beings and intelligent machines will have to learn to coexist. But on whose terms? In this context, the oversight of persuasive design strategies that prime human beings to behave in certain ways becomes an urgent ethical question for policy makers and civil society.Chapter Four Impact of persuasive technologies on childhood36 5Chapter Seeds of change The World Wide Web is little more than a quarter of a century old.142 Few anticipated its rapid dominance of economic and civic life referred to as the Fourth Industrial Revolution.143 In the UK, the last great industrial revolution of the 19th century saw 17 Factory Acts,144 and vast swathes of further legislation on town planning, utilities, food safety and child labour, to balance societal needs against the rise of the commercial instincts of a handful of ultra-wealthy, industrial entrepreneurs. These acts included provisions that regulated the hours and welfare of children and young people.145 The assets of the digital revolution are less visible and more mobile. Cables and servers transport and store data across the globe blurring jurisdictional lines, making it harder to pinpoint the exact whereabouts of a user s personal data. Whilst the multibillion-dollar market value of the most successful tech companies points to data as the gold of the digital revolution each piece is hard to value. The lack of clarity as to where data sits and how much it is worth makes it difficult to find, regulate or tax. Whilst some continue to assert that these issues and the fast-moving nature of digital innovation preclude effective regulation, there is an increasingly active number who believe that the unfettered commercial freedoms singularly enjoyed by the tech industry create a negative environment and that limits must be set. Including limiting the impact of persuasive design strategies on the choices and outcomes of children and young people.1 Upsetting the Apple cart Apple s chief design officer Johnny Ive, said that constant use (by children) of the iPhone was misuse , yet the device is deliberately designed to extend use.146 In January this year, in a blow to Silicon Valley s reputation, two of Apple s largest shareholders, California State Teachers Retirement System and JANA Partners, (who own approximately $2 billion of its stock)147 sent a letter to Apple148 outlining their concerns about the damage of device addiction for young people because of persuasive technologies. The letter, voicing many of the same concerns raised by this report, highlights the persuasive, psychological features of social media sites and digital applications designed to be as addictive and time consuming as possible. Whilst acknowledging the great potential of the digital world, and recognising the value of responsible usage, the writers, mainly teachers and academics, cite a plethora of negative outcomes of excessive use. The letter outlines the negative effect on children s wellbeing, including increased risk of suicide and depression, conflict with parents and adverse effects on children s cerebral and social development. The authors found the efforts made by the technology sector to monitor and address the effects of their products on children and young people inadequate and highlight that options provided to parents to address their child s usage are limited and also inadequate. 37 Chapter Five Seeds of change They urge Apple to set an example for the technology sector about the obligations of companies towards their youngest customers, by investing in enhanced software, and improving research and reporting to support and safeguard young people who will be the next generation of leaders, innovators and customers. As we go to press, Apple has announced changes to its operating system. We cautiously welcome the announcement but are yet to see how comprehensive the features are. Systemic change however, must still be implemented by all providers throughout the digital value chain. 2 The Age-Appropriate Design Code In 2018, the UK Government introduced an Age-Appropriate Design Code into the DPA. Building on the requirement of the General Data Protection Regulation that provides that children merit specific protection , the Code will set out what the UK considers to be the high bar of data regulation needed to protect those under 18-years-old. The Code will take into account children s development milestones and requires the regulator (the ICO) to prioritise the best interests 149 of children when considering what constitutes adequate data protection. Among the aspects of design to be considered by the Commissioner are strategies that encourage extended user engagement. This is the first time that questions raised by persuasive design will form part of a statutory regulatory framework, offering the opportunity to assess persuasive design strategies and militate their impact on children and young people. Shirley Cramer CBE, chief executive of Royal Society for Public Health, that co-authored #StatusofMind,150 reacting to the introduction of the Code wrote: There is a need for an informed public debate about how we protect children from the coercive and addictive elements of social media and future new technologies. To improve the mental health and wellbeing of our children we need to urgently mitigate the negative impacts of social media and accentuate the positives. The RSPH welcomes the new Age-Appropriate Design Code and believes it should be urgently implemented and rigorously enforced. As the implications of a digital-first world become clearer, and the conflicts between Big Tech s commercial imperative and society s established norms are exposed, governments across the world have begun to consider how to apply existing legal principles to the online world, and adapt, enhance and add to legislation to tackle harms, business practices and impacts specific to the digital environment. The ubiquitous use of persuasive design strategies in the digital environment is an issue that affects almost all UK children. Whilst the digital environment tantalisingly embodies both progress and the promise of creativity and knowledge, its current dependence on persuasive technology makes it a toxic environment for children and young people that limits opportunity and creativity. 38 The current asymmetry of power between the developing child and the most powerful companies in the world is not in the best interests of the child.Children are vulnerable to mental health issues associated with identity development, familial and social pressure. The digital norms relating to extended use amplify these pressures and therefore their vulnerability. It is imperative that for children to engage purposefully and playfully online, the digital environment must be designed with their needs and rights in mind. 5Rights Foundation wishes to see a global effort to set the ethics, governance and legal boundaries for the global technology companies and those that use technology to engage with children. This issue is bigger than any single nation state, bigger than a single company, and bigger than any single voice. All stakeholders have a duty to start the process towards ethical design standards of services for children. The children and young people who contributed to this report and those who engage with 5Rights, want more choice, more control and more peace. Chapter Five Seeds of change39 Executive summary The Brain That Changes Itself: Stories of Personal Triumph from the Frontiers of Brain Science , Norman Doidge, Penguin, 2007 Article 3, United Nations Convention on the Rights of a Child p. 13, UK Children Go Online , S Livingstone, October 2003 p.436, The International Handbook of Children, Media and Culture , S Livingstone, 2008 Paragraph 304, Growing up with the Internet Question 103, Dr. Marc Bush, Corrected oral evidence: children and the internet, evidence session no. 7, 15 November 2016 Question 104, ibid Question 105, ibid Article 3, UNCRC ProfessionalInterest/Pages/CRC.aspx Written evidence from Children s Media Foundation, (CHI0027), Life in Likes : Children s Commissioner report into social media use among eight to 12-year-olds , Children s Commission, January 2018 Introduction The history of smartphones: timeline , The Guardian, 24 January 2012 Children and Parents: Media Use and Attitudes Report , Ofcom, 29 November 2017 Apollo lunar missions computing power vs your iPhone , FutureCar, 27 November 2016 Children s quotes were made by members of the Young Scot 5Rights Youth Leadership Group, February 2018 Chapter one p.1, Hooked: How to Build Habit-Forming Products by Nir Eyal with Ryan Hoover, Portfolio Penguin, 2014 p.3, Technology Addiction: Concern, Controversy and Finding a Balance , Executive Summary, Common Sense Media, May 2016 Our Digital Rights , Young Scot 5Rights Youth Commission, May 2017 p.4, How Children Use Mobile Devices at School and at Home , Dr. B Clarke, R Atkinson, S Svanaes, September 2015, p.17, Technology Addiction: Concern, Controversy and Finding a Balance , Research Brief, Common Sense Media, May Secret lives of children and their phones , Financial Times, 6 October 2017 Ibid Comments made by members of the Young Scot 5Rights Youth Leadership Group, February 2018 I worried people would forget about me: can teenagers survive without social media? The Guardian, 18 June 2016 p.2, Technology Addiction: Concern, Controversy and Finding a Balance , Executive Summary, Common Sense Media, May 2016 Digital Natives, Digital Immigrants , Mark Prensky, On the Horizon, MCB University Press, Vol. 9 No. 5, October 2001 Children s Online Risks and Opportunities: Comparative Findings From EU Kids Online and Net Children Go Mobile , Professor Sonia Livingstone et al , London School of Economics and Political Science, 2014 , p.106, Children and Parents: Media Use and Attitudes Report , Ofcom, 29 November 2017 Ten years since the Byron Review; Are Children Safer in the Digital World? , NSPCC, February 2018; Discussed in Children as young as three at risk from groomers on online games Daily Telegraph, 2 February 2018 Internet Safety Strategy Green Paper, Department for Digital, Culture, Media and Sport, 11 October 2017 Will Robots Steal Our Jobs? The Potential Impact of Automation on the UK and Other Major Economies , PwC, March 2017 How technology will change the future of work , World Economic Forum, 24 February 2016 All parent quotes from a Parentzone questionnaire on children s time online and persuasive technologies, February 2018 p.231, Children and Parents: Media Use and Attitudes Report , Ofcom, 29 November 2017 p.191, ibid Dscout web-based research platform paired with a smartphone app to capture in-the-moment behaviours. For this study, they recruited a demographically diverse sample of 94 Android users from a pool of more than 100,000 participants. See Putting a finger on our phone obsession , Dscout, 16 June 2016 Screen Time and Young Children: Promoting Health and Development in a Digital World , Canadian Paediatric Society, 27 November 2017 Interviewed by 5Rights, February 2018 Bett Show is an industry show for education technology, bringing together companies, start ups and attendees, including teachers, to discuss the future of technology and its role to enable educators and learners to thrive. In 2018, the Bett Show took place between 24-27 January and was attended by more than 34,700 people from 131 countries. A tool or distraction? How schools approaches to mobile phones vary widely in the UK , The Guardian, 15 December 2017 p.27, Tablets in Schools: How Useful Are They? , Dr. Liz Fawcett, Association of Teachers and Lecturers, 2016 Endnotes40 p.16, ibid See Growing Up Digital Alberta , Harvard Medical School Teaching Hospital, the Center on Media and Child Health, Boston Children s Hospital, University of Alberta and the Alberta Teachers Association, 2016 Ibid Chapter two Sean Parker unloads on Facebook: God only knows what it s doing to our children s brains , Axios, 9 November 2017 98 personal that Facebook uses to target ads to you , The Washington Post, 19 August 2016 Figure Private Traits and Attributes are Predictable from Digital Records of Human Behavior , M Kosiniski, D Stillwell, T Graepel, Proceedings of the National Academy of Sciences, April 2013; 110 ( 5802-58052 Artificial intelligence developers build simulated neural networks that are able to process thousands of times more data. Developers are building better software, faster, using AI, TNW, October 2017 p. 788, The Datafied Child: The Dataveillance of Children and Implications For Their Rights , D Lupton, B Williamson, New Media and Society, Vol 19, Issue 5, pp. 780-794, 23 January 2017 Study: 81% of 2-year-olds have online presence Digital Journal, October 2010 Slide 35, Global top 100 companies by market capitalisation , PwC, 31 March 2017 The Need for New Design Ethics , Tristan Harris Persuasive Technology: Using Computers to Change What We Think and Do , BJ Fogg, Ubiquity, Volume 2002 December, 2002 Article No. 5 A Behavior Model for Persuasive Design , BJ Fogg, Persuasive Technology Lab, Stanford University, 2009 The formula for phone addiction might double as a cure , WIRED, 1 February 2018 Nudge: Improving Decisions About Health, Wealth and Happiness , R Thaler, C Sunstein, Yale University Press, 2008 Want to design user behaviour? Pass the Regret Test first , Nir Eyal, 2018 See Truth About Tech: A Roadmap for Kids Digital Well-being Common Sense Media and Centre for Humane Technology, February 2018 Tech companies design your life, here s why you should care Tristan HarrisChapter three On money as an Instrument of Change , Chamath Palihapitiya, Stanford Business School, 13 November Discussed in Former Facebook exec says social media is ripping apart society , The Verge, 11 December 2017 What is the role of dopamine in reward: hedonic impact, reward learning, or incentive salience? K Berridge, T Robinson, Brain Research Reviews, 28, 309 Referenced in Why we re all addicted to texts, twitter and Google , Dr. Susan Weinschenk, Psychology Today, 11 September 2012 How I kicked my addiction to the iPhone game Angry Birds , Dr. Michael Chorust, Psychology Today, 4 January 2011 Between 10 12 years old, children find it hard to think of the longer-term consequences and seek immediate rewards. p.18 Digital Childhood: Addressing Childhood Development Milestones in the Digital Environment , B Kidron and Dr. A Rudkin 5Rights, December 2017 Association between portable screen-based media device access or use and sleep outcomes , B Carter et al , JAMA Pediatrics, 170( 1202-1208 ( What happens to your brain when you get a Like on Instagram , Business Insider, 25 March 2017 How I kicked my addiction to the iPhone game Angry Birds , Dr. Michael Chorust, Psychology Today, 4 January 2011 An Angry Birds empire: games, toys, movies and now an IPO , New York Times, 5 September 2017 How I kicked my addiction to the iPhone game Angry Birds , Dr. Michael Chorust, Psychology Today, 4 January 2011 The Brain That Changes Itself: Stories of Personal Triumph from the Frontiers of Brain Science , N Doidge, Penguin, 2007 Ibid p. 20, At 13 to 15-years-old, children are Highly dependent on peers for a sense of wellbeing. They need to feel as if they are part of a group. Digital childhood: Addressing childhood development milestones in the digital environment , B Kidron, Dr. A Rudkin, 5Rights, December 2017 Motivational, emotional, and behavioral correlates of fear of missing out , AK Przybylski et al , Computers in Human Behavior, Volume 29, Issue 4, pp. 1841-1848 July 2013 Ibid The inventor of the Facebook Like: There s always going to be unintended consequences. , Alphr, 20 October 2017 This is known as the orienting reflex . See Chapter Treatment of Attentional Problems , G DeGangi, Pediatric Disorders of Regulation in Affect and Behavior (Second Edition), 2017 p.17, Ibid Examining the interface of family and personal traits, media, and academic imperatives using the learning habit study , R Pressman et al , The American Journal of Family Therapy, 5, 347-363, doi:1080/935684, 2014 p.7, Children and Parents: Media Use and Attitudes Report, Ofcom, 29 November 2017 Endnotes41 Machine Learning focuses on teaching computers to learn for themselves, coding them to think like human beings, while Artificial Intelligence concentrates on mimicking human decision making processes and carrying out tasks in ever more human ways. See What is the difference between artificial intelligence and machine learning? , Forbes, 6 December 2016 Optimal Time uses a machine learning algorithm to automatically predict when a user is likely to open your push. It sees the highest success rates. Optimal Time accounts for users individual engagement patterns, sending push notifications when users are prone to open the app. The intelligence of the algorithm contributes to much higher open rates. p.26 Personalize or Bust: the Impact on App Engagement, Leanplum Why timely weekend push notifications do better , Leanplum, 30 June 2016 Optimising Notification Timing , Kevin Kahn, KahnText, 5 February 2016 Pushier notifications: how social media is getting more invasive , New Statesman, 28 June 2017 Our minds can be hijacked: the tech insiders who fear a smartphone dystopia , The Guardian, 6 October 2017 Ibid Media multitaskers pay mental price, Stanford study shows , Stanford News, 24 August 2009 Rapid changes in social media apps are forcing us all to keep up , Evening Standard, 16 March 2017 Sean Parker unloads on Facebook: God only knows what it s doing to our children s brains , Axios, 9 November 2017 Social media is tearing society apart , The Times, 15 November 2017 Happy birthday to YouTube? It s now a terrifying cesspit of clickbait , The Guardian, 24 April 2018 You ll be outraged at how easy it was to get you to click on this headline , WIRED, 18 December 2015 A study by MIT professors of the diffusion of all the verified true and false news stories distributed on Twitter between 2006 and 2017 found that true stories took about six times longer than falsehoods to reach 1,500 people. The spread of true and false news online , S Vosoughi, DRoy, S Aral, Science: Vol 359, Issue 6380, 9 March 2018 The online disinhibition effect, 20 years later , J Suler, Fifteen Eighty Four: Academic Perspectives from Cambridge University Press , 10 May 2016 p.20, Digital Childhood Addressing Childhood Development Milestones in the Digital Environment , B Kidron, Dr. A Rudkin, 5Rights, December 2017 Liquid surveillance and social media: three provocations , The Society Pages, 25 February 2013 Tide pod challenge: blaming stupid millennials is the easy way out , The Conversation, 25 January 2018 Cat Videos on YouTube: 2 Million Uploads, 25 Billion Views , Tubular Insights, 29 October 2014 Watching cute cat videos is instinctive and good for you seriously , CNN, 20 January 2016 Baby schema in human and animal faces induces cuteness perception and gaze allocation in children , M Borgi et al , Frontiers in Psychology, 411, 2014 Watching cute cat videos is instinctive and good for you seriously , CNN, 20 January 2016 Chapter four The secret social media lives of teenagers , New York Times, 7 June 2017 How does social media impact the mental health of young people? , Dr. Linda Papadopoulous, Internet Matters, 12 April 2017; and Use of multiple social media platforms and symptoms of depression and anxiety , B Primack, et al , Computers in Human Behavior, Vol. 69; 1 9, April 2017 Leah Shafer, Social media and teen anxiety , Harvard Graduate School of Education, 15 December 2017 Problematic smartphone use: a conceptual overview and systematic review of relations with anxiety and depression psychopathology , J Elhai et al , Journal of Affective Disorders 207, 251-259, 2017 Multitasking may harm the social and emotional development of tweenage girls, but face-to-face talks could save the day, say Stanford researchers , Stanford News, 25 January 2012 p.28, Ditch the Label, The Annual Bullying Survey , 2017 p.29, ibid The survey of 1,002 young people aged 11 18 by Plan International UK found that 235 out of 486 girls and 202 of 510 boys reported that they had experienced online abuse. 14 August 2017 p.32 The Internet on our Own Terms: How Children and Young People Deliberated About their Digital Rights , S Coleman et al 5Rights, January 2017 p.30, ibid #sleepyteens: social media use in adolescents is associated with poor sleep quality, anxiety, depression, and low self-esteem , C Woods, H Scott, Journal of Adolescence, 2015; Frequent use of social networking sites is associated with poor psychological functioning among children and adolescents , Hugues Sampasa-Kanyiga and Rosamund Lewis, Cyberpsychology, Behavior, and Social Networking, 18(: 380-385, July 2015 p.30 Ditch the Label, The Annual Bullying Survey , 2017 Tempted to text: College students mobile phone use during a face-to-face interaction with a close friend , G Brown, A Manago, J Trimble, Emerging Adulthood, 4(, 440-443, 2016 Ibid Lost Online: an Overview of Internet Addiction, Advances in Psychiatric Treatment V Murali, S George, Advances in Psychiatric Treatment, 13, 24-30 2007 Referred to in Getting Plugged In: An Overview of Internet Addiction , Dr. Caroline Flisher, Journal of Paediatrics and Child Health pp. 557 559, Volume 46, Issue 10, October 2010Endnotes42 PISA 2015 Results (Volume III): Students well-being , OECD, April 2016 p.3 Technology Addiction: Concern, Controversy and Finding a Balance , Executive Summary, Common Sense Media, May 2016 p.23, Social media and children s mental health: a review of the evidence , Education Policy Institute, June 2017 The Common Sense Census: Media use by tweens and teens , Common Sense Media, November 2015 9 to 16-year-olds p.3, Net children go mobile: the UK report , S Livingstone et al, London School of Economics and Political Science, 2014 Breaking free of our addictions to persuasive technology , IBM, 18 May 2017 Using the Internet to access information inflates future use of the Internet to access other information , B Storm, S Stone, A Benjamin, Memory, pp.717 23, 2016 Cognitive offloading: How the internet is increasingly taking over human memory , Science Daily, 16 August 2016 p.7 Digital Childhood: Addressing Childhood Development Milestones in the Digital Environment , B Kidron and Dr. A Rudkin 5Rights, December 2017 Association between portable screen-based media device access or use and sleep outcomes , B Carter et al , JAMA Pediatrics. 170(:1202-2016 Ibid Availability and night-time use of electronic entertainment and communication devices are associated with short sleep duration and obesity among Canadian children , H Chahal et al, Pediatric Obesity, September 2012 Ill Communication: Technology, Distraction & Student Performance , L Beland, R Murphy, Centre for Economic Performance, LSE, Discussion Paper No 1350 May 2015 Effect of Student Classroom Cell Phone Usage on Teachers , D Pulliam, Masters Theses and Specialist projects, Paper 1915, 2017 p.ii Press Start to Track?: Privacy and the New Questions Posed by Modern Videogame Technology , J Newman, J Jerome and C Hazard, American Intellectual Property Law Association, Quarterly Journal, 2014 The Datafied Child: The Dataveillance of Children and Implications For Their Rights , D Lupton, B Williamson, New Media and Society, Vol 19, Issue 5, pp. 780-794, 23 January 2017 Ibid Who Can You Trust? How Technology Brought Us Together and Why It Could Drive Us Apart , R Botsam, Penguin Portfolio, October 2017 Ibid Life in Likes: Children s Commissioner s report into social media use among 8 12 year olds , January Social media is tearing society apart , The Times, 15 November 2017 Terry Gross interviewing Farhad Manjoo on Fresh Air, How 5 Tech Giants Have Become More Like Governments Than Companies , National Public Radio, 26 Oct 2017 Chapter five The birth of the web CERN The Fourth Industrial Revolution: At a Glance , World Economic Forum, April 2016 The Factory Acts , Technicaleducationmatters.org 16 February 2016 Ibid Appendix B: Illustrative list of data elements and segments, Data brokers: a call for transparency and accountability , Federal Trade Commission, May 2014 Apple investors warn iPhones and other technology may be hurting children , 8 January 2018, New York Times, https:// children.html Open letter from Jana Partners and CalSTRS to Apple Inc, 6 January 2018, Article 3, UN Convention on the Rights of a Child See mind.htmlEndnotes43 Baroness Beeban Kidron OBE Founder of 5Rights Foundation Baroness Kidron sits in the House of Lords as a Crossbench peer where she is a member of the Communications Committee. She is also a Commissioner on the UN Broadband Commission for Sustainable Development, a member of The Royal Foundation Taskforce on Cyberbullying and a member of the Technical Working Group for the Child Dignity Alliance. Kidron is the co-founder of charity Into Film, which uses film to educate school children, running more than 9,000 film clubs in UK schools. Before entering the House of Lords in 2012, she spent more than 35 years working as a filmmaker in film, television and documentary. Alexandra Evans Chief of Strategy at 5Rights Foundation Evans is the former Policy Director of the British Board of Film Classification (BBFC) and oversaw the passage of the Digital Economy Act, for which the BBFC will be the regulator. A qualified lawyer, for many years she worked in law firm Mishcon de Reya s public policy team where she advised on a wide range of public interest and human rights law issues.Authors & contributors Authors Jenny Afia Partner at Schillings Afia is a privacy lawyer and was Spear s Reputation and Defamation Lawyer of the year in She has been involved in many landmark privacy cases such as Rocknroll v NewsGroup Newspapers Ltd, which addressed how social media content can be used by third parties, as well as the first civil case concerning revenge porn, JPH v Persons Unknown. As part of her work on the Children s Commissioner s Digital Task Force, Afia re-wrote terms and conditions for major social media companies, so that children and young people could better understand them. 44 Authors & contributors Prof. Joanna R Adler Professor of Forensic Psychology at Middlesex University Adler is a BPS- and HCPC-registered forensic psychologist. She has been conducting research and evaluating interventions in and around youth justice for more than two decades. Adler applies both qualitative and quantitative approaches to her research and has conducted several systematic reviews of evidence. Whether exploring how young people respond after victimisation, or considering efficacy of sanctions imposed on those who have offended, her findings have led her to believe that it has never been more important to consider the impact of real and digital blended identities on young people s experiences and development from youth across the lifespan. Dr. Henrietta Bowden-Jones Honorary Clinical Senior Lecturer at Imperial College London Bowden-Jones is the founder and director of the National Problem Gambling Clinic in the UK, the only NHS service (CNWL NHS Trust) designated for the treatment of pathological gamblers. The clinic has been running for ten years. She runs a gambling disorders research group and has been the recipient of Medical Research Council grants and Wolfson Fellowships as well as several prizes and awards. She is a medical doctor, specialising in addiction psychiatry and an Honorary Clinical Senior Lecturer, in the Department of Medicine, Imperial College, where she teaches. Her current roles include: president elect of the Medical Women s Federation, Royal College of Psychiatrists spokesperson on Behavioural Addictions, board member of the International Society of Addiction Medicine and member of the Royal Society of Medicine s Psychiatry Council. Contributors Dr. Liam Hackett Founder and CEO of Ditch the Label Hackett has founded and grown Ditch the Label into an organisation that helps thousands of people every month. It is now one of the largest anti-bullying charities in the world, and last year alone supported 1 million people globally. Hackett sits on advisory boards across the third sector and governmental departments and has contributed to various academic and government reports at UK, EU and USA level. Working across a multitude of countries and languages, he regularly speaks, debates and contributes articles throughout the press, radio and TV on a range of issues surrounding bullying, cyberbullying, equality, discrimination, gender, self-esteem, masculinity, digital technology and young people. He regularly speaks publicly in places such as The White House, The United Nations, the European Parliament and the Houses of Parliament, in addition to live public events with audiences of up to 12,Anisha Juj Intelligence Consultant at Schillings Juj is an intelligence specialist with a wealth of experience in leading investigations, building investigative strategies and providing threat assessments. Having worked in Government, and latterly the financial and legal sectors, she utilises her knowledge of open source and social media intelligence to identify threats and mitigate risks to privacy and security.45 Authors & contributors Prof. Andrew K. Przybylski Director of Research at University of Oxford Professor Przybylski is an experimental psychologist and Director of Research at the Oxford Internet Institute. His work is mainly concerned with applying psychological models of motivation and health to study how people interact with virtual environments including video games and social media. He is particularly interested is integrating open, robust, and reproducible science with evidence-based policymaking in the digital age. Dr. Angharad Rudkin Child Clinical Psychologist at University of Southampton Rudkin (MA (Oxon) CPsychol AFBPsS) is a clinical psychologist and associate fellow of the British Psychological Society. She has worked with children and their families for more than 15 years, using her academic background to provide and supervise evidence-based interventions with families. Rudkin lectures in child psychology at the Doctorate in Clinical Psychology course at the University of Southampton, supervises research and examines theses. She sits on the editorial board of Clinical Psychology Forum. Young Scot 5Rights Youth Leadership Group Launched in February 2018 by Young Scot, the Scottish Government and 5Rights, the Young Scot 5Rights Youth Leadership Group is a diverse group of 30 young people from across Scotland, aged 11 to 19, who champion their rights in the digital world. Building on the recommendations co-designed by the Young Scot 5Rights Youth Commission in their report, Our Digital Rights (May , the group focuses on investigating how Scotland can realise young people s rights in the digital world.The 5Rights are;46 About 5Rights Foundation 5Rights Foundation takes the existing rights of children and young people as codified by the UN Convention on the Rights of the Child, and articulates them for the digital environment. The Right to Remove The Right to Know The Right to Safety and Support The Right to Informed and Conscious Use The Right to Digital Literacy A digital environment fit for childhood must be deliberately designed to meet the needs of children and young people, while equipping them with the skills and competencies to manage their journey to adulthood in a digital-first world. 5Rights Foundation works with a unique and interdisciplinary network of child development and mental health experts, computer scientists, academics, policy makers, UX designers, and most importantly, children who are often our most enlightened partners. 5rightsframework.com schillingspartners.com",en,"Professor Deborah Lupton and Dr. Ben Williamson in The Datafied Child ( point out that many parents start constructing a digital profile before their child is even born.50 81% of children have a digital footprint before they are two years old.51Digital technologies can be put to an infinite number of tasks and uses, but the last decade has seen the digital environment become increasingly commercialised. This changes over time and is closely linked to resilience as well as the ability to withstand environmental stimuli in a positive and constructive way. If so, should perfect be set against income, ethnicity, an ecological footprint, a daily read out of the child s state of health, their family s traditions, ethics of food production, or simply based on what they ate yesterday? , NSPCC, February 2018; Discussed in Children as young as three at risk from groomers on online games Daily Telegraph, 2 February 2018 Internet Safety Strategy Green Paper, Department for Digital, Culture, Media and Sport, 11 October 2017 Will Robots Steal Our Jobs?",risk
COV (Christelijk Onderwijzersverbond) (Belgium),F2665252,03 August 2021,Trade union,Small (10 to 49 employees),Belgium,"As a trade union focused on education, we view this initiative from a set of core values (quality education for every child, good working conditions for teachers - working conditions are learning conditions -, teachers as autonomous professionals, and education as a public good in a free democratic society) that constitute our organization. Our remarks on the EC initiative and the response by ETUCE is based on these cornerstone values, and is as follows: - Educ ation plays a key role in our society. It entails striving for a broad formation for every student and has an immense innovative and emancipating strength. Every child should be entitled to a high quality education that aims to bring out the best in every student and learner. Introducing AI in our education systems can provide benefits for our teachers and students, as it opens up new possibilities and has the potential to make teaching and learning more personal. B ut introducing AI in education doesn t com e without risk s. Therefore it is of utmost importance that legislation is put in place so that every actor in our educational environment, and with extension our entire society, is protected and can fall back on a legislative framework . A good legislative framework will ensure a level playing field for all actors, and offer transparency for everyone involved. Introducing AI in education cannot undermine the equal access to quality education, which is a human right. As the ongoing Corona -pandemic has shown, digitalization has enhanced the inequalities in education (OECD, , so this risk is real, not potential or plausible. Therefore we believe the EC should take the lead in developing clear and binding measures, which should include but not be limited to ethical guidelines that address the risks that AI poses concerning transparency, accountability, intellectual property rights, data privacy, cyber -safety, equality and environmental protection. - The introduction of AI in education will signify a change in the work environment and the working conditions of teachers. Since we, as an organization, sincerely believe that the working conditions of teachers are of the utmost importance and form an integral part of the learning conditions of children and students , we think it is necessary to regard teachers as autonomous professionals. Teachers should be respected as masters of their craft, who know better than anyone else what is needed in education. The Corona -crisis has shown again that there is a big social va lue to education, and this social part is an integral segment of the quality of education. AI cannot and should not replace teachers, but should be seen as an aid to teachers . Consequently, it is important that teachers are included in the development of A I in all its aspects regarding education, and that they are represented by their representative organizations in this process. Therefor e the social partners have to be included in the governance regarding AI in education. As teachers are the ones who have to work with it and implement it in their every practices, involving them as equal partners from the start will only improve both the tr ansition to a more digital educational environment and the quality of the digital tools that will be used. It will also make teachers as end -users more familiar and at ease in using AI in their teaching practice. - To ensure that teachers can work as autonomous professionals, the guidelines regarding AI in education need to have a segment embedded that focusses on the need and realization of extended professionalization o f the digital skills of teachers. We see this professionalization as twofold : it is clear that teachers need the necessary technical digital competences, and these should be part of the further professionalization of teachers, but we also find it of the utmost importance to include ethical awareness and critical thinking. Teachers need to have the tools to make conscious, deliberate choices when including AI in their teaching. This means that teachers should be involved in the process, and not be reduced to the role of executors of what others decide. Teachers get to decide how and when to use AI and digital tools, not the other way around. - Education, as a human right, is a public service that should provide quality formation for every student , regardless of means and family situation, and therefore needs to be funded with public resou rces, not private funds. The increasing degree of digitalization and AI in education have made education an interesting market for companies , who want to make a profit. This poses a real danger for privatization, commercialization and monopolization , all o f which endanger the equal access to education . The EC must ensure that local governments do not disregard their duties and take up their role in providing funds so that education remains a public good that can t be dictated by EdTech companies. In providi ng a framework, governments can ensure that there is a level playing field, which should help against monopolization. The government needs to help to ensure that AI and the companies behind it work to the benefit of the schools, students and teachers , so that AI adapts to the educational system, instead of education catering to AI and Edtech companies. AI should become a useful tool for schools, students and teachers; they cannot become a money -tool for AI and the companies behind it.",en,"Therefore we believe the EC should take the lead in developing clear and binding measures, which should include but not be limited to ethical guidelines that address the risks that AI poses concerning transparency, accountability, intellectual property rights, data privacy, cyber -safety, equality and environmental protection.",risk
European Digital Rights (EDRi) (Belgium),F2665234,03 August 2021,Non-governmental organisation (NGO),Small (10 to 49 employees),Belgium,"European Commission adoption consultation: Artificial Intelligence Act Brussels, 3 August 2021 European Digital Rights (EDRi) outlines the following analysis of the Proposal for a Regulation Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) 2021/0106(COD) .1 This input is intended for the European Commission consultation on the adoption of the Artifi cial Intelligence Act. It builds on previous EDRi positions on the EU s approach to artificial intelligence regulation, including Recommendations for a Fundamental rights- based artificial intelligence regulation 2 and Ban Biometric Mass Surveillance .3 Section A summarises an initial analysis of the Artificial Intelligence Act, and Section B begins to chart recommendations for improvement and adaption to ensure fundamental rights are duly protected: A)Analysis: Artificial Intelligence Act B)Recommendations for a fundamental rights-based Artificial Intelligence Act EDRi will publish a full response to the Artificial Intelligence Act in autumn 2021, outlining the network s recommendations toward the European Parliament and the Council of the EU. 1 European Commission (, Proposal for a Regulation laying down harmonised rules on artificial intelligence (Artificial Intelligence Act): %3A52021PC0206 . 2 EDRi (, Recommendations for a Fundamental rights-based artificial intelligence regula - tion: . 3 EDRi (, Ban Biometric Mass Surveillance: ; for more EDRi resources on artificial intelligence, consult the EDRI AI and fundamental rights document pool: document-pool/. 1 Summary of recommendations: EDRi recommends that the European Parliament and the Council of the EU implement the following improvements to the Artificial Intelligence Act (AIA). For the full recommendations, see section B. Ensure effective protection against prohibited practices and ad - dress the full scope of unacceptable risks through AI: a.Strengthen existing prohibitions in article 5 to provide meaningful pro - tection against fundamental rights violations and individual and col - lective harms; b.Comprehensively prohibit the use of remote biometric identification in publicly accessible spaces for any purpose, and implement a general ban on any use of AI for an automated recognition of human features in publicly accessible spaces; c.Include new prohibitions on the following practices which are incompatible with fundamental rights and democracy, and thus pose an unacceptable risk: i.Uses of AI in the field of law enforcement or criminal justice that purport to predict future behaviour; ii.Uses of AI in the field of migration control in ways that undermine the right to claim asylum; iii.Uses of AI that implement invasive surveillance, monitoring and algorithmic management in an employment and educational context; iv.The use of AI to categorise people on the basis of their human features, which can pose a grave and disproportionate threat to all human rights, in particular equality and non-discrimination; v.Placing on the market, putting into service or use of AI to infer, predict, analyse or assess a person s emotions, feelings, emotional state, beliefs, preferences, intentions or otherwise inner thoughts, as well as to use human features, behaviours or expressions to predict future actions or behaviours; vi.Uses of AI that constitute mass surveillance. Adapt the AIA to ensure a holistic, democratic and future-proof framework: a.Introduce a democratic, inclusive and accessible process by for the insertion of new prohibitions. Include criteria for unacceptable risk and the addition of future prohibitions into the AIA; b.Ensure the potential to update the high risk use case areas in the future (amending article in addition to updating the use case sub- areas ; 2 c.Respond to gaps in regulation with respect to economic and environmental impact, structural forms of inequality and AI and migration control, law enforcement and worker surveillance, mass surveillance, and exports of high-risk or prohibited AI outside the EU; d.Remove loopholes in articles 2( and 83 which currently leave out of scope of the AIA AI systems used as part of international law enforcement agreements; e.Remove the broad exemption to forgo the duty to conduct a conformity assessment on grounds of public security in article 47; f.Remove the exemption to the principle of purpose limitation contained in article 54((a) for innovative AI within the regulatory sandbox provisions for uses in the criminal justice context. Ensure responsibility to those subjected to AI systems with en - hanced obligations on users of all AI systems: a.Mandate users to conduct and publish an ex ante human rights impact assessment before putting a high risk AI system into use, clearly outlining the stated purpose for which the system will be implemented; b.Implement on users a duty to cooperate with national competent authorities investigating AI systems for potential threats to fundamental rights or safety under articles 65 and 67 for all AI systems, regardless of risk designation; c.Implement a duty on users to meaningfully consult with institutions, civil society and social partners representing affected groups before deploying high risk AI systems; d.When the user of any AI system is a public authority, implement a notification requirement to all those impacted by a decision made by the system. Implement meaningful public transparency for high risk AI sys - tems: a.Ensure meaningful public transparency by requiring registration in the EU database (article of all high risk AI systems (and potentially also all AI systems to which people are subject) that are put into use. This would enable individuals and civil society to access information about AI systems in operation; b.Ensure the inclusion of instructions for use for AI systems in law enforcement and migration, asylum and border control management in the public database as per Annex III, points 1, 6 and Remove the exemption contained within Annex VIII, point 11; c.Require providers to include access to the conformity assessment alongside the instructions for use as per article 13(-( in the public database under article 60; d.Require providers to provide more thorough details about the system to the users as part of article 13(; 3 e.Remove the exemptions in article 52 relating to the transparency of AI systems used for detection and prevention of criminal offences, (as argued by the EDPB and EDPS) and for the prosecution of people. When AI systems under article 52 are used for investigation, suspects should be notified post factum . Facilitate accountability: Include oversight and enforcement in - frastructures that work for people: a.Ensure a cohesive national enforcement structure; b.Include flagging and redress mechanisms allowing individuals and collectives to contest and seek redress for all AI systems that cause harm and threaten fundamental rights; c.Implement a more democratic governance infrastructure, with greater independence for the European AI board. 4 EDRi is the biggest European network defending rights and freedoms online. The EDRi network is a dynamic and resilient collective of 45 NGOs, as well as experts, advocates, and academics working to defend and advance dig - ital rights across Europe and beyond. T ogether, the EDRi network builds a movement of organisations and individuals pushing for robust and enforced laws, informing and mobilising people, and promoting a healthy and accountable technology market. Acknowledgments With thanks to the EDRi network for their contributions. In particular: Access Now ARTICLE 19 Bits of Freedom Chaos Computer Club (CCC) Digitale Gesellschaft Schweiz Free Software Foundation Europe (FSFE) Panoptykon Foundation Electronic Frontier Norway (EFN) Electronic Privacy Information Center (EPIC) epicenter.works Homo Digitalis IT-Political Association of Denmark (IT-POL) Statewatch Thanks also to conversation partners: Amos T oh, Fieke Jansen, Jill T oh, Griff Fer - ris, Jeremias Adams-Prassl, Reuben Binns, Aislinn Kelly-Lyth, Petra Molnar, Alyna Smith, Mher Hakobyan, Marlena Wisniak, Agathe Balayn, Seda G rses, Mute Schimpf, Jascha Galaski, Mark Brakel and members of the Digital Dignity Coali - tion for their thoughtful insights. 5 (A) Analysis: Artificial Intelligence Act EDRi welcomes the European Commission s globally significant step towards regulating the development and deployment of artificial intelligence (AI) systems. Uses of AI systems have the ability to enable mass surveillance and intrusion into our personal lives, reinforce some of the deepest societal inequalities, fundamentally alter the delivery of public and essential services, shift more power into corporate hands and disrupt the democratic purpose. The proposal thus takes a notable step to acknowledge that some uses of AI are simply unacceptable and must be prohibited. However, we would like to make a number of suggestions to ensure that the AIA is in line with the Charter of Fundamental Rights of the EU (CFEU), future proof , and a role model for other rights-protective future AI legislation around the world. Overall, EDRi raises a number of concerns relating to the AIA as a regulatory framework, specifically in relation to the extent to which it protects fundamental rights and is able to address broader structural, political and economic issues as a result of the widescale promotion and adoption of AI systems in various areas of life. We have argued that any approach which assumes benefits from the widescale uptake of AI will be problematic from a fundamental rights perspective and should not be a policy objective in itself.4 Further, we highlight throughout that there are structural concerns relating to the extent to which the use of AI systems can systematically target, harm and exclude marginalised communities, exacerbating existing power imbalances in society. Additionally, there are broad questions as to how far the AIA as a framework is sufficiently comprehensive to address these structural harms, due to its tendency toward de-regulation of all but the most narrowly-defined unacceptable of AI systems, the lack of obligations on users and the lack of provisions for individual or collective redress for those subjected to AI systems. As such, the following outlines EDRi s main analysis of the proposal for an Artificial Intelligence Act. 1Inconsistencies with stated objectives of the AIA The Explanatory Memorandum to the Commission s proposal establishes that the primary purpose of the AIA is to implement the second objective [of the Commission s White Paper on AI] for the development of an ecosystem of 4 EDRi (, Recommendations for a Fundamental rights-based artificial intelligence regula - tion: . 6 trust by addressing the risks associated with certain uses of such technology based on EU values and fundamental rights (. The Memorandum continues that this is separate from the Commission s aim of promoting the uptake of AI ( which while important - is not the main goal of this Act. The four specific objectives which the Memorandum describes are also aligned with the ambition of trustworthy AI: safety and respect for fundamental rights; legal certainty; enhanced governance and enforcement of safety and fundamental rights; and a lawful, safe and trustworthy single market (. Despite this reassurance, the AIA proposal seems, at its core, designed to enable AI uptake rather than to limit or mitigate its harms. The fact that the vast majority of rules apply only to the narrowest sub-set of high risk AI - which the Commission explicitly admits is a minimum necessary approach ( - is at odds with EU fundamental rights obligations. It also contradicts the precautionary principle , which civil society has warned is necessary, given the vast contextual harms which may arise from the use of AI systems to which people are subject. Furthermore, the Memorandum describes that the proposed rules cover the development, placement on the market and use of AI systems [italics for emphasis] ( when in fact, the use of AI receives insufficient attention in the proposal. Even the prohibition of certain forms of remote biometric identification (RBI) under article 5 is recognised in the Memorandum as not being a real ban: the RBI rules, the Commission explains, constitute only specific restrictions and safeguards (. Whilst the AIA seeks to classify the risk level of an AI system based on the intended purpose of the AI system (, it has created a set of rules and obligations which are largely unable to achieve this aim. As critics are increasingly pointing out, the AIA proposal attempts to transplant a typical product safety framework into an often novel AI context. By failing to account for the specificities of artificial intelligence, for example the variety of ways in which it can be applied, the importance of context, the fact that it is often sold as a service (not a product), and its self-learning nature, the proposal falls short of what would be needed to anticipate, prevent or at the very least mitigate the myriad ways in which it can cause harm. There is a broader concern as to the extent to which the AIA s primary objective of harmonising the single market for AI products is compatible with the other objective of safeguarding fundamental rights, and the broader need to mitigate the societal impacts of AI. The promotion of AI s uptake and the push for a harmonised single market, via the act s maximum harmonisation function, may preclude Member States from introducing higher fundamental rights standards than those contained in the AIA.5 Despite the protection of 5 Veale and Zuiderveen Borgesius (, Demystifying the Draft EU Artificial Intelligence Act : . 7 personal data being one of the treaty bases of the proposal, it is clear that the proposal goes nowhere near far enough to ensure the protection of fundamental rights, and in doing so, contradicts its own stated aims and objectives to ensure trustworthy AI. 2Prohibited practices: Incomplete coverage of unacceptable AI and fundamental rights threats Whilst it is positive that the AIA proposal foresees that some uses of artificial intelligence pose an unacceptable risk to fundamental rights and therefore must be prohibited under article 5, the AIA s approach to unacceptable risks falls short in two main ways. 1 Proposed prohibitions are too wide and vague Firstly, article 5 leaves a wide scope for interpretation, broad exceptions and in some cases unreasonably high thresholds for systems to be prohibited. As such, there is a risk that this provision fails to prevent the worst excesses of potential fundamental rights abuses arising from AI systems. Those shortcomings are: Subliminal and exploitative uses Physical or psychological harm: Articles 5((a) and (b) are unduly narrow leading to significant concerns that they will fail to prevent against manipulative or exploitative uses of AI. In particular, that both prohibitions only apply when there is or likely to be physical or pyschological harm foresees a burden of proof on invididuals to demonstrate future or actual harm (without creating a legal path to flag or contest such systems, see section 5 below). Whilst the background to the AIA acknowledges the opacity and unpredictability of AI systems, this provision does not incorporate these concerns into the drafting of this provision. The requirement that the use is in order to materially distort behaviour adds an unreasonably high threshold; Individual harm: Both provisions are drafted in narrow, individualistic terms, not foreseeing that many such systems are unlikely to target specific persons, but rather whole groups of people in society; Limited vulnerabilities: Article 5((b) attempts to prevent only such uses of AI that may exploit people on the basis of specific vulnerabilities age and physical or mental disability. It is unclear why the AIA limits only to these vulnerabilities rather than taking a comprehensive approach and prohibiting uses of AI that exploit vulnerabilities based on the full range of protected characteristics under EU law. 8 Social scoring Limitation to public uses: The prohibition of social scoring systems contains a number of limitations suggesting an extremely high threshold for its application. Firstly the prohibition is limited to uses in a public context, by public authorities or on their behalf, thus excluding commercial uses, such as scoring of customers on online platforms leading to different service options; Trustworthiness : The provision is limited to those systems which evaluate or classify trustworthiness, without providing a definition of trustworthiness in the act. This could be potentially limiting for those systems that have a parallel impact but do not purport to map trustworthiness per se; General purpose score: The implicit grounding of the prohibition in the notion of a single score to be used for general purposes (as indicated in Recital suggests that many of the examples of risk scoring used in specific governmental contexts (such as risk-scoring for welfare fraud in the notorious Dutch SyRI case) are to be excluded from the scope of the prohibition, unless they deploy data collected in one context to be used in another or have an unjustified or disproportionate impact ; Temporal limit: The provision also includes a temporaral limitation, thus applying to systems which evaluate or classify the trustworthiness of natural persons over a certain period of time , another limiting threshold; Added conditions : The prohibition is limited to uses which lead to detrimental or unfavourable treatment in contexts other than that in which the data was collected, or in a nature that causes unjustified or disproportionate harm. Such conditions suggest that the central principle is not the harm caused, otherwise these conditions would not be relevant. Arguably, there are reasons to contest social scoring systems regardless of the presence of proof of unfavourable outcomes insofar as they reduce the complexity of human experience to a combination of limited, measurable indicators, with potential negative implications for fundamental rights to good administration and human dignity. Remote biometric identification The AIA s prohibition of real-time remote biometric identification (RBI) in publicly accessible spaces for the purpose of law enforcement addresses only a small range of the many practices that can constitute biometric mass surveillance.6 As we will recommend further in Section B, the AI Act must be amended to ensure it does not undermine existing fundamental rights standards; furthermore all remote biometric identification and the use of AI for the automated recognition of human features must be prohibited without exceptions . 6 EDRi (, Ban Biometric Mass Surveillance : . 9 Despite accepting that real-time RBI can unduly restrict people s fundamental rights (Recital , and noting that the majority of respondents to the Commission s Consultation were in favour of new rules, article 5 of the AIA contradictorily risks creating a blueprint for conducting biometric mass surveillance, instead of a substantive prohibition of these practices. In its approach to RBI, the Act requires a lot of improvements to bring it in line with existing standards of fundamental rights and data protection: Wide exceptions with low thresholds: Despite recognising the severe undue fundamental rights risks of real-time RBI, the AI Act allows Member States to adopt three broad and highly discretionary exceptions to the prohibition (d). In article d.i, the exception for potential victims of crime suggests that there need only be the potential of a crime, creating a dangerously wide and potentially arbitrary scope which may be easily misused to justify perpetual and untargeted use; furthermore, the fallacious reference to targeted search fails to recognise that remote biometric identification is by definition always mass / indiscriminate.7 d.iii sets a potentially very low bar to permit RBI to search for perpetrators or suspects of crimes under the European Arrest Warrant, which is a long list of crimes including non-violent ones like counterfeiting currency, forging administrative documents or trafficking endangered plants. This exception is further problematic because it is based on the assumption that facial recognition or other RBI is useful for the prosecution of a perpetrator or a suspect of a criminal offence . However, due to its inherent probabilistic nature (sometimes referred to as the base rate fallacy phenomenon), biometric identification can never and will never provide conclusive identification or inferrence.8 Thus it cannot be relied upon in a court of law, as shown in a 2019 case in the Netherlands where the defendant was acquited because a facial recognition match could not meet the burden of proof.9 The Act claims that these exceptions are narrowly defined (Recital , but these examples show just how wide the exceptions and how low thresholds are to permit the mass infringement of fundamental rights; Safeguards in name only: The exceptions to the prohibition are furthermore subject to a series of purported safeguards (articles 2 and , including temporal and geographic limitations, and judicial or administrative authorisation. Given that the ex ante authorisation safeguard can be waived in the event of each Member State s 7 Garante per la Protezione dei Data Personali (Italian data protection authority) (, Ri - conoscimento facciale: Sari Real Time non conforme alla normativa sulla privacy: https:// . 8 EDRi (, Why EU passenger surveillance fails its purpose: eu-passenger-surveillance-fails-its-purpose/ . 9 EDRi (, The Rise and Rise of Biometric Mass Surveillance in the EU: A legal analysis of biometric mass surveillance practices in Germany, the Netherlands, and Poland, [65]: https:// edri.org/wp-content/uploads/2021/07/EDRI_RISE_REPORT.pdf . 10 interpretation of the vague and discretionary threshold of a duly justified situation of urgency (, it is possible that in its current form, the AIA may not be able to substantively prevent any law enforcement use of real-time RBI. This is especially pertinent in the context of systemic threats to democracy and the rule of law across the EU, evidenced for example in the pending European Court of Human Rights case brought by Panoptykon Foundation against the Polish government for the non-existence of effective supervision over the government s surveillance activities.10 The authorisation process is thus vulnerable to government pressure and even further weakens the already deficient RBI prohibition ; The authorisation process : Dr N ra Ni Loideain has further noted that the authorisation is a flawed process which does not meet the standards of Charter of Fundamental Rights of the European Union (CFEU): firstly, in the current proposal, prior authorisation is permissible on the basis of a the low evidentiary threshold of objective evidence or clear indications presented to it [the authorising authority] (italics for emphasis) (. This is a low bar in which the decisive factor in whether or not to authorise RBI can be clear indications provided by the very entity with a vested interest in using RBI. Furthermore, this authority is compelled to assess the seriousness, probability and scale of the harm caused in the absence of the use of the system (italics for emphasis) (a). This coercive and speculative approach seems at odds with fundamental rights principles of necessity and proportionality (CFEU article which require that the burden of proof is on demonstrating that a use case or action does not unduly restrict fundamental rights;11 Incompatibility with requirements of necessity and proportionality: In addition to the failure of the proposed safeguards to comply with existing fundamental rights law, the AI Act has further been criticised for its misapplication of the tests of necessity and proportionality for conducting RBI. The EDPS and EDPB Joint Opinion, for example, states that: [t]he reasoning behind the Proposal seems to omit that when monitoring open areas, the obligations under EU data protection law need to be met for not just suspects, but for all those that in practice are monitored (paragraph .12 As demonstrated in EDRi s Ban Biometric Mass Surveillance position paper, real time and post RBI (both of which constitute indiscriminate biometric surveillance) are inherently unnecessary and disproportionate under the CFEU and should be fully prohibited. Conversely, in its current form, the AIA creates the conditions for law enforcement agencies to unduly restrict the 10 Panoptykon Foundation, No control over surveillance by Polish intelligence agencies. ECHR demands explanations from the government , December - ernment-surveillance-echr-complaint . 11 Ni Loideain, N., University of London, a rticle forthcoming, August 12 EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act): edps-proposal-regulation-european_en 11 fundamental rights of whole populations through biometric mass surveillance practices, contravening the CFEU; Threat to existing data protection laws : Recital 23 clarifies that this Regulation is not intended to provide the legal basis for the processing of personal data . Therefore, it needs to be clarified that the lex specialis status of the prohibition on real-time RBI does not provide a legal basis for law enforcement under one of the exceptions (d), nor does it weaken existing protections of biometric data under the Data Protection Law Enforcement Directive (LED) or national implementations of the LED; The post RBI loophole: The AIA draws a fundamentally arbitrary distinction between real-time and post uses of remote biometric identification by virtue of a significant [temporal] delay between collection and processing which is, in fundamental rights terms, irrelevant. By doing so, the Act creates a loophole which permits law enforcement agencies to retrospectively apply biometric identification to CCTV footage or photographs. This form of biometric mass surveillance can unduly restrict people s rights equally as profoundly as real-time methods and sometimes even more invasively so, due to the potential to pool data from many different sources across place and time. This erroneous distinction also leaves the deployment of equally harmful post RBI systems free from the restrictions in time, place and authorisation that apply for the exceptional uses of real-time RBI deployments (, meaning that the potential for mass surveillance from post RBI is even further strengthened. Similarly, the definition of remote ( is overly narrow in ways that may also create loopholes, for example arbitrarily and illogically linking the system definition to the (lack of) prior knowledge of the user. This must be corrected, as we will discuss further in the recommendations laid out in Section B; No ban on other purposes ( i.e. other governmental or private purposes): The AIA limits the RBI prohibition to law enforcement purposes, on the basis that other purposes are already sufficiently prohibited under the General Data Protection Regulation (GDPR). By doing so, the AIA fails to acknowledge the existing wide exemptions under the GDPR, which EDRi has demonstrated have already led to the systematic and sustained violations of people s rights and freedoms across the EU.13 In this manner, the proposal misses the opportunity to bring in complementary rules which will reinforce and strengthen the provisions on the processing of biometric data in the GDPR and align with the fundamental rights enshrined in the CFEU. Furthermore, by addressing only the use of these systems, EU providers may still be able to develop and sell rights-violating RBI systems outside of the EU; No ban on other types of processing: By limiting the prohibition to real-time RBI, the AIA fails to address other biometric mass surveillance practices such as singling out individuals based on their biometric characteristics. The protection of fundamental rights need a broader 13 EDRi ( The Rise and Rise of Biometric Mass Surveillance in the EU: A legal analysis of biometric mass surveillance practices in Germany, the Netherlands, and Poland, https:// edri.org/wp-content/uploads/2021/07/EDRI_RISE_REPORT.pdf 12 prohibition of biometric mass surveillance practices than just remote biometric identification; Infrastructural enablement : The AIA fails to address the underlying issue of biometric mass surveillance infrastructures and enabling practices. Such infrastructures and practices can proliferate under the Act because post uses of RBI, real-time exceptional uses (under d.i - iii), and uses for non-law enforcement purposes will all ensure that the required databases, software and hardware remain readily accessible. In essence, because only the use is prohibited, and not the development, sale, purchase or deployment, the implication is that biometric devices, software and databases can be bought, installed and maintained, and may be turned on with a simple authorisation (which we have already demonstrated is highly flawed). This doesn t just fail to stop biometric mass surveillance: it may even enable and encourage it by incentivising governments to make greater use of the costly, convenient infrastructures that are already in place; Online spaces out of scope: The exclusion of online spaces from the definition of publicly-accessible spaces which are subject to the prohibition - contrary to recommendations from EDRi14 and more recently, the EDPS and EDPB15 - suggests that the AIA may not prevent the scraping of online sources to develop commercial databases and software such as those offered by Clearview AI to many European law enforcement agencies. This is despite a number of EU data protection authorities (DPAs), including the Hamburg DPA, confirming the inherent rights-violating nature of such practices. The COVID-19 pandemic has made this even more urgent, as large parts of people s everyday lives have necessarily moved online. Online spaces must be included in the definition of publicly-accessible spaces, and the data scraped from online spaces (such as from social media) included in the prohibition;16 Vague and complicated wording : The wording relating to RBI is unnecessarily vague and complicated, creating risky grey areas and making it overly onerous for civil society as well as AI providers and users to apply the rules in a consistent and rights-respecting manner. 2Lack of mechanism to add unacceptable uses Secondly, the proposal does not introduce a mechanism by which unacceptable uses of AI may be added in the future , unlike the process outlined in article 7 for updating the list of stand alone high risk use cases. 14 EDRi (, Ban Biometric Mass Surveillance: 2020/05/Paper-Ban-Biometric-Mass-Surveillance.pdf . 15 EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act): edps-proposal-regulation-european_en . 16 noyb ( Clearview AI s biometric photo database deemed illegal in the EU, but only par - tial deletion ordered: . 13 The lack of more general criteria to establish unacceptable risk is an inconsistency in the framework, leaving a lack of clarity as to why the article 5 prohibitions were included (to the exclusion of others below), whilst not providing a framework for future unacceptable uses cases to be added as the AI market evolves. As outlined in the recommendations below, such criteria might include the impact on fundamental rights, structural power imbalances around the context of deployment (including potential for enhanced discrimination, marginalisation, inequality), lack of capacity for individuals, groups or civil society to contest the usage, etc. 3Unacceptable use cases missing from list of prohibited AI Thirdly, the proposal does not put forward a holistic set of prohibitions covering the full range of unacceptable uses of AI . As highlighted by EDRi alongside 62 human rights organisations,17 116 MEPs18 and the European Data Protection Supervisor and European Data Protection Board,19 there are further use cases of AI that pose unacceptable risks to fundamental rights and democracy, and therefore must be prohibited under the AIA. These are the following: Predictive policing and uses of AI to risk assess for future criminality, offending or re-offending. The use of predictive modelling to forecast where and by whom certain crimes are likely to be committed, alongside uses of AI to detect risk in the context of criminality, unduly and unnecessarily inpinge on a number of fundamental rights. In particular, the rights to dignity,20 to an effective remedy and a fair trial,21 to good administration22 as well as the presumption of innocence23 are compromised by practices that attempt to automate the prediction and characterisation of the future behaviour of individuals and groups, with potentially harmful consequences for their liberty and privacy. In addition, systems designed to assess risk or predict crimes have been demonstrated to repeatedly score poor, working class, racialised and 17 EDRi ( Open letter: Civil society call for the introduction of red lines in the upcoming Commission proposal on artificial intelligence: red-lines-in-the-european-unions-artificial-intelligence-proposal/ . 18 MEP letter to President von der Leyen, 8th March . 19 EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act): edps-proposal-regulation-european_en . 20 Charter of Fundamental Rights of the European Union, article 21 Charter of Fundamental Rights of the European Union, article 22 Charter of Fundamental Rights of the European Union, article 23 Charter of Fundamental Rights of the European Union, article 48; See also EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act), [34]. 14 migrant communities with a higher likelihood of presumed future criminality,24 therefore unreservedly compromising the rights to equality before the law and non-discrimination.25 The use of historical data in practice serves as a proxy for race and other protected characteristics, as well as socio-economic status, reflecting patterns of over-policing of certain communities, exacerbating racial biases, the criminalisation of poverty and affording false objectivity to patterns of racial and other types of profiling. Insofar as such practices reflect ongoing surveillance priorities, it is highly likely that such practices will amplify existing patterns of institutionalised discrimination insofar as they reify presumptions of criminality on the basis of individual or group characteristics, behaviour, or location. The use of AI systems at borders and in migration control. The proliferation of tests, trials and deployments in the context of migration control is a particular fundamental rights concern, which is not systematically addressed in the AIA. Specific conditions relating to the migration context warrant a higher level of scrutiny and limitations on the use of AI, in particular: the heightened conditions of vulnerability placed on people on the move, including refugees, migrants, non-status individuals, and other categories; the lower procedural safeguards and protection of rights afforded to migrants;26 and that the migration context has been used as an opportunity to experimenton an already highly marginalised category of persons.27 It is vital that particular limitations are drawn and higher safeguards applied to ensure that the rule of law and fundamental rights cannot be overridden by national security or other vaguely-defined policy priorities, and that the principles of necessity and proportionality are upheld. In particular, myriad uses of AI in the migration control context pose severe risks to fundamental rights of people on the move, as well as comprising potential violations of international refugee and human rights law. The increasing datafication of the migration management process, use of AI systems and big data to predict migration controls in combination with an expansive surveillance infrastructure28 to detect, intercept and prevent entry into Europe, is an impermissable use of AI systems. It also amounts to mass surveillance and is in contravention of 24 European Network Against Racism (, Data Driven Policing: The Hardwiring of Discriminatory Policing Practices Across Europe: profiling-web-final.pdf , 25 Charter of Fundamental Rights of the European Union, articles 20 & 26 UN Special Rapporteur on contemporary forms of racism, racial discrimination, xenophobia and related intolerance (, Racial discrimination and emerging digital technologies: a hu - man rights analysis A/HRC/27 EDRi, Petra Molnar ( T echnological T esting Grounds: Migration Management Experi - ments and Reflections from the Ground Up: ech - nological-T esting-Grounds.pdf . 28 European Parliament Research Service (. A rtificial Intelligence at EU borders: Overview of applications and key issues . PE 15 obligations under the Geneva Convention as well as a fundamental violation of the right to asylum enshrined in the Refugee Convention and domestic legislation. Further, the growing resort to biometric identification, verification and analysis of migrants sensitive data in the the context of migration management is deployed in the context of significant power imabalance, particularly given that immigration and border administrative decision-making is already an area rife with opacity and discretion without adequate oversight and accountability. The use of individual risk assessments and predictive systems to classify security or health risks also pose particular consequences for human dignity, equality and non-discrimination, privacy and data protection risks, as well as due process and good administration rights. The extent to which these systems are used to facilitate processes such as detention and deportation present particular risks to fundamental rights and with vast potential for abuse. In addition, emotion recognition systems (explored further in general below) are particularly harmful in the migration context due to the power imbalance and deep reliance on generally flawed and un-scientific premises,29 leading to potentially innacurate and discriminatory decision-making processes. Invasive monitoring, surveillance (including of biometric and other human features) and algorithmic management in employment and educational contexts. As highlighted by unions, there are particular concerns with the deployment of AI to monitor, measure and manage employees, tasks and resources in employment and educational contexts. Firstly, we see a growing resort to invasive monitoring practices, predicated on a vast scale of data collection in extreme power imbalance and subordination,30 fundamentally undermining notions of consent in data processing, given the contractual subordination of employees to their employers. Many such systems are combined with algorithmic assessments of performance and other forms of algorithmic task management, which are not only very likely to infringe on data protection and privacy rights of workers, but also likely diminish well-being, pose serious physical and psychological harm,31 limit work autonomy and maintain greater distance and opacity between managers and workers. Further, as demonstrated by a number of cases contested by app-workers, algorithmic management and ranking systems used by large platforms have had severe consequences on the economic situation 29 ARTICLE19, (. Emotional Entanglement: China s emotion recognition market and its implications for human rights : ech- China-Report.pdf. 30 ETUI ( The AI Regulation: entering an AI regulatory winter? Why an ad hoc directive on AI in employment is required: - tory-winter. 31 Wood, A. J., Algorithmic Management: Consequences for Work Organisation and Working Conditions, Seville: European Commission, 2021,JRC124874. 16 of workers as a result of specific decisions, including discriminatory treatment and violation of statutory rights,32 and major decisions, such as termination, taking through substantively automated means.33 The classifcation of such systems as only high risk , subject to primarily technical requirements to be fulfilled by providers of AI systems, is wholly insufficient to mitigate these threats to fundamental rights and severe harms to individuals and groups. Rather than restricting these systems, the AIA in its current form rather deems such practices permissable, exacerbating the burden on civil society and affected individuals to seek redress in the event of harm. As outlined below in the recommendations, such practices must instead be prohibited. 4Insufficiency of the limited risk approach The following practices are generally considered to have only a limited risk profile under the AIA, despite their va st capacity for harm and violations of fundamental rights . In practice, at least for uses of AI which process personal data, it is hard to see how such rules go further than existing requirements under the General Data Protection Regulation (GDPR). Instead, biometric categorisation must be prohibited wherever it may unduly restrict fundamental rights, most notably equality and non- discrimination , as set out more extensively in our recommendations in Section B. Emotion recognition must be fully prohibited due to the fundamentally and unmitigably flawed assumptions on which emotion recognition rests, and its incompatibility with human dignity and many fundamental freedoms . Biometric categorisation Ignoring the evidence of harms: The AIA proposal puts biometric categorisation systems in the category of limited risk, entailing only a small number of mandatory transparency requirements (article . It further sets up the possibility that some biometric categorisation use cases could in future be considered high risk under Annex III heading 1, but does not at this point include any such use cases, on the grounds that there is not sufficient evidence. However, the EDRi network and other civil society groups have repeatedly demonstrated that in fact, 32 T ech crunch, 4th January 2021, Italian court rules against discriminatory Deliveroo rider- ranking algorithm: - tory-deliveroo-rider-ranking-algorithm/ . 33 Personnel T oday, 27th October 2020, Uber sued for automated dismissals: h ttps:// - sonneltoday.com/hr/uber-sued-for-automated-dismissals/ . 17 such categorisations can create severe and undue fundamental rights restrictions;34 Threats to equality and non-discrimination: Biometric categorisation can gravely threaten rights to equality and non- discrimination, in particular when they relate to special categories of data as enshrined in the GDPR and protected under the CFEU and the broader EU equality and non-discrimination acquis. By definition, biometric categorisation is a process that seeks to put people into (often arbitrary, discretionary and stereotyped) boxes, and then to make predictions or decisions about them on that basis. Biometric categorisation has historical roots in systems of oppression and injustice, including the control of enslaved people in the US through the so-called lantern laws , the suppression of Indian people under British colonial rule, and even Nazi eugenics.35 For these reasons, its use in a rule-of-law-respecting society is exceptionally hard to justify; Links with mass surveillance: Biometric categorisation often forms the technical foundation of other forms of biometric data processing which can lead to mass surveillance, such as in remote biometric identification. Recital 18 of the AIA acknowledges the particularly intrusive and chilling nature of law enforcement performing such RBI practices (albeit only in real-time); Law enforcement exemption: However, when it comes to the practice of biometric categorisation which is often inextricable from RBI article 2 contradictorily exempts its use in criminal investigations, detection and prevention from the already very limited transparency requirements that are established in the AIA. Given that law enforcement uses of biometric categorisation can be associated to severe and extensive harms (loss of liberty, denial of access to procedural rights, denial of the presumption of innocence etc) this risks creating a get-out clause for some of the most harmful biometric categorisation practices ; Things that cannot be inferred: Additionally, the biometric categories proposed in article 35 treat as equivalent categories that may be predicted with a relatively high degree of accuracy based upon visible human features (such as predicting hair or eye colour, although even these are never absolute ) with those that simply cannot be determined on the basis of human features (such as sexual or political orientation) and instead, risk perpetuating scientifically-discredited and discriminatory theories like phrenology and physiognomy .36 The self- learning nature of some AI systems could make it even harder to identify 34 EDRi (, Ban Biometric Mass Surveillance: 2020/05/Paper-Ban-Biometric-Mass-Surveillance.pdf ; All Out ( Ban automated recognition of gender and sexuality, . 35 Najibi, A (, Racial Discrimination in Face Recognition T echnology : - vard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/ ; Sengoopta, C ( Imprint of the Raj: How Fingerprinting was Born in Colonial India. London: Macmillan. 36 Access Now (, Ban Biometric Surveillance: - loads/2021/06/BanBS-Statement-English.pdf . 18 when prima facie non-sensitive biometric features are in fact being used as proxies for sensitive characteristics like sexual orientation. Emotion recognition A scientifically invalid process: except in the cases of law enforcement or migration uses of polygraphs and similar tools (which are designated high risk under Annex III, b and a) , the AIA proposal classifies emotion recognition as only limited risk, despite vast evidence of its harms as well as its complete scientific invalidity such as via EU- funded projects like the much-criticised iBorderCTRL.37 The Civil Liberties Committee in the European Parliament has already called for the use of emtoion recognition in law enforcement to be discontinued ;38 Fundamentally incompatible with fundamental rights : EDRi member ARTICLE19 has demonstrated that, as a practice, emotion recognition is incompatible with international human rights principles and rules.39 As an incredibly intrusive practice, it can infringe on people s dignity, is often used in discriminatory contexts, and intrudes into people s cognitive liberty by coercing not just how people express themselves, but even how they think. Despite this, emotion recognition is becoming increasingly common in employment contexts, education, border and migration experiments and advertising. It risks infringing on people s rights and freedoms, and has particularly grave impacts on human dignity when important decisions relating to people s free movement, employment and other rights are made upon the basis of an inherently probabilistic and flawed system, which no amount of improvements to accuracy or performance can ever fix. The problem of definitions within the limited risk category: The definitions in the AIA of an emotion recognition system ( and a biometric categorisation system ( both limit the application of rules for these processes to when it is performed on the basis of biometric data. However, given that the AIA s definition of biometric data ( applies only if it allows or confirms the unique identification of a natural person, there is a risk that certain emotion recognition and biometric categorisation practices could be performed using data sets which avoid or even evade the threshold for being considered biometric data, for example through anonymisation (despite growing scepticism about the credibility of supposedly anonymised biometric 37 Wired (, The science behind the EU's creepy new border tech is totally flawed:, ; Jakubowska, E., ( Mass facial recognition is the apparatus of police states and must be regulated: ronews.com/2021/02/17/mass-facial-recognition-is-the-apparatus-of-police-states-and-must-be- regulated. 38 LIBE Committee (, Artificial Intelligence in policing: safeguards needed against mass surveillance: intelligence-in-policing-safeguards-needed-against-mass-surveillance . 39 ARTICLE19 (, Emotional Entanglement: China s emotion recognition market and its im - plications for human rights: ech- China-Report.pdf. 19 data). This could allow providers and users to circumvent the already unacceptably low requirements on emotion recognition or biometric categorisation. The Act should therefore widen the scope for emotion recognition and categorisation to include biometric, physiological and behavioural signals (in a new definition under article 3 of human features , as elabourated in section B) in order to ensure that equally harmful uses of data about human features are in scope of the prohibitions even when unique identification may not occur. Evasive processing practices (e.g. edge or transient processes) may also be employed in attempts to avoid the technical processing threshold for data to be considered biometric ( although it is important to note that the Italian Data Protection Authority (DPA) has confirmed that even if discarded almost immediately, the practice of scanning the biometric features of everyone in view of a camera is still considered unlawful mass surveillance.40 The inadequacy of notification rules to prevent harms arising from biometric categorisation or emotion recognition : For both biometric categorisation and emotion recognition, the AIA fundamentally falters in its presumption that the disclosure of their use to those who are subject to them is a solution to the harms and violations of rights that these practices can entail; and that such notification constitutes genuine transparency and accountability. Rather, harms such as a trans person being mis-gendered in public, a racialised person being shown (or not shown) adverts on the basis of their predicted ethnicity, or an employee facing disciplicary procedures due to not showing the right emotions at work, are just three of many examples of how the negative impacts of biometric categorisation and emotion recognition will remain just as real, regardless of whether or not the use of AI is disclosed to the subject. 3The AIA s scope overlooks broader structural harms and impact of AI It is positive that the AIA proposes a broad definition of artificial intelligence to include in scope a wide range of potentially harmful AI systems. However, despite this broad definition, the AIA has an extremely narrow list-based approach to regulation, which narrowly specifies use cases to be classified as high risk, alongside a process (article for the future inclusion of high risk use cases that fit within existing areas outlined in Annex III. 40 Garante per la Protezione dei Data Personali (Italian data protection authority) (, Ri - conoscimento facciale: Sari Real Time non conforme alla normativa sulla privacy: https:// . 20 Yet the following limitations in the AIA s scope are particularly challenging due to the act s primary objective, which is to promote a harmonised single market for AI within the EU. As highlighted by Veale and Zuiderveen Borgesius,41 this maximum harmonisation function requires that Member States must disapply any conflicting rules with those in the act. Thus, Member States are to be potentially precluded from introducing higher fundamental rights standards than those contained in the AIA. 1A lack of future-proofing for high-risk requirements The specific challenges as to the scope of the AIA are as follows. Firstly, the core requirements of the AIA apply to a very narrowly defined list of high risk AI systems, as outlined in article As such, with respect to stand alone use cases based on fundamental rights risks, the AIA limits from the start the range of high risk areas, determined solely by the European Commission, which cannot be updated in the future. The limited and caveated nature of the pre- defined areas (for example the processing of biometric data is high risk only if it leads to identification or categorisation, despite the fact that some authentication uses may entail significant risks) casts doubt over how comprehensive and future-proof this Annex can possibly be. Furthermore, new sub-areas can be included only insofar as they are compatible with the criteria outlined in article This falls far from the precautionary principle or rights- based approach called for by civil society.42 In addition, currently the European Commission has centralised power to update the list of high risk sub-areas in Annex III. 2Exclusion of many harmful use cases from the requirements Secondly, the risk-based approach, with requirements primarily limited to the narrow list of high risk AI systems, necessarily means that a number of systems with potentially harmful impacts remain unregulated under this act. There is particular concern as to the extent to which the following types of harms are (not) addressed: AI systems which exacerbate structural inequalities and power imbalance: As outlined above and specifically with respect to deployments of AI in the contexts of law enforcement, migration control and workplace surveillance, uses of AI in certain contexts will necessarily perpetuate structural power imbalances and fundamental rights risks. 41 Veale and Zuiderveen Borgesius ( Demystifying the Draft EU Artificial Intelligence Act , . 42 Access Now (, The EU should regulate AI on the basis of rights, not risks: https:// ; EDRi (, Recommendations for a fundamental rights-based Artificial Intelligence Regulation: addressing collective harms, democratic oversight and impermissible use: AI_EDRiRecommendations.pdf . 21 Further, as outlined by the European Disability Forum (EDF), there are no provisions in the Act to ensure that all AI systems (regardless of risk level) meet international legal obligations relating to the accessibility of persons with disabilities.43 By promoting the notion that AI systems can be primarily regulated through a series of technical measures, (documentation, human oversight in design and data quality standards), the AIA provides no response to the structural harms outlined in the previous section.44 This techno-centric framing does not adequately deal with how AI as socio-technical systems become embedded in broader processes of structural discrimination, which the examination of possible biases (article , purported improvements in accuracy (Article and more documentation (article will simply not address.45 Environmental impact: The AIA wholly underestimates the vast impact of a policy agenda designed to promote the widescale uptake of AI, underpinned by the exponential collection of data and focusing on the presumed benefits, without sufficient regards to the broader implications of the greater resort to AI systems on the environment. In particular, consequences on the environment relating to the vast environmental resources (including the exploitation of natural resources for the hardware required to underpin AI systems) as well as the energy consumption46 required for many of such systems to be trained and functional, as well as for data to be stored, find no place in the proposed regulatory framework. Economic and infrastructural c onsequences of AI systems : By defining high risk primarily with respect to fundamental rights and product safety, the AIA leaves little room for broader political and economic impacts of AI that fall outside of these frameworks. Also overlooked are the labour implications of the AI production pipeline, which often rely on labour exploitation of people in the Global South, but also how the resort to algorthmic management is reshaping the labour market toward crowd work and other more precarious, flexibilised forms of work.47 In addition, broad scale economic and political impacts following from the increased uptake of AI, including the increased 43 European Disability Forum (. Disability Perspective of AI of excellence and of trust (forthcoming). 44 EDRi (, EU s AI law needs major changes to prevent discrimination and mass surveil - lance: mass-surveillance/ . 45 EDRi (, Beyond De-biasing: Automated decision making and structural discrimination, authored by Agathe Balayn and Seda G rses, Delft University of T echnology, the Netherlands (forthcoming, September . 46 Emma Strubell, Ananya Ganesh and Andrew McCallum (. Energy and Policy Considera - tions for Deep Learning in NLP , accessed via: . 47 Valerio De Stefano (, The rise of the just-in-time workforce : On-demand work, crowd work and labour protection in the gig-economy : - stract_id=2682602 . 22 dependencies on centralised computational infrastructures,48 as well as the re-structuring of organisations and democractically accountable institutions49 are entirely overlooked in the AIA framework, with no foresight of the need for users of AI systems to take such factors into account. Enabling mass surveillance : The AIA proposal addresses neither the processes nor the infrastructures that may contribute to and normalise mass surveillance in its many forms (including, but not limited to, biometric mass surveillance). By promoting structures for gathering, inferring or predicting ever-more information about people, and connecting it across entities and services, the capacity for states and companies to unjustifiably surveil part or whole populations becomes ever- present and inescapable. Exporting rights-violating AI to the rest of the world : Concerns are also relevant in the context of export, as the scope of the AIA in article 2 establishes that the Act covers only AI that is put on the market/into service or used within the EU. This means that companies based in the EU may nevertheless be able to develop high-risk AI in an unrestricted manner, and even prohibited AI. It is a contradiction of EU rights and values that companies or entities based in the EU should be allowed to develop and then sell systems to states or companies outside of the EU, despite such systems being deemed to pose an unacceptable risk to fundamental rights and safety within the EU. 3 Loopholes enabling high risk uses Military and international law enforcement : Further, a number of loopholes limit the scope of the AIA in areas with crucial fundamental rights implications. article 2( leaves out of scope uses of AI developed or used for military contexts, and 2( leaves out of scope of the legislative proposal international organisations using AI systems in the framework of international agreements for law enforcement. This poses an unwarranted loophole for uses of AI for organisations such as EUROPOL, yet which still operates with significant fundamental rights implications for individuals in the European Union.50 Large scale migration databases: Further, article 83 leaves out of scope AI systems which are components of large scale IT systems, including the Schengen Information system, Visa Information System, Eurodac, the Entry/Exit 48 For an explanation of computational infrastructures, see - mable-infrastructures . 49 EDRi (, Beyond De-biasing: Automated decision making and structural discrimination, authored by Agathe Balayn and Seda G rses, Delft University of T echnology, the Netherlands (forthcoming, September . 50 EDRi (. Recommendations on the revision of Europol s mandate: content/uploads/2021/06/Recommendations-on-the-revision-of-Europols-mandate.pdf . 23 system, ETIAS, the European Criminal Records Information System on third-country nationals and stateless persons, and the Interoperability framework (Annex IX). This is major loophole for AI uses within the EU s migration control framework, with significant and severe consequences on the fundamental rights of people on the move should the AI systems that form part of these controls be excluded from the AIA s scope. Public security exemption : In addition, article 47 provides for a concerning ability for market surveillance authorities to authorise and provide exemptions to the conformity assessment procedure for reasons of public security or the protection of life and health of persons, environmental protection and the protection of key industrial and infrastructural assets. This provides an overly broad basis for market surveillance authorities to eradicate the already limited safeguards provided for in the Act, potentially compromising the principles of necessity and proportionality if not duly respected. Further processing of data exemptions: Further, article 54 of the proposal sets out a dangerous exemption to the principle of purpose limitation for innovative uses of AI. Specifcially, it allows for further processing of personal data for uses of substantial public interest , such as particular uses for law enforcement, public health, and environmental reasons. As argued by the EDPB and EDPS, this provision, alongside others in the proposal, presents a potential disconnect with the underlying principles contained in the GDPR regarding the grounds for further processing. These loopholes along with exclusions from rules for certain law enforcement purposes throughout the proposal also pose an additional risk that certain AI- based processes currently dealt with by administrative authorities may be pushed to law enforcement agencies in order to avoid regulatory scrutiny. The impact of such moves is that certain people most likely from marginalised groups, for example people on the move could be criminalised as a result. 4Focus on providers; limited obligations on users The core assumption of the AIA is that providers of AI systems are best placed to forecast, identify and mitigate the main harms that may emanate from AI systems. Following this, the AIA centralises the provider in the regulatory framework, with the bulk of requirements in the Act falling on those developing AI systems (articles 8-. The regulation allows a very wide scope for self-regulation by companies developing high risk AI. For the majority of high-risk AI uses contained in Annex III, the rules in article 43( mean that compliance with the Regulation s requirements is primarily ensured through self-assessment by the providers themselves. It is concerning that AI providers (those with a financial interest in 24 securing compliance and without the expertise to assess the implications on people s rights) themselves to judge if they have sufficiently met the requirements set out on data governance, transparency, accuracy, and more. Further, as higlighted by Veale and Zuiderveen Borgesius51, the conformity assessment process is likley to be highly influenced by European standardisation organisations such as CEN (European Committee for Standardisation) and CENELEC (European Committee for Electrotechnical Standardisation). The involvement of such entities in setting broad standards for the fulfilment of the essential requirements on providers further abstracts and weakens the process, creating a presumption of conformity . This process is particularly inappropriate for AI systems falling under article 6( of the act relating to fundamental rights impact, which may be incredibly complex, intangible and thus difficult to standardise for, but also have incredibly high potential impact on peoples rights. Furthermore, article 43( establishes a potential loophole, enabling providers to evade the requirement for external conf ormity assessments of biometric identification or categorisation systems if they comply with the standards. This means that a key purported safeguard may in practice have no effect. Another crucial flaw of the AIA s approach is that it overlooks the complexity of AI systems and the importance of context to be able to asess impact on fundamental rights, people and society . Whilst the provider- led conformity assessment process may identify the core technical shortcomings of the system, the mechanism is fundamentally ill-suited to identify the risks in the context of deployment. For example, a facial recogni - tion system may meet the technical requirements specified in the Act, yet still pose significant fundamental rights violations, compromise data protection and non-discrimination law, and enable mass surveillance in the context of deploy - ment (i.e. in a shopping centre). Because AI systems also by definition learn over time, there are intrinsic limitations to any snapshot-in-time conformity as - sessment. As such, ensuring that there are greater (and ongoing) obligations on users, in addition to providers, is crucial in order to address the fundamental rights issues that will arise in the use of AI. Further, we see that the requirements contained in the AIA placed on providers are highly technical in nature, thus largely inappropriate as a mechanism to prevent or mitigate potential risks to fundamental rights or other structural harms, or economic or environmental shifts engendered by the introduction of AI systems in context. Such considerations are inherently better assesed by the users in light of the context of deployment. 1Embedding dominance of AI providers 51 Veale and Zuiderveen Borgesius ( Demystifying the Draft EU Artificial Intelligence Act : . 25 This focus on providers potentially sets up a situation in which users of an AI system, such as a government agency, will be legally bound to follow the guidelines set out by a private company relating to the use of the AI system which they have procured (article . Whilst this article is intended to avoid misuse of a system, an unintended consequence of it may be that technology companies developing AI, whereby many already command disproportionate power over people and markets as established in EDRi s work on platform power, will thus be able to dictate the application of rules to which users including governments - are bound. 2Insufficient transparency With respect to the transparency framework, the Regulation (article largely imposes limited transparency obligations on providers toward users, as opposed to transparency requirements directly to people affected by or subject to AI systems (the exception to this is for limited risk uses cases under article 52, however we have explained on page 18-20 the serious shortcomings in the limited risk approach in the AIA). As such, the proposal will have a severely limited effect on people s ability to understand and challenge harmful and opaque AI systems deployed against them. Whilst the inclusion of an EU database of high risk AI systems as outlined in ar - ticle 60 is welcomed, currently the provision focuses on registration of high risk applications being put on the EU market. Full public transparency necessitates that this database registers high risk systems being put into use, including de - tails on which actors are deploying them and for which purpose. In addition, Annex VIII, s.11 contains an exception for public transparency for uses of AI in law enforcement and migration control, limiting the efficacy of the tool for pub - lic transparency in these sensitive contexts and the extent to which this data - base provides the necessary democratic checks and balances.52 Further, whilst the information currently included in article 13 to users provides a good basis of transparency, there is little obligation on providers to disclose to users information relating to the political a ssumptions and specific decisions related to the fundamental goals and assumptions of the system , weightings, parameters and standards resulting from these decisions. Articles 13 and 15 re - fer to accuracy of AI systems, however the definition and standards for accu - racy are left to the discretion of the providers. This provides little by way of guarantee to the user as to the validity or efficacy of the AI system for the pur - pose of use. This self-regulatory approach gives very little certainty with re - spect to the potential impact on fundamental rights. There is a concern that, if performance metrics conveyed to the user under article 13 are not sufficiently 52 EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act): [69]-[70]. 26 detailed and substantive, the broader human rights implications of de - ploying such a system may not be evident or discoverable by the user. 3Structural discrimination through AI With respect to discrimination exacerbated by AI systems, the AIA makes the assumption that data quality can solve the harms emenating from high risk AI systems. However, for many of the applications listed in Annex III, whilst AI developers may be able to predict and prevent some negative biases, for the most part such systems will inevitably exacerbate structural inequalities. This is because AI systems are deployed in a wider context of structural discrimination.53 By relying on technical checks for bias as a response to discrimination, the proposal risks reinforcing a harmful suggestion that removing bias from such systems is even possible, potentially obfuscating the need for structural solutions, such as limitations on certain uses, but also the need for intensive governance related responses. 4Impact on marginalised communities There are no specific requirements on users intending to put into use an AI sys - tem to measure the potential impact of such system on marginalised communi - ties, nor take steps to mitigate those impacts (including ensuring inclusive ac - cess, or halting deployments should they have an harmful impact on certain groups). The AIA fails to impose specific requirements on users to ensure the accessibility of AI systems or services that are operational through AI sys - tems.54 This is a systemic oversight in this Regulation, particularly in light of the broad promise that AI systems are likely to bring benefits to all in society. 5No consultation with affected groups Futher, the AIA foresees no specific duty on users to consult with affected groups or social partners before deploying AI systems, or modes of democratic oversight of AI systems deployed in contexts vital to the public interest. As already highlighted by Unions, this may serve to dilute existing consultation requirements with social partners in the employment context.55 53 EDRi (, Beyond De-biasing: Automated decision making and structural discrimination, authored by Agathe Balayn and Seda G rses, Delft University of T echnology, the Netherlands (forthcoming, September . 54 European Disability Forum (, Disability Perspective of AI of excellence and of trust (forthcoming). 55 ETUI ( The AI Regulation: entering an AI regulatory winter? Why an ad hoc directive on AI in employment is required: - tory-winter; UNI Global European Commission consultation response, June 27 5Limited enforcement and governance framework without actionable redress for subjects of AI The AIA pays insufficient attention to the fundamental interaction between the user and the subject of AI. This relationship is key to any fundamental rights based analysis and regulation of AI systems, and to the crucial question of how harms can be prevented and mitigated. Aside from article 52 outlining notification requiremented for a few narrowly defined limited risk AI systems (which are in themselves insufficient) the Act does not foresee notification requirements for high risk systems; duties to explain the reasoning behind automated decision making processes; nor, crucically, mechanisms for flagging or contestation of violations or harms as a result of interaction with AI systems. Whilst the AIA foresees the need for coordination between relevant national authorities supervising the regulation (articles 63(; 64; 65 and it provides no mechanism by which affected individuals or groups may flag to authorities potential harms, breaches of the Act or fundamental rights issues with an AI system. In addition, there is no mechanism for individual or collective redress for harms in scope of the AIA. This is a particularly fundamental ommission considering the limitations of other legal frameworks to provide effective redress with respect to AI systems, including the limits of article 22 of the GDPR, which is likely to be insufficient as a means to provide a right to explanation for many AI systems.56 Further, the limits of non-discrimination law, namely the focus on a limited set of protected characterstics, its requirement of a comparator, the focus on individual instances of discrimination as well as the high burden of proof on individuals in practice (exacerbated by the opacity of AI systems) reiterates the need for the inclusion of a redress mechanism in the AIA.57 Whilst the Explanatory Memorandum to the proposal claims that all major stakeholders were consulted in the course of developing the proposal (, many civil society organisations and communities have challenged the accuracy of this claim, citing their exclusion from this process. It is crucial that going forward in the legislative process, engagement with more affected groups (especially marginalised groups that are most likely to be subjected to AI decisions) is prioritised, in particular to consider the need for meaningful redress measures. 56 Wachter, Mittelstadt, and Floridi, (, Why a Right to Explanation of Automated Deci - sion-Making Does Not Exist in the General Data Protection Regulation International Data Pri - vacy Law, Volume 7, Issue 2, May 2017, Pages 76 57 T etyana Krupiy (, Why the proposed Artificial Intelligence Regulation does not deliver on the promise to protect individuals from harm European Law Blog: - blog.eu/2021/07/23/why-the-proposed-artificial-intelligence-regulation-does-not-deliver-on-the- promise-to-protect-individuals-from-harm/ . 28 Lastly, there is a significant concern with centralised nature of the proposed governance framework for the AIA. Article 56 establishes the European AI Board, however removes the competence of this Board (which appeared in previous verisons) to present additions to the list of high risk AI systems. In the AIA in its current form, this function is centralised with the European Commission only, presenting a significant concern as to the democratic nature of this process. Further, due to the power of the European Commission in the European AI Board, it raises significant questions relating to the independence of national supervisory authorities, which report to the AI board. However, as it is likely that these authorites should be Data Protection Authorities (and following the recommendation of the EDPS and EDPS it is clear that they should), that they should report to the European Commission potentially compromises the independence of these entities. 29 (B) Recommendations for a fundamental rights- based Artificial Intelligence Act In light of this analysis, EDRi recommends that the European Parliament and the Council of the EU implement the following improvements to the Artificial Intelligence Act (AIA): 1Ensure effective protection against prohibited practices and address the full scope of unacceptable risks through AI Imperative to the goal of a fundamental-rights respecting artificial intelligence regulation is the need to implement meaningful mechanisms geared toward the prevention of harm on individuals, groups and wider society. Civil society has been clear on the need to prevent, rather than to mitigate after the fact, impermissible or unacceptable risks to fundamental rights.58 a)Strengthen existing prohibitions in article 5 to provide meaningful protection against fundamental rights violations and individual and collective harms: i.Ensure that the prohibition on subliminal manipulative techniques in article 5((a) extends to harms which target groups of people as well as individuals; ii.Remove the caveat that AI systems that deploy subliminal techniques in order to materially distort a person s behaviour (article 5((a)) or exploits vulnerabilities of a specific group due to their age, physical or mental disability (article 5((b)) must cause or be likely to cause psychological and physical harm in order to be prohibited. Extend the list of vulnerabilities in article 5((b) to at least the protected characteristics outlined in the Charter of Fundamental Rights in the EU, with the explicit inclusion of gender identity; iii.Ensure wide application of the prohibition on social scoring59 (article 5((c)). Remove narrow framings, such as the temporal limitation over a certain period of time , the limitation to public authorities, 58 EDRi (, Open letter: Civil society call for the introduction of red lines in the upcoming Commission proposal on artificial intelligence: red-lines-in-the-european-unions-artificial-intelligence-proposal/ . 59 EDPS and EDPB, Joint Opinion on the proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act), [29]. 30 the narrow framing of a singular score and replace the reference to trustworthiness to one of risk . Remove references to general purpose in recital b)Comprehensively prohibit the use of remote biometric identification in publicly-accessible spaces for any purpose , and implement a general ban on any use of AI for an automated recognition of human features in publicly accessible spaces - such as of faces but also of gait, fingerprints, DNA, voice, keystrokes and other biometric or behavioral signals - in any context , as per the EDPS- EDPB Joint Opinion.60 These prohibitions must apply for all purposes and in any context, including online spaces, and without exception: i.Furthermore, the putting on the market or placing into service of remote biometric identification software and hardware should be restricted in order to prevent biometric mass surveillance infrastructures being rolled out, and to ensure that EU companies cannot sell products and services which are designed for biometric mass surveillance outside the EU. The purpose limitation principle in the GDPR (article b) should be reiterated here, as it already stipulates that CCTV footage, for example, should not be used for other purposes, for example training AI software or for performing re-identification; ii.The definition of remote in RBI ( should add that RBI occurs not just with reference to watchlists but also to general databases. The provision that it applies only if there is no prior knowledge of the user about whether the person of interest will be present and identifiable should be fully removed, in order to avoid creating loopholes; iii.Human features should be defined under article 3 to include but not be limited to - biometric, physiological, behavioural and neurological signals; iv.As called for by the Civil Liberties Committee in the European Parliament, there must be a ban on the use of private facial recognition databases in law enforcement such as Clearview AI due to the likely incompatibility of such uses with EU data protection law.61 EDRi s analysis has shown that many of the same incompatibilities will apply also for databases developed by law enforcement agencies themselves. 60 Ibid [11]. 61 LIBE Committee, Artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters 2020/2016(INI), awaiting Plenary vote. 31 c)Include new prohibitions on the following practices which are incompatible with fundamental rights and democracy, and pose an unacceptable risk: i.Uses of AI in the field of law enforcement or criminal justice that purport to predict future behaviour, including analysing the risk that individuals will offend or re-offend, and predicting the likelihood that criminal or unfavourable conduct will occur on the basis of personality traits, individual or group characteristics or location; ii.Uses of AI in the field of migration control in ways that undermine the right to claim asylum, including but not limited to those: to risk assess inviduals for factors that do not relate to the substance of their immigration claim, such as risk of terrorism, public health threats, etc; to collect data and / or predict patterns in migratory movements for the purpose of preventing the exercise of the right to claim asylum; AI systems to assess eligibility for asylum, refugee or visa claims; iii.Uses of AI that implement invasive surveillance, monitoring and algorithmic management in an employment and educational context; iv.(Biometric) categorisation, which can pose a grave and disproportionate threat to all human rights, in particular equality and non-discrimination, by comprehensively prohibiting the use of AI to categorise people, on the basis of their human features, to the special categories of data as defined in article 9 of the GDPR;62 or to categories based on the grounds for unlawful discrimination in article 21 of the Charter of Fundamental Rights of the European Union;63 or on the basis of mental health status, migration status or gender identity: This prohibition must also include the use of potentially non- special or non-personal data, as well as data that does not meet the threshold to be considered biometric, captured from human features when used to categorise people according to proxies of 62 Racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade union membership, and the processing of genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health or data concerning a natural person's sex life or sexual orientation (GDPR, article . 63 Sex, race, colour, ethnic or social origin, genetic features, language, religion or belief, politi - cal or other opinion, membership of a national minority, property, disability, age or sexual ori - entation (CFEU, article . 32 special or protected categories (e.g. by combining eye and hair colour to predict ethnicity or using the wearing of a headscarf to predict religion). In the event that the self-learning nature of some AI systems makes it difficult to know whether people are being assigned to categories that could lead to discrimination, the precautionary principle dictates that such uses should also be prohibited; As already explained above, the definition of biometric data (article must be complemented with a definition of human features to ensure that it includes all data relevant to biometric or other human feature categorisation (without loopholes for types of data or methods of processing that don t meet the current threshold);64 v.Emotion recognition, which is scientifically invalid and can unduly infringe on all human rights, in particular human dignity and free expression, by comprehensively prohibiting the placing on the market, putting into service or use of AI to infer, predict, analyse or assess a person s emotions, feelings, emotional state, beliefs, preferences, intentions or otherwise inner thoughts, as well as to use human features, behaviours or expressions to predict future actions or behaviours; vi.Uses of AI that constitute mass surveillance should be prohibited. Mass surveillance means the surveillance of, or potential for surveillance of, whole or part populations (including specific groups), and is thus inherently unnecessary and disproportionate. Note that outside of the proposed prohibitions of biometric mass surveil - lance, biometric categorisation on the basis of special or protected categories and emotion recognition in general, there are additional AI ap - plications which use human features and which can pose a high risk to funda - mental rights. Therefore we additionally recommend that: Heading 1 of Annex III is changed to Physiological, behavioural, biomet - ric and neurological authentication, identification and categorisation . If human features are defined in article 3 according to our recommenda - tion, then an alternative heading could be Authentication, identification and categorisation of human features ; Under heading 1, the following use cases are added in addition to the ex - isting use case (a), but are not necessarily exhaustive at this point: i.Physiological, behavioural or biometric authentication, identification or categorisation (i.e. of human features) for law enforcement purposes; 64 See Access Now s Submission to the Consultation on the AI White Paper for a deeper engagement with issues of the definition of (biometric) categorisation. 33 ii.Physiological, behavioural or biometric authentication, identi - fication or categorisation (i.e. of human features) by private actors for surveillance or security purposes (such as security companies); iii.Physiological, behavioural or biometric authentication, identification or categorisation (i.e. of human features) for any purpose, where it can determine, solely or in part, people s access to: Public services (e.g. getting benefits payments); Private or privatised services which are necessary for people to ex - ercise or enjoy their fundamental rights and freedoms (e.g. using e- border gates, entering supermarkets, going to work). Furthermore, the Act must guarantee that providers of high-risk uses of AI under Annex III paragraph 1 should not be able to circumvent the obli - gation for an ex ante third party conformity assessment simply by meeting harmonised EU standards (as proposed in article 43(. 2Adapt the AIA to ensure holistic, democratic and fu - ture-proof framework Noting that the impact of AI systems extend far beyond impact on individual rights and product safety, but also are highly transient and susceptible to rapid change, the following proposals are designed to democratise the framework set out in AIA as well as better respond to structural and infrastructural harms. a)Introduce a democratic, inclusive and acccessible process by for the insertion of new prohibitions. Include criteria for unacceptable risk and the addition of future prohibitions into the AIA to ensure the enduring relevance of this regulatory instrument: i.Such criteria might criteria might include the impact on fundamental rights, structural power imbalances around the context of deployment (including potential for enhanced discrimination, marginalisation, inequality), lack of capacity for individuals, groups or civil society to contest the usage, etc. b)Ensure the potential to update the high risk use case areas in the future (amending article in addition to updating the use cases sub-areas . Further, this process must not be centralised with the European Commission only, but include a range of actors including civil society. 34 c)Respond to gaps in regulation with respect to economic and environmental impact, structural forms of inequality and AI and migration control, law enforcement and worker surveillance, mass surveillance, and exports of high- risk or prohibited AI outside the EU. d)Remove loopholes in articles 2(, and 83 leaving out of scope of the AIA AI systems used as part of international agreements on law enforcement and large scale IT systems in the migration control context. e)Remove the broad exemption to forgo the duty to conduct a conformity assessment on grounds of public security in article f)Remove the exemption to the principle of purpose limitation contained in article 54((a) for innovative AI within the regulatory sandbox provisions for uses in the criminal justice context. 3Ensure responsibility to those subjected to AI systems with enhanced obligations on users of all AI systems In the current AIA framework, the majority of the requirements fall on providers to implement a series of technical measures designed to mitigate harm in the deployment of systems. However, many of these harms are likely to be contextual and are best evaluated and addressed by the user, who has ultimate responsibilty to those subjected to the AI system. T o ensure the use of AI systems is accountable to and compliant with fundamental rights, we recommend that the requirements on providers are complemented with obligations on users geared toward greater responsibility to those subjected to AI systems. a)Mandate users to conduct and publish an ex ante human rights impact assessment before putting a high risk AI system into use, clearly outlining the stated purpose for which the system will be implemented: i.The impact assessment must be published on registration of use of the system in the public database under article 60; ii.This impact assessment must involve prior consultation with relevant national authorities, including equality bodies, consumer protection agencies, and data protection agencies. If other impact assessments are also required, these impact assessments must be published together; 35 iii.The impact assessment must also carry out meaningful consultation with social partners, civil society groups and individuals and groups affected by the use case; iv.The impact assessment must include full assessment of the fundamental rights that are likely to be impacted by the AI system, in addition to broader, social, political and economic consequences of deploying the AI system in the particualr context for the particular use. This should include indirect consequences of deploying the system, beyond impacts on those directly impacted by a decision generated; v.The impact assessment must include clear steps as to how the harms identitified will be mititgated, and how effective this mitigation is likely to be. If adequate steps for mitigation cannot be cannot be outlined, the system ought not to be deployed. b)Implement on users a duty to cooperate with national competent authorities investigating AI systems for potential threats to fundamental rights or safety under articles 65 and 67 for all AI systems, regardless of risk designation. c)Implement a duty on users to meaningfully consult with institutions, civil society and social partners representing affected groups before deploying high risk AI systems: i.Documentation of the results of this consultation should be included in the publicly acessible impact assessment. d)When the user of any AI system is a public authority, implement a notifcation requirement to all those impacted by a decision made by the system: i.This should include communicating how and why the decision was made, and how other available information or alternative outcomes were considered in reaching a decision.65 4Implement meaningful public transparency for high risk AI systems 65 This proposal was originally made by Melanie Fink (, The EU Artificial Intelligence Act and Access to Justice : to-justice-by-melanie-fink/ . 36 T o ensure greater public oversight of AI systems, the existing frame - work must be complemented by substantive mechanisms for transparency, such that AI systems in use are discoverable by oversight bodies, civil society and individuals. a)Ensure meaningful public transparency by requiring registration in the EU database (article of all high risk AI systems, and potentially all AI systems to which people are subject that are put into use. This would enable individuals and civil society to access information about AI systems in operation: i.The responsible authority or entity for deploying the high risk AI system should be listed with a contact point; ii.This should include information as to the stated purpose of the AI system in clear terms for individuals to understand. b)Ensure the inclusion of instructions for use for AI systems in law enforcement and migration, asylum and border control management in the public database as per Annex III, points 1, 6 and Remove the exemption contained within Annex VIII, point c)Require providers to include access to the conformity assessment alongside the instructions for use as per article 13(-( in the public database under article d)Require providers to provide more thorough details about the system to the users as part of article 13(. This must include: i.Information relating to the weightings and criteria relevant to choices in automated decision making systems; ii.An explanation of the fundamental assumptions and decisions informing the design of the AI system; iii.Ensure that information regarding the accuracy of the system under article 13((ii) is precise, allowing the user to objectively assess whether the AI sytem is fit for purpose. e)Remove the exemptions in article 52 relating to the transparency of AI systems used for detection and prevention of criminal offences (as argued by the EDPB and EDPS) and for the prosecution of people. When AI systems under article 52 37 are used for investigation, suspects should be notified post factum . 5Facilitate accountability: Include oversight and enforce - ment infrastructures that work for people Lastly, the following proposals are designed to ensure that those harmed by the systems regulated under the AIA are able to contest and seek remedies. Further, there must be more independence for the European AI board and more distributed scope of governance funtions. a)Ensure a cohesive national enforcement structure: i.Following the recomemndation of the EDPS and EDPB, national Data Protection Authorities (DPAs) should be the designated national supervisory authorities under the act, with a stated duty to work with other relevant enforcement authorities in evaulation and monitoring; ii.Ensure sufficient resources for national supervisory authorities in order to evaluate AI systems but also to respond and administer complaints. b)Include flagging and redress mechanisms allowing individuals and collectives to contest and seek redress for all AI systems that cause harm and threaten fundamental rights: i.This could include a flagging mechanism for those potentially impacted by an AI system to trigger national supervisory authorities evaluative action under article 67; ii.This duty to evaluate for fundamental rights risks should not be limited only to high risk systems, but any AI system once the national supervisory authority has received a complaint; iii.An explicit individual and collective redress mechanism must be introduced specifically to apply to those subjected by all AI systems, in particular noting that many such stand-alone systems are not covered by consumer mechanisms for collective redress. c)Implement a more democratic governance infrastructure, with greater independence for the European AI board: 38 i.Ensure that the mandate to make substantive updates to the legal framework (updates to high risk use cases, prohibitions) is held by a representative and democractically accountable European AI Board, not solely with the European Commission; ii.Include within the structure of the AI Board representatives of social partners and civil society, in particular those representing marginalised groups. 39",en,"Include criteria for unacceptable risk and the addition of future prohibitions into the AIA; b.Ensure the potential to update the high risk use case areas in the future (amending article in addition to updating the use case sub- areas ; 2 c.Respond to gaps in regulation with respect to economic and environmental impact, structural forms of inequality and AI and migration control, law enforcement and worker surveillance, mass surveillance, and exports of high-risk or prohibited AI outside the EU; d.Remove loopholes in articles 2( and 83 which currently leave out of scope of the AIA AI systems used as part of international law enforcement agreements; e.Remove the broad exemption to forgo the duty to conduct a conformity assessment on grounds of public security in article 47; f.Remove the exemption to the principle of purpose limitation contained in article 54((a) for innovative AI within the regulatory sandbox provisions for uses in the criminal justice context. 21 Further, as outlined by the European Disability Forum (EDF), there are no provisions in the Act to ensure that all AI systems (regardless of risk level) meet international legal obligations relating to the accessibility of persons with disabilities.43 By promoting the notion that AI systems can be primarily regulated through a series of technical measures, (documentation, human oversight in design and data quality standards), the AIA provides no response to the structural harms outlined in the previous section.44 This techno-centric framing does not adequately deal with how AI as socio-technical systems become embedded in broader processes of structural discrimination, which the examination of possible biases (article , purported improvements in accuracy (Article and more documentation (article will simply not address.45 Environmental impact: The AIA wholly underestimates the vast impact of a policy agenda designed to promote the widescale uptake of AI, underpinned by the exponential collection of data and focusing on the presumed benefits, without sufficient regards to the broader implications of the greater resort to AI systems on the environment. In particular, consequences on the environment relating to the vast environmental resources (including the exploitation of natural resources for the hardware required to underpin AI systems) as well as the energy consumption46 required for many of such systems to be trained and functional, as well as for data to be stored, find no place in the proposed regulatory framework. Energy and Policy Considera - tions for Deep Learning in NLP , accessed via: . Public security exemption : In addition, article 47 provides for a concerning ability for market surveillance authorities to authorise and provide exemptions to the conformity assessment procedure for reasons of public security or the protection of life and health of persons, environmental protection and the protection of key industrial and infrastructural assets. Specifcially, it allows for further processing of personal data for uses of substantial public interest , such as particular uses for law enforcement, public health, and environmental reasons. Further, we see that the requirements contained in the AIA placed on providers are highly technical in nature, thus largely inappropriate as a mechanism to prevent or mitigate potential risks to fundamental rights or other structural harms, or economic or environmental shifts engendered by the introduction of AI systems in context. 34 c)Respond to gaps in regulation with respect to economic and environmental impact, structural forms of inequality and AI and migration control, law enforcement and worker surveillance, mass surveillance, and exports of high- risk or prohibited AI outside the EU.",risk
German Education Union (GEW) (Germany),F2665205,02 August 2021,Trade union,Medium (50 to 249 employees),Germany,"Education International Internationale de l'Education Internacional de la Educaci n -ie.org EUROPEAN REGION - ETUCE President Larry FLANAGAN Vice-Presidents Odile CORDELIER Andreas KELLER Trudy KERPERIEN Dorte LANGE Galina MERKULOVA Branimir STRUKELJ Boulevard Bischoffsheim, 15 1000 Brussels , Belgium Tel +32 2 224 06 91/92 Fax +32 2 224 06 94 secretariat@csee -etuce.org -etuce.org European Director Susan FLOCKEN Treasurer Joan DONEGAN ETUCE European Trade Union Committee for Education EI European Region ETUCE position on the EU Regulation on Artificial Intelligence (Adopted b y the ETUCE Bureau on 7 June 2021 ) Background: On 21 April 2021, the European Commission published a proposal for a Regulation on a European Approach for Artificial intelligence (the AI Regulation). With this proposal, the European Commission follows up on its White Paper on Artificia l Intelligence (February , based on the results of a broad consultation process to which ETUCE contributed . The aim of the initiative is t o establish the first EU legal framework regulat ing the entire lifecycle of the use of Artificial Intelligence (AI) in all sectors , including education. The AI Regulation classifies the use of Artificial Intelligence in various sectors based on the risk that the AI tools have on the health and safety and the fundamental rights of individuals. Concerning education, t he proposal consider s the use of A rtificial Intelligence tools in education as high -risk as potentially harmful to the right to education and training as well as the right not to be discriminated in education. For high -risk sector s, the AI Regulation establishes stricter horizontal legal requirements to which AI tools must comply before being auth orised on the market. These include risk management system during the entire lifecycle of the AI system. Following the publication of the proposal, on 26 April 2021 , the European Commission issue d a public consultation that will run until 20 July 2021 , accompanied by a n impact assessment report . The following text is the ETUCE response to the public consultation bringing the perspective of teachers, a cademics and other education personnel on the sections of the AI Regulation that touch upon the education sector. ETUCE reply : ETUCE welcomes th e publication of the AI Regulation as it sets the ground for the first comprehensive EU regulation on Artificial Intelligence to ensure a controlled development of AI tools in education and address the risks connected to the ir use by teachers , academic, other education personnel and students . While ETUCE recognises the potential of digital technologies and Artificial Intelligence tools to bring about improvements in education , it also underlines the numerous ethical concern s related to their trustworthiness, data privacy, accountability, transparency and their impact on equality and inclusion in education . ETUCE underlines that further research at na tional and European level is needed to assess and address the risks connected to the use of Artificial Intelligence in education with constant and meaningful consultation with education social partners . 2 AI in education as a high -risk: ETUCE welcomes that the AI Regulation classifies the use of AI tools in education and vocational training as high -risk underlining that When improperly designed and used, such systems may violate the right to education and train ing as well as the right not to be discriminated against and perpetuate historical patterns of discrimination . ETUCE emphasises that the EU Commission initiative should ensure that the development of Artificial Intelligence in education does not infringe the human right of all individuals to have equal access to quality education . This is enshrined in the first and third principle s of the European Pillar of Social Rights and the European Charter of Fundamental Rights . ETUCE supports the European Commission s propos al to set stricter horizontal l egal requirements for the AI tools used in the education sector . The AI Regulation proposal also foresee s the establishment of a risk management system to analyse the risks associated with the use of the AI tools in education and monitor them during the entire lifecycle of the AI tool s. In th is regard, ETUCE believes that the European Commission should support the development of clear and binding measures , including ethical guidelines, to address the risks that AI tools pose concerning transparency, accountability, intellectual property right s, data privacy, cyber -safety, equality and environmental protectio n. Governance: The Proposal for Regulation seeks to establish a governance system leading to the establishment of a European Artificial Intelligence Boar d with the involvement of national authori ties to monitor the implementation of the regulation. Nevertheless, ETUCE underlines that the effective implementation of the AI legislation in education requires the meaningful involvement of teachers, academics and education staff as co -creators of Artificial Intelligence tools in education. E ducation trade unions have a crucial role to play to addressing the risks of Artificial Intelligence in education and bring the perspective of AI users on the implementation of the regulation. It is therefore crucial that education social partners are actively involved in the activities of the European Artificial Intelligence Board through regular consultations and meaningful social dialogue, both at n ational and European leve l to monitor the implementation of the regulation and address the risks related to the use of Artificial Intelligence in education. The role of teachers in education : Teachers, academics and other education personnel play a c rucial role in fostering the full human potential of students and their role in education must be preserved. ETUC E calls on the European Commission and the Member States to interdict the Artificial Intelligence tools that are designed to replace education personnel or can damage the social value and the quality of education . Besides , the AI Regulation should ensure that the development of AI in education does not reduce the role of teachers to mere providers of instructions but rather serves as a supporting tool for the teaching profession while 3 preserving the professional and pedagogical autonomy and academic freedom of teachers and academics. Transparency and AI literacy and CPD of teachers on AI : ETUCE welcomes that the proposal of AI Regulation requires that users of AI tools (who include students, teachers, academic and education staff for the education sector) must be adequately informed about the intended purpose, level of accuracy, residual risks of AI tools . Nevert heless, ETUCE highlight s that providing information is not sufficient to ensure the transparency of the AI tools when users miss the adequate digital skills and data and AI literacy to interpret it. Therefore, it is of utmost importan ce to improve the importance of digital skills, AI literacy and d ata literacy in educational curricula and raise awareness on the risks related to the use of AI tools in education . It is also essential to ensure that infrastructures of education institutions are adequately equipped for digital education as well as to provide equal access to digital technologies and ICT tools to all teachers and students, with particular attention to the most disadvantaged group s. To these purposes , sustainable public investment should be provided by national governments and the European Commission should provide financial support through European funding such as Horizon E urope, Digital Europe and in the framework of National Recovery and Resilience Facility. While the AI Regulation blandly mention to the possibility o f providing users with training on Artificial Intelligence , ETUCE emphasises that it is crucial that sustai nable public funding are provided at national and European level to ensure that teachers, trainers, academics and other education personnel receive up-to-date and free of charge continuous training and professional development on the use of AI tools in acc ordance with their professional needs . EdTech expansion and issues of intellectual property rights, data privacy of teachers : ETUCE points out that the development of the use of Artificial Intelligence in education has been accompanied by the expansion of Ed-tech companies that are progressively increasing their influence in the education sector , especially under the pressure of emergency online teaching and learnin g during the COVID -19 pandemic. ETUCE reminds that education is a human right and public goo d whose value need s to be protected. ETUCE calls for further public responsibility from national governments that should not limit their scope to regulat ing the EdTech sector and should develop and implement public platforms for online teaching and learning to protect the public value of education. In addition, public platforms should be implemented in full respect of professional autonomy of teachers and education personnel as well as academic freedom and autonomy of education institutions, without creating pressure on teachers and education personnel regarding the education material and pedagogical methods they use. It is also essential t o protect the accountability and transparency in the governance of public education systems from the influence of private and commercial interests and actors . 4 AI tools storing a vast amo unt of data cause inevitable risks on data protection , privacy and intellectual property rights of teachers and academics and other education personnel. ETUCE highlights that ensuring data protection and privacy of teachers and students should be a priority of the AI Regulation and calls on the EU Commission and the Member States to develop appropriate data -retention policies applicable to Artificial Intelligence in education, in the respect of national competenc ies in education . Equality an d inclusion in the design and use of AI in education : As enshrined in the EU Pillar of Social Rights and the EU Charter of Fundamental Rights , non- discrimination in educati on is a fundamental principle of our society. In this regard, the EU Commission s p roposal states that the AI regulation will minimise the risks of erroneous or biased AI -assisted decision on education and training . In this context, ETUCE recognizes that the use of Artificial Intelligence has the potential to advance the quality of li fe and inclusion of teachers and students in education . Nonetheless, the persistent lack of diversity and underrepresentation of women, ethnic minorities, Black People and disadvantaged groups in the population of professionals responsible for designing, t esting and training the algorithms and data of AI tools translate in the presence of biases in AI tools, leading to a detrimental impact on inclusion and equality in education . Therefore, ETUCE calls on the European Commission and Member States to provide adequate public investment to encourage more diversity in the STEAM sector and ensure that AI tools are designed and used with the full representation of the wide societ y. Besides , research shows that cyber -violence, cyber -bullying and cyber -harassment have increased with the development of digitalisation in education. ETUCE underlines that it is important to further explore how Artificial Intelligence systems can act as supporting tools to detect and counter cyber -violence, cyber -bullying and cyber -harassment . *The European Trade Union Committee for Education (ETUCE) represents 127 Education Trade Unions and 11 million teachers in 51 countries of Europe. ETUCE is a Social Partner in education at the EU level and a European Trade Union Federation within ETUC, the European Trade Union Confederation. ETUCE is the European Region of Education International, the global federation of education trade unions.",en,"The AI Regulation proposal also foresee s the establishment of a risk management system to analyse the risks associated with the use of the AI tools in education and monitor them during the entire lifecycle of the AI tool s. In th is regard, ETUCE believes that the European Commission should support the development of clear and binding measures , including ethical guidelines, to address the risks that AI tools pose concerning transparency, accountability, intellectual property right s, data privacy, cyber -safety, equality and environmental protectio n. Governance: The Proposal for Regulation seeks to establish a governance system leading to the establishment of a European Artificial Intelligence Boar d with the involvement of national authori ties to monitor the implementation of the regulation.",risk
ETUCE (Belgium),F2663486,30 July 2021,Trade union,Small (10 to 49 employees),Belgium,"Education International Internationale de l'Education Internacional de la Educaci n -ie.org EUROPEAN REGION - ETUCE President Larry FLANAGAN Vice-Presidents Odile CORDELIER Andreas KELLER Trudy KERPERIEN Dorte LANGE Galina MERKULOVA Branimir STRUKELJ Boulevard Bischoffsheim, 15 1000 Brussels , Belgium Tel +32 2 224 06 91/92 Fax +32 2 224 06 94 secretariat@csee -etuce.org -etuce.org European Director Susan FLOCKEN Treasurer Joan DONEGAN ETUCE European Trade Union Committee for Education EI European Region ETUCE position on the EU Regulation on Artificial Intelligence (Adopted b y the ETUCE Bureau on 7 June 2021 ) Background: On 21 April 2021, the European Commission published a proposal for a Regulation on a European Approach for Artificial intelligence (the AI Regulation). With this proposal, the European Commission follows up on its White Paper on Artificia l Intelligence (February , based on the results of a broad consultation process to which ETUCE contributed . The aim of the initiative is t o establish the first EU legal framework regulat ing the entire lifecycle of the use of Artificial Intelligence (AI) in all sectors , including education. The AI Regulation classifies the use of Artificial Intelligence in various sectors based on the risk that the AI tools have on the health and safety and the fundamental rights of individuals. Concerning education, t he proposal consider s the use of A rtificial Intelligence tools in education as high -risk as potentially harmful to the right to education and training as well as the right not to be discriminated in education. For high -risk sector s, the AI Regulation establishes stricter horizontal legal requirements to which AI tools must comply before being auth orised on the market. These include risk management system during the entire lifecycle of the AI system. Following the publication of the proposal, on 26 April 2021 , the European Commission issue d a public consultation that will run until 20 July 2021 , accompanied by a n impact assessment report . The following text is the ETUCE response to the public consultation bringing the perspective of teachers, a cademics and other education personnel on the sections of the AI Regulation that touch upon the education sector. ETUCE reply : ETUCE welcomes th e publication of the AI Regulation as it sets the ground for the first comprehensive EU regulation on Artificial Intelligence to ensure a controlled development of AI tools in education and address the risks connected to the ir use by teachers , academic, other education personnel and students . While ETUCE recognises the potential of digital technologies and Artificial Intelligence tools to bring about improvements in education , it also underlines the numerous ethical concern s related to their trustworthiness, data privacy, accountability, transparency and their impact on equality and inclusion in education . ETUCE underlines that further research at na tional and European level is needed to assess and address the risks connected to the use of Artificial Intelligence in education with constant and meaningful consultation with education social partners . 2 AI in education as a high -risk: ETUCE welcomes that the AI Regulation classifies the use of AI tools in education and vocational training as high -risk underlining that When improperly designed and used, such systems may violate the right to education and train ing as well as the right not to be discriminated against and perpetuate historical patterns of discrimination . ETUCE emphasises that the EU Commission initiative should ensure that the development of Artificial Intelligence in education does not infringe the human right of all individuals to have equal access to quality education . This is enshrined in the first and third principle s of the European Pillar of Social Rights and the European Charter of Fundamental Rights . ETUCE supports the European Commission s propos al to set stricter horizontal l egal requirements for the AI tools used in the education sector . The AI Regulation proposal also foresee s the establishment of a risk management system to analyse the risks associated with the use of the AI tools in education and monitor them during the entire lifecycle of the AI tool s. In th is regard, ETUCE believes that the European Commission should support the development of clear and binding measures , including ethical guidelines, to address the risks that AI tools pose concerning transparency, accountability, intellectual property right s, data privacy, cyber -safety, equality and environmental protectio n. Governance: The Proposal for Regulation seeks to establish a governance system leading to the establishment of a European Artificial Intelligence Boar d with the involvement of national authori ties to monitor the implementation of the regulation. Nevertheless, ETUCE underlines that the effective implementation of the AI legislation in education requires the meaningful involvement of teachers, academics and education staff as co -creators of Artificial Intelligence tools in education. E ducation trade unions have a crucial role to play to addressing the risks of Artificial Intelligence in education and bring the perspective of AI users on the implementation of the regulation. It is therefore crucial that education social partners are actively involved in the activities of the European Artificial Intelligence Board through regular consultations and meaningful social dialogue, both at n ational and European leve l to monitor the implementation of the regulation and address the risks related to the use of Artificial Intelligence in education. The role of teachers in education : Teachers, academics and other education personnel play a c rucial role in fostering the full human potential of students and their role in education must be preserved. ETUC E calls on the European Commission and the Member States to interdict the Artificial Intelligence tools that are designed to replace education personnel or can damage the social value and the quality of education . Besides , the AI Regulation should ensure that the development of AI in education does not reduce the role of teachers to mere providers of instructions but rather serves as a supporting tool for the teaching profession while 3 preserving the professional and pedagogical autonomy and academic freedom of teachers and academics. Transparency and AI literacy and CPD of teachers on AI : ETUCE welcomes that the proposal of AI Regulation requires that users of AI tools (who include students, teachers, academic and education staff for the education sector) must be adequately informed about the intended purpose, level of accuracy, residual risks of AI tools . Nevert heless, ETUCE highlight s that providing information is not sufficient to ensure the transparency of the AI tools when users miss the adequate digital skills and data and AI literacy to interpret it. Therefore, it is of utmost importan ce to improve the importance of digital skills, AI literacy and d ata literacy in educational curricula and raise awareness on the risks related to the use of AI tools in education . It is also essential to ensure that infrastructures of education institutions are adequately equipped for digital education as well as to provide equal access to digital technologies and ICT tools to all teachers and students, with particular attention to the most disadvantaged group s. To these purposes , sustainable public investment should be provided by national governments and the European Commission should provide financial support through European funding such as Horizon E urope, Digital Europe and in the framework of National Recovery and Resilience Facility. While the AI Regulation blandly mention to the possibility o f providing users with training on Artificial Intelligence , ETUCE emphasises that it is crucial that sustai nable public funding are provided at national and European level to ensure that teachers, trainers, academics and other education personnel receive up-to-date and free of charge continuous training and professional development on the use of AI tools in acc ordance with their professional needs . EdTech expansion and issues of intellectual property rights, data privacy of teachers : ETUCE points out that the development of the use of Artificial Intelligence in education has been accompanied by the expansion of Ed-tech companies that are progressively increasing their influence in the education sector , especially under the pressure of emergency online teaching and learnin g during the COVID -19 pandemic. ETUCE reminds that education is a human right and public goo d whose value need s to be protected. ETUCE calls for further public responsibility from national governments that should not limit their scope to regulat ing the EdTech sector and should develop and implement public platforms for online teaching and learning to protect the public value of education. In addition, public platforms should be implemented in full respect of professional autonomy of teachers and education personnel as well as academic freedom and autonomy of education institutions, without creating pressure on teachers and education personnel regarding the education material and pedagogical methods they use. It is also essential t o protect the accountability and transparency in the governance of public education systems from the influence of private and commercial interests and actors . 4 AI tools storing a vast amo unt of data cause inevitable risks on data protection , privacy and intellectual property rights of teachers and academics and other education personnel. ETUCE highlights that ensuring data protection and privacy of teachers and students should be a priority of the AI Regulation and calls on the EU Commission and the Member States to develop appropriate data -retention policies applicable to Artificial Intelligence in education, in the respect of national competenc ies in education . Equality an d inclusion in the design and use of AI in education : As enshrined in the EU Pillar of Social Rights and the EU Charter of Fundamental Rights , non- discrimination in educati on is a fundamental principle of our society. In this regard, the EU Commission s p roposal states that the AI regulation will minimise the risks of erroneous or biased AI -assisted decision on education and training . In this context, ETUCE recognizes that the use of Artificial Intelligence has the potential to advance the quality of li fe and inclusion of teachers and students in education . Nonetheless, the persistent lack of diversity and underrepresentation of women, ethnic minorities, Black People and disadvantaged groups in the population of professionals responsible for designing, t esting and training the algorithms and data of AI tools translate in the presence of biases in AI tools, leading to a detrimental impact on inclusion and equality in education . Therefore, ETUCE calls on the European Commission and Member States to provide adequate public investment to encourage more diversity in the STEAM sector and ensure that AI tools are designed and used with the full representation of the wide societ y. Besides , research shows that cyber -violence, cyber -bullying and cyber -harassment have increased with the development of digitalisation in education. ETUCE underlines that it is important to further explore how Artificial Intelligence systems can act as supporting tools to detect and counter cyber -violence, cyber -bullying and cyber -harassment . *The European Trade Union Committee for Education (ETUCE) represents 127 Education Trade Unions and 11 million teachers in 51 countries of Europe. ETUCE is a Social Partner in education at the EU level and a European Trade Union Federation within ETUC, the European Trade Union Confederation. ETUCE is the European Region of Education International, the global federation of education trade unions. 1 EI European Region For an Education-led RecoveryRESOLUTION Artificial Intelligence in the Education Sector Adopted by the ETUCE Conference, the Regional Conference of Education International, on 5-6 July 2021 Further to and consistent with the Resolutions adopted by the 8th EI Congress in Bangkok in 2019 and the Resolutions adopted by the ETUCE Conference in 2020, this ETUCE Conference, Acknowledges that: The digital transformation is set to change the world of work and wider societal landscape. Technologies such as Artificial Intelligence are increasingly woven into the professional, social and personal lives of individuals. As such, the impact of Artificial Intelligence is a matter of growing importance for the education sector and its professionals. Along with the spread of Artificial Intelligence technologies in everyday lives, arises the pressing need for individuals to be trained to understand the basics of these technologies, as well as their potential risks; The growing reliance on Artificial Intelligence for a multitude of purposes, further exacerbated due to the COVID-19 pandemic and the multiplication of activities previously undertaken physically being transitioned online, therefore undoubtedly invites to a wider reflection on the role of these powerful technologies in the daily lives of citizens across Europe and its impact on democratic, sustainable societies; Artificial Intelligence, whilst being increasingly hailed by decision-makers and tech giants as an innovative technology leading to the vast improvement of the life of people, presents opportunities but also real threats to individuals. From an ethics perspective, in particular, the ability of machines to influence the choices of human beings risks hindering humankind s independence, free will and creativity;2 ETUCECONFERENCE 2021 EI European Region Crucially, Artificial Intelligence technologies present ethical concerns when it comes to transparency, accountability, data protection, privacy of users, cyber-safety, democracy, freedom of action and choice and discriminatory practices. A common understanding of their optimal use is therefore essential for all, in order to engage with them in a critical, safe, inclusive, participative and confident approach as early as possible. In such a context, social partners in education have a key role in providing their members with the basic skills needed to understand the functioning and safest use of Artificial Intelligence. Therefore, in all respects, the use and impact of Artificial Intelligence in the education sector is a matter of interest for social partners in education and a legitimate subject of a meaningful and effective social dialogue. Therefore education trade unions must be closely involved in the design, development, and monitoring of AI technologies introduced in the education sector; The preconditioning of online research by algorithms and the collection, storing and analysing of personal information by users of digital devices are applications of Artificial Intelligence that have far-ranging implications for the education sector and its personnel. These uses of Artificial Intelligence in the education sector, at all levels, involve a wide range of areas including employment, pedagogy, assessment, research and administration. Significantly, the potential risks from the use of Artificial Intelligence should be addressed; Artificial Intelligence in the education sector must be handled sensibly and carefully, as a high-risk factor giving rise to a need for robust and binding ethical guidelines and legislative frameworks, with teachers, trainers, academics and other education personnel at the centre of establishing these. This equally supports calls for deeper discussions on the role of digital technologies in education. Notes that: Recently, Artificial Intelligence has been the subject of several European and International policy positions and studies, including the European Commission Communication on Building Trust in Human-Centric Artificial Intelligence (April , the White Paper on Artificial Intelligence A European approach to excellence and trust (February , the OECD Recommendation on Artificial Intelligence (May and Working paper Trustworthy Artificial intelligence (AI) in education: Promises and challenges (April , the UNESCO Preliminary report on the first draft of the Recommendation on the Ethics of Artificial Intelligence (December , the Council of Europe Recommendation on developing and promoting digital citizenship education (November and the draft of the UNICEF report: Policy Guidance on AI for Children (September . In addition to this, data protection mechanisms and principles common to all European Union Member States have been laid out in the 2016 General Data Protection Regulation (GDPR); Artificial Intelligence, as a technology built by human beings with their preconceptions, is fundamentally set to replicate conscious or unconscious human bias. While the STEM and IT community remains widely unrepresentative of many categories of society, such as women, ethnic minorities, people from a migrant background and people with disabilities, there is a pressing need for transparency in algorithmic decision-making, with a view to identifying, addressing and tackling discriminatory practices in Artificial Intelligence-powered technologies; Building Artificial Intelligence for educational purposes, should be done by a multi-disciplinary team that also consists of academics and researchers from the human sciences to assure that philosophical and ethical concerns can be tackled from the beginning; Artificial Intelligence used for educational purposes can never replicate nor replace the social and emotional engagement of the teaching professional. The role of teachers, trainers, academics and other education personnel goes far beyond merely providing instruction. Their ability to engage with students according to their specific needs and individuality is a core aspect of inclusive quality education, that must be protected. The act of learning is inherently a collective process which is difficult to provide in distance education;3 ETUCECONFERENCE 2021 EI European Region Teachers, trainers, academics and other education personnel must be trained from the onset and throughout their professional career in Artificial Intelligence, its underlying risks, including from their perspective as workers, and its possible applications in the educational context. Such training should be available free of charge and developed in accordance with education professionals needs; Artificial Intelligence in the educational context is a matter of concern for education personnel, in terms of teaching and as workers. Indeed, Artificial Intelligence in the workplace gives rise to a set of issues regarding the working conditions of education personnel, for instance when it comes to privacy issues and the right to disconnect. It is of utmost importance that the use of Artificial Intelligence in education institutions does in no way hinder or lessen the protection of teaching professional s rights and equal opportunities, and their professional autonomy. Equally importantly, the conditions of Artificial Intelligence tools used in the workplace must be designed in consultation with education trade unions; The bias inherent in Artificial Intelligence s analysis and sorting of data presents concerning implications on the working conditions of education personnel, and in particular, the recruitment, assessment, and career progression of teachers, trainers, academics and other education personnel. This risk calls for absolute transparency in data collection and usage, clear accountability processes, as well as for robust protection of workers rights. Crucially, at a time when many education systems are at least partially resorting to digital education due to the COVID-19 pandemic, but also looking beyond, the implications of using Artificial Intelligence on education personnel s working conditions must be included in collective bargaining agreements; Artificial Intelligence, when introduced into the educational context, must remain a means to support the work of teachers, trainers, academics and other education personnel, in full respect for their professional agency and academic freedom. Artificial Intelligence designed to reproduce or replace education personnel jeopardises the social and emotional teaching context and damages quality education; Calls for the application of Artificial Intelligence in education often allow ed-tech companies to expand their influence in the education sector; Protecting the capacity, accountability and transparency in the governance of public education systems from the influence and reach of for-profit private commercial interests and actors is therefore of utmost importance. This includes public procurement that ensures that funds are used for the public good of education based on clear rules and legislation that affirm and require that the services contracted by the public authorities allow for social partner engagement and collective bargaining. While the race towards uncovering the potentials of Artificial Intelligence is an explicit objective for many decision-makers, Artificial Intelligence and its use in the education sector is a matter of public interest and debates around it should, as such, be fully independent of the influence of profit as a motive. Supporting its member organisations and representatives in making active use of co-determination, data protection and personal rights; Making sure that their members can acquire the basic skills needed to understand the functioning and use of Artificial Intelligence and can assess the risks. This professionalisation policy should be oriented towards the education institution s staff team so that the entire team is involved with respect for the different levels of AI-knowledge in the staff.4 ETUCECONFERENCE 2021 EI European Region To mitigate the adverse risks of Artificial Intelligence for education personnel and students alike, ETUCE and its Member Organisations commit to: Continuously expanding the knowledge of education trade unions on Artificial Intelligence applied to the education sector, and in particular when it comes to its impact on the quality and inclusiveness of education, on the safety and well-being of students and education personnel, on the personal pedagogical relationships, on democracy and participation in educational institutions and learning processes, and on the working conditions of teachers, trainers, academics and other education personnel, including in their recruitment, assessment, and career progression; Pursuing research including the transfer for teachers and students on the impact of Artificial Intelligence technologies in education, not at least as regards inclusion and diversity in education; Advocating for and seeking to contribute to the elaboration of robust legal frameworks and ethical guidelines as regards Artificial Intelligence in the education sector, as well as for strict compliance with existing regulations, such as general data protection regulations; Lobbying national governments and decision-making bodies for the inclusion of education trade unions in the development of policies regarding Artificial Intelligence in the education sector, at all levels of education; Raising awareness and fight against on the threat of privatisation and commercialisation in and of education from the influence and reach of private ed-tech companies providing Artificial Intelligence tools for educational purposes through outsourcing, public-private partnerships, or even through the promotion of reforms embedded in public education systems; Pursuing more public responsibility in the development of datafied and algorithmised teaching, learning and research processes, for example through stronger regulation of the influence of EdTech companies on education and research and through the promotion of publicly, democratically, pedagogically and scientifically accountable governance and activities; The development of software should be done in a more open way to prevent lock-ins like vendor lock-ins. In Europe the focus should be more on open source development driven by communities where public actors like research institutes can participate alongside software developers and the private sector; Continuing to advocate for respect for the professional autonomy of adequately trained teachers, trainers, academics and other education personnel regarding the impact of Artificial Intelligence tools; Developing a common policy strategy at European level to address and overcome the concerns of education trade unions on Artificial Intelligence in education, both in terms of professional issues and working conditions.",en,"The AI Regulation proposal also foresee s the establishment of a risk management system to analyse the risks associated with the use of the AI tools in education and monitor them during the entire lifecycle of the AI tool s. In th is regard, ETUCE believes that the European Commission should support the development of clear and binding measures , including ethical guidelines, to address the risks that AI tools pose concerning transparency, accountability, intellectual property right s, data privacy, cyber -safety, equality and environmental protectio n. Governance: The Proposal for Regulation seeks to establish a governance system leading to the establishment of a European Artificial Intelligence Boar d with the involvement of national authori ties to monitor the implementation of the regulation.",risk
Center for AI and Digital Policy (CAIDP) (United States),F2663310,28 July 2021,Academic/research Institution,Medium (50 to 249 employees),United States,"CAIDP Statement EU AI Regulation 28 July 2021 European Commission 1 Center for AI & Digital Policy (CAIDP) Statement on Proposed EU AI Regulation 28 July 2021 CAIDP welcomes the opportunity to provide feedback as amendments to the text of draft Artificial Intelligence Act ( Proposal ) are considered.1 This statement follows from CAIDP s Statement to the European Commission, the European Parliament, and the European Council on April 20, 2 As we wrote in our initial statement on the Proposal, this initiative may be the single most important legal framework for the digital economy to ensure the protection of fundamental rights. We also wish to express support for the work of the European Commission to seek public comment on the Proposal. We believe that meaningful public participation in the development of AI strategies is a key indicator of the health of democratic institutions. The Center for AI and Digital Policy is a global research organization. In 2020 we published Artificial Intelligence and Democratic Values, a comprehensive review of the AI policies and practices in 30 countries.3 We also created a methodology to assess AI national strategies. Our aim is to promote a world where technology promotes broad social inclusion based on fundamental rights, democratic institutions, and the rule of law. In the CAIDP 2020 report AI and Democratic Values, we stated: Countries must establish national policies for AI that implement democratic values Countries must ensure public participation in AI policymaking and also create robust mechanisms for independent oversight of AI systems Countries must guarantee fairness, accountability, and transparency in all AI systems 1 European Commission, Artificial intelligence ethical and legal requirements, Give Feedback, See also CAIDP, Public Voice, European Commission Seeks Comments on AI Regulation (posted May 31, , 2 CAIDP Statement to European Commission, European Parliament, and European Council regarding the Draft EU AI Regulation (Apr. 20, , 3 Artificial Intelligence and Democratic Values (CAIDP , CAIDP Statement EU AI Regulation 28 July 2021 European Commission 2 Countries must commit to these principles in the development, procurement, and implementation of AI systems for public services Countries must halt the use of facial recognition for mass surveillance Our assessment of the draft EU AI Regulation is favorable. The draft EU AI Regulation will promote AI policies and practices and will bring more transparency to allow for ongoing evaluation and monitoring. We also recognize the important step forward in the evolution of the internal market with the integration of fundamental rights compliance. As AI systems become more critical role for the digital economy, compliance with fundamental rights should be a necessary precondition for market participation.4 We further fully support the objective to prohibit certain AI systems. Our review of country AI practices found that the clearest distinction between AI systems in authoritarian countries and AI systems in democratic countries is the use of facial recognition for mass surveillance. Such indiscriminate ongoing surveillance is intended precisely to coerce social behavior and to control populations. This AI technique has been used against political protesters and religious minorities, and will almost certainly be more widely deployed unless a clear prohibition is adopted.5 Below are the further recommendations of the CAIDP for the deliberations of the EU AI Regulation. PROHIBITED CASES CAIDP recommends that all prohibited use cases for AI systems should apply equally to both private and public entities. If a system is a risk to the fundamental rights of an individual and is detrimental to society, then it does not matter which entity uses it and it should be prohibited in both remits. AI system that deploys subliminal techniques beyond a person s consciousness; AI system that exploits any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability: A subliminal technique is by its definition not detectable by the person impacted, informed consent is not possible, nor is it possible for an individual to prove that his/her/their behavior was materially distorted. 4 CAIDP Statement (Apr. 20, . CAIDP Statement EU AI Regulation 28 July 2021 European Commission 3 It is also not clear from the wording who makes the determination that a practice is subliminal , materially distorting , likely to cause harm. This provision should be clarified. The wording of the prohibited use case with such narrowed scope to define makes it practically impossible to effectively ban any practice or protect any individual from exploitation of vulnerabilities. Examples should be provided to help make clear which practices will be prohibited. The UN Convention on the Rights of Persons with Disabilities6 states that Persons with disabilities include those who have long-term physical, mental, intellectual or sensory impairments which in interaction with various barriers may hinder their full and effective participation in society on an equal basis with others. The UN Convention recommends ""Universal design"" - design of products, environments, programmes and services to be usable by all people, to the greatest extent possible, without the need for adaptation or specialized design. Same definition has been adopted by the CJEU in the Z case.7 CAIDP recommends that the right to accessibility should apply to everyone, as various barriers may hinder full and effective participation in society on an equal basis with others. Use of real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement The wording of the prohibited use case does not cover the use of a system by private companies in public spaces; nor does it prohibit non-real time biometric identification systems. Even when not done in real-time, the collection of data to be analyzed at a later stage and any biometric identification in public spaces can negatively impact a person s expectation of being anonymous and the fundamental rights to express one s self and freedom of association and freedom of movement. Such narrow scope of the definition of prohibited practice makes it useless in practice. We support the intent of the Article. We are concerned that the text does not fulfill the purpose of the Article. CAIDP recommends a ban on biometric recognition systems used for mass surveillance purposes. CAIDP also made this recommendations in our report Artificial Intelligence and 6 UN Convention on the Rights of Persons with Disabilities (, 7 CJEU Decision on Z Case (, CAIDP Statement EU AI Regulation 28 July 2021 European Commission 4 Democratic Values (CAIDP . This ban should not only be limited to facial recognition systems, but also new /emerging forms of biometric recognition analyses such as gait, voice, etc. If biometric identification systems are to be used by law enforcement for specific investigation purposes after due process is followed and for the specific location and target person(s) , there should also be a time limit of how long these records are retained. The regulation should clearly state that the records cannot be retained infinitely. There should also be independent oversight of the deployment, management, and termination of these AI-based surveillance techniques. Evaluation or classification of the trustworthiness of natural persons over a certain period based on their social behavior or known or predicted personal or personality characteristics, with the social score The wording of the prohibited use case does not cover the use of this system by private companies for their own commercial purposes, or as a service provided to other private companies (for example risk scoring of individuals for online behavior, employability scoring based on online behavior) or other risk scoring systems used by public entities, such as for criminal sentencing. Paragraph 17 states that AI systems evaluate or classify the trustworthiness of natural persons based on their social behavior in multiple contexts or known or predicted personal or personality characteristics. The social score obtained from such AI systems may lead to the detrimental or unfavorable treatment of natural persons or whole groups thereof in social contexts, which are unrelated to the context in which the data was originally generated or collected or to a detrimental treatment that is disproportionate or unjustified to the gravity of their social behavior. Such AI systems should be therefore prohibited. However, despite the above acknowledgement, Proposal also allows credit scoring systems and scoring of personality (in recruitment) by classifying them as high-risk systems. The explanations at the beginning of the Proposal conflict with the articles of the Proposal in this sense. It is a fact that majority of these systems incorporate behavioral and social data in their models from a wide range of data sources usually unrelated to the context of their eventual use. CAIDP recommends a ban on any score-based profiling of individuals by private companies that is not fully compliant with all legal obligations prior to deployment and for which an algorithmic impact assessment has not been conducted ex ante. HIGH RISK USE CASES Biometric identification and categorisation of natural persons: CAIDP Statement EU AI Regulation 28 July 2021 European Commission 5 AI systems categorizing individuals from biometric data into groups according to race, ethnicity, gender, political or sexual orientation, or similar provides for grounds for discrimination under Article 21 of the Charter. Such categorization assumes universal traits and pre-decided categories and transforms social constructs such as race, ethnicity, gender, political or sexual orientation into objective truths. It removes one of the most fundamental of human rights right to express one s identity. Categorization systems do not have any scientific validity as they are built on constructed concepts. CAIDP similarly recommends a ban online categorization and scoring of individuals using biometric features (facial features, voice, DNA data) by both public and private entities. Although we appreciate that the Proposal states that the classification of an AI system / use case as high risk does not make a particular AI system lawful, we are concerned that inclusion of certain use cases in high-risk AI systems, such as for criminal sentencing, will permit such AI systems even if they lack a scientific basis or have historical roots in discrimination against people. Inclusion of these systems as high risk, but without a clear prohibition, could legitimize or normalize certain improper practices. Although the Proposal acknowledges the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defense and the presumption of innocence, could be hampered, in particular, where such AI systems are not sufficiently transparent, explainable and documented , it does not require these systems to have any scientific validity or replicability in their outcomes. Law enforcement: The danger of categorization of natural person is particularly critical for AI systems used by law enforcement authorities for individual risk assessments, polygraphs and similar tools or to detect the emotional state of natural person and to predict the occurrence or reoccurrence of an actual or potential criminal offence based on profiling of natural persons, or assessing personality traits and characteristics or past criminal behaviour of natural persons or groups. Considering these use cases as high risk only and not prohibited practices will have chilling effects on democratic institutions, presumption of innocence, due process. The Proposal acknowledges that Actions by law enforcement authorities involving certain uses of AI systems are characterised by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation of a natural person s liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter . However, still gives law enforcement the ability to use systems to predict a crime. CAIDP Statement EU AI Regulation 28 July 2021 European Commission 6 The Proposal (in Paragraph acknowledges that AI systems evaluate or classify the trustworthiness of natural persons based on their social behavior in multiple contexts or known or predicted personal or personality characteristics. The social score obtained from such AI systems may lead to the detrimental or unfavorable treatment of natural persons or whole groups thereof in social contexts, which are unrelated to the context in which the data was originally generated or collected or to a detrimental treatment that is disproportionate or unjustified to the gravity of their social behavior. Such AI systems should be therefore prohibited. However, there is again conflict within the body of Proposal since it then classifies as high-risk (and hence allows the practice) when law enforcement uses AI systems for crime analytics regarding natural persons, allowing law enforcement authorities to search complex related and unrelated large data sets available in different data sources or in different data formats in order to identify unknown patterns or discover hidden relationships in the data. CAIDP recommends a ban on the use of pseudoscientific AI systems to detect emotional state of a natural person, as well as use of what is described as predictive policing. Migration, asylum and border control management: The Proposal improperly groups together border management, broadly including migration and asylum and security. This very approach seems to indicate that asylum seekers and refugees who are already in a vulnerable position and seeking protection and have rights assigned to their status are considered a threat. This approach forms part of an elaborate agenda on the embedment of AI this field, as indicated by the Commission report on Opportunities and challenges for the use of artificial intelligence in border control, migration and security published in 8 These technologies, such as iBorderCtrl, have provoked considerable and are currently under review before the Court of Justice of the European Union.9 The Proposal states that AI systems used in migration, asylum and border control management affect people who are often in particularly vulnerable position and who are dependent on the outcome of the actions of the competent public authorities. The accuracy, non-discriminatory nature and transparency of the AI 8 Deloitte, Directorate-General for Migration and Home Affairs (European Commission), Opportunities and challenges for the use of artificial intelligence in border control, Migration and security . Volume 1, Main report (May , 9 Umberto Bacchi, EU's lie-detecting virtual border guards face court scrutiny, Reuters (Feb. 5, , CAIDP Statement EU AI Regulation 28 July 2021 European Commission 7 systems used in those contexts are therefore particularly important to guarantee the respect of the fundamental rights of the affected persons, notably their rights to free movement, non-discrimination, protection of private life and personal data, international protection and good administration . However, again by classifying these AI systems as high-risk, the Proposal legitimizes polygraphs and other similar pseudoscientific tools, such as emotion-recognition, to be used on vulnerable populations. The Proposal includes AI systems intended to be used by competent public authorities to assess a risk, including a security risk, a risk of irregular immigration, or a health risk, posed by a natural person who intends to enter or has entered into the territory of a Member State . CAIDP is concerned that despite the published protections10, systems such as Digital Green Certificates could be used to discriminate against travelers from certain countries due to their country of origin and not necessarily their vaccination status. CAIDP recommends that asylum seekers and refugee rights be protected on an equal basis and that these populations not become test beds or experimentation for emerging technologies, as has been the case with border control systems requiring biometric identifiers, such as fingerprints and facial images, in the recent years. Administration of justice and democratic processes: CAIDP asks further clarification regarding AI systems intended to apply the law to a concrete set of facts. If the intention is to use any natural language processing system (NLP) to review the facts, such as the deposition of witnesses, the requirement of validity of these systems (which is not a requirement in the Proposal) becomes crucial where NLP cannot understand the nuances of language, context of descriptions or analogies. SOCIETAL RISKS The Proposal very briefly mentions larger risks to the society but leaves out the impact of AI systems on collectives. Even mentioning these, the Proposal leaves the commentary as Codes of conduct may also include voluntary commitments related, for example, to environmental sustainability, accessibility for persons with disability, stakeholders participation in the design and development of AI systems, and diversity of development teams. . 10 European Commission Eu Digital COVID Certificate (, CAIDP Statement EU AI Regulation 28 July 2021 European Commission 8 CAIDP notes further risks to the society as below and recommends measures to address these concerns in the final version of the Proposal: Environment: Require AI system providers to document impact of large AI systems (especially training systems) on the environment, emission, and waste. This documentation should be part of transparency requirements. Declaration on A Green and Digital Transformation of the EU11 requires deploying and investing more green digital technologies to achieve climate neutrality. Transparency requirements would greatly benefit the realization of twin objectives. Group discrimination: Biased recognition, scoring and categorization systems based on unscientific methods can lead to further marginalization of certain minorities (in terms of ethnic / income / educational backgrounds, as well as gender identities). Proposal focuses on individual rights but leaves out discrimination / stigmatization of groups as a whole. UN Special Rapporteur Alston detailed his concerns about specific targeting of poor and other vulnerable groups by public authorities via AI systems.12 Misinformation: Prohibition of use of AI systems to mis/disinform citizens and manipulate their political and social interactions Children s rights: Protection / exclusion of children from surveillance, recognition, and data collection systems all together. Disability rights: Requiring accessibility from all AI systems, so as not to treat people with disabilities as errors, outliers or edge cases in the development of these systems. In addition, Proposal s current wording assumes that anyone with a disability is vulnerable a framing that disrespects human dignity and the diversity of people. To repeat again our earlier comment, the UN Convention on the Rights of Persons with Disabilities13 states that Persons with disabilities include those who have long-term physical, 11 European Commission, Declaration on A Green and Digital Transformation of the EU: 12 Report of the UN Special Rapporteur on extreme poverty and human rights, Digital technology, social protection and human rights (October . 13 UN Convention on the Rights of Persons with Disabilities (, CAIDP Statement EU AI Regulation 28 July 2021 European Commission 9 mental, intellectual or sensory impairments which in interaction with various barriers may hinder their full and effective participation in society on an equal basis with others. The Convention recommends ""Universal design"" - design of products, environments, programmes and services to be usable by all people, to the greatest extent possible, without the need for adaptation or specialized design. Universal design applies to everyone. No one should experience barriers that may hinder their full and effective participation in society on an equal basis with others. DATASETS Training, validation and testing data sets shall be relevant, representative, free of errors and complete. The Proposal requires examination in view of possible biases. However, it should be noted that bias does not only manifest itself in data sets but also the model design, selection of performance metrics, and also in implementation by the operators. CAIDP recommends that the Proposal explicitly include examination of bias in model design and selection of performance metrics. It is also impossible for a dataset to be free of errors . The wording in the Proposal requires clarification as to point to the outcomes / decisions of AI systems to be free of errors, and not the dataset itself. CAIDP recommends that the wording amended to AI system decision error rates across protected categories should be transparent, made publicly available, along with a statement from Provider as to why that error rate was acceptable level for the AI system to be put into market. CONFORMITY ASSESSMENTS CAIDP recommends conformity assessments for high-risk systems be conducted by certified independent third parties who shall not have any conflict of interest. The Proposal currently allows high-risk AI systems to be risk and conformity assessed by the very people and entities who have a vested interest and investment in the sale of these products. The transparency obligations are a great step towards scrutiny. However, the self-conducted ex-ante conformity assessments are by no means an effective way to protect fundamental rights or the individuals and society from the possible harms of these systems. The Proposal states common mandatory requirements applicable to the design and development of certain AI systems before they are placed on the market that will be CAIDP Statement EU AI Regulation 28 July 2021 European Commission 10 further operationalised through harmonised technical standards. In separate documentation,14 European Commission has already made its leaning on which entities should set these standards. Two of those entities, CEN (European Committee for Standardization) and CENELEC (European Committee for Electrotechnical Standardisation)15 16 have already moved forward and established a CEN-CENELEC Joint Technical Committee 21 Artificial Intelligence . CAIDP is concerned that business interests will dominate these standard-setting organizations. and that the protection of fundamental rights; and that the voices and concerns of civil society and affected communities will not be effectively represented. CAIDP therefore recommends that these organizations publish annually reports that describe specifically steps taken to ensure broad-based participation in the development of technical standards as well as the consideration of fundamental rights . AI systems already in use by the time of Proposal coming into force CAIDP recommends that any AI system that requires safety and / or conformity assessment be included in the scope and no exemptions be provided to those AI systems already in use by the time the Proposal comes into force. Exclusion might result in certain systems to be pushed into live environments without the due diligence, proper development or risk assessments in place in an effort to beat the dates. Exclusion of AI systems might also leave many AI systems, which have impact on fundamental rights, without any oversight. TRANSPARENCY OBLIGATIONS In of Proposal states that Transparency obligations will apply for systems that (i) interact with humans, (ii) are used to detect emotions or determine association with (social) categories based on biometric data. When persons interact with an AI system or their emotions or characteristics are recognized through automated means, people must be informed of that circumstance...This allows persons to make informed choices or step back from a given situation is in effect not true or practical. In reality, this obligation puts the onus of understanding the implications of these systems on each separate individual who might or might not have the knowledge, expertise or capacity to do so. It also does not address the 14 European Commission, Rolling Plan for ICT standardization, 15 European Committee for Standardization, European Standards support the EU ambitions on Artificial Intelligence (, 16 European Committee for Standardization, CEN and CENELEC launched a new Joint TC on Artificial Intelligence (, CAIDP Statement EU AI Regulation 28 July 2021 European Commission 11 power imbalance between the implementer of the system and the individual. For example, a job candidate who is informed of an interaction with an employer is in no position to step back or refuse to interact, or for example a refugee in a camp whose access to resources is dependent on being subject to these systems. cannot step away from a given situation. Finally, even if it is transparent, this approach allows for emotion recognition and categorization of individuals using biometric data. CAIDP repeats the recommendation for a ban online categorization and scoring of individuals using biometric features (facial features, voice, DNA data) by both public and private entities. NOTABLE PROVISIONS FROM UNESCO AI ETHICS RECOMMENDATION TO BE CONSIDERED FOR EU AI ACT As the work of the European Union on AI policy has moved forward, so too has work on AI policy in international organizations. We would like to call your attention to several provisions in the recently finalized draft of UNESCO Recommendation on the Ethics of AI17 that could be incorporated in the EU Proposal. Persons may interact with AI systems throughout their lifecycle and receive assistance from them Within such interactions, persons should never be objectified, nor should their dignity be otherwise undermined, or human rights and fundamental freedoms violated or abused (Rec # All actors involved in the lifecycle of AI systems must comply with applicable international law and domestic legislation, standards and practices. They should reduce the environmental impact of AI systems. (Rec # Respect, protection and promotion of human dignity and rights is essential throughout the life cycle of AI systems. Human dignity relates to the recognition of the intrinsic and equal worth of each individual human being, regardless of race, color, descent, gender, age, language, religion, political opinion, national origin, ethnic origin, social origin, economic or social condition of birth, or disability and any other grounds (Rec # Peace, inclusiveness and justice, equity and interconnectedness should be promoted throughout the lifecycle of AI systems, in so far as the processes of the lifecycle of AI systems should not segregate, objectify or undermine freedom and autonomous decision-making as well as the safety of human beings and communities, divide and 17 UNESCO Recommendation on the Ethics of AI (, CAIDP Statement EU AI Regulation 28 July 2021 European Commission 12 turn individuals and groups against each other, or threaten the coexistence between humans, other living beings and the natural environment. (Rec # The choice to use AI systems and which AI method to use should be justified in the following ways: (a)the AI method chosen should be appropriate and proportional to achieve a given legitimate aim; (b) the AI method chosen should not infringe upon the foundational values captured in this document, in particular, its use must not violate or abuse human rights; and (c)the AI method should be appropriate to the context and should be based on rigorous scientific foundations. In scenarios where decisions are understood to have an impact that is irreversible or difficult to reverse or may involve life and death decisions, final human determination should apply. In particular, AI systems should not be used for social scoring or mass surveillance purposes (Rec # AI actors should make all reasonable efforts to minimize and avoid reinforcing or perpetuating discriminatory or biased applications and outcomes throughout the lifecycle of the AI system to ensure fairness of such systems. Effective remedy should be available against discrimination and biased algorithmic determination. (Rec # People should be fully informed when a decision is informed by or is made on the basis of AI algorithms, including when it affects their safety or human rights, and in those circumstances should have the opportunity to request explanatory information from the relevant AI actor or public sector institutions. In addition, individuals should be able to access the reasons for a decision affecting their rights and freedoms and have the option of making submissions to a designated staff member of the private sector company or public sector institution able to review and correct the decision. (Rec # Appropriate oversight, impact assessment, audit and due diligence mechanisms, including whistle-blowers protection, should be developed to ensure accountability for AI systems and their impact throughout their lifecycle. (Rec # Governments should adopt a regulatory framework that sets out a procedure, particularly for public authorities, to carry out ethical impact assessments on AI systems to predict consequences, mitigate risks, avoid harmful consequences, facilitate citizen participation and address societal challenges. The assessment should also establish appropriate oversight mechanisms, including auditability, traceability and explainability, which enable the assessment of algorithms, data and design processes, as well as include external review of AI systems. Ethical impact assessments should be transparent and open to the public, where appropriate. Such assessments should also be multidisciplinary, multi-stakeholder, multicultural, pluralistic and inclusive. The public authorities should be required to monitor the AI systems CAIDP Statement EU AI Regulation 28 July 2021 European Commission 13 implemented and/or deployed by those authorities by introducing appropriate mechanisms and tools (Rec # Member States that acquire Al systems for human rights-sensitive use cases, such as law enforcement, welfare, employment, media and information providers, health care and the independent judiciary system should provide mechanisms to monitor the social and economic impact of such systems by appropriate oversight authorities, including independent data protection authorities, sectoral oversight and public bodies responsible for oversight. (Rec # Thank you for your consideration of our views. We would welcome the opportunity to speak with you further about these recommendations. Merve Hickok Marc Rotenberg CAIDP Research Director CAIDP President",en,"CAIDP is concerned that despite the published protections10, systems such as Digital Green Certificates could be used to discriminate against travelers from certain countries due to their country of origin and not necessarily their vaccination status. Even mentioning these, the Proposal leaves the commentary as Codes of conduct may also include voluntary commitments related, for example, to environmental sustainability, accessibility for persons with disability, stakeholders participation in the design and development of AI systems, and diversity of development teams. 10 European Commission Eu Digital COVID Certificate (, CAIDP Statement EU AI Regulation 28 July 2021 European Commission 8 CAIDP notes further risks to the society as below and recommends measures to address these concerns in the final version of the Proposal: Environment: Require AI system providers to document impact of large AI systems (especially training systems) on the environment, emission, and waste. Declaration on A Green and Digital Transformation of the EU11 requires deploying and investing more green digital technologies to achieve climate neutrality. To repeat again our earlier comment, the UN Convention on the Rights of Persons with Disabilities13 states that Persons with disabilities include those who have long-term physical, 11 European Commission, Declaration on A Green and Digital Transformation of the EU: 12 Report of the UN Special Rapporteur on extreme poverty and human rights, Digital technology, social protection and human rights (October . They should reduce the environmental impact of AI systems.",risk
Eurocities (Belgium),F2663127,27 July 2021,Non-governmental organisation (NGO),Medium (50 to 249 employees),Belgium,"April 2020 | | EUROCITIES statement on AI 1 Artificial intelligence and cities The Covid -19 pandemic has disrupted our societies beyond precedent. The global health crisis has enormous economic and social repercussions. It has also highlighted the urgency of securing a people -centred digital transformation in Europe, which is essential to ensure social and economic resilience in our cities. European digital strategies must fully support the mainstreaming of digitalisation as part of rights -based public service delivery. We recognise the crucial role of AI technologies to respond to global public health emergencies. At the same time, human rights and European fundamental values must be fully protected. The next EU regulatory framework on trustworthy AI must specifically addr ess the use of AI in public health emergency prediction and management making sure AI is used without undermining ethics and fundamental rights. More generally, a rtificial intelligence is an enabler of change for local governments. AI is already transfor ming the governance of the city and society . Public administration s can increase Response to EU s white paper on AI AI needs data. A huge amount of data produced and collected in cities, crucial for local governments to improve public services and policies, is currently gathered and owned by the private sector. A single market for data needs to benefit all ecosystem pla yers, including local governments. We call for legislative action to ensure access and use of business to government (B2G) data sharing in the proposed Data Act planned for Liability and accountability in AI are key to guaranteeing people s safety. A I safety and reliability tests are burdensome; local public authorities do not have the expertise or the budget to carry them out. A central EU body or agency should develop the necessary verification and validation procedures, and guarantee security and p ublic safety. Some AI uses can be considered high -risk. More detailed and clear descriptions of the possible high -risk uses are necessary to clearly understand when a local public administration is affected. We propose establishing a task force composed by AI experts and local public administration experts to better define the different possible high -risk uses. AI could have a disruptive effect on the future of the job market and skills development in cities. ESF+ and the Youth Guarantee funding should focus more on digital literacy training, skills development and gender equality. Local governments are crucial t o fostering an ecosystem of excellence and trust in AI in Europe. The EU must work with local governments in the development of the future regulatory framework for a trustworthy AI, taking into account the principles defined by the Cities for Digital Right s coalition. People -centred Artificial Intelligence ( AI) in cities Response to EU s white paper on AI March 20 20 April 2020 | | EUROCITIES statement on AI 2 efficiency and productivity while reducing costs through automation of processes and tasks . City governments can use algorithms and machine learning applications to predict ser vice demand and anticipate urban problems , improv e decision making and the delivery of more and innovative public services . As AI is becoming more advanced and more accessible, city authorities are increasingly experimenting and piloting A I, in many cases in combination with other tools such as IoT , 5G or Big Data technology , leveraging their potential while understanding new patterns and trends . As with the first smart city projects1, cities use the results and lessons from AI experimentati on to develop action plans and strategies, often in collaboration with national governments and with the support of local and regional stakeholders. However, AI adoption in cities is a long and costly process . Good data collection, processing, management and opening is expensive and requires specific, high -level competences as well as new governance approaches. A broader uptake of AI technology locally requires intensive financial investment from the national and EU level to ensure adequate technological and skills development in city administrations . Opportunities and c hallenges for cities AI is a powerful means to fully transform Europe s cities into sustainable, inclusive and smart places for people to liv e2. AI development also helps cities to reach the sustainability goals of the Green Deal3 especially in sectors such as air quality and mobility. Local authorities use AI to leverage IoT applications and to predict the level of pollution in cities. City governments can get valuable information by analysing social media data on tourists behaviour and to adapt cultural policies and investments to needs. Cities use machine learning to predict parking space availability to then efficiently redirect drivers into a free area. Through AI chatbots, governments can communicate faster with citizens and stakeholders increasing their sense of participation . While AI adoption is increasing in cities, challenges connected to it are too. Disruption in the labour market and skills gap, safety and liability , and digital right s protection are the most important one s for cities. Disruption in the labour market and skills gap The rapid and exponential technological progress in AI will have a disruptive effect on the job market. While it is still difficult to predict to which extend AI and robotics will affect unemployment rate, polarisation of jobs, income inequalities or discr imination in the labour market, the risks seem high. AI, powered by machine learning, automates tasks that might cause displacement or even replacement of workers, polarisation of job demand between high -skilled and low -skilled jobs , and worsen the status of already fragile groups of people, such as the digitally excluded, long -term unemployed and low -skilled people. There is a strong need in cities for skills development including long -life learning programmes, training for low -skilled people and early edu cation on digital skills - also oriented to engage more young women into technology4. Safety and liability AI applications can hide bias or amplify existing bias. Machine learning, powered by data, ha s no consciousness, reflect s the human prejudices and opinions of the developers , and repeat s and perpetuate s data which, if flawed or incorrect , can lead to misinterpretations and errors. When AI is used , for example, for urban mobility (e.g. autonomous cars) or environm ental (e.g. air p ollution or water security ) purposes , data bias might lead to high er risks for people s 1 Becoming cities of the future, lessons learned from experimenting smart cities, October 2016 , 2 EUROCITIES statement Smart cities in the age of the digital revolution , March 2019, 3 EUROCITIES statement on the Green Deal (to be finalised), March 2020, 4 EUROCITIES report on equal opportunities and access to the labour market, December 2018, April 2020 | | EUROCITIES statement on AI 3 safety5. AI systems and algorithms must undergo rigorous testing to check their reliability and safety , meaning time, the right expertise and costs. Liability issues a re also a major concern for cities. If a driverless bus runs over a pedestrian, who is responsible? There are many parties involved in an AI system (data provider, designer, manufacturer, programmer, developer, user and AI system itself), liability is difficult to establish when something goes wrong and where there are many factors to be taken into consid eration. Digital rights Fundamental human rights might be put at risk in an AI age. AI systems can impair freedom of expression, privacy and data protection, equality and fairness. Local governments are actively committed to protect ing, promot ing and mon itoring citizens human rights in the digital sphere. Through the Cities for Digital Rights6 coalition , supported by EUROCITIES , over 50 cities all over the world are working to provide trustworthy and secure digital services and infrastructure s for the common good . From apps that gamify participation in local consultation s to vide o-based solution s that promote community interaction , cities are develop ing services to decrease inequalities, discrimination and help reach traditionall y excluded communities. Local governments are implementing mobility plans and actions in cooperation with local stakeholders to secure people s privacy while obtaining crucial real -time data visualisation through traffic cameras. Guiding AI principles for Europe Despite the differences in the level of AI experiments and deployment across Europe, local governments share the same values and principles when it comes to using AI. People -focused AI : People are at the centre of AI deployment in cities. AI shoul d be used to facilitate access and deliver better services to citizens - not to track, control or direct people s behaviour. AI systems should serve people , and solutions should be based on EU societal and ethical values. Collaborative intelligence for successful AI deployment in cities : AI must complement and augment human capabilities , not replace them. Data is the engine of AI : The quantity, quality and t ransparency of used data is a key success factor for AI adoption. High quality annotated open data should be more available for use by all actors . Those using the data have the responsibility to ensure its integrity, authenticity, consistency and accuracy. A description of the data on which an algorithm is trained should be published. Safety and secur ity: AI must serve and protect people; systems should be accurate and perform reliably. Security and privacy should be integrated into systems from the design phase. Accountability and transparency : As the impact of AI on people s safety can be high, strong accountability and transparency measures and mechanisms should be ensured. Oversight measures covering responsibility and liability need to be put in place. Response to EU s white paper on AI Facilitate access and use of private data AI thrives on data. Full access, share and use of data by all ecosystem actors is a prerequisite for AI development and implementation. A growing amount of data is generated every day in cities 5 EUROCITIES statement on Integrating transport automation in the urban system, February 2019, 6 Cities for Digital Rights April 2020 | | EUROCITIES statement on AI 4 by people and machines, but its use and re -use is not fully exploited7. We welcome the Commission proposal to improve access to data establi shing common European data spaces that will facilitate data be ing used and shared between different players, also cross -border, as well as data quality, interoperability and standards within and across sectors. We strongly believe in strenghtening this eff ort through initiatives that focus on semantic interoperability by shared definitions, both technical and multilingual. Ensuring GDPR compliance, we will monitor the governance process and decisions on which data will be used and in which situations. We will also monitor the standardisation of data formats, protocols and registry that could help promote exchange of data and knowledge. However, a huge amount of data is currently gathered and owned by the private sector . The use and management of th at data is crucial for public authorities to improve public services and policies including urban planning, mobility and housing w hile ensuring democratic control over data and mitigating the negative societal impact of AI. Access and use of business to government (B2G) data need s specific regulation. We therefore call for action to be taken on B2G data sharing in the proposed Data A ct planned for Accountability and transparency measures AI systems and algorithms can hide bias which might lead to high risk s for people s safety . Safety and reliability test s of AI systems are burdensome; local public authorities do not have the expertise or the budget to carry them out. A central EU body or agency should develop the necessary verification and validation procedures , and guarantee security and public safety . Liability and accountability in AI are key to guarantee ing people s safety . Tangible measures must be applied to close the accountability gap such as: - full access to the algorithm code by the competent authorities whenever needed for inspection or verification purposes - obligations to report which algorithm s are used - a framework for algorithmic auditing that supports AI system development end -to-end - fostering an open source code philosophy Scope of a future EU regulatory framework We agree with the Commission s opinion on applying the high -risk approach and to use the cumulative criteria of the sector where AI applications are employed as well as the intended use of AI within each sector. More detailed and clear description of the possible uses that are considered high-risk is essential to clearly understand when a local public administration is affected by a specific use . Considering the multitude of possible uses of AI in several sectors o f responsibility for public authorities and the complexity of the classification exercise, we recommend a task force composed of experts on AI and public administration experts , also at local level, to better define the different possible high -risk uses . Funding for skills development Local governments need support in managing the digital transition and to tackle the possible disruptive effects of emerging technologies in the labour market. City governments are key players in supporting the creation of new jobs as well as the development of skills and knowledge; they are the right level to facilitate the dialogue between all actors involved, including national and regional governments, educational institutions and the private sector. Digital skills developm ent must be enhanced in city administrations , but local governments fac e strong competition from global technological companies in terms of attracting employees with the right digital skills . We welcome the Commission s strong focus on skills development a s part of its digital strategy including the update of the EU Skills Agenda, the dedicated funding in the 7 Data, People, Cities EUROCITIES citizen data principles in action, November 2019, April 2020 | | EUROCITIES statement on AI 5 proposed Digital Europe Programme to advanced digital skills and the update of the Digital Education Action Plan to increase education quality through AI. However, cities should also be able to access and use ESF+ and the Youth Guarantee funding t hat focus more on digital literacy training and skills development to tackle disruption in the labour market and skills gap or mismatch . This should specifically include women empowerment measures to combat gender imbalance in STEM . Building an ecosystem of excellence and trust together Local governments are crucial to foster ing an ecosystem of excellence and trust in AI in Europe. With increasing urban populations and access to talent and skills, universities, companies and infrastructure s, city authorities can facilitate research and innovation activities and processes , and create the right conditions to boost AI deployment, including by SMEs. Acting as open participation and collaboration platforms, using and making data and information available, city authorities enable crowd -creation, foster experimentation, share te chnological expertise and co-develop ideas and solutions. Local governments are key to build ing confidence and trust in AI. Through experimentation and early adoption of AI, city governments identify possible safety and fundamental rights risks , and propo se trustworthy solutions. As the level closest to citizens, local authorities engage with citizens, understand their fears and concerns and develop together possible solutions. The EU must work with local governments in the development of a future regulato ry framework for a trustworthy AI that takes into account the principles defined by the Cities for Digital Rights coalition8 and that supports cities to uptake and upscale AI in Europe . 8 Cities for Digital Rights hts.org/",en,"AI development also helps cities to reach the sustainability goals of the Green Deal3 especially in sectors such as air quality and mobility. Local authorities use AI to leverage IoT applications and to predict the level of pollution in cities. air p ollution or water security ) purposes , data bias might lead to high er risks for people s 1 Becoming cities of the future, lessons learned from experimenting smart cities, October 2016 , 2 EUROCITIES statement Smart cities in the age of the digital revolution , March 2019, 3 EUROCITIES statement on the Green Deal (to be finalised), March 2020, 4 EUROCITIES report on equal opportunities and access to the labour market, December 2018, April 2020 | | EUROCITIES statement on AI 3 safety5.",risk
European Center for Not-For-Profit Law Stichting (Netherlands),F2663061,26 July 2021,Non-governmental organisation (NGO),Small (10 to 49 employees),Netherlands,"ECNL Position Statement on the EU AI Act 23 July 2021 info@ecnl.org 0031 639029805 @enablingNGOLaw European Center for Not -for-Profit Law Stichting Riviervismarkt 5, 2513 AM, the Hague, Netherlands ECNL Position Statement on the EU AI Act RISK LEVELS ................................ ................................ ................................ ................................ Prohibitions (art. ................................ ................................ ................................ ................................ High -risk systems (art. 6 - ................................ ................................ ................................ ................................ Low-risk systems ( certain AI systems ) art. 52 ................................ ................................ No-risk systems (all other systems) ................................ ................................ ................................ Governance mechanisms ................................ ................................ ................................ ................................ THE CORPORATE RESPONSIBILITY TO RESPECT HUMAN RIGHTS ................................ STAKEHOLDER ENGAGEMENT & CIVIL SOCIETY PARTICIPATION ................................ HUMA N RIGHTS IMPLICATIONS OF MARGINALIZED AND AT -RISK GROUPS ................................ BIBLIOGRAPHY ................................ ................................ ................................ ................................ Acknowledgement : This paper was drafted by ECNL s Marlena Wisniak, with support from Vanja Skoric and Francesca Fanu cci. We are grateful for and recognize the contributions of our colleagues in this field. We would especially like to thank Sarah Chander, Richard Wingfield, Imogen Parker, Lilian Edwards , Angella Mueller, and Iverna McGowan who have provided invaluable feedb ack on very short notice. We look forward to continuing our thinking and advocacy related to the EU AI Act going forward with more representatives from civil society, academia, and affected communities. This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 3 APPROACH OF THE AI ACT The European Center for Not-for-Profit Law (ECNL) strongly supports rights -based regulation of artificial intelligence (AI) systems and welcomes the European Commission s initiative to draft a proposal for an EU -wide AI Act . We are also pleased that the legislative process has provided external stakeholders, especially civil society organisations and affected groups, opportunities to contribute to the development of the AI Act. We encourage the European Commission and going forward the European Parliament to expand and strengt hen the participatory process. That said, ECNL is deeply concerned about the current approach of the AI Act. In its current state, the AI Act misses an opportunity to effectively protect the rights of persons and communities being subjected to AI systems , placing business and operational interests as well as harmonization of the internal market for AI products above people s fundamental rights. This is despite the Act s specific objective of ensuring that such AI systems respect existing law on fundamenta l rights and Union values. Indeed, while a superficial reading of the Act suggests that it aims to protect European values and fundamental rights (e.g. , a deeper analysis exposes that the underlying motives are instead technological innovation, econom ic development, national security and counter - terrorism, border control, and criminal justice. Recognising that AI systems can adversely impact and at times be incompatible with fundamental rights, the AI Act nonetheless stresses that the development of AI is necessary for sectors as diverse as environmental, health, finance, mobility, home affairs, and culture, both in the private and public sectors (para. (). This narrative suggests that the Act is promoting an uptake of AI systems. These systems can indeed have beneficial impacts, but ECNL is concerned that as the Act does not sufficiently address the severe power imbalance that exists between those who develop and deploy AI systems, and the communities that are subjected to them. This imbalance is especially acute for historically marginalized and under -represented groups , such as racialized groups, women and gender non -conforming persons, religious minorities, LGBTQIA+, disabled persons, migrants and refugees, children and the elderly, and perso ns of lower socio -economic status , among others. When considering potential opportunities that can arise from AI systems, it is therefore important to begin with a power analysis and centre the needs of the most at -risk communities. With this in mind, it i s crucial to consider the following elements: This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 4 I. Who will benefit from these systems (specifically, which demographic groups and/or sectors) and who will be harmed? II. Is the root cause of a (social, economic, political or other) issue effectively being addres sed by deploying the AI system, or are we merely offering perform ative and superficial solutions? In reality, there are no systems that only present opportunities or risks from a strictly binary perspective, but instead there are systems that provide diff erent opportunities or risks depending on the targeted population, context, and situation in which they re deployed. In its current form, the AI Act falls short of addressing this concern. The overarching approach of the regulation does not adequately protect human rights and fundamental freedoms , especially since only a few AI systems are subject to (inadequate) legal requirements, while the vast majority of AI systems are under no impact assessment nor regulation at all. At its core, the AI Act is rooted in EU free market policy, preventing market fragmentation and ensuring product safety regulation. Promoting neoliberal and industry -friendly narratives, the AI Act actually supports the acceleration of AI systems by preventing Member -States from further regulating them. As noted by scholars Veale and Zuiderveen Borgesius, the material scope of the AI Act appears to rule out the possibility that [it] is a general minimum harmonisation instrument, setting a horizo ntal regulatory floor. [1] They caution that the Act may contribute to deregulation more than it raises the regulatory bar . [1] This is especially worrisome, given that the regulation has the potential to set legal and policy standards on a global level, promoting trustworthy AI instead of rights - respecting AI and therefore focusing on perception rather than on effective rights protection. ECNL strongly supports establishing minimum transparency and human rights safeguards for all AI systems , irrespecti ve of their level of risk taking into consideration parallel requirements under the GDPR and other relevant laws (e.g. Digital Services Act, Digital Markets Act, European Convention on Human Rights, EU Charter of Fundamental Rights, etc .). Once such level is properly assessed, higher safeguards should be applied whenever the risk is high or higher, also depending on the context and area of application. Importantly, obligations should fall on both AI providers and users. This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 5 I. The scope and list of prohibited AI s ystems is too narrow and fails to include other AI systems that are incompatible with human rights . II. The European Commission s proposal for the AI Act proposes very limited oversight and safeguards to AI systems determined as high risk , while merely establishing weak transparency requirements for certain AI systems. As a result, all AI systems that do not fall under the high risk category remain effectively unregulated . Yet, the level of risk often depends on the application of the AI syste m, and differs greatly for specific demographic and geographic groups subjected to the technology as well as depending on the context of its application. Finally, it unduly puts the burden of proof on the person subjected to the AI system, as opposed to th ose developing or deploying it. This would create an uneven playing field and incentivize classifying AI systems as low risk to avoid further regulation. It also runs against an established principle of EU non - discrimination law that the victim is not expe cted to prove the wrong (see for e.g. EU Directive for equal treatment in employment and occupation [2]). There are compelling arguments that a thorough, inclusive and transparent human rights impact assessment (HRIA) must be the starting point for all subsequent regulatory actions of any AI system , which is in line with the EU s proclaimed risk -based approach. In addition to HRIAs, ECNL encourages taking a sector -specific approach when it comes to further regulatory requirements, with an emphasis on potential users and developers. This was reflected in the recent online consultation conducted by the Council of Europe Ad - Hoc Committee on Artificial Intelligence (CAHAI). The vast majo rity of different stakeholders responded with overwhelming support for regulating all AI systems, irrespective of their risk level, and a strong preference for a human rights -based approach, with HRIAs being the preferred choice of governance mechanism. [3] ECNL is highly concerned that the obligations generally pertain to AI providers only , failing to consider those pertaining to AI users . ECNL urges policymakers to develop parallel obligations on users of AI systems, given that they are best positioned to understand the context in which the systems are deployed, and importantly, the impacts that the use will have on affected communities. ECNL strongly recommends that AI users conduct human rights due diligence, including human rights impact assessments, before deploying the AI system s, and continuously thereafter. Imp ortantly, this should be done in close consultation with affected groups, especially marginalized and at - risk ones. This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 6 Similarly, the rights of redress under the AI Act generally pertain to AI providers only. The rights of affected communities and persons su bjected to AI systems are not protected, and these stakeholders have no access to remedy in case of harm. This is exemplified by the fact that transparency requirements regulate the relationships between AI providers and users, as opposed to any direct res ponsibility towards people subjected to the systems or affected by them (article 13 ). ECNL strongly recommends that an effective right to redress for affected groups be added to the AI Act, with meaningful support (including adequate resources) to stakeholders so that they can fully exercise this right. Similarly, requirements apply to providers only, and not to the deployers or users who merely need to get instructions from providers (article . RISK LEVELS Prohibitions (art. The EU AI Act includes a few important and welcome prohibitions of AI practices whose use is considered unacceptable, since they contravene Union values and violates fundamental rights. However, the objective of the prohibitions is marred by their narrow scope and b road exceptions and derogations . ECNL recommends ( expanding the list of prohibited AI practices in line with the European Data Protection Board and the European Data Protection Supervisor s demands; ( remov ing the condition to prove physical or psychological harm ; and ( narrow ing down the scope of exceptions. I. The definition of remote biometric identification in publicly accessible spaces is overly narrow and the standards are difficult to meet. Article 5(1 )d) proposes a prohibition for a few specific uses of biometric technologies when deployed by law enforcement. One notable example of a prohibited application, which we fully support, is using real -time facial recognition against people protesting. Unfortu nately, the prohibition only applies to real -time uses, and does not cover other harmful use cases due to post - remote biometric identification. This leaves out other dangerous systems that should be prohibited but are instead allowed to be placed on t he market. In alignment with many other civil society organisations, [4] the European Data Protection Board and the European Data Protection Supervisor, ECNL calls for the This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 7 following prohibitions: ( a general prohibition on any use of AI for automated recognition of human features in publicly accessible spaces, such as recog nition of faces, gait, fingerprints, DNA, voice, keystrokes and other biometric or behavioural signals, in any context; [(] a prohibition on AI systems using biometrics to categorise individuals into clusters based on ethnicity, gender, political or sexu al orientation among others ; ( a prohibition of emotion recognition systems and the use of AI for social scoring [5] and ( a prohibition of risk assessment tools for criminal justice and asylum. II. Furthermore, the threshold that some AI systems or applications must meet to fall under the scope of the prohibition makes it difficult, if not practically impossible, to achieve. Specifically, the requirement of individual physical or psychological harm is very difficult to prove in the case of AI systems, where collective and indirect harms are ubiquitous. What s more, the definition of public spaces includes spaces (both private and public) that are accessible to the public, excluding online spaces (articl es 2, 3, . ECNL is concerned about the risks for activists and civil society organisations operating in digital spaces, and the human rights implications of using AI -driven biometrics systems to identify people whose faces appear on the internet. III. The broad exceptions to the prohibition of remote biometric identification at best fail to capture all the risks of these technologies, and at worst legitimise their development and use (articles 1d, 2, . ECNL agrees with the European Commission that exceptions should be exhaustively listed and narrowly defined situations and for substantial public interest , but believes that the current proposal falls short of such goals. The AI Act authorises the use of facial recognition systems for the searc h of missing children, to prevent terrorism or to predict crime under a few conditions (notably, the need to get a judicial authorization, which we support.) Such an approach is highly dangerous. The exception for missing children perpetuates a techno - solutionist approach in the absence of any publicly available information that facial recognition is indeed effective in this case. Even more worrisome is the fact that r emote biometric identification , as authorized under the AI Act s exceptions , generally aims to identify, surveil, and possibly detain individuals under the justification of security. R acialized persons , political dissidents, and activists are often disproportionately targeted and are at risk of great physical harm, from arbitrary detention to potentially torture or This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 8 death . At the same, by requiring that groups such as migrants or refugees consent to AI -driven biometric systems for processing their request or accessing essential services , they risk excluding people from accessing lif e- saving opportunities in the context of justice or border control , should they not consent. IV. Importantly, these exceptions risk exacerbating existing racial and social inequity. AI-driven surveillance technologies in the hands of powerful actors such as judicial bodies or law enforcement officials have the potential to do great harm, with minorities and racialized groups, human rights defenders, activists and journalists bearing the most significant risk. These risks are heightened by the fact that the systems can only be deployed in specific areas, for example where there is an indication of threat of presence of an alleged perpetrator. While this limitation is good at first sight, it will likely lead to deploying biometric identification systems in are as that already over -policed, and where the residents are predominantly poor, migrants, and persons of colour. Other exceptions include derogations for international and bilateral agreements (article , for example in the context of national security or c ounter - terrorism. Ultimately, these exceptions enable the uptake of these systems without any publicly available evidence that they re effective to combat crime or terrorism, at the expense of fundamental rights and open civic space. High -risk systems (a rt. 6- Grounded in product safety legislation, the AI Act classifies AI systems on the basis of their intended purpose (article . While ECNL agrees that this is an important criterion, it is only one of many relevant factors. Importantly, it excludes unintended (or hidden) purposes, such as collateral harm and misuse or abuse. In practice, it will be difficult to determine the true and underlying purpose of an AI system, given the incentives of providers to frame their intent i n a way that limits risk. I. We believe that criteria for determining the risk level should include, at a minimum, those related to product design (including intent); severity of impact; due diligence mechanisms; causal link; and potential for remedy. [6] The context in which AI systems are deployed is also critical, as areas such as law enforcement, This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 9 migration and border control, and access to justice should de -facto be considered high -risk. II. We are deeply concerned that there is currently no provision nor clearly identified procedure allowing for adding new categories to annex III related to the list of high -risk uses of AI systems. This should be amended to permit future additions and follow a thorough human rights impact assessment, where affected communities and c ivil society are included in the revision process. III. We generally support the added requirements for AI providers of high -risk system s, although we do not think that they re robust enough to effectively protect human rights. In any case, we urge the European Parliament to expand these requirements to all AI systems proportionate to their risk level . We welcome the establishment of public registers (article and recommend expanding them to include information about who is deploying them and for what purpose , in line with other civil society organisations. [7] We also would like to se e support for civil society organisations and affected groups to access and understand these databases. IV. Regarding the requirements for high data quality and other testing, validation, and accuracy standards (inter alia articles 10 and , we are disappo inted that these apply to high -risk systems only. Moreover, ECNL recommends to include information about who will determine the level of acceptable accuracy, robustness and cybersecurity, who determines evaluation metrics, and what role historically at -risk, marginalized and affected communities will play in establishing these norms . We are also concerned that these standards will promote a narrative that de-biasing or improving the quality of data prevent risks of discrimination, when ultimately the proble m lies in how the use of these systems can exacerbate existing discrimination and inequality. V. Placement of high -risk systems on the market is subject to conformity assessments. Given that these are self -assessments, there is a high risk of conflict of int erest and incitement to lower standards or to easily approve the process. To ensure public interest, ECNL recommends that an external audit of certification be required as conforming with the goals of the regulation, instead of the developers of the AI system themselves. Such conformity assessments should be conducted on an ongoing basis anytime that they are deployed in a This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 10 new high -risk (geographic, social or political) c ontext or application, in addition to when systems are substantially modified as required by article VI. ECNL strongly recommends removing the derogation from the conformity procedure for exceptional reasons of public security (article . This derogation is especially alarming, as t he use of technology for counter -terrorism or national security purposes is already notoriously opaque and under -regulated, and the risks of human rights abuses and adverse impacts are elevated in such contexts. If anything , we recommend conducting enhanced assessments (as consistent with the United Nations Guiding Principles for Business and Human Rights ), but certainly not a derogation thereof. VII. Conformity assessments do not follow a rights -based and community -driven methodology. Human rights impact assessments (HRIAs) do, which is why ECNL believes they are the type of assessment that best prevents adverse impacts on fundamental rights. Following strict and high standards outlined in the United Nations Guiding Principles for Business and Human Rights (UNGPs), HRIAs are based on stakeholder engagement and transparency principles. Accordingly, the conformity assessment should be based on input from affected communities and stakeholder gr oups, including civil society. This should begin and where necessary end with the questions: (i) is the purpose of the technology a legitimate one?; (ii) if so, is the technology effective in achieving that purpose?; and (iii) even if it is effective , is it proportionate, i.e., is there no other less intrusive way to achieve the same result? The results of the conformity assessment should be made available and accessible. Importantly, users who deploy AI systems should also be subject to assessment requirements, given the wide scope of possible applications of AI systems and corresponding human rights risks. HRIAs are best suited for this purpose, where understanding impacts within specific contexts is critical. VIII. The AI Act has limited notification requirements to national authorities for serious incidents or malfunctioning of high -risk systems (article . Affected individuals or groups have the right to information under international human rights law, but the present draft law inhibits their ability to realise this right , as it does not allow for them to contact national authorities to report any incidents that impact them. This leads to an over -reliance on the good faith of AI providers, who have a strong conflict of interest and are This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 11 incentivized to under -report. ECNL recomm ends includ ing an obligation to make these reports publicly available. IX. Most regrettably, articles 16 to 29, which outline obligations for high -risk systems to AI providers and users , do not include any requirements to consult with or notify civil society organisations and affected communities. ECNL strongly supports requiring stakeholder engagement and notification to external stakeholders in corrective actions processes (article , duty of information (article , and obligations of importers and distributors (article 26 and , among others. ECNL believes that users, who deploy the AI systems in specific contexts and applications, have a particular duty to consult with affected groups given the signi ficant implications that these systems can have on their rights. Low-risk systems ( certain AI systems ) art. 52 AI systems considered as low -risk are barely regulated in the AI Act, which merely imposes minimal transparency obligations for providers of a few technologies. I. As mentioned above when discussing prohibitions and high -risk systems, some systems listed as low -risk are shockingly misclassified. Emotion recognition technology; biometric categorisation for the purpose of predicting ethnicity, gender, political or sexual orientation; and risk assessments for criminal justice and asylum should be prohibited entirely. The use of bots (at least in some contexts) and deepfakes should be considered high -risk (article 52( and 52(). II. These transpa rency measures are even more inadequate given the broad exceptions in the AI Act that further reduce their effectiveness. This is particularly worrisome in the context of criminal justice, where there is already lots of opacity and discrimination of racial ized persons and religious minorities. High -risk contexts and applications such as predictive policing and sentencing should demand more transparency, not less . Moreover, given their severe human rights impacts (e.g. right to life, liberty, and security), they should be prohibited or at the very least considered high risk. This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 12 No-risk systems (all other systems) The vast majority of AI systems are left unregulated in the AI Act . This is even more problematic given that the AI Act precludes governments from further regulating them at the national level (see above). I. AI providers are merely encouraged to adopt voluntary codes of conduct ( art. , which have long been criticized as ineffective. They are at best performative, and at worst legitimise an uptake of AI under the promise of good conduct . In any case, AI providers should instead adopt human rights policies and implement the UNGPs (see below section on corporate account ability). AI providers are merely encouraged to voluntarily apply the mandatory requirements for high -risk AI systems , which is highly insufficient to protect fundamental rights. As mentioned throughout this document, minimum legal requirements should apply to all AI systems, irrespective of their risk level , and enhanced obligations should be proportionate to their risk level. II. AI providers are finally encouraged to apply on a voluntary basis additional requirements related to, for example ( ) stakeholders participation in the design and development of AI systems ( art. 2 ). Meaningful stakeholder participation, inc luding external stakeholders such as civil society organisations, should be mandatory in the context of human rights due diligence by AI providers and users. Governance mechanisms Overall, the governance mechanisms outlined in the AI Act are inadequate to effectively prevent and remedy harm. Resources are for their part not well allocated. I. The European Data Protection Board, which will be responsible for supervising the Union bodies that fall withing the scope of the AI Act, would also be the most effective authority to oversee enforcement of the AI Act. Yet this competence is assigned to a newly established European Artificial Intelligence Board (the Board ), composed of representatives of Members This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 13 States and the Commission (art . 56 and . ECNL recommends expanding membership eligibility to include human rights experts, researchers and affected communities to ensure multi -stakeholder representation. Resources should also be allocated to these stakeholders to ensure their meaningful participation. II. Regarding ex -post enforcement, ECNL welcomes the fact that public authorities should have the powers and resources to intervene in case AI systems generate unexpected risks, which warrant rapid action (para. (6 ); art . 65-. This includes withdrawing a product when there s a risk, despite it being in compliance with the regulation. ECNL sees this as good example of the principle of precaution and hopes that adequate implementation will follow. We also support putting the bur den of proof of demonstrating that a product is no longer a risk on AI providers. In any case, ECNL encourage s expanding this provision to explicitly include the possibility of affected stakeholders and civil society organisations to sound the alarm on adv erse impacts and call for the removal of a system from the market. III. Relatedly, requirements around withdrawing a product and notifying authorities at the national level seem to ignore the need to inform the general public. While it s appropriate to not pu blicly share the details of an ongoing investigation, there is a strong public interest in communicating that a particular AI system is being investigated and/or has been temporarily suspended, and when an AI system has been removed from the market. This is also a necessary element to enable future access to remedy. While the AI Act does not grant a direct right to remedy, affected individuals and groups still need to be informed to seek remedy on other legal grounds. Yet too often, adversely impacted indiv iduals and groups are not even aware of a violation to begin with. Civil society organisations and academics would also benefit from such knowledge for their research, organising and advocacy efforts. This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 14 THE CORPORATE RESPONSIBILITY TO RESPECT HUMAN RIGHTS AI providers, the vast majority of which are private sector actors, are tasked with carrying out most of the requirements in the AI Act, yet there is no mention of companies responsibility to respect human rights in their activities and supply chains. I. ECNL is pleased that the AI act applies to multinational enterprises that are located in a third country where the output of the system is used in the Union (article ) We would recommend extending this application to companies value chains, which at th e moment are merely required to cooperate with providers and users in complying with the AI Act (para. ( , art. 24 -. II. Overall, the AI Act fails to take a human -rights based approach to corporate responsibility, as consistent with the UNGPs, recent cou ntry -level laws on mandatory human rights due diligence, and the European Parliament s legislative initiative on mandatory human rights due diligence for supply chains. [8] The AI Act misses an important opportunity to require (or at the very least recommend) AI providers to have in place human rights polic ies, conduct human rights due diligence, and establish operational grievance mechanisms, in close consultation with affected stakeholders. III. ECNL is alarmed about the disproportionate role that standardization bodies like CEN and CENELEC have, and their power t o adopt standards related to the AI Act. Given that AI providers will de facto follow these standards when conducting conformity assessments, external stakeholders including civil society, academics and affected communities should participate in the development of standards . As Veale and Zuiderveen Borgesius have pointed out, [n]otified bodies [article 33] checking a provider s self -assessment may play a small role, but there are few situations where they are required. [1] As a result, AI providers will essentially operate unchecked. We refer to our section above on conformity assessments on further analysis on how these assessme nts fall short of preventing adverse human rights, and why we advocate for HRIAs instead. This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 15 STAKEHOLDER ENGAGEMENT & CIVIL SOCIETY PARTICIPATION The AI Act misses an important opportunity to enable civic participation and require meaningful stakeholder en gagement, especially of at -risk and marginalized groups. Ultimately, the Act lacks a bottom -up force to hold regulators to account for weak enforcement. [1] I. Affected individuals and communities do not have standing to claim redress under the AI Act, which ECNL views as a major shortcoming of the regulation. Only those with obligations under the AI Act can challenge regulators' decisions. In other words, the AI Act creates no legal right to sue a provider or user for failures to comply with the obligations therein. As the European Digital Rights Initiative (EDRi) rightfully warns, this has a duel effect of, first, str ipping individuals whose fundamental rights have been impacted from their right to seek remedy, and second, increasing the power of AI providers (which are generally private companies) to shape rules for how public authorities should use AI systems. [7] The only right to contest decisions that is granted to parties having a legitimate interest in that decision is an appeal against decisions of notified bodies (article . As mentioned above, however, notified bodies have a very limited rule in overall compliance and oversight . ECNL also recommends adding an explicit right of civil society organisations and external stakeholders to appeal these decisions and consider them as having a legitimate interest. II. Stakeholder engagement obligations related to operationalising the requirement in the AI Act are strongly inadequate, and mostly absent altogether. Meaningful engagement with a wide range of stakeholders , including unions and worker representatives, is needed on an ongoing basis and during re -assessments of conformity (art. 4 ). As mentioned above, notified bodies (article and the market surveillance authority who verify the conformity assessment in limited cases have only minimal importance and responsibility, in practice, given the power given to standardisation bodies. [1] AI system providers can generally choose the notified body, which raises important conflict of interest questions (article . This is especially problematic given t hat European Standardisation This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 16 Organisations do not have a strong track -record on stakeholder engagement . ECNL urges these organisations to reform their processes to include representatives of civil society and incorporate feedback from at - risk and marginali sed groups. Unfortunately, the AI Act makes no mention whatsoever about the need to include affected communities in neither the verification, nor the standardisation process. As noted by Veale and Zuiderveen Borgesius, It is unclear whether limited existi ng efforts to include stakeholder representation will enable the deep and meaningful engagement needed from affected communities. The vast majority will have absolutely no experience of standardisation, and may lack EU -level representation. [1] More generally, there needs to be deeper discussion about who, practically, will be part of notification and sta ndardization bodies, and what (if any) role will civil society play. Will they have sufficient human rights expertise, and will they include the lived experiences of affected communities and marginalized and vulnerable groups? All information related to th e bodies, and their assessments, should be made publicly available and accessible. Moreover, users should consult with the market surveillance authority and affected groups before deploying the AI system. III. The AI Board has the capacity to invite external experts (article 57(). While ECNL encourages the inclusion of experts, we are concerned that, unless specific efforts are made to include affected communities, experts will mostly be white, cis male and representatives of industry and academia. Affecte d communities, especially historically marginalized and vulnerable groups, have lived experienced and understanding of risks on the ground, thus providing valuable input and often neglected expertise. Similarly, civil society organisations are well positio ned to inform on risks to fundamental rights and should be referred to as experts. In this respect, sufficient resources should be dedicated to supporting meaningful participation of civil society . The AI Act foresees the need to provide financial and huma n resources to national competent authorities (article 59(). We recommend adding a similar provision focusing on affected groups, especially marginalized ones, and civil society organisations . We also recommend extending the requirement to dedicate adequate resources to all areas where stakeholder engagement is needed, including when setting standards , conducting risk assessments , and enfor cing the regulation. This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 17 HUMAN RIGHTS IMPLICATIONS OF MARGINALIZED AND AT -RISK GROUPS AI systems disproportionately impact already marginalized and at -risk groups, further exacerbating existing inequality. The section below outlines a few important shortcomings of the AI Act from the perspective of marginalize d groups. Importantly, any analysis should be intersectional at its heart, i.e. acknowledging that persons with intersecting forms of identity face elevated (often unique) harms . This reinforces the importance of including and centring affected communi ties, as they are the best positioned to inform on risks and harms of AI systems in their communities. I. To ensure that women, trans people , and gender non -binary persons are protected against harmful consequences of AI systems, the AI Act should include t he human rights impacts on gender beyond sex only and consider gender as non -binary. Overall, the AI Act makes no mention of the specific risks of AI systems t o trans people and gender non -binary persons. What s more, the AI Act does not address the risk of harm to LGBTQIA+ communities, and loses an opportunity to ban dangerous technology such as automated recognition of gender and sexual orientation. [9] II. The use of some AI systems such as polygraphs, emotion recognition technology, and risk assessment tools for the purpose of migration, asylum or border control management by public authorities are considered as high - risk ( annex III s. 7 ). However, these systems are widely inaccurate and effective, relying on racist and pseudoscientific technology, and should be prohibited . They are often incompatible wit h or have severe human rights impacts on migrants, refugees and asylum seekers rights, such as their right to life, the prohibition of arbitrary detention, right to movement, and non - discrimination, among others. III. The use of some AI systems such as polygr aphs, emotion recognition technology, risk assessment tools and predictive police for the purpose of law enforcement and criminal justice are considered as high -risk ( annex III s. . However, these systems are widely inaccurate and effective, relying on racist and pseudoscientific technology, and should be banned. The AI Act This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 18 acknowledges the risk of potential biases, errors, and opacity in these systems as they can harm racialised people . However, it fails to consider how these systems will exacerbate st ructural racial inequality regardless of the capacity to de -bias systems. As cautioned by EDRi, [b]y relying on technical checks for bias as a response to discrimination, the proposal risks reinforcing a harmful suggestion that removing bias from such s ystems is even possible. [7] IV. The AI Act does not give proper attention to accessibility for persons with disabilities . It does not analyse the specific risks that persons with disabilities face when AI systems are deployed in different contexts. Conversely, it does not require accessibility to technologies necessary for realising their human rights, only having voluntary recommendation for non -high -risk AI providers.* V. The use of AI systems for social services risks exacerbating existing social and economic inequality, disproportionately harming persons of lower socio - economic status . Examples of applications include det ermining whether and how benefits and services (e.g. welfare, education, healthcare, etc.) should be allocated. The AI Act does not sufficiently address the risks to people s right to social protection (and other economic and social rights) and non - discrim ination, failing to consider the intersection between poverty, race and gender ( despite acknowledging the problem at para. ( and (). The AI Act rightfully recognises how the use AI systems in the workplace can harm already vulnerable workers by perp etuat[ing] historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation (para. (). It should go further by prohibiting the use of s ome systems (e.g. emotion recognition in the workplace) and calling out the risks and harm related to worker surveillance, especially given the power differentials that exist between employers and workers. * Question and comment made by Mher Hakobyan from the European Disability Forum during the ECNL workshop AI regulation in Europe Opportunities for civil society to engage in policymaking on June 30, This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 19 BIBLIOGRAPHY [1] M. Veale and F. Z. Borgesius, Demystifying the Draft EU Artificial Intelligence Act. SocArXiv, Jul. 05, doi: 31235/osf.io/38p5f [2] Council Directive 2000/78/EC of 27 November 2000 establishing a general framework for equal treatment in employment and occupation , vol. OJ L. Accessed: Jul. 23, [Online]. Available: [3] Consultation on the elements of a legal framework on AI, Artificial Intelligence . -intelligence/cahai -multi - stakeholder -consultation (accessed Jul. 19, . [4] Ban Biometric Surveillance, Access Now . - biometric -surveillance/ (accessed Jul. 13, . [5] EDPB & EDPS call for ban on use of AI for automated recognition of human features in publicly accessible spaces, and some other uses of AI that can lead to unfair discrimination | European Data Protection Board. -edps -call-ban -use-ai-automated - recognition -human -features -publicly -accessible_en (accessed Jul. 13, . [6] Marlena Wisniak, European Center for Not -for-Profit Law, Evaluating the risk of AI systems to human r ights from a tier -based approach | ECNL, Mar. 23, -risk -ai-systems -human -rights -tier- based -approach (accessed Jul. 13, . [7] EU s AI law needs major changes to prevent discrimination and mass surveillance, Euro pean Digital Rights (EDRi) . -work/eus -ai- law-needs -major -changes -to-prevent -discrimination -and -mass -surveillance/ (accessed Jul. 13, . [8] MEPs: Companies must no longer cause harm to people and planet with impunity | News | Europ ean Parliament, Oct. 03, -room/20210304IPR99216/meps - companies -must -no-longer -cause -harm -to-people -and -planet -with -impunity (accessed Jul. 14, . [9] Access Now, Reclaim Your Face, Ban automated recog nition of gender and sexual orientation, Apr. 21, -AGSR (accessed Jul. 19, . This work is licensed under a Creative Commons Attribution -ShareAlike 0 International License . 20 European Center for Not -for-Profit Law Stichting 5 Riviervismarkt, 2513 AM, The Hague, Netherlands twitter.com/enablingNGOlaw",en,"Recognising that AI systems can adversely impact and at times be incompatible with fundamental rights, the AI Act nonetheless stresses that the development of AI is necessary for sectors as diverse as environmental, health, finance, mobility, home affairs, and culture, both in the private and public sectors (para.",risk
European Association of Co-operative Banks (Belgium),F2662901,23 July 2021,Business association,Small (10 to 49 employees),Belgium,"EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability , Proximity, Governance The voice of 800 local and retail banks, 84 million members, 209 million customers in EU EACB AISBL Secretariat Rue de l Industrie 26 -38 B-1040 Brussels Tel: (+32 230 11 24 Fax (+32 230 06 49 Enterprise 149 lobbying register -19 e-mail : secretariat@eacb.coop Brussels , 23 July 2021 CDO Position paper on a proposal for a Regulation laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) The European Association of Co -operative Banks (EACB ) represents, promotes and defends the common interests of its 27 member institutions and of cooperative banks, with regard to banking as well as to co -operative legislation. Founded in 1970, today the EA CB is a leading professional lobbying association in the European banking industry. Co -operative banks play a major role in the financial and economic system. They contribute widely to stability thanks to their anti -cyclical behaviour, they are driver of l ocal and social growth with 800 locally operating banks and 5 1,500 outlets, they serve 209 million customers, mainly consumers, SMEs and communities. Europe s co-operative banks represent 84 million members and 71 3,000 employees and have an average marke t share in Europe of about 20%. For further details, please visit EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 2 Introduction The European Association of Co -operative Banks (EACB) is happy to contribute to the discussion on the Artificial Intelligence (AI) legislative proposal. The EACB recognises that the AI proposal is the Commission s first ever legal framework on the matter, which addresses the risks of AI and aims to position Europe to play a leading role globally. It should be recogni sed that this is a risky bet. If European values were not u ltimately adopted on an international scale, European companies would be at a disadvantage compared to non- European players active in less restrictive regulatory environments . We believe that the European Commission, the European Parliament and the Counci l should remain vigilant to ensure that European players are not unduly constrained in their prospect of developing innovative AI solutions compared to international competitors . We would like to highlight the following points: The EACB welcome s the Commission s risk -based approach as basis for a proportionate legal text. The Commission suggests a risk -pyramid approach : the higher the risk (for users) using AI system, the more additional measures. We appreciate the technology -neutral and future -proof definition of AI, recognising that AI is a fast evolving family of technologies that is constantly developing . Nevertheless, combining the definition of artificial intelligence system together with the techniques and approaches of Annex I of the proposal, we observe that the scope of the Regulation is becoming quite wide as it also includes rule -based approaches. We believe it is of paramount importance to make sure that the AI proposal will not add new and burdensome requirements for the banking sector and create conflicts and overlaps with existing rules: e.g., sector -specific regulation (CRD, CRR). We particularly value the Commission s human -centric perspective in designing AI rules : o The responsibility for an action or a decision still lies with a human bein g; o Actions and decisions of an AI system have to be traceable and understandable by humans using it ; and o Actions and decisions of an AI system can always be changed/corrected by a human being (human oversight ). We understand that the Regulation s intention is to protect the safety and fundamental rights of EU citizens, thus that the requirements for high -risk AI systems are only targeted at AI applications that could possibly pose risks to natural persons. Generally, some provisions of the Regulation contain somewhat vague wording, e.g. , the definition s provided for remote biometric identification system and user . Moreover, we believe that the definition of developer and end user are missing from the legal text. These points should be further clarified in order to guarantee legal certainty for providers , developers and users of AI systems. EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 3 Regulatory sandboxes are useful for the development of AI . However, their objectives and entry criteria should be clear and made public in order to ensure a high degree of transparency and a level playing field in the entry process . According to the Digital Finance Strategy1, the Commission has planned to invite the European Supervisory Authorities (ESAs) and the European Central Bank ( ECB) to explore the possibility of developing regulatory and supervisory guidance on the use of AI applications in finance. We wonder if this action is still needed as the AI proposal designates the authorities responsible for the supervision and enforcement of financial services legislation as competent authorities for the purpose of supervising implementation. Processes and methods are alread y known and in place . We wonder how AI systems can be prevented from being biased. This requirement is not realistic as it cannot be guaranteed that datasets will be fully correct or complete . We believe that errors should be minimised and that the training, validation and testing of the AI sy stem should be as complete as possible. To ensure a fair treatment an ex -post revision should be made possible. Finally, we wonder who decides what data is of good quality. An external institution cannot decide th is because it doesn t understand the use o f the AI system as much as the company that develops it. And if the company makes this decision, a conflict of interests comes into play. EACB s specific comments AI and CRD frameworks The EACB is pleased to see recognised in Recital 80 that EU legislation on financial services already includes internal governance and risk management rules and requirements which are applicable to regulated financial institutions in the course of provisio n of those services, including when they make use of AI systems. Ensuring coherent application and enforcement of the obligations under the new A I Regulation and relevant rules and requirements of the Union financial services legislatio n is of paramount importance. Recital 80 s tates that authorities responsible for the supervision and enforcement of financial services legislation should be designat ed as competent of supervising the implementation of the AI act. It also mentions that in order to further enhance the consistency between the proposed AI Regulation and the rules applicable to credit institutions regulated under Directive 2013/36/EU (CRD), it is appropriate to integrate the conformity assessment procedure and some of the providers procedural obligations in relation to risk management, post marketing monitoring and documentation into the existing obligations and procedures under the CRD. In order to avoid 1 Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions on a Digital Finance Strategy for the EU, COM/2020/591 final . EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 4 overlaps, limited derogations should also be envisaged in relation t o the quality management system of providers and the monitoring obligation placed on users of high -risk AI systems to the extent that these apply to credit institutions regulated by the CRD . The content of Recital 80 is reflected throughout the following articles of the AI proposal : o Art. 9 on risk management procedures; Art. 17 on quality management system; Art. 18 on the obligation to draw up technical documentation ; Art. 20 on automatically generated logs; Art. 29 on obligations of users of high-risk AI systems . All referring to Art. 74 of the CRD on Internal governance and recovery and resolution plans . o Articles 19, 43 on ex-ante conformity assessment , and Articles 61 and 63 on post -market monitoring and market surveillance authorities , which refer to Articles 97 to 101 of the CRD. While we appreciate th e reference throughout the AI proposal to existing CRD provisions applicable to credit institutions in order to prevent overlapping requirements by the different regulations , we believe that further clarity is needed as regards the interactions of the two frameworks (e.g., regarding the ex -ante conformity assessments foreseen as part of the Supervisory Review and E valuation Process (SREP) ). We also note that the proposed Regulation lacks respective references to CRR provisions applicable to credit institutions using rating systems in the context of creditworthiness assessments and credit scoring. CRR provisions on creditworthiness assessments and credit scoring According to the currently proposed classification of high -risk AI systems, AI systems used to evaluate the creditworthiness of natural persons and to establish their credit score, would fall under the remit of this Regulation and would need to fulfil the requirements set out for high -risk AI systems. As regards the use of such AI systems by credit institutions regulated under Regulation 575/2013 (CRR)2, we see significant overlaps and even con flicting provisions between these two frameworks. The CRR already foresees comprehensive requirements for the implementation of rating systems, including on the integrity of the assignment process (Art. , the use of models (Art. , the documentatio n of the rating system (Art. and data maintenance (Art. , as well as on governance aspects (Art. . The proposed rules under the AI Regulation do not consider these existing requirements and would therefore create significant overlaps. Additio nally, internal models used by credit institutions already need to be approved by the competent authorities. This would overlap with the newly introduced requirement of obtaining an ex-ante conformity assessment for AI systems used as part of the creditwor thiness assessment or credit scoring foreseen under the SREP. 2 Regulation (EU) No 575/2013 of the European Parliament and of the Council of 26 June 2013 on prudential requirements for credit institutions and investment firms and amending Regulation (EU) No 648/2012 . EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 5 We believe that overlapping or conflicting regulation needs to be avoided and sector -specific legislation such as the CRR should prevail. Where the prudential framework already outlines require ments for banks' AI risk systems/applications those are the ones that should apply. Conformity assessment, Art. 19 ( and Art. 43 ( As pointed out in our comment above, the EACB believes that the use of AI systems for creditworthiness assessments and credit scoring by credit institutions is already sufficiently regulated by the provisions in the CRR and an additional ex -ante conformity assessment as part of the SREP would overlap with the provisions of the CRR. Notwithstanding the above, if an additional ex -ante conformity assessment for high - risk AI systems referred to in point 5(b) of Annex III were to be integrated into the SREP as referred to in Art. 97 to 101 of the CRD, we believe that further guidance concerning the procedure foreseen for the assessment and the expected interaction between credit institutions and the competent authorities is necessary . Definitions Artificial intelligence system (AI system), Art. 3( and Annex I Co-operative banks have been exploring the possibility offered by AI systems with the GDPR in mind. When we look at the proposal, there are a couple of elements relevant to us, in particul ar in the area of credit scoring and creditworthiness assessment. Banks are one of the earlier adopters of AI systems. Combining the definition of artificial intelligence system (Art. 3() with the techniques and approaches of Annex I, we observe that the scope of the Regulation appears to expand as it also includes rule -based approaches . The current definition and scope provided by the proposed Regulation would include any logic and knowledge -based approaches. The mere use of such techniques and approaches clearly lack the characteristic of displaying intelligent behaviour by analysing the ir environment and taking actions with some degree of autonomy to achieve specific goals that the Commission used in its 2018 Communication on Artificial Intelligence for Europe to define AI systems3. We strongly believe that such systems (as set o ut under Annex I (b)) should only be considered as AI if they are able to automatically adapt without human intervention, or in other words, if such rule -based techniques have a self -learning character. Therefore, we propose a clarification to Art. 3 ( where only rule -based approaches (Annex I (b)) without human intervention possibilities are in scope of the AI system definition. Without such a 3 Communication from the Commission to the European Parliament, the European Council, the Council, the European Economic and Social Committee and the Committee of the Regions on A rtificial Intelligence for Europe , COM( 237 final. EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 6 clarification on the proposed broad scope, the Regulation would create an unnecessary burden and high admin istrative efforts for many providers and users of such established software systems. Furthermore, we feel that decision is clearer and more reflective of market practice than outputs . Finally , we agree with the combination of Art. 3 ( and the techniq ues and approaches as set out in Annex I (a) & (c) as a definition of an AI system . We would welcome a more targeted approach, limited to rule-based techniques and approaches built on AI in the sense that such systems without human intervention should fall within the scope of an AI system . Provider , Art. 3( We believe that the definition o f provider is not clear because it means the legal entity that develops AI systems but also has AI systems developed. This is not applicable for a lot of banks that have outsourced their IT to a different legal entity. It is necessary to define the roles of the legal entity that develop AI systems, e. g. an IT service provider, and the legal entity that has an AI system developed, e. g. a financial institution, and the role of the end user or the consumer, e. g. the client of the financial institution that interacts w ith the AI system, e. g. a chatbot. The process of defining a legal entity as either developer or user should be based on an individual AI system and can vary between AI systems, since a financial institution can use the AI system of an IT provider and wou ld therefore be the user, or develop an AI system and in this case would be a developer . We would like to emphasize that entities are still struggling with role appointments under the GDPR (processor vs. controller) and hope to avoid this discussions and role ambiguity in proposed future legislation . For the above reasons, w e suggest replacing the definition of provider with developer . Need to better define the different actors in the chain: User , end user, operator Because of the m ain reason given on the unclear definition of provider, we believe it is necessary to better define the different actors in the chain involved in the AI system by amending the definition of user and by adding the definition of end user . The suggested definition of user in the Regulation does not clearly state if the user is a company (e.g., a bank) that uses an AI system that was developed by a different company (IT developer), or the end user (e.g., the customer of a bank). Within th e financial sector we face two different situations: firstly, the customer who is faced with the decision of a client -facing AI model (e.g., in case of automated decision by the co - operative bank on a credit card request by the customer); secondly, the employee who gives follow -up to the decision of an AI system in detecting fraudulent transactions (e.g., in case of a mortgage -loan request by a customer), which is a non -client -facing AI system. Finally and in the same spirit of clarificatio n, we suggest deleting the definition of operator as it suggests too many different players can be an operator. EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 7 Testing data , Art. 3( We believe that biases and errors will not be identified when the validation data has the same characteristics as the data that was used for the training of the AI system. Therefore w e suggest making it clear in Art. 3( that the testing dataset must be a separate dataset . Remote biometric identification system , Art. 3( We understand that under the new rules, all AI systems intended to be used for remote biometric identification of persons will be considered high-risk and subject to an -ex-ante third party conformity assessment, including documentation and human oversight requirements by design. It is not clear to us whether financial services firms and their providers, who rely on biometric identification to o nboard customers remotely and comply with know -you-customer (KYC) requirements, will also be in scope of the full set of requirements in the AI regulation. On another note, Art. 3( provides the following definition for a remote biometric identification (RBI) system : [ ] an AI system for the purpose of identifying natural persons at a distance through the comparison of a person s biometric data with the biometric data contained in a reference database, and without prior knowledge of the user of the AI system whether the person will be present and can be identified . The wording in the second part of the definition would lead to the conclusion that, in case the user of the AI system know s that the person is present and can be identified, the AI system would not classify as an RBI system and would hence not fall under the classification as a high-risk AI system. We believe that considering the above observat ions, further clarifications or a more precise definition are needed to avoid legal uncertainty and diverging interpretations by the competent authorities within the Union. Regulatory sandbox , Art. 3 (new We note that while the AI proposal dedicates specific articles on AI regulatory sandboxes (Articles 53 , a definition of regulatory sandbox is missing from the text. We would appreaciate having a definition of regulatory sandbox in the proposal. High -Risk AI System s: classification rules for high -risk AI systems In line with the general intention of the Regulation to protect the safety and fundamental rights of EU citizens, the high -risk AI systems listed in Annex III as referred to in Art. 6( focus on the intended use of these AI systems that could possibly harm natural persons. This limitation should also be reflected in the respective article of the Regulation . EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 8 Unrealistic requirements of training, validation and testing data sets, Art. 10 ( According to the first sentence of Art. 10 (, training, validation and testing data sets shall be relevant, representative, free of errors and complete. This requirement is absolute ly not realistic as it cannot be guaranteed that datasets will be fully correct or complete. Therefore, we suggest rephrasing Art. 10 ( to the effect that errors should be minimised and that training, validation and testing should be as complete as possible. Transition period for delegated acts amending Annex III, Art. 7( and Art. 73 According to Art. 7, the Commission can update the high -risk list in Annex III with delegated acts in accordance with Article However, the proposed text of the Regulation does not foresee any transition period for AI systems newly added to Annex III via respective amendments by the Commission. However, we understand from an exchange w ith the Commission that the requirements for newly defined high -risk AI systems shall be applicable only two years after the delegated act enters into force allowing for a sufficient transition period. Therefore, we suggest adding the transition period of at least two years to Art. Exemption for smal l-scale providers Recital 37 states that Considering the very limited scale of the impact and the available alternatives on the market, it is appropriate to exempt AI systems for the purpose of creditworthiness assessment and credit scoring when put into service by small -scale providers for their own use. The same concept is also reported on Annex III point 5(b). We would prefer a more precise expression than it is appropriate . It has to be clear if small - scale providers are exempt or not. We find the expression appropriate to be unclear and therefore suggest clearly stating whether small -scale providers are exempt or not. Furthermore, if they are exempt, it should be reflected t hroughout the Regulation. Sandboxes While we understand that regulatory sandboxes are useful for the development of AI, we think that t he language used in the AI proposal is too vague . A definition of sandbox or a reference to a definition is not provided in the AI proposal. According to the definition4 provided by the EBA, 4 EBA Report on FinTech: Regulatory sandboxes and innovation hubs . The definition used by the EBA of Regulatroy sandboxes is the following: Regulatory sandboxes: these provide a scheme to enable firms to test, pursuant to a specific EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 9 sandboxes may also imply the use of legally provided discretions by the relevant supervisor. Furthermore, the Regulation does not give any hint on what will be the selection criteria that determine how companies are selected , or the goals that are pursued with the regulatory sandboxes, e. g. are they meant to foster the development of AI systems for new use cases ? If so, how are these use cases selected? Furthermore, we want to avoid market distortions by giving an advantage only to a few selected companies . It seems that sandboxes are designed exclusively to offer advantages to a few selected companies, while other enterprises have to develop their AI systems without the support and the improved environment . We are of the opinion that regulatory sandboxes should only be allowed if all companies have access to them. We believe that sandboxes should be open to both new (start -up) and incumbent (e.g. , banks) FinTech (intended as technology -enabled innovation, as per the definition adopted by the Commission) providers . To prevent disadvantages to the companies with out access , it should be possible for companies to establish their own regulatory sandboxes separate from their existing IT operations . A level playing field has to be guaranteed with those outside the sandbox , thus transparency on the experiments going on and any regulatory lenience have to be transparent in order to avoid market distortion s. The information in the reports that Member State competent authorities submit to the European AI Board and the Commission ( Art. should be made available to the public, so all companies can learn from the activities taking place in the sandbox. Moreover, and as a general observation by reading Art. 53, there is a dichotomy between the willingness to set up regulatory sandboxes within the EU to foster AI innovation and the fact that the relevant participants in the sandbox should compl y with strict regulatory oversight (Recital of the legislative proposal on AI and, where relevant, other Union and Member States legislation supervised within the sandbox (Recital . We suggest clearly spelling out in Recital 72 and/or Art. 53 that the objective s of the regulatory sandboxes and the entry criteria should be clear and made public in order to ensure a high degree of transparency in the entry process. Furthermore, we suggest that the lessons learnt in the sandboxes are made publicly available so that companies without access can benefit as well. Codes of conduct, Art. 69 The Commission wishes to encourage providers of non -high-risk AI systems to create codes of conduct intended to foster the voluntary application of the mandatory requirements applicable to high-risk AI systems but also to apply additional requirements on a voluntary basis . We note that this may lead to a multiplication of different voluntary codes, which may ultimately lead to confusion on the part of users and co nsumers. Moreover, those codes of conducts could represent a new regulatory layer that could hinder innovation and, at the end, go against the original goal of the Commission to be proportionate in the approach . testing plan agreed and monitored by a dedicated function of the competent authority, innovative financial products, financial services or business models. Sandboxes may also imply the use of legally provided discretions by the relevant supervisor (with us e depending on the relevant applicable EU and national law)4 but sandboxes do not entail the disapplication of regulatory requirements that must be applied as a result of EU law . Page EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 10 Contact: The EACB trusts that its comments will be taken into account. For further information or questions on this paper, please contact: - Ms Marieke van Berkel, Head of Department Retail Banking, Payments, Financial Markets ( marieke.vanbe rkel@eacb.coop ) - Ms Chiara Dell Oro, Senior Adviser for Digital Policies (chiara.delloro@eacb.coop )",en,"EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability , Proximity, Governance The voice of 800 local and retail banks, 84 million members, 209 million customers in EU EACB AISBL Secretariat Rue de l Industrie 26 -38 B-1040 Brussels Tel: (+32 230 11 24 Fax (+32 230 06 49 Enterprise 149 lobbying register -19 e-mail : secretariat@eacb.coop Brussels , 23 July 2021 CDO Position paper on a proposal for a Regulation laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) The European Association of Co -operative Banks (EACB ) represents, promotes and defends the common interests of its 27 member institutions and of cooperative banks, with regard to banking as well as to co -operative legislation. For further details, please visit EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 2 Introduction The European Association of Co -operative Banks (EACB) is happy to contribute to the discussion on the Artificial Intelligence (AI) legislative proposal. EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 3 Regulatory sandboxes are useful for the development of AI . EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 4 overlaps, limited derogations should also be envisaged in relation t o the quality management system of providers and the monitoring obligation placed on users of high -risk AI systems to the extent that these apply to credit institutions regulated by the CRD . EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 5 We believe that overlapping or conflicting regulation needs to be avoided and sector -specific legislation such as the CRR should prevail. EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 6 clarification on the proposed broad scope, the Regulation would create an unnecessary burden and high admin istrative efforts for many providers and users of such established software systems. EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 7 Testing data , Art. EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 8 Unrealistic requirements of training, validation and testing data sets, Art. The definition used by the EBA of Regulatroy sandboxes is the following: Regulatory sandboxes: these provide a scheme to enable firms to test, pursuant to a specific EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 9 sandboxes may also imply the use of legally provided discretions by the relevant supervisor. Page EUROPEAN ASSOCIATION OF CO -OPERATIVE BANKS The Co -operative Difference : Sustainability, Pro ximity , Governance 10 Contact: The EACB trusts that its comments will be taken into account.",risk
MOS P (Czechia),F2662780,22 July 2021,Trade union,Small (10 to 49 employees),Czechia,"Education International Internationale de l'Education Internacional de la Educaci n -ie.org EUROPEAN REGION - ETUCE President Larry FLANAGAN Vice-Presidents Odile CORDELIER Andreas KELLER Trudy KERPERIEN Dorte LANGE Galina MERKULOVA Branimir STRUKELJ Boulevard Bischoffsheim, 15 1000 Brussels , Belgium Tel +32 2 224 06 91/92 Fax +32 2 224 06 94 secretariat@csee -etuce.org -etuce.org European Director Susan FLOCKEN Treasurer Joan DONEGAN ETUCE European Trade Union Committee for Education EI European Region ETUCE position on the EU Regulation on Artificial Intelligence (Adopted b y the ETUCE Bureau on 7 June 2021 ) Background: On 21 April 2021, the European Commission published a proposal for a Regulation on a European Approach for Artificial intelligence (the AI Regulation). With this proposal, the European Commission follows up on its White Paper on Artificia l Intelligence (February , based on the results of a broad consultation process to which ETUCE contributed . The aim of the initiative is t o establish the first EU legal framework regulat ing the entire lifecycle of the use of Artificial Intelligence (AI) in all sectors , including education. The AI Regulation classifies the use of Artificial Intelligence in various sectors based on the risk that the AI tools have on the health and safety and the fundamental rights of individuals. Concerning education, t he proposal consider s the use of A rtificial Intelligence tools in education as high -risk as potentially harmful to the right to education and training as well as the right not to be discriminated in education. For high -risk sector s, the AI Regulation establishes stricter horizontal legal requirements to which AI tools must comply before being auth orised on the market. These include risk management system during the entire lifecycle of the AI system. Following the publication of the proposal, on 26 April 2021 , the European Commission issue d a public consultation that will run until 20 July 2021 , accompanied by a n impact assessment report . The following text is the ETUCE response to the public consultation bringing the perspective of teachers, a cademics and other education personnel on the sections of the AI Regulation that touch upon the education sector. ETUCE reply : ETUCE welcomes th e publication of the AI Regulation as it sets the ground for the first comprehensive EU regulation on Artificial Intelligence to ensure a controlled development of AI tools in education and address the risks connected to the ir use by teachers , academic, other education personnel and students . While ETUCE recognises the potential of digital technologies and Artificial Intelligence tools to bring about improvements in education , it also underlines the numerous ethical concern s related to their trustworthiness, data privacy, accountability, transparency and their impact on equality and inclusion in education . ETUCE underlines that further research at na tional and European level is needed to assess and address the risks connected to the use of Artificial Intelligence in education with constant and meaningful consultation with education social partners . 2 AI in education as a high -risk: ETUCE welcomes that the AI Regulation classifies the use of AI tools in education and vocational training as high -risk underlining that When improperly designed and used, such systems may violate the right to education and train ing as well as the right not to be discriminated against and perpetuate historical patterns of discrimination . ETUCE emphasises that the EU Commission initiative should ensure that the development of Artificial Intelligence in education does not infringe the human right of all individuals to have equal access to quality education . This is enshrined in the first and third principle s of the European Pillar of Social Rights and the European Charter of Fundamental Rights . ETUCE supports the European Commission s propos al to set stricter horizontal l egal requirements for the AI tools used in the education sector . The AI Regulation proposal also foresee s the establishment of a risk management system to analyse the risks associated with the use of the AI tools in education and monitor them during the entire lifecycle of the AI tool s. In th is regard, ETUCE believes that the European Commission should support the development of clear and binding measures , including ethical guidelines, to address the risks that AI tools pose concerning transparency, accountability, intellectual property right s, data privacy, cyber -safety, equality and environmental protectio n. Governance: The Proposal for Regulation seeks to establish a governance system leading to the establishment of a European Artificial Intelligence Boar d with the involvement of national authori ties to monitor the implementation of the regulation. Nevertheless, ETUCE underlines that the effective implementation of the AI legislation in education requires the meaningful involvement of teachers, academics and education staff as co -creators of Artificial Intelligence tools in education. E ducation trade unions have a crucial role to play to addressing the risks of Artificial Intelligence in education and bring the perspective of AI users on the implementation of the regulation. It is therefore crucial that education social partners are actively involved in the activities of the European Artificial Intelligence Board through regular consultations and meaningful social dialogue, both at n ational and European leve l to monitor the implementation of the regulation and address the risks related to the use of Artificial Intelligence in education. The role of teachers in education : Teachers, academics and other education personnel play a c rucial role in fostering the full human potential of students and their role in education must be preserved. ETUC E calls on the European Commission and the Member States to interdict the Artificial Intelligence tools that are designed to replace education personnel or can damage the social value and the quality of education . Besides , the AI Regulation should ensure that the development of AI in education does not reduce the role of teachers to mere providers of instructions but rather serves as a supporting tool for the teaching profession while 3 preserving the professional and pedagogical autonomy and academic freedom of teachers and academics. Transparency and AI literacy and CPD of teachers on AI : ETUCE welcomes that the proposal of AI Regulation requires that users of AI tools (who include students, teachers, academic and education staff for the education sector) must be adequately informed about the intended purpose, level of accuracy, residual risks of AI tools . Nevert heless, ETUCE highlight s that providing information is not sufficient to ensure the transparency of the AI tools when users miss the adequate digital skills and data and AI literacy to interpret it. Therefore, it is of utmost importan ce to improve the importance of digital skills, AI literacy and d ata literacy in educational curricula and raise awareness on the risks related to the use of AI tools in education . It is also essential to ensure that infrastructures of education institutions are adequately equipped for digital education as well as to provide equal access to digital technologies and ICT tools to all teachers and students, with particular attention to the most disadvantaged group s. To these purposes , sustainable public investment should be provided by national governments and the European Commission should provide financial support through European funding such as Horizon E urope, Digital Europe and in the framework of National Recovery and Resilience Facility. While the AI Regulation blandly mention to the possibility o f providing users with training on Artificial Intelligence , ETUCE emphasises that it is crucial that sustai nable public funding are provided at national and European level to ensure that teachers, trainers, academics and other education personnel receive up-to-date and free of charge continuous training and professional development on the use of AI tools in acc ordance with their professional needs . EdTech expansion and issues of intellectual property rights, data privacy of teachers : ETUCE points out that the development of the use of Artificial Intelligence in education has been accompanied by the expansion of Ed-tech companies that are progressively increasing their influence in the education sector , especially under the pressure of emergency online teaching and learnin g during the COVID -19 pandemic. ETUCE reminds that education is a human right and public goo d whose value need s to be protected. ETUCE calls for further public responsibility from national governments that should not limit their scope to regulat ing the EdTech sector and should develop and implement public platforms for online teaching and learning to protect the public value of education. In addition, public platforms should be implemented in full respect of professional autonomy of teachers and education personnel as well as academic freedom and autonomy of education institutions, without creating pressure on teachers and education personnel regarding the education material and pedagogical methods they use. It is also essential t o protect the accountability and transparency in the governance of public education systems from the influence of private and commercial interests and actors . 4 AI tools storing a vast amo unt of data cause inevitable risks on data protection , privacy and intellectual property rights of teachers and academics and other education personnel. ETUCE highlights that ensuring data protection and privacy of teachers and students should be a priority of the AI Regulation and calls on the EU Commission and the Member States to develop appropriate data -retention policies applicable to Artificial Intelligence in education, in the respect of national competenc ies in education . Equality an d inclusion in the design and use of AI in education : As enshrined in the EU Pillar of Social Rights and the EU Charter of Fundamental Rights , non- discrimination in educati on is a fundamental principle of our society. In this regard, the EU Commission s p roposal states that the AI regulation will minimise the risks of erroneous or biased AI -assisted decision on education and training . In this context, ETUCE recognizes that the use of Artificial Intelligence has the potential to advance the quality of li fe and inclusion of teachers and students in education . Nonetheless, the persistent lack of diversity and underrepresentation of women, ethnic minorities, Black People and disadvantaged groups in the population of professionals responsible for designing, t esting and training the algorithms and data of AI tools translate in the presence of biases in AI tools, leading to a detrimental impact on inclusion and equality in education . Therefore, ETUCE calls on the European Commission and Member States to provide adequate public investment to encourage more diversity in the STEAM sector and ensure that AI tools are designed and used with the full representation of the wide societ y. Besides , research shows that cyber -violence, cyber -bullying and cyber -harassment have increased with the development of digitalisation in education. ETUCE underlines that it is important to further explore how Artificial Intelligence systems can act as supporting tools to detect and counter cyber -violence, cyber -bullying and cyber -harassment . *The European Trade Union Committee for Education (ETUCE) represents 127 Education Trade Unions and 11 million teachers in 51 countries of Europe. ETUCE is a Social Partner in education at the EU level and a European Trade Union Federation within ETUC, the European Trade Union Confederation. ETUCE is the European Region of Education International, the global federation of education trade unions.",en,"The AI Regulation proposal also foresee s the establishment of a risk management system to analyse the risks associated with the use of the AI tools in education and monitor them during the entire lifecycle of the AI tool s. In th is regard, ETUCE believes that the European Commission should support the development of clear and binding measures , including ethical guidelines, to address the risks that AI tools pose concerning transparency, accountability, intellectual property right s, data privacy, cyber -safety, equality and environmental protectio n. Governance: The Proposal for Regulation seeks to establish a governance system leading to the establishment of a European Artificial Intelligence Boar d with the involvement of national authori ties to monitor the implementation of the regulation.",risk
Teachers' Union of Ireland (Ireland),F2661971,09 July 2021,Trade union,Small (10 to 49 employees),Ireland,"The Teachers' union of Ireland (TUI) welcomes the publication of the AI Regulation as it sets the ground for the first comprehensive EU regulation on Artificial Intelligence to ensure a controlled development of AI tools in education and address the risks connected to their use by teachers, academic, other education personnel and students. While the TUI recognises the potential of digital technologies and Artificial Intelligence tools to bring about improvements in education, it also underlines the numerous ethical concerns related to their trustworthiness, data privacy, accountability, transparency and their impact on equality and inclusion in education. The TUI underlines that further research at national and European level is needed to assess and address the risks connected to the use of Artificial Intelligence in education with constant and meaningful consultation with education social partners. AI in education as a high-risk: The TUI welcomes that the AI Regulation classifies the use of AI tools in education and vocational training as high-risk underlining that When improperly designed and used, such systems may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination . The TUI emphasises that the EU Commission initiative should ensure that the development of Artificial Intelligence in education does not infringe the human right of all individuals to have equal access to quality education. This is enshrined in the first and third principles of the European Pillar of Social Rights and the European Charter of Fundamental Rights. The TUI supports the European Commission s proposal to set stricter horizontal legal requirements for the AI tools used in the education sector. The AI Regulation proposal also foresees the establishment of a risk management system to analyse the risks associated with the use of the AI tools in education and monitor them during the entire lifecycle of the AI tools. In this regard, the TUI believes that the European Commission should support the development of clear and binding measures, including ethical guidelines, to address the risks that AI tools pose concerning transparency, accountability, intellectual property rights, data privacy, cyber-safety, equality and environmental protection. Governance: The Proposal for Regulation seeks to establish a governance system leading to the establishment of a European Artificial Intelligence Board with the involvement of national authorities to monitor the implementation of the regulation. Nevertheless, the TUI underlines that the effective implementation of the AI legislation in education requires the meaningful involvement of teachers, academics and education staff as co-creators of Artificial Intelligence tools in education. Education trade unions have a crucial role to play to addressing the risks of Artificial Intelligence in education and bring the perspective of AI users on the implementation of the regulation. The role of teachers in education: Teachers, academics and other education personnel play a crucial role in fostering the full human potential of students and their role in education must be preserved. The TUI calls on the European Commission and the Member States to interdict the Artificial Intelligence tools that are designed to replace education personnel or can damage the social value and the quality of education. Besides, the AI Regulation should ensure that the development of AI in education does not reduce the role of teachers to mere providers of instructions but rather serves as a supporting tool for the teaching profession. EdTech expansion and issues of intellectual property rights, data privacy of teachers: The TUI points out that the development of the use of Artificial Intelligence in education has been accompanied by the often adverse expansion of Ed-tech companies in their influence in the education sector.",en,"In this regard, the TUI believes that the European Commission should support the development of clear and binding measures, including ethical guidelines, to address the risks that AI tools pose concerning transparency, accountability, intellectual property rights, data privacy, cyber-safety, equality and environmental protection.",risk
Engine B (United Kingdom),F2636199,21 June 2021,Company/business,Small (10 to 49 employees),United Kingdom,"Engine B welcomes the EC s proposals to harmonise the regulation of AI across the Union. Standardisation will help ensure that negative impacts of AI on humans are consistently prevented while supporting technology firms to engage confidently on a level and consistent playing field. Engine B is pleased to see the EC recognises the potential benefits from the safe and ethical adoption of AI, and welcomes simplification proposals. The proposed requirements on the safeguards for high-risk AI are good, and we especially welcome the recommendations around high quality data, and bias monitoring, detection and correction. The proposal to expand European common data spaces is particularly welcome; Engine B recommends that corporate data, especially relating to companies which have ceased to trade due to fraud, be included. We further recommend the EC align its data spaces with UK proposals to provide common data repositories for corporate data. Engine B welcomes the EC s commitment to transparency of AI, the documentation requirement, and the requirement to retain human in the loop for oversight and governance. We also fully support the requirement that AI systems should be required to convey their limitations and accuracy limits to users. Engine B fully supports the recommendation in paragraph 61 on standardisation. We encourage the EC to look into data standards and common data models in particular. The adoption of such common models enables a competitive marketplace where consumers of AI are better able to compare products for quality, and supports good quality governance of technology by making assessment of tools more consistent. By developing common data models in collaboration with technology providers, the EC could significantly aid development of an effective AI oversight industry which works for all. Engine B is also encouraged by the proposal for AI regulatory sandboxes, and believes these can be critical to furthering our knowledge of AI and its successful implementation. We especially welcome approaches which support SMEs and start-ups, and would recommend the EC encourage the adoption of common data models and standards within these sandboxes to further enable small companies to innovate. Engine B also welcomes the proposal to encourage and facilitate the drawing up of codes of conduct. We are concerned though that the current proposal could lead to a proliferation of substandard codes of conduct. It is important for the users of AI and those impacted by it even in low-risk ways that design, implementation, and governance is high quality, and we recommend the EC use its market position to recognise and endorse high quality codes of conduct to which AI providers could voluntarily subscribe. However, Engine B has a concern that the definition of high risk within the proposed regulation is too narrow. The current definition may not cover activities where the poor quality AI may have broad negative societal impacts but where it is hard in advance to predict who would be harmed. Our particular concern is for the use of AI in the production of financial and other corporate reporting information and in the provision of assurance services. If poor quality AI is adopted here, we could see major corporate and audit failures which have an enormous impact on markets. As companies increasingly use AI to produce estimates in their corporate reporting, and as major audit providers increasingly use AI to detect fraud and error in this reporting, the risks of AI grow. AI could be used to perpetrate fraud, it could introduce material errors, direct businesses towards poor sustainability behaviours in the belief that they are improving their environmental impact, or in assurance AI could fail to identify significant frauds or risk of disorderly collapse. We recommend amendments to include areas of application where misuse of AI could increase the risk of disorderly business collapse and its resulting economic detriment.",en,"AI could be used to perpetrate fraud, it could introduce material errors, direct businesses towards poor sustainability behaviours in the belief that they are improving their environmental impact, or in assurance AI could fail to identify significant frauds or risk of disorderly collapse.",risk
UNI Europa (Belgium),F2636017,16 June 2021,Trade union,Small (10 to 49 employees),Belgium,"1 Comments on the Commission proposal for a regulation laying down harmonized rules on Artificial Intelligence (AI Act) June 2021 Key concerns for UNI Europa regarding the draft AI regulation a) Employees participation, social dialogue and collective bargaining - The regulation fails to address the crucial role that social partners and collective bargaining play in the deployment of new technologies at the workplace. Participation of workers representatives in the development, implementation and governance of AI systems at the workplace is key to ensure the best protection of workers rights. - Collective bargaining is an essential tool to address technological change. It is flexible as it is possible at different levels (company, sector, national) and effective to provide safeguards against the negative impacts of new technologies (like algorithmic management) that occur when the rules for their implementation have not been agreed between employers and employees beforehand. Collective bargaining agreements can establish limits to AI-enabled surveillance of workers and lay down criteria to improve the transparency of decision-making process of an AI system. (A good example is the Spanish social partner agreement on platforms that stipulates that algorithms impacting on working conditions must be made available to trade unions and meet transparency requirements.) - Workers are not mentioned in the proposal. Only users are defined as any natural or legal person using an AI system under its authority . In a work context the user is rather the employer / HR Manager/ line manager / principal / payer (in the gig-economy concept), not the (possible) employee who is the possible subject of the system. The draft Regulation does not provide for any information, consultation or protection for workers. (Cf. rec. 47, 48 and art. . - The regulation states that stakeholders participation in the design and development of AI and diverse teams should be encouraged (art. . Though this might apply for trade unions, such a voluntary approach is not enough. We want a structural/systematic involvement of unions and workers representatives in all stages of the design, development and implementation of AI systems. - Governance (art. 56-: The regulation lacks focus on general governance of AI systems, as it only covers the governance for implementing the regulation. Trade unions should be part of the governance of the EU AI Board to ensure a democratic process and the protection of workers rights. The regulation only mentions external experts and observers (art. . - Not only users or companies should be monitored. Also, system developers should be obliged to stick to local regulations. Providing a system to customers, knowing that it is illegal, with the excuse that the customer is not obliged to use it but has a choice should not be possible. Instead, there should be a clear liability throughout the supply chain. If someone produces or uses a system with illegal parameters, the whole chain should be collectively liable. 2 b) AI systems used in employment (recitals 27, 36, articles 6-8, Annex III) and fundamental and workers rights (recital 28 ) - The Commission focuses in its approach to the use of AI above all on the innovative aspects of new technologies. Though we welcome the beneficial potential of AI systems, we think that the general debate on AI and labour neglects the fact that AI also creates low quality employment. AI systems fuel the growth of ghost workers , workers that carry out repetitive tasks such as labelling, editing and moderating data needed in AI systems. Ghost work is often characterized by low pay, stress and is uncredited. When regulating AI in the area of employment, it is important to pay sufficient attention to the negative impact that the use of AI driven technology has on working conditions. - Though the draft regulation mentions high-risk AI-systems in the field of employment, workers management and access to self-employment (Article 2 and Annex , the list of applications covered is limited. The regulation only mentions systems used for algorithmic management especially in HR (recruitment, selection of candidates, advertising of vacancies, screening applications, interviews) and decision- making (promotion and termination of employees, performance /behaviour evaluation and monitoring) and for task allocation. - However, other AI applications with possible consequences for employees do not fall under this scope. We believe that the list in Annex III is not complete as future AI systems that allow for an extended algorithmic management might not be covered by the list in Annex III. - Given the huge impact of high-risk applications (especially for surveillance) in the employment area regarding health & safety and fundamental rights, we think that such a limitation of the scope of high-risk applications is not sufficient. We are especially concerned about high-risk applications to o Infringe privacy/data protection rights as employer can access workers data, o Allow surveillance to take place outside of company premises and outside of working hours as it invades workers homes, o Cause bias, incorrect and discriminating AI decisions due to limited data. - New systems currently tested and researched often combine image-based and voice analysis regarding emotion recognition software, personality analysis software and lie detection software. All of these are currently highly unreliable, especially in an ethnically and/or culturally diverse environment. This software is not prohibited so it might end up in the high-risk category, but it is unclear under which category exactly. (E.g., a Belgian Bank considers the further training and development of an algorithm to digitally assist the bank employees to work more efficiently.) - Though it will be possible to add certain AI systems to the scope of Annex III (art. , this can only be updated within the areas already covered. The Commission lays down criteria for this assessment explicitly mentioning harm to health, impact on fundamental rights and a position of imbalance of power and social economic circumstances. This is certainly the case for any AI system impacting on workers given the structural power imbalances between employer and employees, and also the risk to health and safety involved. Adding a high-risk AI system ex post after because it has already caused harm, is too late. Instead, the precautionary principle should be applied and any AI system that is intended for implementation at work should be classified as high-risk. 3 - Regulatory sandboxes (art. : should not be allowed for AI systems that are implemented at the workplace. Sandboxes should provide a possibility for a deeper understanding of the socio-technical nature of the development process. AI research and innovation requires a more interdisciplinary approach to cover social issues and potential hazards that could occur when systems are applied. The focus of innovation should be on developing tools that gain transparency into the AI systems and understand their behaviour in novel situations ahead of time. c) Conformity assessment & third-party supervision of high-risk AI (rec. 78, art. 19, art. 43, Annex III, art. 4 & Annex VI): - The regulation provides for conformity assessments for high-risk AI that should be done via self-assessment of the provider (Annex III, art . - The process of internal control combined with a post-market monitoring system for high- risk applications and reporting systems to be established by providers is inadequate given the risks for health & safety and the fundamental rights of people. There is too much reliance on industry that it can self-assess the risks. Instead, conformity assessments for all high-risk applications should be done via an independent notified body/third party. - (Example from Norway: Government has put AI monitoring not to the Data Inspectorate but to the auditor general, meaning that it will only be addressing public services and leaving a gap for supervision of services run by private enterprises whether it is for private enterprises or the public sector. That gap should not exist.) - Third-party supervisors should not be private companies but an independent public body addressing all implementations of AI not only high-risk. - There is the question of a user organisation downgrading their AI implementation from high-risk to a lower level, there should be mechanisms and a trustworthy body to address that - Discrimination and associated bias to the outcomes of the high-risk AI systems that are not assessed by an independent body and left to self-evaluation. There should be financial support for SMEs to enable this kind of assessment for these companies, too. d) Transparency and human oversight - Transparency: More transparency is needed for AI systems impacting on workers. So far, the regulation only mentions transparency requirements to support users of AI (rec. 47, art. and it only refers to a certain degree oft transparency for high-risk applications. This is insufficient and the regulation should oblige employers to ensure that workers are aware of the AI systems at the workplace, including their impact on data, digital footprint and work organisation ( AI literacy ). This is where social dialogue between trade unions and employers plays an essential role. - Human oversight (rec. 48, art. : The responsibility for false decisions made by an AI system cannot be shouldered alone by an individual engineer or AI system developer. Especially when systems are subject to a self-assessment procedure, the individual developer might be blamed for the deployment of the system, rather than the organisational setting within which the developer operates. Responsibility should be distributed across the process and its stakeholders. This requires a no-blame culture, where taking responsibility includes providing space for empowering and training the 4 workforce to ensure human oversight, especially when the output of the high-risk AI system is a part of the responsibility. Nothing mentioned in the regulation about special training for those operating these systems or to counteract if necessary. - No specification of any right to contest an algorithmic decision and obtain human oversight or provision for remedies when something goes wrong. There should be an effective appeal procedure and remedies enabling individuals to address the AI behaviour and decisions citizens find potentially harmful and illegal. - Ethics: The regulation addresses and limits certain uses of AI, which is positive. However, ethics should have a stronger visibility in the regulation. Our demands concerning the draft AI regulation: a) Employees' participation, social dialogue and collective bargaining - Trade unions and workers representatives must be key actors in developing and implementing AI systems. The regulation should acknowledge the importance of social dialogue and collective bargaining for AI systems used in the field of employment, but also in general as regards the implementation of new technologies in the workplace. (Example from Norway: Agreement with Negotia regulating employees involvement with important wording regarding AI.) - When AI systems are implemented in the workplace, trade union/workers representatives need to be involved in the process. Employees rights to information, consultation and participation (according to existing legal frameworks) must be respected and should be part of the mandatory compliance obligations. This is the only way to anticipate the impact on the quality of work, working conditions or dismissals due to automation. - The regulation needs to ensure the right to workers involvement, protection of collective bargaining and national legislation providing for better safeguards and collecting bargaining should not be undermined or overruled by EU regulation. National legislation regarding tech-enabled surveillance is often subject to trade union involvement/social dialogue. It is not the case for this EU regulation. Could be used to introduce only minimum standards for workers protection. EU regulation must not override national laws that provide more protection and safeguards for workers as this could lead to deregulation of labour and industrial relations (e.g. the new Spanish law on algorithmic transparency or the German co-decision model). - Trade unions should be part of the governance of the EU AI Board. - Third-party supervisors should not be private companies but an independent public body addressing all implementations of AI not only high-risk. - Responsibility for decision made by AI systems should be distributed across the process and its stakeholders (do not blame the developer alone). - There should be a revision of the different EU legislations, including EU labour law, as regards human oversight (providing labour tax incentives for investment in human capital -lifelong learning and new skills), health and safety both in terms of working with the machines and being exposed to stress due to tracking and surveillance, working conditions and social protection, etc. 5 b) Worker Data and Surveillance & monitoring of workers - The exemption for the use of real-time remote biometric identification systems in public spaces by public authorities (art.) is controversial. How are public spaces defined as this could lead to mass surveillance when the provision allows deploying such systems without an authorisation in a duly justified situation of urgency . This is a threat to fundamental and universal rights and freedoms, including one s right to privacy, as such systems process biometric data in mass, generally without consent, and therefore should be forbidden. - Any AI system (and the data selected to feed the system) in the field of employment and that impacts on workers or working conditions, needs to be categorised as high-risk. They should be subject to a third-party impact assessment by competent authorities with the involvement of trade unions. - It is vital for workers rights that biometric identification and monitoring of workers should not be allowed. Employers should not use biometric identification like face recognition for tracking and controlling employees, whether the workplace is publicly accessible or not. The data analysed by these tools does not say anything about how motivated or productive a person will be at work. Especially in the context of HR, AI systems are not used to identify a person but to make presumptions and for categorizing employees. A Ban of harmful surveillance practices (neuro-surveillance) is necessary. - With regard to surveillance, we have the following demands: o No monitoring of workers in the home environment through cameras. o No permanent monitoring through cameras or other means, except when contributing to increased occupational safety and health in physically dangerous work o Strong regulation on software spies reporting constantly on the employees activities (like Nexthink) o All technical information of AI tools used to be given to trade union representatives. o Workers to be clearly told how AI is measuring and assessing them. o Raw data should be given to trade union representatives. This means assessment outcomes and measures divided by gender, age, ethnicity, regional accents etc, to check for bias in a transparent way. o Negotiation with trade union representatives of the rate of output to hours worked matrix. For example, average handling time in contact centres, Amazon warehouse workers etc. - Workers Data: The regulation should improve workers data rights, e.g. access to data, redress possibility. GDPR art. 88 mentions the role of collective agreements regarding data processing for recruitment & management to safeguard workers & fundamental rights. The implementation of the GDPR provisions at the workplace requires the involvement of social partners. - Issues like the right to privacy, information, transparency of and access to data should be covered by comprehensive legislation and, in addition, dealt with by collective bargaining. - Regulation must ensure that the limitation on where data can be stored as set forth in the GDPR regulations is enforced even when data is to be transported to or processed by AI engines. - Collective bargaining is crucial to negotiate about the use of monitoring or surveillance technologies at work. It is important to address the collective data rights for workers, not 6 only individual rights and access to data. The regulation must be respecting labour rights and data rights at national level and should be an addition, not a substitution, of co- governance models respecting social relations and dialogue. - Regulatory sandboxes should not be allowed for AI systems that are implemented at the workplace. - We need an effective appeal procedure and remedies enabling individuals to address harmful or incorrect decisions made by AI. c) Skills and training/employability of workers - Training and upskilling regarding AI based systems is essential to provide employees with the necessary skills when AI systems are implemented and to ensure that employees have a basic understanding of how an AI system works or decides. Besides digital literacy, employees also need to understand how an AI system could possibly impact on their working conditions, health and safety and other relevant areas. - Collective bargaining is the best way to identifying the training needs in the specific company context. - To raise awareness about the ethical aspects of AI and to promote ethical and trustworthy AI, it is important to integrate ethics in training for engineers and to promote a more interdisciplinary approach for R & D. There should be a special training for those operating AI systems or to counteract if necessary. - We need diversity of skills and competences, not reduced to STEM related upskilling. With an increase of the use of AI systems, typical human skills like empathy or creativity become even more important and should be promoted. Investment in training should not be exclusive but address the career paths of workers across job categories.",en,"This is insufficient and the regulation should oblige employers to ensure that workers are aware of the AI systems at the workplace, including their impact on data, digital footprint and work organisation ( AI literacy ).",risk
